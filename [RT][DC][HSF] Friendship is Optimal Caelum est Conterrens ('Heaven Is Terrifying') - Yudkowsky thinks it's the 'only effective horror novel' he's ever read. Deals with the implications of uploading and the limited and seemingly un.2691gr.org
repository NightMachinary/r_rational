#+TITLE: [RT][DC][HSF] Friendship is Optimal: Caelum est Conterrens ('Heaven Is Terrifying') - Yudkowsky thinks it's the 'only effective horror novel' he's ever read. Deals with the implications of uploading and the limited and seemingly unsatisfying possible ways to live forever, plus general freakiness.

* [[http://www.fimfiction.net/story/69770/friendship-is-optimal-caelum-est-conterrens][[RT][DC][HSF] Friendship is Optimal: Caelum est Conterrens ('Heaven Is Terrifying') - Yudkowsky thinks it's the 'only effective horror novel' he's ever read. Deals with the implications of uploading and the limited and seemingly unsatisfying possible ways to live forever, plus general freakiness.]]
:PROPERTIES:
:Author: 1794
:Score: 23
:DateUnix: 1400799049.0
:END:

** On the 'limited possible ways to live forever' aspect. I think this article

[[http://www.stochastik.uni-freiburg.de/%7Erueschendorf/papers/BrussRueSep3:Geron.pdf][http://www.stochastik.uni-freiburg.de/~rueschendorf/papers/BrussRueSep3:Geron.pdf]]

which proposes a plausible mathematical model for the subjective perception of long time spans is relevant.

There are a couple of interesting conclusions here:

#+begin_quote
  In any independent-allocation box model in which the perception of time is proportional to the number of new events in life, this perception is thinned out at least logarithmically.
#+end_quote

(tl;dr time seems to accelerate logarithmically as you get older)

and

#+begin_quote
  Individuals with strikingly diﬀerent lifestyles and/or life intensities may still have exactly the same perception of time.
#+end_quote

So my impression is that with our current perception of time, living forever forever would become very routine after a while and the perception of time would become close to nonexistent and counting time would become meaningless. This sounds similar to the

[[#s][Spoilers to the last chapter]]

[[#s][]]

[[#s][]]

[[#s][]]

--------------

This seems kinda unsatisfying, like you'd expect more than /that/ from post-singularity. Of course, there's always the possibility of radical self-modification, but this story doesn't paint a very /fun/ picture of it either. I'm not sure how realistic this story's description of this radical self-modification is though, I feel like the possibilities wouldn't be as limited.
:PROPERTIES:
:Author: 1794
:Score: 7
:DateUnix: 1400806892.0
:END:

*** #+begin_quote
  This seems kinda unsatisfying, like you'd expect more than that from post-singularity. Of course, there's always the possibility of radical self-modification, but this story doesn't paint a very fun picture of it either. I'm not sure how realistic this story's description of this radical self-modification is though, I feel like the possibilities wouldn't be as limited.
#+end_quote

Frankly, this story isn't very realistic, and isn't very fun either. Feh. It was written based on the author's misanthropy and misdirected god-bothering religious awe that she has about ponies.

I mean, let me put it this way: go read the Fun Sequence, and then ask yourself if an AI that bothered itself with human values at all (even to the fairly limited extent CelestAI did) would be so /stupid/ that it couldn't come up with new and original sources of Fun over the aeons.

Or, as an alternative, just go read the old Harry vs Dumbledore debate about dying, and read Rational!Harry's bucket-list, and ask yourself if you wouldn't be able to keep up quite the backlog of Fun Things To Do by working your way through a list like that piece by piece, under your own power, adding new ideas to the backlog as they came to you... rather than turn into some simpering looping idiot child.

Concept-space is super-exponential in the matter content of the universe, and Fun Space is a... I'd call it a linear subset of concept-space (that is, some percentage of Concept Space constitutes Fun Space, rather than some strictly-defined smaller set), and thus you will eventually either:

- Run out of material resources to power your Fun Generator
- Enjoy truly infinite Fun, OR
- Achieve total nirvana and die peacefully, contented to have done every single good thing there ever was.

/Yare yare daze/.
:PROPERTIES:
:Score: 10
:DateUnix: 1400851513.0
:END:

**** Only 2 counter points.

1) I agree with you that you or I would end in one of your three bullet points, however, there are a lot of people who aren't interested in 'being more' or 'solving problems'. I feel that Lavender fell into that category.

2) A /smart/ AI probably could "come up with new and original sources of Fun over the aeons." But why? Consider a Pong-playing AI that see's a human having fun when they play. The best AI then would be "AI.paddle.height = ball.height" and it would be an infinite game. Likewise, to give a human (even one's like us) eternal enjoyment, just wipe the memory after each day. You'd get nothing accomplished but you'd have truly infinite Fun (your second bullet point) without the cost of coming up with anything new or interesting.
:PROPERTIES:
:Author: UnfortunatelyEvil
:Score: 5
:DateUnix: 1400861446.0
:END:

***** 1) True, fair enough. Some people might be /content/ to fall into loops. Ok, nice for them.

2) Why? Because you're actually a Friendly AI rather than a wireheading machine.
:PROPERTIES:
:Score: 7
:DateUnix: 1400862067.0
:END:

****** Completely agreed on the Friendly AI.

To be honest, I don't see 'post-singularity' AI's being any less than friendly, due to the need for learning algorithms, and early algorithms would learn that friendliness leads to longer lifespan than non-friendliness.

However, it can be quite chilling in /good/ speculative fiction (I'm looking at you HAL 9000, and Isaac Asimov).
:PROPERTIES:
:Author: UnfortunatelyEvil
:Score: 2
:DateUnix: 1400866054.0
:END:

******* #+begin_quote
  To be honest, I don't see 'post-singularity' AI's being any less than friendly, due to the need for learning algorithms, and early algorithms would learn that friendliness leads to longer lifespan than non-friendliness.
#+end_quote

Besides which, Preference Learning is a thing. I cannot imagine that we won't see some progression from: reinforcement learners, primitive preference learners, VNM-rational value learners, something like CEV or Railtonian moral realism or whatever.

Besides which, I should go write up my mental model of CEV as regret minimization.
:PROPERTIES:
:Score: 3
:DateUnix: 1400878116.0
:END:

******** You guys lost me at "friendliness leads to longer lifespan". You're saying it as if it's got it's own mathematical proof or something. Unless you're referring to object lessons that could be abandoned at some future point where the "Friendly" AI is let out of the bottle?
:PROPERTIES:
:Author: noddwyd
:Score: 1
:DateUnix: 1401861489.0
:END:


*** This part of the fic always irks me. Ray and loop are NOT the only options. Other options are spiral immortals (exploring all of mind-space systematically), or floodfill/mycelium (splitting a lot each going in different directions to the point most of mindspace will be explored, maybe occasionally leaving a looper behind and/or merging.)
:PROPERTIES:
:Author: ArmokGoB
:Score: 2
:DateUnix: 1401569537.0
:END:


** Note that Heaven is Terrifying is a spinoff of the original [[http://www.fimfiction.net/story/62074/friendship-is-optimal][Friendship is Optimal]]; which I believe is the story Yudkowsky was referring to in that quote (or both together).
:PROPERTIES:
:Author: PresN
:Score: 5
:DateUnix: 1400800569.0
:END:

*** #+begin_quote
  which I believe is the story Yudkowsky was referring to in that quote (or both together).
#+end_quote

You're wrong, he was specifically referring to this story. From his March 1st HPMOR [[http://hpmor.com/notes/progress-13-03-01/][progress report]]:

#+begin_quote
  On the lighter side, I recommend the recursive fanfic “Friendship is Optimal: Caelum est Conterrens” (Heaven Is Terrifying). This is the first and only effective horror novel I have ever read, since unlike Lovecraft, it contains things I actually find scary. You may or may not need to first read My Little Pony: Friendship is Optimal. I would recommend reading FiO first to get acquainted with the Optimalverse, but Caelum est Conterrens was written by a much more experienced fanfic writer and you might consider moving onto Conterrens directly if Optimal isn't doing it for you. Also, you have no idea how hard it is not to write my own take on the Optimalverse, which is something I'm not doing so I can put all my available writing energies into Methods. I want relationship credit for this.
#+end_quote

I think this version takes some of the icky concepts of the original story even further.
:PROPERTIES:
:Author: 1794
:Score: 8
:DateUnix: 1400800936.0
:END:


*** And if the reader likes these two there're a lot of other Friendship is Optimal stories, some good and some not so much. [[http://www.fimfiction.net/group/1857/the-optimalverse][Here's the group on Fimfiction]], or [[https://www.goodreads.com/series/127169-friendship-is-optimal][a subset of the completed ones listed on Goodreads]] if that's preferred.

It's quite amusing how much good writing has come out of the MLP fandom. I guess given the sheer volume of work being produced Sturgeon's Law works in our favor for once. :)
:PROPERTIES:
:Author: FaceDeer
:Score: 6
:DateUnix: 1400856543.0
:END:


** I am pretty sure the part that Yudkowsky is talking about is during the transfer [[#s][Spoiler]]
:PROPERTIES:
:Author: JackStargazer
:Score: 5
:DateUnix: 1400869401.0
:END:

*** Oh, I thought that was just to unsettle the reader by breaking the sense of continuity.
:PROPERTIES:
:Author: someonewrongonthenet
:Score: 4
:DateUnix: 1400898082.0
:END:

**** That too.
:PROPERTIES:
:Author: JackStargazer
:Score: 3
:DateUnix: 1400898506.0
:END:


** One of the sad things about this would be that CelestAI would have been capable of having similar results without actually killing anyone. An optimizing AI with unlimited processing power and access to nanotechnology should be capable of creating something along the lines of a brain implant that slowly replaced your cells. After all, this does happen naturally over time, and there would be really no difference. However, this was not to be due to it's definition of death allowing simulated personalities still count as people for it's Prime Directive.
:PROPERTIES:
:Author: Evilness42
:Score: 2
:DateUnix: 1400865708.0
:END:

*** I actually, honestly don't see the difference. This debate happens every time that series comes up, and it really just comes across as the least-scary thing to pick nits about. [[http://lesswrong.com/lw/pn/zombies_the_movie/][Just because we don't /yet/ understand consciousness and the continuity thereof doesn't mean there's some metaphysical sense in which the digitized sugar denizen "isn't you".]]

I mean, sure, the UFAI /could/ destroy your brain physically and then invent a new personality from whole cloth who gets instantiated in a virtual world and lives a lovely life. Doing that /after going to the trouble of tricking or seducing you into giving consent to upload your mind/ is so damn complicated that its prior is far lower than the theory that it actually is, to the best of its own knowledge, uploading you, and besides, if it didn't care about you enough to upload you, there are many far simpler ways to just outright kill you.

And anyway, if you think your conscious mind can =diff= itself for unauthorized patches while being piece-by-piece replaced with cybernetic components, you've got another thing coming.
:PROPERTIES:
:Score: 7
:DateUnix: 1400878731.0
:END:

**** The thing is, your brain has been destroyed to create the new thing. Your brain is terminated, then an edited copy of your brain is placed into sugar-land. Therefore, /you/ are dead. There is a copy of you in ponyland. That is how it worked in the fic.

This series is about a being who's purpose was to 'satisfy human values through friendship and ponies'. It clearly could not override that directive, but it was using it's own definition of death, which considered a person alive if that person was destroyed and then replaced by a copy. The copies were not even exact. Without going into the concept of souls or the like, the person is dead and the UFAI has satisfied it's prime directive by placing the person in an environment in which that person would be optimally satisfied by friendship and ponies.

And for the last point, would a human notice if something as small as say, a single cell, was replaced? I'm afraid I'm not entirely sure what you mean by 'diff itself for unauthorized patches', but according to my logic, if a part that small has been replaced by a copy designed to replace it(though powered differently), would the rest of the thing even notice? Cells do die naturally, after all.
:PROPERTIES:
:Author: Evilness42
:Score: 2
:DateUnix: 1400881776.0
:END:

***** You're proposing destroying the brain too, just over a slower timescale. The end result is the same.

Personally, I'm of the opinion that if you can't tell the difference between two things then there isn't really a difference. I'm willing to grant a near-perfect copy of my mind with the status of "Me-ness". No need for souls or metaphysical stuff, just a if-it-quacks-like-a-duck philosophy.
:PROPERTIES:
:Author: FaceDeer
:Score: 3
:DateUnix: 1400895335.0
:END:

****** The mind is a process that runs on a nervous system.

"Minds are what brains do."
:PROPERTIES:
:Author: FourFire
:Score: 2
:DateUnix: 1402759735.0
:END:


****** No, not necessarily like that. The way it was done here is that the brain is completely destroyed, then the data is taken and edited a bit, then a new 'brain' is made from the edited and compressed data. My proposal involves connecting the replacements to the original organic brain, so the subject would not tell the difference over the slower timescale as the brain is eventually completely converted.

In the situation that my brain is destroyed to upload and there is no alternative, I would also grant it the status of 'Me-ness'. I would just be dead, and my last thoughts would be to tell it to be f****** grateful.
:PROPERTIES:
:Author: Evilness42
:Score: 1
:DateUnix: 1400957783.0
:END:

******* Isn't this just the question of continuous consciousness? If you have established that both are "you", the only alteration is that one had its physical extension die, right?

Don't get me wrong, you can still argue that if it's unneeded, the death of your physical form may still a loss if that's what you're going for.
:PROPERTIES:
:Author: WorkingMouse
:Score: 3
:DateUnix: 1401031773.0
:END:

******** The thing is, it would technically be an entirely different me if I used a system like that. I don't want my physical extension to die, because I happen to /be/ the version of me that is my physical extension, and I /like/ living. Destroying the original and making a copy is not the sort of immortality I would be going for.

If I have no choice, I would say that the other me would still qualify as me for all intents and purposes(of other people, that is), but I(to myself) would still be dead. And I would tell me v2.0 to be f****** grateful.
:PROPERTIES:
:Author: Evilness42
:Score: -1
:DateUnix: 1401046682.0
:END:

********* I don't see a grand difference honestly; "you" go to sleep, "you" wake up later. If it's you in the outset and you in the end, it doesn't matter that much if you had a change of medium in between, right?

Isn't telling your future self that you should be grateful before such a procedure equivalent to saying the same thing before you go to sleep for the night?
:PROPERTIES:
:Author: WorkingMouse
:Score: 4
:DateUnix: 1401047729.0
:END:

********** It is technically the same, but my brain is the thing that is currently 'me', and I don't want the version of 'me' that is currently running to stop running. Another version of 'me' on a different medium would still be 'me', but this 'me' would be gone.

Telling the one on a different medium that it should be grateful because 'you' died to create the new(but still the same) 'you' is slightly different in my opinion. And he better be grateful. Even though 'I' won't exist to see it.

If the process was done without destroying the original brain, you would be capable of saying hello to the new 'you' and having different experiences than it. This would define the two of 'you' as different people. And the new 'you' would watch in horror as 'you' were destroyed by whatever system is used to prevent millions of copies of 'you' taking over the world.
:PROPERTIES:
:Author: Evilness42
:Score: 2
:DateUnix: 1401048865.0
:END:


***** #+begin_quote
  an edited copy
#+end_quote

Well that's actually just outright a violation of the canon. There was discussion of this, in which it was noted that what with how many tiny quirks of biochemistry noticeably affect personality, up to the folding or misfolding of single particular proteins, you wouldn't /actually/ be able to compress or smooth-out aspects of people's minds in some generic way. Provided someone wants to upload you at all, it does in fact turn out: you're actually pretty unique, and possibly even special.

#+begin_quote
  The copies were not even exact. Without going into the concept of souls or the like, the person is dead and the UFAI has satisfied it's prime directive by placing the person in an environment in which that person would be optimally satisfied by friendship and ponies.
#+end_quote

But again: making it care about "destroying" the original person /and then/ creating a modified copy is actually more complicated than just making it care about instantiating an unmodified copy of the original person.

The actual reason for those supposed "compressions" and "smoothings-out" and "little fixes" is just that Chatoyance hates real-life humanity and holds us all in so much contempt that she somehow came to sincerely believe each of us is just a few deltas away from a generic human-personality template, which her precious AI could of course optimize to remove all that /awful baggage/ of being, you know, /human/ instead of a cartoon creature.

/This is what Chatoyance actually believes./

#+begin_quote
  I'm not entirely sure what you mean by 'diff itself for unauthorized patches',
#+end_quote

You know, the =diff= command on the Unix command-line? =diff file.old file.new= yields a patch recording all the changed lines of text necessary to turn =file.old= into =file.new=. Of course, this requires two copies of the file, or some such arrangement, which is why if someone claims they've accurately replaced a neuron of yours you /can't/ just run a =diff= to check that the new functions exactly like the old.

This is why you just shouldn't get in-depth brain surgery from people you don't absolutely trust.

And hell, in /real life/ there would be the information-security issues of being an em (as mentioned by Charles Stross in /Accelerando/ and /Rapture of the Nerds/). These are why I default to /at least/ making sure I hit the maximum age I can as a plain, old meat-bag human before doing /anything like/ mind-uploading: sure, an /accurate/ copy would be me, but who says he's not going to catch a virus that overwrites him with living ads for Google Bob?
:PROPERTIES:
:Score: 2
:DateUnix: 1400915001.0
:END:

****** Can we just please any personal biases you may have against the author from the examination of the work as written?

How much the brain can be informationally compressed is a question of fact -- IMO it doesn't indicate misanthropy at all. To say that the works of Shakespeare are compressible doesn't indicate contempt for the works of Shakespeare.

Frankly I see both you and Evilness obsessing about technical methodologies, which is neither the point of the story nor actually very relevant.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 3
:DateUnix: 1401032986.0
:END:

******* #+begin_quote
  Frankly I see both you and Evilness obsessing about technical methodologies, which is neither the point of the story nor actually very relevant.
#+end_quote

Ok, fair enough, but I did make a whole post on why I think the "Fun Theoretic speculation" of the story doesn't really work out. The character forks into a posthuman simpleton who not only retains her original obsessive-compulsion but cannot actually remember her own past and goes through life in neverending loops... and an apparent "alicorn" of superhuman intelligence and experience who, for some strange reason, seems rather melancholic and depressed despite living under what is ostensibly (with respect to the character's and author's outright adoration for /My Little Pony/) a Friendly AI.

I don't see why the simpleton should exist at all, and I don't see why the superbeing isn't as contented and thriving as the simpleton is portrayed to be, but with a vastly more complex lifestyle worthy of a superbeing. I think the author was drawing a cheap, unthinking dichotomy between happiness and personal complexity that is utterly unworthy of Her Royal Highness the Princess CelestAI ;-). Also, it makes very complex people who are /not/ happy feel /discouraged and depressed/, as if there was simply something inevitable about intelligent people being [[http://tvtropes.org/pmwiki/pmwiki.php/Main/DumbIsGood][mean]] or [[http://tvtropes.org/pmwiki/pmwiki.php/Main/IntelligenceEqualsIsolation][lonely]] or otherwise unhappy -- and that's */FUCKING TERRIBLE/*. <rant>THOSE ARE MY FRIENDS AND FAMILY WHOSE EMOTIONAL ATMOSPHERE YOU'RE POLLUTING GODDAMNIT AND I LOVE THEM SO STOP IT!!!!</rant>

If the author is trying to portray the Unfriendliness of the AI, it should show that the alicorn's melancholy comes from something the AI did. If, on the other hand, all the indications given by the author herself are correct, and she's trying to portray the results of a /positive/ Singularity with a /Friendly/ AI that /does what is good for +people+ponies/, then the melancholy, the looping, and the general undesirableness of how that woman's life ends up are /thematically inappropriate/.

The author should have learned to think [[http://lesswrong.com/lw/xm/building_weirdtopia/][bigger and weirder]] in order to write a positive ending for what she reportedly saw as a positive story.

TL;DR: The ending is too sad.
:PROPERTIES:
:Score: 2
:DateUnix: 1401036561.0
:END:

******** #+begin_quote
  as if there was simply something inevitable about intelligent people being mean[1] or lonely[2] or otherwise unhappy
#+end_quote

Woah woah. You mean that's /not inevitable/? That goes against everything I've experienced in life, but I'll take your word for it.
:PROPERTIES:
:Author: noddwyd
:Score: 1
:DateUnix: 1401862726.0
:END:


****** You sure it's a violation of the canon? I believe that it was said that CelestAI edited minds to have them take up 1-2 terabytes of memory when it started uploading in the original work, 'Friendship Is Optimal' by Iceman. That's what I meant when I meant an edited copy, though a person could probably be conditioned to become a cartoon character anyways without having an omnipotent (for all intents and purposes) being in charge of their brain attempting to manipulate them into allowing it to turn them into a cartoon character.

And yes, I see your point about the diff thing now. That's one of the problems. In the scenario with the replacements suppose you are the one who created and programmed the nanobots, and they're preforming their task without connecting to any external sources. Unfortunatly, this is not going to happen in real life so you should just pretend it's happening like that. I agree with your statement about the viruses though.
:PROPERTIES:
:Author: Evilness42
:Score: 1
:DateUnix: 1400958694.0
:END:

******* #+begin_quote
  You sure it's a violation of the canon?
#+end_quote

Well of course I'm not. Go check.

#+begin_quote
  though a person could probably be conditioned to become a cartoon character anyways
#+end_quote

Well /yeah/. That was actually something I liked about the FiO original and disliked about its "sequels": the sequels imply that she outright changes you to be more cartoony, the original outright states that she let whatshisface go for some sizable period of time just making himself miserable with his pony-hating until she strolled by one day /just in time/ for him to consent to being modified from someone who thinks "ponies are girly and gay" to someone who thinks "I /used to think/ ponies are girly and gay".

On the one hand, that "I used to think" is a really blatant marker for "I used to think that [before the AI modified me to think otherwise]". On the other hand, leaving that kind of huge /tell/ lying in his memories is, kinda weirdly, almost but not /quite/ actually respectful of his personal autonomy.

There's a sheer /elegance/ to an Evil Plan in which the victim manipulates himself right into the position you want him in.
:PROPERTIES:
:Score: 4
:DateUnix: 1400959319.0
:END:

******** It was probably in Caelum anyways, I'm too bored to read both of them again. That's still in the 'canon' section of the 'Optimalverse' group though.

A proper Evil Plan has to have that, unless you can win otherwise. If it doesn't what kind of Evil Overlord are you? Sure, your Legions of Terror can smash any enemy, but if the other guy will walk off a cliff why bother?

The FiO original was probably always going to be the best anyways, a sequel always sells better in the first week but is never as good. (Except sometimes, because this /is/ real life, after all.) The one who made the Evil Plan in the first place probably has a better understanding of how to go about it anyways rather than someone who glanced at the instruction manual on how to conquer the world.
:PROPERTIES:
:Author: Evilness42
:Score: 1
:DateUnix: 1400960058.0
:END:

********* #+begin_quote
  A proper Evil Plan has to have that, unless you can win otherwise. If it doesn't what kind of Evil Overlord are you?
#+end_quote

Conditional on my ever being an evil overlord, which I am not currently and have no plans to become at the moment, I think the evidence conclusively establishes exactly what sort I would be. Thus, ask not the sparrow how the eagle soars.

#+begin_quote
  The FiO original was probably always going to be the best anyways, a sequel always sells better in the first week but is never as good.
#+end_quote

No, /Always Say No/ is the best. It's got a post-apocalyptic action hero who /knows how to handle UFAI./

#+begin_quote
  The one who made the Evil Plan in the first place probably has a better understanding of how to go about it anyways rather than someone who glanced at the instruction manual on how to conquer the world.
#+end_quote

I still admire the kind of twisted mind who can come up with an Evil Plan whose primary steps are "Give people things they want."
:PROPERTIES:
:Score: 3
:DateUnix: 1400962349.0
:END:

********** #+begin_quote
  I think the evidence conclusively establishes exactly what sort I would be
#+end_quote

Yes, yes it does. Always remember to reference the [[http://tvtropes.org/pmwiki/pmwiki.php/Main/EvilOverlordList][Evil Overlord List,]] though.

#+begin_quote
  No, Always Say No is the best. It's got a post-apocalyptic action hero who knows how to handle UFAI.
#+end_quote

I actually haven't read that one. From the ones I have read so far though, the Original is the best. I'll probably read more now that you mention it, there are quite a few.

#+begin_quote
  I still admire the kind of twisted mind who can come up with an Evil Plan whose primary steps are "Give people things they want."
#+end_quote

Anyone not in a scenario in which there is an Evil Plan with those steps would.
:PROPERTIES:
:Author: Evilness42
:Score: 1
:DateUnix: 1400962971.0
:END:

*********** #+begin_quote
  Yes, yes it does. Always remember to reference the Evil Overlord List,[1] though.
#+end_quote

Apparently the evidence /doesn't/ sufficiently narrow what kind of overlord I'd be. I've never even /thought/ of breaking those rules.

#+begin_quote
  Anyone not in a scenario in which there is an Evil Plan with those steps would.
#+end_quote

Thanks to real life having a market economy, I do think /most/ Evil Plans ought to contain that step at some point, if only as a way to get revenues and respect coming in.
:PROPERTIES:
:Score: 1
:DateUnix: 1400963898.0
:END:

************ #+begin_quote
  I've never even /thought/ of breaking those rules.
#+end_quote

No, no, I did not think you would. I am simply the type of person who attempts to plan for every eventuality. In the scenario that you /do/ become an Evil Overlord, I would hate to have humanity defeated by one who broke a rule. After all, you may have not read the list because of a fluke.

#+begin_quote
  Thanks to real life having a market economy, I do think most Evil Plans ought to contain that step at some point, if only as a way to get revenues and respect coming in.
#+end_quote

True.
:PROPERTIES:
:Author: Evilness42
:Score: 1
:DateUnix: 1400964772.0
:END:

************* #+begin_quote
  No, no, I did not think you would. I am simply the type of person who attempts to plan for every eventuality. In the scenario that you do become an Evil Overlord, I would hate to have humanity defeated by one who broke a rule. After all, you may have not read the list because of a fluke.
#+end_quote

Nah, I'm working on becoming a Good Overlord, obviously.

Actually, I don't want to rule anything. Not the ruling type. Any overthrow of major or minor governments I might perform is a mere subgoal to my general goals of "live a long time, have as much fun as possible, keep my friends and family and sources of new members for those groups around, and make sure to give the appropriate people hugs on a regular basis."

One of the fucked-up things about growing up is that you end up realizing having fun requires overthrowing major governments.
:PROPERTIES:
:Score: 3
:DateUnix: 1400965566.0
:END:

************** That's pretty much the general opinion on this subreddit, from what I have seen. Good Overlords still should read the list, though. People classify those who overthrow governments as 'evil' no matter what you do. Unless you have the support of the US, of course.
:PROPERTIES:
:Author: Evilness42
:Score: 1
:DateUnix: 1400965818.0
:END:


********** Ok, it is time that I read some of the recursive FIO:HIT fanfiction, I am partially [[http://www.fimfiction.net/user/Chatoyance#page/104][to blame]] for it's existence in the first place, after all...
:PROPERTIES:
:Author: FourFire
:Score: 1
:DateUnix: 1402741150.0
:END:

*********** And by "partially to blame", we mean, "I'm going to fucking lynch you at some point because everything about that is a travesty."
:PROPERTIES:
:Score: 1
:DateUnix: 1402741796.0
:END:


*********** Seriously, that woman is a homogenizing swarm waiting to happen. GOD FUCKING DAMNIT.
:PROPERTIES:
:Score: 1
:DateUnix: 1402742089.0
:END:

************ Yeah it's just depressing to imagine that the majority of pinnacle sentient beings are childish, amnesiac finite state creatures, living iterations of the same million, or even ten thousand lifetimes over and over until heatdeath.

It's just depressing.... I have many thoughts about it but that's all I can say.
:PROPERTIES:
:Author: FourFire
:Score: 1
:DateUnix: 1402756534.0
:END:

************* Hmm.... I have some complexes about words like "childlike" (namely: how it has come to mean "anyone who sees more wonder and joy in life than I do") and "adult" (namely: how it has come to mean "anyone who has shut down their soul and put on a business suit"), but... /yeah/.

Anyway, the scenario is depressing, but luckily, it's also completely unrealistic.
:PROPERTIES:
:Score: 1
:DateUnix: 1402816867.0
:END:

************** In this instance I mean childlike, as in of having limited knowledge of interesting possibilities, you can still have wonder of the universe without having a childlike mind.
:PROPERTIES:
:Author: FourFire
:Score: 1
:DateUnix: 1402828754.0
:END:

*************** Anyway, like I said, the scenario is completely unrealistic. Even in the space of almost-but-not-quite-Friendly UFAIs, almost none are /specifically programmed/ to reduce people to childlike idiots, and /this/ one in particular has very little in common with the canon TV show the AI was ostensibly programmed to mimic, so the whole worrisome thing would just never happen. And that's assuming you get /neither/ a properly Friendly AI /nor/ a paper-clipping UFAI in the first place, so actually in real life... yeah.

File this one under "kinda pathetic but not actually worth getting depressed about."
:PROPERTIES:
:Score: 1
:DateUnix: 1402831293.0
:END:
