#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 13
:DateUnix: 1492441436.0
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** This is perhaps slightly off topic, but I ended up at a church service again the other day. I'm assuming like myself most of the people here aren't​ religious, but every time I do go to a church service I'm stuck again by the staying power of the institution. It's a meme in the fullest of the original meaning I feel. With a more negative mind you can see how the various institutions work to propel it on. The social censor is obviously mostly gone now, but I still felt bad having to step out of the line going up for communion. You can really see how in days gone by the church would be nearly inescapable.

Relatedly this has reminded me about my biggest problem with the idea of a totally material universe. Something about consciousness always seems weird to me coming purely out of meat. I acknowledge that it almost certainly does, but there's a part of me that wants to go. I think therefore there is something more to meat to me.
:PROPERTIES:
:Author: space_fountain
:Score: 12
:DateUnix: 1492447472.0
:END:

*** #+begin_quote
  Relatedly this has reminded me about my biggest problem with the idea of a totally material universe. Something about consciousness always seems weird to me coming purely out of meat. I acknowledge that it almost certainly does, but there's a part of me that wants to go. I think therefore there is something more to meat to me.
#+end_quote

Try reading up on predictive coding and see if your intuitions don't start to shift the other way. For me, the Hard Problem sounds like a problem if I think of the brain as a passive, bottom-up signal transducer, but not if I think of it as an active, top-down signal predictor.
:PROPERTIES:
:Score: 11
:DateUnix: 1492450092.0
:END:


*** #+begin_quote
  Relatedly this has reminded me about my biggest problem with the idea of a totally material universe. Something about consciousness always seems weird to me coming purely out of meat. I acknowledge that it almost certainly does, but there's a part of me that wants to go. I think therefore there is something more to meat to me.
#+end_quote

What dissuaded me from this perspective is the idea that if the mind is nonphysical that doesn't help questions about consciousness. "Just" because there's something nonphysical interacting somehow with brains, why does /that/ lead to consciousness?
:PROPERTIES:
:Author: TimTravel
:Score: 7
:DateUnix: 1492463090.0
:END:


*** If we are going to be talking about memes, then you should read [[https://www.inkitt.com/stories/horror/7465?ref=v_c35fab34-bc27-42a5-ab24-07ad6bd4457d&started_reading=true][Triangle Opportunity]] by [[https://www.inkitt.com/AlexBeyman?ref=v_c35fab34-bc27-42a5-ab24-07ad6bd4457d][Alex Beyman]]. It's a commonly reoccurring theme in this stories where memes (especially ones to do with religion) act a lot like an infectious disease. There's a [[https://www.inkitt.com/stories/scifi/13278?ref=v_c35fab34-bc27-42a5-ab24-07ad6bd4457d][sequel]] too.

EDIT: This isn't meant to be belittling any emotional difficulties you may have, but rather to prove a similar story for reading that relates to the topic.
:PROPERTIES:
:Author: xamueljones
:Score: 5
:DateUnix: 1492478805.0
:END:

**** Pinging [[/u/Aquareon]]
:PROPERTIES:
:Author: traverseda
:Score: 1
:DateUnix: 1492484590.0
:END:


**** Scary read. Something messing with my cognition is one of the worst things I can think of.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1492684718.0
:END:


*** I think of religion as an exploit. I think that we are evolutionary rewarded for figuring things out, understanding our environment, and feel low level fear when we don't.

As we became more and more intelligent, we came upon questions that can not be answered with things like "tiger bad", "blue berries good". Questions about life, death, purpose. Being unable to fully understand our "environment" we feel anxiety.

Along comes religion and gives simple, understandable answers. And once we accept those answers, the anxiety drops away and we are rewarded by ancient systems put in place to promote adaption.

Combine this vulnerability with another vulnerability - our sponge-like to ability to soak up information as young, often with little or no verification, and you have a potent security hole for a "memetic virus".

Other traits, like wanting to conform to the tribe, be in the "in" group, etc. add to this effect.

Struggling with the nature of consciousness is fine. Taking the easy answers not from evidence but just to feel good is in my opinion less fine, in any context.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 2
:DateUnix: 1492684629.0
:END:


** I'm trying to find my rhythm doing sports; I really don't know what I'm doing, but I'm trying to be more fit and more... I dunno, energetic?

Do you guys have any advice on what kind of training regimen I should adopt? How should I even decide? I'm not trying to lose weight (I'm kinda scrawny), and while I would like to gain muscle mass, it's not really a priority for me. What worries me most is akrasia; I've still haven't managed to keep doing the same sport for two years, and often miss sessions every few weeks/months (my beeminder page has a few bumps).
:PROPERTIES:
:Author: CouteauBleu
:Score: 5
:DateUnix: 1492443919.0
:END:

*** Just do a bunch of intensive cardio. Doesn't matter what it is, really. You'll feel more energetic when you do it consistently.
:PROPERTIES:
:Score: 8
:DateUnix: 1492449992.0
:END:


*** #+begin_quote
  akrasia\\
  /əˈkreɪzɪə/\\
  noun

  --------------

  1. \\
     (philosophy) weakness of will; acting in a way contrary to one's sincerely held moral values\\
#+end_quote

--------------

#+begin_quote
  Derived Forms\\
  akratic, adjective\\
  Word Origin\\
  C20: from a- ² + Greek kratos power
#+end_quote

TIL. I'll put this here for anyone else who doesn't know.
:PROPERTIES:
:Author: appropriate-username
:Score: 5
:DateUnix: 1492446026.0
:END:


*** I haven't done this yet but my friends recommend doing a dance class.
:PROPERTIES:
:Author: Anderkent
:Score: 3
:DateUnix: 1492459940.0
:END:


*** [[http://www.coolrunning.com/engine/2/2_3/181.shtml][Couch to 5k]] is a fantastic Android app that walks you through a multi-week training for running, with built-in timers for warmups and cooldowns.

I find that having a specific "running Thing" has made it way easier for me to get out and get running.
:PROPERTIES:
:Author: owenshen24
:Score: 2
:DateUnix: 1492459714.0
:END:


*** If you like weightlifting, do it, otherwise-- you said you're trying to trying to do sports; do you have any you're currently doing?
:PROPERTIES:
:Author: Loiathal
:Score: 2
:DateUnix: 1492467734.0
:END:


*** Find what (general categories​ of things) you like to do.

I don't like running so much unless it involves chasing a ball. I like the speed of cycling. I am allergic to chlorine pool treatments. I like hitting things. I like hauling stuff up a mountain.

So I do tennis, cycling, and backpacking.
:PROPERTIES:
:Author: CarVac
:Score: 2
:DateUnix: 1492484583.0
:END:


*** I'm a cycle commuter, it's the only way I'd get exercise apart from walking the dog.

Using a bicycle in your daily life is an easy way to force yourself to exercise, I would recommend it if you can make it work. Even one or two days a week (like I used to do when I had a 14km commute each way) can be really helpful.
:PROPERTIES:
:Author: MagicWeasel
:Score: 2
:DateUnix: 1492488088.0
:END:


*** For me, one of the key things is that I have to want to do it. It took a while for me to find activities that I look forward to doing rather than feel like I am doing out of some kind of obligation or guilt.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1492684866.0
:END:


** I've been thinking about irrational artificial intelligences.

If humans had well-defined utility functions, would they become paperclippers? I'm thinking not, given that humans have a number of utility functions that often conflict, and that no human has consolidated and ranked their utility functions in order of utility. Is it because humans are irrational that they don't end up becoming paperclippers? Or is it because they can't integrate their utility functions?

Following from that thought: where do human utility functions come from? At the most basic level of evolution, humans are merely a collection of selfish genes, each "aiming" to self-replicate (because really it's more of an anthropic principle: we only see the genes that are able to self-replicate). All behaviours derive from the function/interaction of the genes, and thus our drives, simple (reproduction, survival) and complex (beauty, justice, social status) all derive from the functions of the genes. How do these goals arise from the self-replication of genes? And can we create a "safe" AI with emergent utility functions from these principles?

(Would it have to be irrational by definition? After all, a fully rational AI should be able integrate all utility functions and still become a paperclipper.)
:PROPERTIES:
:Author: eniteris
:Score: 5
:DateUnix: 1492455092.0
:END:

*** Rationality or lack thereof has nothing to do with paperclipping, I think. Something that blindly maximizes paperclips is, well, a paperclipper from our point of view, but humans are paperclippers in our own way to anything that doesn't share enough of our values.
:PROPERTIES:
:Author: callmebrotherg
:Score: 8
:DateUnix: 1492455691.0
:END:

**** What combination of traits leads to paperclipping?

A well-defined utility function is a must. (Most) humans don't have a well-defined utility function. Is that sufficient? If we could work out the formula for the human utility function, would that automagically make all humans into paperclippers?

Actually, the human utility function probably integrates a bunch of diminishing returns and loss aversion and scope blindness, so that probably balances out and makes it seem like humans aren't paperclippers.

Programming in multiple utility functions with diminishing returns? Probably someone smarter than me has already thought of that one before.
:PROPERTIES:
:Author: eniteris
:Score: 4
:DateUnix: 1492457266.0
:END:

***** #+begin_quote
  (Most) humans don't have a well-defined utility function. Is that sufficient? If we could work out the formula for the human utility function, would that automagically make all humans into paperclippers?
#+end_quote

I think that we generally use "paperclipper" to talk about things that maximize a sole thing, relative to our human perspective.

If you're calling "anything that works to maximize its values" a paperclipper, I think the definition stops being very useful.

Once we extend the definition, everything starts to look like it maximizes stuff.

Sure, I think that humans can probably be modeled as maximizing some multi-variate, complex function that's cobbled together by evolution.

It's generally agreed upon, though, that we're not demonstrating the single-minded focus of an optimization process. (Esp. as paperclipping tends to be defined relative to humans, anyway.)

One could argue that the satisficing actions we take in life actually maximize some meta-function that focuses on both maximizing human values plus some other constraints for feasibility, morals, etc., but then /everything/ would be defined as maximizing things.
:PROPERTIES:
:Author: owenshen24
:Score: 11
:DateUnix: 1492460097.0
:END:


**** I don't quite think so. There are sensory experiences we can have (eg: rewards) which /change/ the internal models our brains use to represent motivation and plan action. A paperclipper, by definition, never updates its motivations. Thus, with a human, you can argue: you can bring to their attention facts which will update their motivations. With a paperclipper, you can't: unless you're giving them information about paper-clips, they'll just keep doing the paper-clip thing.
:PROPERTIES:
:Score: 2
:DateUnix: 1492635739.0
:END:


*** Humans can't become paperclippers because most human goals cannot be endlessly maximized. For example if someone wants to have free time than thinking to much about optimalizing is counterproductive. If someone wants to have children he doesn't think about infinite amount. /"The one small garden of a free gardener was all his need and due, not a garden swollen to a realm."/
:PROPERTIES:
:Author: Wiron
:Score: 3
:DateUnix: 1492465709.0
:END:

**** Maybe the smaller garden had greater value to him than a large garden? So by choosing the smaller garden he WAS maximizing his values. And perhaps if he spent too much time pondering how to make his garden exactly how he likes it, he will have less time to make the garden exactly how he likes it, and even less time to spend in it overall. So by not taking too much time to think about the decision of big garden or small garden, he was also maximizing his values?

Just a thought.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 3
:DateUnix: 1492466224.0
:END:


*** What do you mean by "paperclipping"? Clearly not the literal meaning.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1492462535.0
:END:

**** A 'paperclipper' is an AI that has a utility function which is aligned with some goal that isn't very useful to us, and then pursues that goal relentlessly.

It's from an example of what a failed self-improving general artificial intelligence could look like, where someone manually types in how much it 'values' each item it could produce. If they accidentally mistype something (e.g. how much the AI values paperclips), you end up with a ruthless optimisation process that wants to transform its future light cone into paperclips.

From our point of view, a paperclip maximiser is obviously bad.
:PROPERTIES:
:Author: waylandertheslayer
:Score: 2
:DateUnix: 1492463162.0
:END:

***** I know what a paperclip maximizer is.

[[/u/eniteris]] seems to be using it in a nonstandard way, given "is it because humans are irrational that they don't end up obsessed with paperclips?" doesn't make much sense.
:PROPERTIES:
:Author: MugaSofer
:Score: 2
:DateUnix: 1492468857.0
:END:

****** The main question is "why can't we make an AI in the human mindspace"

What is the difference between a human and a paperclipper? Why is it that humans don't seek to maximise (what seems to be) their utility (whether it be wealth, reproduction or status). Why does akrasia exist, and why do humans behave counter to their own goals?

And are there ways to implement these into AIs?

Although that is a good question. Why don't humans end up as paperclippers? Why do we have maximal limits on our goals, and why don't we fall prey to the fallacies that AIs do? (ie: spending the rest of the universe's mass-energy double-checking that the right number of paperclips are made)
:PROPERTIES:
:Author: eniteris
:Score: 3
:DateUnix: 1492473716.0
:END:

******* I think that you're misunderstanding the issues behind a paperclipper, and why we want to avoid making one.

#+begin_quote
  Why don't humans end up as paperclippers?
#+end_quote

In common parlance in these circles, "what is a paperclipper, really?" would best be answered by the definition "any agent with values that are orthogonal or even inimical to our own."

It doesn't matter whether the paperclipper actually values paperclips, or values something else entirely, so long as they are incompatible or conflict with human values.

In other words, humans /are/ paperclippers, to anything that does not value what we value.

#+begin_quote
  Why do we have maximal limits on our goals, and why don't we fall prey to the fallacies that AIs do? (ie: spending the rest of the universe's mass-energy double-checking that the right number of paperclips are made)
#+end_quote

The classic paperclipper isn't going to spend mass-energy "double-checking" that the right number of paperclips are made. It is going to spend mass-energy making more paperclips, because the "right number" is "as many as can possibly be made."

From the point of view of the paperclipper, however, we are the paperclippers, because we are interested in spending mass-energy on [human values] rather than on supremely interesting and self-evidently valuable things like paperclips.

"How do we avoid creating a paperclipper?" is not a question that we are asking because the hypothetical paperclipper is necessarily more or less rational than humans, or because we can define it in an objective sense such that the paperclipper would consider /itself/ to be a paperclipper.

We are asking this question because, fundamentally, what we are trying to do is avoid the creation of an intelligence whose values do not align with our own. If said intelligence is supremely irrational and incapable of effectively pursuing its goals then we sure did luck out there, but that's beside the point of the discussion.

The simplicity of a paperclipper's value system is also beside the point; we could posit a paperclipper whose values were as complicated and weird as human values, which were also as inimical to human values as the classic paperclipper, and it would qualify as a paperclipper in the important sense that it is part of the class of things that we are trying to avoid when we talk about paperclippers and value alignment. Similarly, we could give this intelligence the whole bevy of human shortcomings, from akrasia to cognitive fallacies, and it would remain a paperclipper, albeit a less competent one.

The reason that we generally talk about a simpler type of paperclipper is that adding all this other stuff distracts from the point that is trying to be made (or at the very least doesn't add to the discussion).
:PROPERTIES:
:Author: callmebrotherg
:Score: 4
:DateUnix: 1492475096.0
:END:


****** As far as I can tell, he's only used the word 'paperclipper[s]' (and that with the standard meaning), rather than verbing it. The rest of the argument might be a bit hard to follow, though.
:PROPERTIES:
:Author: waylandertheslayer
:Score: 1
:DateUnix: 1492470991.0
:END:


*** I believe that humans, and any ration agent, can be modeled using one single utility function, but the output of the function looks like a weighted average of a bunch of more basic utility functions. Humans numerous things like health, sex, love, satisfaction, lack of pain, popping bubble wrap, etc... Each of these imparts some value to the true utility function, with different weights depending on the individual person, and also depending on the time and circumstances they occur in. So, if we want an AI to be well behaved, I think we need something similar. To get more specific, I think the features that are relevant here are:

Robustness: There are a wide range of actions that provide positive utility, and a wide range that provide negative utility. This means that if certain actions are unavailable, others can be taken instead in the meantime. Some people go there entire lives without eating a certain food that someone else eats every day. Some people enjoy learning about random things, some people hate it and would rather carve sculptures. This allows for specialization among individuals, it allows for adapting to new circumstances that never existed when evolution or programming occurred initially, and it prevents existential breakdowns when your favorite activity becomes impossible. Even if all actions exist to serve the spreading of your genes, sex doesn't need to be the only thing you think about since you only need to do it a few times in your entire life (or even zero if you help by supporting other humans with similar genes). A robust utility function will probably look like a weighted average of a bunch of simpler utility functions.

Diminishing Returns: The amount of utility gained from actions tends to decrease as those actions are repeated. Maybe you get 10 points the first time you do something, then 8, then 6.4, and so on. Maybe it's exponential, maybe it's linear, who knows, but the point is it goes down so that eventually it stops being worth the cost and you go do something else instead. People get bored of doing the same thing repeatedly, but also people get used to bad things so they don't hurt as much. Usually the utility goes back up over time, like with eating or sleeping, but it might be at different rates for different activities.

I think these two combined prevent paper-clipping. Even if you deliberately program a machine to make paper-clips, you can prevent it from taking over the world if you give it a robust and diminishing utility function instead of just saying "maximize paperclips". A robust machine will also care about preserving human life, protecting the environment, maintaining production of whatever the paperclips are used for, preserving the health of the company that built it and is selling the paperclips, etc. Manufacturing paperclips is likely its primary goal and the most significant weight in its utility function, but if it starts to make so many that they can't be sold anymore then it will slow down production since the costs start to outweigh the diminishing gains.
:PROPERTIES:
:Author: hh26
:Score: 1
:DateUnix: 1492745646.0
:END:


** an edit of something I posted to spacebattles a little earlier, explaining why I don't think UBI will happen. Does anyone have counterpoint? I'm honestly a little iffy about my own reasoning, and it's the sort of thing I don't want to be wrong about because it affects my long-term plans.

--------------

I don't think UBI is going to happen, but not for the reasons everyone else has been talking about. Assume computers can automate basically every job, and assume that computers can do so in a way that's better and cheaper that people can. Considering how cheap cost of living can be for humans, that would mean cost of living can become even cheaper. Thus, people are cheap enough to hire not because it's necessary, but because it's /prestigious./ Imagine an MMO that simulates wars where most people play as mercenaries, and the rich can hire them for a dozen dollars a day, with the company that owns the IP getting a cut of that payment. Right now, something like that doesn't work primarily for networking reasons-- whales already exist that will drop hundreds a day on a game.

So I predict the confluence of extremely cheap labor and better AI will result in the continuing existence of a job market no matter how good automation gets. There will probably be a period where massive job deficits exist and cause civil unrest, but COL still isn't low enough for this to work, but I don't think that period will last long enough to cause the political will to have UBI.

My back of the envelope calculation goes like this:

Let's assume Moore's law more-or-less holds, and a human brain requires ~an exaflop of computing power. An i7-4790k has a theoretical maximum of 43.92 gigaflops. Obviously that's never getting hit, but it's an older machine regardless. Therefore it'll be about 2*log2(10^18/(43.92*10^9))=~49 years until a home computer is as computationally powerful as the human brain. That doesn't necessarily mean we're getting strong AI then, but AI will still be incredibly smart and relatively cheap by at most 2070. And considering that's just for near-human-level AI, which isn't necessary for most jobs, I think we'll be hitting peak automation at least a decade earlier for basically every single job. so that gives us 40 years to play with, so until ~2060.

Meanwhile, coming from this end of the scale, according to the [[https://www.bls.gov/emp/ep_table_201.htm][Bureau of Labor Statistics]], by 2024 they expect that ~25% of jobs will be in "Goods-producing, excluding agriculture," or "Retail trade" or "transportation and warehousing." These will probably get automated first, but it'll take a while, and some people will sucessfully re-train. On the downside though, losing that many jobs will likely cause a recession of some sort. But still, I don't see unemployment breaching the mid-thirties until past 2035 or so. And even that won't be enough for massive civil unrest if Greece is any indication.

That effectively leaves about 25 years for UBI to be implemented. Now, it's not impossible that UBI gets implemented in that window-- 25 years is a decent amount of time, but I personally don't think a government will be able to reform the entire welfare system around it in anywhere near that timeframe.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 3
:DateUnix: 1492486527.0
:END:

*** There are three issues with your timing:

1. It's possible to distribute calculations across multiple computers.
2. Graphics cards have significantly more operations per second (11.3 teraflops for an NVIDIA GTX 1080 Ti, ~257 times faster than an i77-4790k) for parallelisable functions, and lots of machine learning algorithms are suitable.
3. The human brain doesn't really work like a computer. Its "real" computational power is almost certainly at least a factor of 100 smaller than 1 exaflops.

As an example (a slightly misleading one) of point 3, a human can generally perform under 1 floating point operation a second (maybe up to 10 flops for a savant, but even that would be virtually impossible).

The brain simply hasn't had long enough to evolve optimal calculation processes. A $2 calculator can outperform every human alive when performing complex operations, and a desktop PC can probably beat out every human combined with room to spare.

The difficulty with artificial intelligences is that they don't have the built-in processing faculties that a human brain does (so vision, for example, requires us to come up with the algorithms anew). This is also their strength, because they can potentially do it far more efficiently.

Consider that if humans truly have 1 exaflops of computational power, the world's total artificial computational power (hard to find a figure, but probably under 1,000 exaflops) ought to be exceeded by a small town. So why use computers at all, if a single human is smarter than ~100,000 high-end GPUs?

I contend that computers, especially supercomputers, are more than fast enough to exceed apparent human intelligence already. We're just trying to catch up on evolution, which has relentlessly optimised for a problem space that computers are naive to.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 5
:DateUnix: 1492517253.0
:END:

**** #+begin_quote
  There are three issues with your timing:
#+end_quote

My timing is designed to be very permissible. I'm not saying "we have to wait until 2070 until there's strong AI," I'm saying "We're absolutely guaranteed to get strong AI by 2070," even if we have to resort to EMs and human uploading to do it. I do that instead of an earlier estimate because we don't actually know if moore's law will hold and optimism bias can be a scary think.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 2
:DateUnix: 1492531754.0
:END:


*** #+begin_quote
  Imagine an MMO that simulates wars where most people play as mercenaries, and the rich can hire them for a dozen dollars a day, with the company that owns the IP getting a cut of that payment. Right now, something like that doesn't work primarily for networking reasons-- whales already exist that will drop hundreds a day on a game.
#+end_quote

This is a /really cool/ idea, but I'm not sure where the demand would come from.

- Because you need lots of players on your side to win? Bots are generally better than humans.
- Because you want servants to do the boring parts so you can focus on the fun stuff? We have this, it's called gold farming. It doesn't really look like what you describe.
- Because it makes them feel good to boss around lesser players? Maybe. But under current systems, whales get to /beat up/ lesser players, or to lead them in exchange for in-game scraps rather than real money.
- Because you just want to spend money to show off how rich you are? Here's an in-game hat that costs a million dollars, knock yourself out.

Game developers have no incentive to build games that funnel money to people who are not game developers. When it happens (again, see gold farming), they generally try to stamp it out and/or replace it with a version where the money goes to them rather than other people.

And game devs have a natural advantage here. It's pretty much always going to be cheaper for them to provide whales with gold conjured out of nowhere, NPC minions, or "I win" buttons than it is for other players to do the same. And if it's not, then they can easily change the game rules until it is.

Of course, this is just an example.

But at the end of the day ... if the "dancing for rich people's amusement" industry is worth a billion dollars, and feeding everyone on Earth costs two billion dollars, we're going to have a problem.

Even if feeding everyone on Earth only costs half a billion dollars, what if there's only demand for two billion rich-person-dancers? There are diminishing returns to these things. Once you've exhausted even the truly horrible ways to amuse rich people, like genuine hand-made pyramids, what then?

I think it takes more than "well, labour will be cheaper if living expenses are cheaper" to demonstrate things are going to be OK.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1492528273.0
:END:

**** #+begin_quote
  This is a really cool idea, but I'm not sure where the demand would come from.
#+end_quote

Video games already tend to have policies it banning bots. They would be /better/ at the job, but they just wouldn't be allowed to play.

The decision, would be between paying underlings, or not having underlings (at least for very large groups). And as it turns out, stuff like that already happens-- esports. Of course, I'm not expecting those to be the direct motivator.

Rather, I expect companies to design their business model /exclusively/ around whales, and making them feel powerful as they lead massive armies/cut through disposable pawns, as the regular person won't be able to afford the in-game cash shop. But then a problem occurs-- if an average person is getting shit on by whales, why even play a certain MMO over another? And I think the answer to that is out-of-game compensation by companies, in a similar way as youtube pays people who make content so they draw other people to youtube, and then youtube takes a cut of their profit.

It's similar to what I see in Planetside 2-- even though the devs primarily target whales (that is, people willing to pay for a subscription), they still need to consider non-paying players. Because they're effectively the product used to keep the whales playing.

From there, while my "armies of online mercenaries" may or may not be the way game companies choose to orient their business model, it's still possible to see how wealth can be redistributed on a large scale through capitalism even when robots are mostly better than humans in every scenario.

Of course, you're right:

#+begin_quote
  But at the end of the day ... if the "dancing for rich people's amusement" industry is worth a billion dollars, and feeding everyone on Earth costs two billion dollars, we're going to have a problem
#+end_quote

But then the solution might not necessarily be UBI, but reducing the population of the planet by half. After bear-human-level AI, poor people won't be able to impose their political will through force, because military robots don't feel bad about killing poor people. So my argument is basically that in the transition period, there will still be enough employment (when combined with COL decreases) to prevent the sort of violent unrest that would provoke the political will to have UBI.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1492534291.0
:END:

***** #+begin_quote
  But then the solution might not necessarily be UBI, but reducing the population of the planet by half. After bear-human-level AI, poor people won't be able to impose their political will through force, because military robots don't feel bad about killing poor people.
#+end_quote

I feel like "war between the poor and the rich kills half the planet" is the very /definition/ of "a problem". This is exactly the sort of thing UBI is intended to prevent!

You may be right that it still wouldn't produce the political will to institute UBI because "poor people won't be able to impose their political will through force", but ... at what point in this scenario was democracy abolished? The moment strikes ceased to be effective?
:PROPERTIES:
:Author: MugaSofer
:Score: 2
:DateUnix: 1492539843.0
:END:

****** #+begin_quote
  but ... at what point in this scenario was democracy abolished? The moment strikes ceased to be effective?
#+end_quote

It isn't that democracy is abandoned, it's that a democratic solution won't happen because of further and further concentration of power leading to endemic corruption and politicians listening less to people.

Well, maybe. I admit that I'm taking a deliberately pessimistic view as a form of self-motivation.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1492560517.0
:END:
