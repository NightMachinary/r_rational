#+TITLE: Route to world optimization, assuming infinite motivation

* Route to world optimization, assuming infinite motivation
:PROPERTIES:
:Score: 8
:DateUnix: 1480034274.0
:END:
[deleted]


** I don't think simple motivation is the limiting factor here, compared to people having a limited amount of resources (including knowledge of how to do certain things) capable of leading to such change.

For example, if we were to use the optimization criteria of "Maximize the odds of DataPacRat's long-term survival", and managed to avoid the weird potential edge-cases that don't actually include what that criteria is meant to mean, then giving me infinite motivation won't get me much further than I'm already doing. I'd probably exercise more and eat better, I might manage to acquire a few more bucks in a way that doesn't increase my long-term stress which I could use to buy some nootropics or other off-beat pepper-upper, maybe I'd run for the board of directors of my cryo organization and try to push them in one direction or another; but short of winning lottery-level resources, about the only other things that have a shot at improving that goal would be working towards other methods of winning something like a lottery, such as trying to become a best-settling authour or picking a set of zillionaires to lobby on the off-chance their resources could be redirected. And given the level of odds we're dealing with, it's at least plausible that making the attempts would increase the odds of my dying in the short term more than they'd increase my odds of long-term surviving.
:PROPERTIES:
:Author: DataPacRat
:Score: 5
:DateUnix: 1480037999.0
:END:

*** Well if I gave you large quantities of useful resources it wouldn't really fit with the question I was asking, or I guess trying to imply. The idea is that your life does not currently revolve around improving the world, it presumably revolves around improving your life and the lives of people around you. This includes a lot of downtime that you could be using to study, or some other activity that would get you ahead in life but drive you absolutely insane if you never had any free time from.
:PROPERTIES:
:Author: TBestIG
:Score: 1
:DateUnix: 1480039162.0
:END:

**** "What do you mean 'you', kemosabe?" :)

I can make a plausible argument that everything I do is part of achieving my larger goals, which includes whatever amount of world-optimization I can manage with whatever resources and personal ability I can get. Of course, I'm not your average camper, so you may want to focus your question on people who haven't spent time figuring out what their own fundamental values are, or some similar nudge.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1480039972.0
:END:

***** Well, that's why I included "presumably." I know virtually nothing about you so I'm making a few generalizations. For the most part and for many reasons I assume no human is living their life in the most efficient way possible to improve the world, so the question is about what said hypothetical human would be doing, assuming they were psychologically capable of putting in the work and effort required.
:PROPERTIES:
:Author: TBestIG
:Score: 1
:DateUnix: 1480040329.0
:END:

****** Don't forget, there are costs to increasing efficiency - and, in economic terms, other factors like the difference in value between $1 today and $1 tomorrow. So, at best, humans can only approach the 'most efficient way possible', even with maximum possible motivation.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1480041261.0
:END:

******* When I said possible I suppose I actually meant feasible. Still, it's more than I picture most people being capable of doing, and even fewer would have the desire to.
:PROPERTIES:
:Author: TBestIG
:Score: 1
:DateUnix: 1480041803.0
:END:


** My specific goals, in world-optimisation terms, are pretty vanilla: get to a friendly AI singularity as quickly as possible, and minimise suffering in the meantime. For this, I go to 80,000 hours, check out their high-impact career recommendations, and then do all of them, simultaneously (okay, maybe not all of them, but as much as possible). I figure out how to spin my inhuman motivation levels into encouraging other people to do the same.

Also, I get a legal name change to the Comet King, because at infinite motivation I'm totally allowed to do that.
:PROPERTIES:
:Author: holomanga
:Score: 2
:DateUnix: 1480176288.0
:END:


** When the next munchkin thread rolls around please repost this.
:PROPERTIES:
:Author: chaosmosis
:Score: 2
:DateUnix: 1480402894.0
:END:


** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1480037168.0
:END:

*** #+begin_quote
  This presupposes a benevolent AGI which I would explain as the natural consequence of near-omniscience.
#+end_quote

[[https://wiki.lesswrong.com/wiki/Orthogonality_thesis]]
:PROPERTIES:
:Author: Gurkenglas
:Score: 7
:DateUnix: 1480040382.0
:END:

**** this a thousand times over

If you read the sequences at all, and get anything out of them, this is probably in the list of Top 5 things you need to stick in your head.
:PROPERTIES:
:Author: Tandemmirror
:Score: 4
:DateUnix: 1480045836.0
:END:


*** I took some notes a while ago about how to write a good novel that takes place in a utopian post-singularity society.

In order for readers to care about the story, about the characters and their experiences, there needs to be an actual story. There needs to be a plot. Of the seven basic plots, Overcoming the Monster, Rags to Riches, the Quest, Voyage and Return, Comedy, Tragedy, and Rebirth, all of them seem plausible for a utopian post-singularity society except for Overcoming the Monster and Tragedy. When you have all the time in the universe to accomplish your goals, the stakes are lower, but that doesn't mean there are no stakes.

So what are the stakes? What kind of goals are worth working towards? And of those, what kind of tasks have a possibility of success or failure, or even just of going well or going poorly over a certain period of time? That way there is still dramatic tension and readers care about the outcomes in the story. What do you write about when no one is dying and no one is being tortured or raped, etc.? Well, you just write about /everything else/.

For example:

Money Romance Relationships Reputation Exploration Learning Adventure Drama Humor Aliens

Will anything be serious at all? Will there be ways to help and be helped by other people?

Here's a story prompt I wrote, from the perspective of someone who is born shortly after the singularity.

"All my life I wanted to do something great. And all my life it seemed like all of the great accomplishments had already been done. There isn't really anything I can do that tops saving the human race from annihilation, is there? Nothing more impressive than saving countless lives, right? Plus, anything I can do that is of such great importance can be done by artificial intelligence. So whatever I do has to be meaningful to the human species in order to be worth doing by humans and not by AI. One thing I could do: Explore the whole Universe and gather data and memories to take with us when our artificial intelligence figures out how to escape from this universe before it runs out of negentropy..."

Maybe I'll write a story to go along with this some day. It will probably be a Star Trek fanfic in which [[#s][spoiler]]

Then again, as long as things don't go to shit I have no doubt that someone would at least name their space exploration ship after the ship in Star Trek.

Also, why can't we have petty disputes? Petty disputes are hilarious, and pretty much everybody has them all the time! If you don't have any petty disputes than you probably aren't close enough to anyone to get on each other's nerves. As long as nobody is being killed or seriously hurt by said petty disputes I see no problem with people having them.

Also, this seems kinda relevant here: [[http://lesswrong.com/lw/y3/value_is_fragile/]]
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 2
:DateUnix: 1480040446.0
:END:


** You're essentially asking what a single person can do with limited free time, limited resources, external limits (for example, ethics and laws) but infinite motivation. The last part is such a small practical factor that the concrete strategy still depends almost exclusively on the other limits.
:PROPERTIES:
:Author: goocy
:Score: 1
:DateUnix: 1480064238.0
:END:

*** Because I've already heard what someone can do with infinite resources and no limits.
:PROPERTIES:
:Author: TBestIG
:Score: 2
:DateUnix: 1480079223.0
:END:

**** If you still think that infinite motivation makes sense in a finite world, you have not yet achieved enlightenment.

That includes me.
:PROPERTIES:
:Score: 1
:DateUnix: 1480142924.0
:END:

***** If you think that questioning realism makes sense in a hypothetical scenario, you have not yet understood the hypothetical
:PROPERTIES:
:Author: TBestIG
:Score: 6
:DateUnix: 1480163662.0
:END:


** My thinking is as follows:

1. Get lots of money.
2. Build self replicating factories.
3. Set up an orbital ring system.
4. Use self replicating space robots to brute force other problems.

That's the crude outline. I'm not sure about all the details. Third step is somewhat optional/substitutable since a self replicating robot in space wouldn't need most of its mass launched (or any of it, in the pure form of the concept), it's just that the bootstrap rate of an ORS seems to be surprisingly rapid (Paul Birch calculated 8 hours to launch its own mass).
:PROPERTIES:
:Author: lsparrish
:Score: 1
:DateUnix: 1480128106.0
:END:
