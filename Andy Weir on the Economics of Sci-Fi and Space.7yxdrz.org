#+TITLE: Andy Weir on the Economics of Sci-Fi and Space

* [[https://medium.com/conversations-with-tyler/tyler-cowen-andy-weir-artemis-the-martian-7087b6873260][Andy Weir on the Economics of Sci-Fi and Space]]
:PROPERTIES:
:Author: Kuiper
:Score: 25
:DateUnix: 1519145559.0
:END:

** #+begin_quote
  WEIR: Now the question is, do you want to buy a car that under certain rare circumstances would choose to sacrifice you for some reason? Like, it concludes that it's like, “Oh, that's a bus . . . Due to events beyond anybody's control, I'm about to crash. I can either hit that bus and my passenger will be OK, or I can go off that cliff and everyone on the bus will be OK and my passenger will be dead.” If you're driving the car, no one blames you for trying to preserve your own life. No one holds you at fault for choosing your own life over anything else in a snap decision.

  COWEN: What does the equilibrium look like? Do we still all go selfish?

  WEIR: The selfish cars would be outlawed, is what I'm saying. This would be a policy issue, not a consumer choice issue.
#+end_quote

Wrong. All cars will be selfish by law. Andy hasn't thought this through -- and he's the one who proposed the scenario. It's as though he just thought "Obviously, from an objective point of view, an ethical AI should save the most lives." Not true! Imagine a bodyguard who has been hired to protect a certain individual. In a critical moment, that bodyguard can save the lives of everyone on the bus at the cost of his client's life. Should he do it? Of course not! He has an ethical obligation to prioritize his client's safety.
:PROPERTIES:
:Author: ben_oni
:Score: 11
:DateUnix: 1519189720.0
:END:

*** Agreed, this is part of the reason why AI development under market economy incentives is inherently unsafe.
:PROPERTIES:
:Author: VirtueOrderDignity
:Score: 3
:DateUnix: 1519207144.0
:END:


*** You have a obligation to protect the most lives possible, regardless of your job.
:PROPERTIES:
:Author: Calsem
:Score: 2
:DateUnix: 1519280281.0
:END:

**** Like hell I do! It's as if you've never even heard of the trolley problem. What if those lives are those of condemned prisoners? Or enemy combatants who are trying to kill me? /Obligation/?! None whatsoever.
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1519287951.0
:END:

***** The state does
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1519294051.0
:END:


***** I was talking about regular people, not condemed prisoners or enemy combatants. Your example was a entire bus full of people. I understand a bodyguard priortizing his clients life over a couple people but an entire bus full of people..... that's a lot.

You can't take an absolutist position and say you always have to do your duty to your job, i guess the same way I shouldn't say you should always save the most lives possible.
:PROPERTIES:
:Author: Calsem
:Score: 1
:DateUnix: 1519316269.0
:END:

****** One of the purposes of the Trolley Problem is to help understand that absolutist positions don't really work. We can always find edge cases where an individual has a judgement call to make, and one person might decide differently than another. So, yes, the bodyguard shouldn't /always/ prioritize their client...

In terms of machine ethics, since we're not talking about AGI or superintelligent machines that can out-think the scenario, we need simple rules to govern how the machines should make certain decisions. In terms of the car, it should make choices that prioritize its passengers' safety. And just as in the trolley problem, we can devise edge cases where we would prefer it behave differently (and possibly scenarios where even the passenger would prefer it act differently).

Now, if the ethics of the machine are dictated by the state, I would think they'll start putting state interests first. While we would hope the government would prioritize the lives of its citizens, even a cursory review of government behavior demonstrates the existence of perverse incentives that lead to extremely non-optimal outcomes.

--------------

For fun, why not consider this scenario: on a high mountain road, someone sets up a cardboard cutout of a bus filled with school children, with sufficient detail to fool a car's sensors. Would you still want a car that can drive its passengers off the cliff?
:PROPERTIES:
:Author: ben_oni
:Score: 4
:DateUnix: 1519332111.0
:END:


** #+begin_quote
  WEIR: I think the best self-replicating machines are ourselves. I mean, we're really a lot better at it than any machines that we'll have by 2080.
#+end_quote

I can just about feel the collective triggering of the rationalist/yudkowskian community as they read/listen to this line in the interview.
:PROPERTIES:
:Author: t3tsubo
:Score: 9
:DateUnix: 1519160398.0
:END:

*** Humans, as far as the task of 'create more instance of species' goes, also fall /way/ short of many, many other species. So even considering organic self-replicators, we aren't nearly the best.
:PROPERTIES:
:Author: Aabcehmu112358
:Score: 7
:DateUnix: 1519178443.0
:END:

**** Other species fail to adapt to the environment of humans killing them if we think they replicate too fast or are generally getting too uppity. Therefore we are the best.
:PROPERTIES:
:Author: sicutumbo
:Score: 8
:DateUnix: 1519184390.0
:END:

***** I would point to a multitude of bacterial species as counter examples. We are on the backfoot on our combat with many of their species, and that's only against those that actually infest human bodies harmfully.
:PROPERTIES:
:Author: Aabcehmu112358
:Score: 5
:DateUnix: 1519188326.0
:END:


**** He was proposing bacteria.
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1519188884.0
:END:


** I recommend listening to this interview in audio form if you have the time (put it on your phone and listen during a commute), but the transcript is contained in the link for those who prefer using their eyeballs.

The first chunk of the interview is especially recommended to those who have yet to read Weir's latest novel, Artemis, as it explores the economics of what might make a moon settlement profitable, and how the economy of a private moon base might be sustained. The later portion of the interview also delves into a number of topics related to science fiction. Some highlights:

On teleportation:

#+begin_quote
  WEIR: That would have massive, tumultuous effects because there would no longer be any such thing as borders or territory. Like, if people can teleport, then how . . . Let's say you've got a country. How do you defend that when your enemies can just teleport into the middle? Ultimately, you would end up, very quickly, with a global government.
#+end_quote

On Isaac Asimov's Three Laws of Robotics:

#+begin_quote
  COWEN: Now, Isaac Asimov, as you know, he came up with his Three Laws of Robotics. No harm, obey, self-preservation, in a strictly hierarchical order. Those date from the 1940s. That's now a long time ago. We've seen a lot more from technology, and, in fact, in robotics. Do you think that you, Andy Weir, today in 2017, could improve on Asimov's Three Laws?

  WEIR: I've got to say yes. Because I was a computer programmer for 25 years, so I'm actually pretty good at that stuff.

  One thing that those three laws hid, and it's OK because science fiction is science fiction, but it requires the robot to make moral and ethical decisions. What constitutes allowing a human to come to harm? And a lot of Asimov's stories explore that. But in order for a robot to have those ethical dilemmas and considerations, there's a lot of programing that has to be done under the hood. [...] You would need a very, very detailed description of what constitutes harming a human. What constitutes allowing a human to come to harm. What constitutes obeying a human, and what constitutes self-preservation.
#+end_quote
:PROPERTIES:
:Author: Kuiper
:Score: 8
:DateUnix: 1519145583.0
:END:

*** #+begin_quote
  But in order for a robot to have those ethical dilemmas and considerations, there's a lot of programing that has to be done under the hood. [...] You would need a very, very detailed description of what constitutes harming a human. What constitutes allowing a human to come to harm. What constitutes obeying a human, and what constitutes self-preservation.
#+end_quote

Given that Asimov's robots all have very advanced natural-language interpretation abilities, they very well might not.
:PROPERTIES:
:Author: nick012000
:Score: 4
:DateUnix: 1519197502.0
:END:

**** The interesting thing about Asimov's three laws is that he often explores their consequences and caveats in his stories. (Ad the above quote also points out). I sometimes read people critiqueing Asimov's three laws with arguments that Asimov himself already explores in his stories. Those people have kind of missed the point.

There's a story where two robots end up defining themselves as human. There's stories about robots with different sets of laws, or with conflicts between the the laws. One very important story has a robot derive a 0th law (do not harm humanity or allow humanity to come to harm) as a consequence of the first, allowing him to harm humans in limited way as long as it helps humanity.

Of course what Asimov doesn't explore are the details of how to program such laws. Thats the hard part.
:PROPERTIES:
:Author: Ozryela
:Score: 9
:DateUnix: 1519222578.0
:END:


**** When Siwenna was the most astounding archaisms. The murky gray light of the meeting ...
:PROPERTIES:
:Author: AsiMouth
:Score: 1
:DateUnix: 1519197508.0
:END:


*** And what constitutes a human. Or, indeed, a "person", which might be a more useful term in those settings where there are more than just human people.
:PROPERTIES:
:Author: Trips-Over-Tail
:Score: 1
:DateUnix: 1519243402.0
:END:


** This submission has been randomly featured in [[/r/serendipity]], a bot-driven subreddit discovery engine. More here: [[/r/Serendipity/comments/7z3odz/andy_weir_on_the_economics_of_scifi_and_space/]]
:PROPERTIES:
:Author: serendipitybot
:Score: 8
:DateUnix: 1519196410.0
:END:


** #+begin_quote
  You know, you've got a rebellion, so “yeah, we'll throw off the yoke,” and it has historical parallels and it's all awesome like that. But I don't necessarily think that's going to be the case. Partially because as long as we keep following the rules of the Outer Space Treaty, which I believe we will, there's no such thing as sovereign territory outside of Earth. So Artemis is, functionally speaking, an offshore platform.
#+end_quote

Seig Zeon.
:PROPERTIES:
:Author: nick012000
:Score: 1
:DateUnix: 1519197382.0
:END:
