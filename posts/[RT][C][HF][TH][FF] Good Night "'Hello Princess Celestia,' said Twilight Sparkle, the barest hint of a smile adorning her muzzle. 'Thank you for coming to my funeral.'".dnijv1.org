#+TITLE: [RT][C][HF][TH][FF] Good Night: "'Hello Princess Celestia,' said Twilight Sparkle, the barest hint of a smile adorning her muzzle. 'Thank you for coming to my funeral.'"

* [[https://www.fimfiction.net/story/212395/7/flashes-of-insight/good-night-572][[RT][C][HF][TH][FF] Good Night: "'Hello Princess Celestia,' said Twilight Sparkle, the barest hint of a smile adorning her muzzle. 'Thank you for coming to my funeral.'"]]
:PROPERTIES:
:Author: erwgv3g34
:Score: 0
:DateUnix: 1572118292.0
:DateShort: 2019-Oct-26
:FlairText: WARNING: PONIES
:END:

** [[https://www.fanfiction.net/r/10643785/7/1/][As horrified Eliezer Yudkowsky.]]
:PROPERTIES:
:Author: erwgv3g34
:Score: 12
:DateUnix: 1572118328.0
:DateShort: 2019-Oct-26
:END:


** ...did you mean to link to the last chapter?

EDIT: ah, nevermind. I didn't realize they were all a series of unrelated one-shots.
:PROPERTIES:
:Author: ketura
:Score: 5
:DateUnix: 1572136249.0
:DateShort: 2019-Oct-27
:END:


** She would rather die than become a superintelligence.

Damn she's stupid.

(Good story though.)
:PROPERTIES:
:Author: Lightwavers
:Score: 6
:DateUnix: 1572140105.0
:DateShort: 2019-Oct-27
:END:

*** That's not fair.

She has a different value system than you do: she values integrity of personality over existence.

Since value systems can only be defined rationally /to a point/, and beyond that point are based off of irrational preferences (there's no law of physics encoding any concept of "better"), it's not "stupid" to value wanting to end your life being recognizably the same person you are now, rather than thinking it's "better" to continue life as something fundamentally different.

It's a value choice. And, while choosing to do something which /won't/ fulfill your core values is irrational, no set of core values can be inherently more rational than another, because none of a person's deepest values come from a place of reason in the first place.

Personally, if I were to be offered immortality, I'd only accept if I were given an escape clause. I would prefer, for example, /not/ to persist in a state of perpetual asphyxia, starvation, dehydration, and solitude, after the heat death of the universe. And that may not be a /rational/ choice (and certainly wouldn't seem so to someone who valued continued existence above all else), but as someone with my core values, I wouldn't consider it a "stupid" one either.
:PROPERTIES:
:Author: Nimelennar
:Score: 11
:DateUnix: 1572193610.0
:DateShort: 2019-Oct-27
:END:

**** But this isn't a choice between abandoning her core values and death---it's a choice between /potentially/ abandoning her core values in a way that would result in her fucking off to the stars somewhere without harming anyone else and death. There's no switch that goes straight from "really you, for sure" to "towering alien abomination of intellect." That it happened with Luna suggests that the scale between those two possibilities is a lot easier to go down than picking a point between them, but it's just one data point. And then there's the fact that psychopathy and intelligence are not necessarily intertwined. It's possible to retain empathy while increasing intelligence, or at least /simulate/ empathy to a significant enough degree that the end result looks enough like the real thing that it doesn't matter whether it /really/ is or not. This isn't just my opinion. Enough researchers are working on FAI to suggest that FAI is possible.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1572205620.0
:DateShort: 2019-Oct-27
:END:

***** And how much deviation from "really you, for sure" is acceptable is a /value choice/. Retaining "yourself" is a lot more about retaining your values than it is about retaining empathy (unless, obviously, you highly value empathy).

If the only example of uplifting that you've come across resulted in massively distorted values, would you really be so eager to go that route?

Or, to put it a different way: let's say you exist in the Stargate: SG1 universe, and a Goa'uld symbiont attaches itself to your brainstem. You don't know the moral alignment of this creature; you do know that it will inhabit your body and possess all of your memories, that it will grant your body long life and health, and a wealth of knowledge. You could be /really/ lucky and it's a Tok'ra Goa'uld, and what will happen is a "blending" of your personality and this other being's, but, in your experience, most Goa'uld aren't Tok'ra, and the personality of the host tends to be brutally suppressed.

Would you choose, in this moment before the choice is taken from you, to end your life, or would you leave your body's future actions and the use of your memories at the whims of this being you don't know?

Valuing the preservation of your self/consciousness/memories/body over that self's integrity/personality/values is a /perfectly legitimate choice/. But so is the other choice. And neither is more "stupid" than the other.
:PROPERTIES:
:Author: Nimelennar
:Score: 7
:DateUnix: 1572212383.0
:DateShort: 2019-Oct-28
:END:

****** u/Lightwavers:
#+begin_quote
  (unless, obviously, you highly value empathy)
#+end_quote

I was positive that was what you were referring to.

#+begin_quote
  If the only example of uplifting that you've come across resulted in massively distorted values, would you really be so eager to go that route?
#+end_quote

I'd at least look into it, especially since there's only one data point and the outcome wasn't a paperclipper.

#+begin_quote
  You could be really lucky and it's a Tok'ra Goa'uld, and what will happen is a "blending" of your personality and this other being's, but, in your experience, most Goa'uld aren't Tok'ra, and the personality of the host tends to be brutally suppressed.
#+end_quote

I have to say this is a false analogy. Again, one data point. You can't really say that most ascensions result in brutal suppression, or even that it's /likely/. All we know is that is /happened./

#+begin_quote
  Valuing the preservation of your self/consciousness/memories/body over that self's integrity/personality/values is a perfectly legitimate choice. But so is the other choice. And neither is more "stupid" than the other.
#+end_quote

That's not what I'm saying is stupid. What is /stupid/ is never even trying to investigate a way to perform an uplift while still holding your previous values. Luna has already demonstrated that she is a massive deviation from the norm---she became /Nightmare Moon/. Perhaps she just never valued others and was just pretending, and ascending allowed her to admit that to herself and just blast off.
:PROPERTIES:
:Author: Lightwavers
:Score: 3
:DateUnix: 1572217029.0
:DateShort: 2019-Oct-28
:END:

******* u/Nimelennar:
#+begin_quote
  I was positive that was what you were referring to.
#+end_quote

I can't see why; I never made any reference to what values are, well, valued, and, while the story hints at a lack of empathy on Luna's part after ascension, all that's made clear is that her values have suddenly become incomprehensible.

#+begin_quote
  I'd at least look into it, especially since there's only one data point and the outcome wasn't a paperclipper.
#+end_quote

Look into it how? The only person Twilight can experiment upon is herself, which risks corrupting her value system. Cadence's mind is functionally gone, and Celestia doesn't seem to be volunteering for experimentation, and /no one else exists/.

It should also be noted that she may consider her value system as /already/ having been corrupted - she has already found, from the last incarnation of Equestria, that she can no longer value the company of new ponies.

#+begin_quote
  I have to say this is a false analogy. Again, one data point. You can't really say that most ascensions result in brutal suppression, or even that it's likely. All we know is that is happened.
#+end_quote

Yes, we have one data point, which means it seems to have happened /one hundred percent of the times it's been tried./ And they don't seem to have any understanding of /why/ it happened, either. That, if anything, says the Goa'uld metaphor is /underselling/ the risk (you've /heard tales/ of these supposed Tok'ra, but neither you nor anyone you've met has actually encountered one; the one Goa'uld any of you have met has been of the "brutally suppress the original personality" variety).

Imagine a rocket that can only launch with human guidance. The first time it launches, it explodes catastrophically, killing its pilot, and you have no idea why that happened, because you can't even simulate it properly without a human consciousness attached and at risk.

How can you ethically test that rocket a second time, knowing that the most likely outcome is that it will explode again and kill the pilot again (and again, and again, until you have done enough simulations to track down the factor which is causing the rocket to explode)?

And that analogy doesn't even do the situation justice, because what we're talking about is a radical shift in core values. The /first/ time, the shift was towards something seemingly harmless, but completely alien, something that looks upon normal people like bacteria, but doesn't care enough to harm them. Yes, the first attempt didn't become a paperclipper, but if you admit the second attempt might turn out /better/ than the first, you should also admit that the second attempt might turn out /worse/.

#+begin_quote
  What is stupid is never even trying to investigate a way to perform an uplift while still holding your previous values.
#+end_quote

By definition, you're creating a new person who thinks differently than you do; if not, what is the point? Since they think differently than you do, you cannot predict how they'd think; if you could predict how a person thinks, you can become that person /without/ an uplift (or, at least, with just a boost in processing power and memory retention, which probably wouldn't do much to fix ennui).

Despite all of that, I'll grant that it might be /possible/ to come up with a way to do a safe upload, where values are retained. But it's made clear that Twilight and Celestia are the last two intelligent life forms on the planet. They'd have to seek out, or create, a whole other civilization in order to start those tests, which will take who-knows-how-long, and Twilight (who already seems to be experiencing value decay) doesn't want to go through that again. And, for a prize which is far out of reach, and which the only data point she has suggests /may not even exist/, why should she?
:PROPERTIES:
:Author: Nimelennar
:Score: 5
:DateUnix: 1572220675.0
:DateShort: 2019-Oct-28
:END:

******** u/Lightwavers:
#+begin_quote
  I can't see why
#+end_quote

I thought it was implied. People value empathy.

#+begin_quote
  which means it seems to have happened one hundred percent of the times it's been tried.
#+end_quote

You've stumbled straight into the [[https://en.wikipedia.org/wiki/Base_rate_fallacy][base rate fallacy]] there. We know of one case where, taken to its extremes, this has seemingly turned someone into an unempathetic jackass who'd rather build things in the stars than talk to people.

#+begin_quote
  and no one else exists.
#+end_quote

Easily solved. Celestia herself contemplated making new ponies at the end of the story. So experiment on them. Or, hell, experiment on Cadance. I'm sure she won't mind.

#+begin_quote
  (you've heard tales of these supposed Tok'ra, but neither you nor anyone you've met has actually encountered one; the one Goa'uld any of you have met has been of the "brutally suppress the original personality" variety).
#+end_quote

This analogy has gotten really far off track. First, there's no suppression going on. We /haven't/ heard of anyone encountering one of these supposed oppressive beings, or unfriendly AI, and the only person who did self-modify was already predisposed to introversion, megalomania, and depression.

#+begin_quote
  How can you ethically test that rocket a second time, knowing that the most likely outcome is that it will explode again and kill the pilot again
#+end_quote

Well first off you don't assume that one failed test means it's going to fail again. Second you recognize that the first test didn't really fail at all---as you yourself said earlier, there's nothing /wrong/ with having values that mean you spend your time playing with starstuff. Third, you make new individuals and you ask for the consent of the suicidal ones, if you're going to make new individuals anyway.

#+begin_quote
  but if you admit the second attempt might turn out better than the first, you should also admit that the second attempt might turn out worse.
#+end_quote

The first AI will have all the power. So far that's Luna, and she doesn't care enough to harm anyone. But assume that the second attempt turns into a genocidal maniac. In story we have Discord, Tirek, and the Elements, all of which could conceivably deal with such a threat.

#+begin_quote
  Since they think differently than you do, you cannot predict how they'd think
#+end_quote

False. So long as you understand how exactly this person deviates, you can definitely predict how they'd think. But what if this person, say, thinks twice as fast and has the ability to instantly make themselves devoted to any task. You can predict how they'd think, /and/ you can see how you can't just become that person without modifying your brain. You don't just need a boost in processing power and memory, but in the ability to modify. In the story, Luna continually modified herself until she became an alien. Just set, say, a max of three modifications per year, with unlimited ability to reverse. Or build a guidance consciousness that reverses any changes she finds abhorrent that polices the process.

#+begin_quote
  And, for a prize which is far out of reach, and which the only data point she has suggests may not even exist, why should she?
#+end_quote

Remember what evil would say if you asked it why it did what it did.
:PROPERTIES:
:Author: Lightwavers
:Score: 3
:DateUnix: 1572221860.0
:DateShort: 2019-Oct-28
:END:

********* u/Nimelennar:
#+begin_quote
  People value empathy.
#+end_quote

Yes, but that's not /all/ they value.

#+begin_quote
  You've stumbled straight into the base rate fallacy there.
#+end_quote

From Wikipedia (emphasis mine): The base rate fallacy, also called base rate neglect or base rate bias, is a fallacy. *If presented with related base rate information (i.e. generic, general information)* and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter."

Can you, perhaps, let me know where the base rate has been provided, to make this a base rate fallacy?

I'll get to the "make new ponies" when it comes up again, but, for now:

#+begin_quote
  Or, hell, experiment on Cadance. I'm sure she won't mind.
#+end_quote

Because she /no longer has a mind/. She's a [[https://www.smbc-comics.com/comic/happy-3][429-particle happiness engine]] with a few octillion extra particles.

#+begin_quote
  We haven't heard of anyone encountering one of these supposed oppressive beings,
#+end_quote

The "oppressive being" is the new, "ascended" person you're creating. If they take your memories and personality, and become a person with different values, then they've successfully suppressed your personality.

#+begin_quote
  the only person who did self-modify was already predisposed to introversion, megalomania, and depression.
#+end_quote

...And yet the people who actually /know/ her are convinced that she's experienced a value shift.

#+begin_quote
  The first AI will have all the power. So far that's Luna, and she doesn't care enough to harm anyone. But assume that the second attempt turns into a genocidal maniac. In story we have Discord, Tirek, and the Elements, all of which could conceivably deal with such a threat.
#+end_quote

To protect Equestria, sure (as much as a place without a population can be said to be "protected"). But have any of these entities been shown to be able to protect the universe /beyond/ Equestria? /(Edit to add: I'm also not sure that any of these entities even exist anymore, as Celestia is described as "last intelligent being on the planet" after Twilight's passing)./

#+begin_quote
  False. So long as you understand how exactly this person deviates, you can definitely predict how they'd think. But what if this person, say, thinks twice as fast and has the ability to instantly make themselves devoted to any task. You can predict how they'd think, and you can see how you can't just become that person without modifying your brain.
#+end_quote

Well, you can pretty much achieve that with the extra processing power ("instantly devoted to a task" is pretty trivial to achieve, and also wouldn't seem to relieve ennui all that well - any task that's sufficiently interesting would probably rate devotion from a superlatively bored person like Twilight even without extra focus, and any insufficiently interesting task won't do anything to alleviate the boredom).

#+begin_quote
  You don't just need a boost in processing power and memory, but in the ability to modify. In the story, Luna continually modified herself until she became an alien. Just set, say, a max of three modifications per year, with unlimited ability to reverse. Or build a guidance consciousness that reverses any changes she finds abhorrent that polices the process.
#+end_quote

You're asking the person designing the upgrade process to build a system that the person /subjected to/ the upgrade process (who will be much smarter than the person designing the process) won't have the ability to subvert. That doesn't strike you as a problem? Heck, some of the smartest people in the world work in computer security, and their efforts are routinely circumvented by amateur hackers. As dead-simple (and computationally secure) as the math behind many cryptographic algorithms is, people are still told not to implement them themselves, because it's so easy for even smart, experienced programmers to make errors that are trivial for hackers to exploit. To [[https://www.xkcd.com/2030/][quote Randall Monroe]]: "Our entire field [of software engineers] is bad at what we do, and if you rely on us, everyone will die." And that's in a comic about /voting software/, not /constraining a superintelligence/.

#+begin_quote
  Remember what evil would say if you asked it why it did what it did.
#+end_quote

That is, "Why not?" Twilight has /told you/ why not. In fact, *I've* told you why /Twilight/ has told you why not (emphasis mine-now, not mine-then):

#+begin_quote
  Twilight (who already seems to be experiencing value decay) /doesn't want to go through that again./
#+end_quote
:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1572229653.0
:DateShort: 2019-Oct-28
:END:

********** u/Lightwavers:
#+begin_quote
  One type of base rate fallacy is the false positive paradox, where false positive tests are more probable than true positive tests, occurring when the overall population has a low incidence of a condition and the incidence rate is lower than the false positive rate. The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population. When the incidence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall. So, in a society with very few infected people---fewer proportionately than the test gives false positives---there will actually be more who test positive for a disease incorrectly and don't have it than those who test positive accurately and do. The paradox has surprised many.

  It is especially counter-intuitive when interpreting a positive result in a test on a low-incidence population after having dealt with positive results drawn from a high-incidence population. If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-incidence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.
#+end_quote

#+begin_quote
  Because she no longer has a mind. She's a 429-particle happiness engine with a few octillion extra particles.
#+end_quote

Excellent. Wipe it clean and start over.

#+begin_quote
  The "oppressive being" is the new, "ascended" person you're creating. If they take your memories and personality, and become a person with different values, then they've successfully suppressed your personality.
#+end_quote

Not so. The original would have simply updated with access to new information. If you want, you can think of the original personality as the utility function. Someone who just honestly doesn't care about people has to interact with them, so at normal intelligence might put on a smile and pretend. This is the stage of a paperclipper's life in which it cooperates with humans. Then the person ascends, and realizes that she was deluding herself all along and she doesn't really want friends---what she really desires is the ability to play out there in the stars with no one else around to disturb her. It's an assumption of course, but it works off the available data. Of which we have /one single data point./

#+begin_quote
  And yet the people who actually know her are convinced that she's experienced a value shift.
#+end_quote

Well, of course they are. After all, they know her. If someone close to you suddenly changes, and they recently started taking a new medicine, it can be tempting to blame that change on the medicine.

#+begin_quote
  But have any of these entities been shown to be able to protect the universe beyond Equestria? (Edit to add: I'm also not sure that any of these entities even exist anymore, as Celestia is described as "last intelligent being on the planet" after Twilight's passing).
#+end_quote

Discord can rip holes in reality and travel between universes, so there's evidence that they can. And the avatar of chaos is immortal. He might be banished, or frozen, or just slumbering like some Lovecraftian god, but he can't /die/. Since the Elements, which are not an intelligent being, can target him (assuming the reason he didn't flee the friendship beam was because he couldn't rather than because he's an idiot) it stands to reason that he couldn't just flee to an alternate plane of existence, and thus they too can defend against universe level threats.

#+begin_quote
  Well, you can pretty much achieve that with the extra processing power
#+end_quote

You can certainly imagine ways to use processing power to emulate this, yes, but you're not engaging with the core point I was making. There are ways we can imagine that modify how we think and that are beneficial.

#+begin_quote
  won't have the ability to subvert.
#+end_quote

Perhaps I failed to convey the point. Copy consciousness. Place it at root, with root access. Set emulation speed at many times higher than the secondary consciousness.

#+begin_quote
  That is, "Why not?" Twilight has told you why not. In fact, I've told you why Twilight has told you why not (emphasis mine-now, not mine-then):
#+end_quote

Wrong angle. These are two questions. Why not die, and why not live. She has answered why she doesn't want to continue /as she is/ and has failed to adequately consider alternatives because she is tired. She has then defaulted to why not die. She has defaulted to the position of evil.
:PROPERTIES:
:Author: Lightwavers
:Score: 3
:DateUnix: 1572231670.0
:DateShort: 2019-Oct-28
:END:

*********** u/Nimelennar:
#+begin_quote
  base rate fallacy
#+end_quote

The base rate fallacy is only a fallacy *if* the base rate is different than the specific information. /We don't know/ what the base rate is. Sure, it's /probably/ not 100%, but if Luna is the only subject who has been upgraded, it's probably not 0.0001% either (or, there'd only have been a 1:1,000,000 chance that she'd be corrupted if it were).

If you have some in-universe information to suggest that Twilight should know that the base rate of value drift when ascending is low enough to be worth the risk, I'm happy to hear it.

#+begin_quote
  Excellent. Wipe it clean and start over.
#+end_quote

...Okay, you've taken a decided turn towards the evil here. Creating new minds to be subjected to experimentation is one thing, but going against the express wishes of a friend as to the disposal of her body/consciousness?

I'll skip the assumptions you're making why Luna became what she became, and state that it doesn't really matter /why/ she did; all that matters is /Twilight's perception/ of why she did. Because that's what she's making her decision based upon (and she can't really obtain more data on this, because Luna has already left). And, in her perception, it was due to the ascension.

And yes, there's only one data point, but one data point is /still a data point/. All you have to weigh /against/ that data point is supposition.

#+begin_quote
  You can certainly imagine ways to use processing power to emulate this, yes, but you're not engaging with the core point I was making. There are ways we can imagine that modify how we think and that are beneficial.
#+end_quote

Yes, but you're missing /my/ point. My point is that any mind that you can sufficiently emulate with your own mind is, pretty much by definition, already present within your own mind. Any mind that you /can't/ emulate, you /can't/ predict. So, anything /safe/ (like processor speed) won't relieve your ennui, because you can pretty much become that person by choice, just slower. Anything sufficiently different from you as to relieve your ennui, if /everything/ bores you, isn't someone you can safely assume will retain your values, because you can't sufficiently emulate them (and, if you could, you wouldn't be stuck in a state of ennui).

#+begin_quote
  Perhaps I failed to convey the point. Copy consciousness. Place it at root, with root access. Set emulation speed at many times higher than the secondary consciousness.
#+end_quote

So, you have a slow-thinking subprocessor. .../How/ exactly is this supposed to relieve ennui?

#+begin_quote
  Wrong angle. These are two questions. Why not die, and why not live. She has answered why she doesn't want to continue /as she is/
#+end_quote

Yes, and, by your own admission, she'd /have to continue as she is/ in order to do the research necessary to safely continue as something else. Which, as you also admit, she /doesn't want to do./

#+begin_quote
  has failed to adequately consider alternatives because she is tired
#+end_quote

Even if I concede this (which I don't; we haven't seen how long she's spent considering alternatives to declare whether it's adequate or not; we certainly can't assume that based on the conclusion she reached), "tired" is not "stupid."

#+begin_quote
  She has then defaulted to why not die. She has defaulted to the position of evil.
#+end_quote

And now we're back to values. You consider her death evil. Which, okay, that's your value judgement. But you're imposing /your/ values on /her./ *Values are not universal constants.* If her values are such that, after many, many lifetimes of rational consideration, she has concluded that it is time for her life to end, I think that is her choice to make. /Her/ values should decide what becomes of /her/ body and /her/ consciousness (just as Cadence's values, a preference that her happiness should be maximized, determined what happened to her).

If you think death is evil, you are well within your rights to never die, if you can manage to pull it off. But, as far as /my/ moral values state, you have no right to make that determination for others.
:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1572234860.0
:DateShort: 2019-Oct-28
:END:

************ u/Lightwavers:
#+begin_quote
  If you have some in-universe information to suggest that Twilight should know that the base rate of value drift when ascending is low enough to be worth the risk, I'm happy to hear it.
#+end_quote

That's the thing, you're working off of one data point. There /is/ no information.

#+begin_quote
  Okay, you've taken a decided turn towards the evil here. Creating new minds to be subjected to experimentation is one thing, but going against the express wishes of a friend as to the disposal of her body/consciousness?
#+end_quote

She's effectively dead. If she didn't want to be found, she should've launched herself into space. I think she'd have been happy to know her body would be used to help her friend after her semi-death.

#+begin_quote
  Because that's what she's making her decision based upon (and she can't really obtain more data on this, because Luna has already left). And yes, there's only one data point, but one data point is still a data point. All you have to weigh against that data point is supposition.
#+end_quote

What do you do when you're lacking data? It's not give up and assume the worst. You /get more data/. If she is just tired and doesn't want to go to the trouble she could admit it and that'd be that, but she didn't.

#+begin_quote
  Any mind that you can't emulate, you can't predict.
#+end_quote

My mistake, definitional issues got in the way. I see what you mean by emulate. This isn't a slow thinking processor. It's a fast one that you would put in charge as the root personality. It would in fact be faster, if less complex, than the evolving consciousness a level above it. It would also have complete access to all thoughts, so if the ascending consciousness thinks “hmm that emulation that is at the core of who I am is annoying,” said emulation shuts it all down and restarts. You get more intelligence and thus more experiences without any Luna-related costs.

#+begin_quote
  Yes, and, by your own admission, she'd have to continue as she is in order to do the research necessary to safely continue as something else. Which, as you also admit, she doesn't want to do.
#+end_quote

Look at the above scenario. Other alternatives include volunteers, as already suggested. I believe here you are using motivated reasoning to simply not think about alternatives routes of research because these are obvious.

#+begin_quote
  "tired" is not "stupid."
#+end_quote

It is slower and more prone to bias and stopping at the first palatable conclusion. So yes, tired is stupid.

#+begin_quote
  And now we're back to values. You consider her death evil.
#+end_quote

Not what I meant. She's asking why not without considering the /reasons/ why not. Guide in a new civilization if immortals to grow with her, perhaps. Now you don't have to worry about the risks of ascension, because the social game evolves with the ages.
:PROPERTIES:
:Author: Lightwavers
:Score: 3
:DateUnix: 1572235526.0
:DateShort: 2019-Oct-28
:END:

************* u/Nimelennar:
#+begin_quote
  That's the thing, you're working off of one data point. There is no information.
#+end_quote

There /is/ information. There is exactly one data point. Basing your decisions off of that data point is only a base rate fallacy /if there is additional information to suggest that data point is not reflective of the base rate/.

Otherwise, if you try something for a first time, the result you get that first time is likely to be a likely result of doing what you did, unlikely to be an unlikely result, and very unlikely to be a very unlikely result.

#+begin_quote
  She's effectively dead.
#+end_quote

Neither Celestia nor Twilight are behaving as such.

#+begin_quote
  If she didn't want to be found, she should've launched herself into space.
#+end_quote

Celestia /teleported/ her to the funeral. I doubt a few million km would have made much of a difference.

#+begin_quote
  I think she'd have been happy to know her body would be used to help her friend after her semi-death.
#+end_quote

Perhaps, but that's why people leave last wills and testaments, so that we know what their wishes are. Cadence's were to be stimulated into bliss for eternity. It's a violation of those expressed wishes to experiment upon her.

#+begin_quote
  What do you do when you're lacking data? It's not give up and assume the worst. You get more data.
#+end_quote

There's /no more data to get/. There are no other survivors, besides the three in this story. Celestia isn't volunteering, and neither is Cadence, and even if both did and both ascended while maintaining their values, that still only brings the base rate down to one in three, which aren't very good odds.

#+begin_quote
  It would in fact be faster, if less complex, than the evolving consciousness a level above it.
#+end_quote

And what makes you think that this isn't the "play nice with the humans" phase of the paperclipper, given that, if you limit a mind more complex than yours to only thoughts you can understand, the mind doesn't actually end up any more complex than yours?

#+begin_quote
  Look at the above scenario. Other alternatives include volunteers, as already suggested.
#+end_quote

Which would still require her to continue in her current state until the ascension process is perfected, /which could take thousands of years/. Longer, even: they don't have a /civilization/ at the moment to conduct this research /in./ Admittedly, it might take less time with the experience Celestia has, but humans have been building their civilization for what, twenty thousand years, and aren't at the point Twilight would need yet.

Twenty /thousand/ more years of ennui, perhaps, for the ephemeral possibility of a reward that may not exist.

#+begin_quote
  I believe here you are using motivated reasoning to simply not think about alternatives routes of research because these are obvious.
#+end_quote

I am trying to simulate the mind of someone trapped in depression and ennui, with a fear that I'm already losing my true personality to value decay. I'm ignoring alternative routes of research because /they'd take too long/.

#+begin_quote
  It is slower and more prone to bias and stopping at the first palatable conclusion.
#+end_quote

Slower, yes, but what is speed to someone who has been considering this for years, if not decades, or centuries, it longer? More prone to bias, perhaps, but that's why you have someone else check your results, and Celestia didn't argue too hard that she was wrong. "Stopping at the first palatable conclusion," certainly not, as, again, it seems to have been an extended period since she started thinking about this, after the fall of the last Equestria and the ascension of Luna.

Besides, this isn't physical, lack of sleep tired, which acts like you describe; this is more akin to depression. Which isn't "stupid" so much as "hopeless."

#+begin_quote
  Not what I meant. She's asking why not without considering the reasons why not. Guide in a new civilization if immortals to grow with her, perhaps.
#+end_quote

Imagine you're being tortured. You're in agony, /all the time/, and yet you never get accustomed to it. You can feel your sanity slipping away, to the point where even if the torture stops, you'll still be a traumatized shell of your former self. And the slippage seems to be accelerating. Now, imagine that your torturer gives you a choice: you can end the suffering now, or you can trust them when they say that they'll let you out in a year's time, by which point you think you'll have been reduced to a drooling, gibbering shell of your former self.

Now, /maybe/ that's not a good analogy for the state Twilight is in. But we /don't know/ the thought process that led her to this point. All we know is that she's reached the conclusion that she would find going through another iteration of Equestria to be unbearable. That she's already stopped forming bonds with new ponies.

You keep insisting that she's stupid for not wanting to go through something unbearable for the /possibility/ of a prize at the end which makes things bearable.

I can't say whether she's making a reasonable decision, because I'm not privy to the entirety of the years (or perhaps /lifetimes/) she's spent coming to that decision. But, for the same reason, I don't think there's enough there to assume that her decision is "stupid," either.
:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1572268652.0
:DateShort: 2019-Oct-28
:END:

************** u/Lightwavers:
#+begin_quote
  if there is additional information to suggest that data point is not reflective of the base rate.
#+end_quote

There is. How did intelligence naturally rise? We have another data point in every intelligent being, of which, in Equestria, there are many species.

#+begin_quote
  Neither Celestia nor Twilight are behaving as such.
#+end_quote

There is a body writhing right there. You're going to treat the twitching corpse of a friend with respect whether or not you believe it's dead.

#+begin_quote
  Celestia teleported her to the funeral. I doubt a few million km would have made much of a difference.
#+end_quote

Teleportation has a range limit.

#+begin_quote
  Cadence's were to be stimulated into bliss for eternity.
#+end_quote

Explicitly, or are we just guessing? It seems as if she did it for lack of anything else to do.

#+begin_quote
  There are no other survivors, besides the three in this story. Celestia isn't volunteering, and neither is Cadence, and even if both did and both ascended while maintaining their values, that still only brings the base rate down to one in three, which aren't very good odds.
#+end_quote

Solution: create new beings. Discord can do it with the snap of his fingers. Or talons.

#+begin_quote
  if you limit a mind more complex than yours to only thoughts you can understand, the mind doesn't actually end up any more complex than yours?
#+end_quote

It is my sincere belief that there is nothing we cannot understand given sufficient time and analysis. The root consciousness would be able to effectively freeze time while it analyzes the changes.

#+begin_quote
  Longer, even: they don't have a civilization at the moment to conduct this research in. Admittedly, it might take less time with the experience Celestia has, but humans have been building their civilization for what, twenty thousand years, and aren't at the point Twilight would need yet.
#+end_quote

Solution: summon Discord. What is usually a coin flip that ends in more harm than good becomes essentially risk-free. There are only three more beings he can torment, two of which don't or can't care, and one of which is used to his antics. Either he creates more beings, or he gets bored and goes away again.

#+begin_quote
  I'm ignoring alternative routes of research because they'd take too long.
#+end_quote

Pre-ascension Twilight can create life out of nothing. This objection is nonsensical.

#+begin_quote
  Besides, this isn't physical, lack of sleep tired, which acts like you describe; this is more akin to depression. Which isn't "stupid" so much as "hopeless."
#+end_quote

Depression makes you stupid. I speak from experience.

#+begin_quote
  Imagine you're being tortured. You're in agony, all the time, and yet you never get accustomed to it.
#+end_quote

I will stop you here. This is not what is happening. Twilight is an Alicorn at peak physical health. This analogy is very, very off-base.

#+begin_quote
  All we know is that she's reached the conclusion that she would find going through another iteration of Equestria to be unbearable. That she's already stopped forming bonds with new ponies.
#+end_quote

Indeed. Which is not even close to torture. Ennui, perhaps. You're also forgetting that there is no jailer. She can end herself at any time.

#+begin_quote
  You keep insisting that she's stupid for not wanting to go through something unbearable for the possibility of a prize at the end which makes things bearable.
#+end_quote

I have said no such thing. She's stupid for not considering alternatives. It is understandable stupidity, but still stupidity. Here's one: make everyone an Alicorn. Simple, free of AI risk, creates novelty and new social situations that simply can't happen with people that aren't even a century old. Another: mirror pool.

#+begin_quote
  But, for the same reason, I don't think there's enough there to assume that her decision is "stupid," either.
#+end_quote

It is unquestionably stupid, but it's also understandable.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1572461558.0
:DateShort: 2019-Oct-30
:END:

*************** u/Nimelennar:
#+begin_quote
  How did intelligence naturally rise? We have another data point in every intelligent being, of which, in Equestria, there are many species.
#+end_quote

Beings which are not capable of exponential self-improvement. There is only one data point in terms of beings which /are/.

#+begin_quote
  There is a body writhing right there. You're going to treat the twitching corpse of a friend with respect whether or not you believe it's dead.
#+end_quote

I find it hard to reconcile "treat the twitching corpse of a friend with respect" with "Wipe it clean and start over."

#+begin_quote

  #+begin_quote
    Cadence's were to be stimulated into bliss for eternity.
  #+end_quote

  Explicitly, or are we just guessing? It seems as if she did it for lack of anything else to do.
#+end_quote

I don't get the point you're trying to make. You seem to be presenting Cadence's motives for making the choice of "stimulated with pleasure into mindlessness," but I don't see how her motives are relevant to the fact that this is how she has chosen to spend eternity.

#+begin_quote
  Pre-ascension Twilight can create life out of nothing.
#+end_quote

Life, sure. But a fully-trained scientist, specializing in artificial intelligence, and the infrastructure that person would need to support the research required to definitively determine how to safely upgrade someone?

Surely you're not suggesting that either Twilight or Celestia, two people who each have a large personal stake in the outcome of the research, conduct that research (or even oversee it) themselves? That seems like an excellent way to pressure the researchers to come down on the side of, "Yes, safe upgrading is possible" (Celestia), or "No, it's not possible, end it already" (Twilight), even if some data has to be massaged to get that result.

#+begin_quote
  Depression makes you stupid. I speak from experience.
#+end_quote

It /can/, yes. It doesn't /necessarily/, and I am /also/ speaking from experience. Heck, take a look at all of the creative individuals who have suffered through depression and yet created /masterpieces/ of intellectual and creative accomplishment.

Depression /may/ be accompanied by cognitive distortion that trap you in a state you think that things are hopeless when they're not, but it can also be a rational reaction to a prolonged period in an /actually/ hopeless situation. Or it could merely be a state ("anhedonia") where the things that used to bring you pleasure, don't anymore (which has /nothing whatsoever/ to do with intelligence or rationality), and that seems to be the state Twilight finds herself in.

There are a lot of different manifestations of major depressive disorder; the only thing they all have in common is that someone is experiencing a prolonged state of a depressed mood.

#+begin_quote
  This is not what is happening. Twilight is an Alicorn at peak physical health.
#+end_quote

/Physical/ health, yes. Emotional health? Mental health? Surely someone who can speak from experience about depression wouldn't say that mental or emotional anguish isn't a thing. I've never experienced ennui on that level, but, then, I've never experienced /centuries/ (or longer) of it.

#+begin_quote
  You're also forgetting that there is no jailer. She can end herself at any time.
#+end_quote

I'm not forgetting. If the goal /is/ achievable, why not end herself when the goal is at its furthest? If it's not, why not end it before she goes through all of the hassle proving that it isn't?

#+begin_quote
  She's stupid for not considering alternatives.
#+end_quote

*SHE HAS HAD CENTURIES TO CONSIDER ALTERNATIVES*, and that's just the time period given since she last saw Cadence. She has lived for a /hundred thousand years./

The fact that a prolonged introspection about all the possible alternatives isn't happening on-page /does not mean it didn't happen./ The fact that her conclusion, after all that time, isn't the same as the one you reached instantly, doesn't mean that there's something wrong with her thought processes.

#+begin_quote
  Here's one: make everyone an Alicorn. Simple, free of AI risk, creates novelty and new social situations that simply can't happen with people that aren't even a century old. Another: mirror pool.
#+end_quote

How does any of that help with "I just don't care about any new ponies I meet?"
:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1572489005.0
:DateShort: 2019-Oct-31
:END:

**************** u/Lightwavers:
#+begin_quote
  Beings which are not capable of exponential self-improvement. There is only one data point in terms of beings which are.
#+end_quote

The latter is but a subset of the former.

#+begin_quote
  I find it hard to reconcile "treat the twitching corpse of a friend with respect" with "Wipe it clean and start over."
#+end_quote

People donate their bodies to science, which is not seen as disrespectful.

#+begin_quote
  I don't see how her motives are relevant to the fact that this is how she has chosen to spend eternity.
#+end_quote

Let's say I'm stuck in a cage. I shoot myself in the head. Are my motives irrelevant when considering whether to attempt resuscitation on my now brain-damaged body?

#+begin_quote
  But a fully-trained scientist, specializing in artificial intelligence, and the infrastructure that person would need to support the research required to definitively determine how to safely upgrade someone?
#+end_quote

If Twilight is not a fully-trained scientist than how has she gotten bored of living? She can also conjure inanimate matter, for the record, and living begins are much more complex so it stands to reason she can make whatever she needs.

#+begin_quote
  That seems like an excellent way to pressure the researchers to come down on the side of, "Yes, safe upgrading is possible" (Celestia), or "No, it's not possible, end it already" (Twilight), even if some data has to be massaged to get that result.
#+end_quote

That is why they would use the scientific method, which has gotten us such theories as evolution even with the bias of many God-fearing scientists.

#+begin_quote
  It can, yes. It doesn't necessarily, and I am also speaking from experience. Heck, take a look at all of the creative individuals who have suffered through depression and yet created masterpieces of intellectual and creative accomplishment.
#+end_quote

Depression literally makes the world less colorful. It has a massive impact on the thought process, one which promotes unhelpful trains of thought and sluggishness.

#+begin_quote
  which has nothing whatsoever to do with intelligence or rationality
#+end_quote

Yes it does. Enormous debates have been had on the matter.

#+begin_quote
  the only thing they all have in common is that someone is experiencing a prolonged state of a depressed mood.
#+end_quote

While accurate, this is not precise. You're ignoring how common each type is.

#+begin_quote
  Emotional health? Mental health?
#+end_quote

Subset of modifications, suggesting safe cures for depression or unsafe one if the ennui surges. If she does not know enough to perform this it is strong evidence for her possessing a type of depression which promotes stupidity. Mentally healthy people have jobs. Extrapolating the ability of the average individual to experience decades of routine with no noticeable increases in the average level of ennui suggests this trend occurs in the future and that Twilight does indeed suffer from some form of depression stemming from mental unwellness. Contrast her with Celestia for further evidence. Should mentally unwell people be allowed to commit suicide when there is a cure for the cause of their suffering?

#+begin_quote
  If the goal is achievable, why not end herself when the goal is at its furthest? If it's not, why not end it before she goes through all of the hassle proving that it isn't?
#+end_quote

Very well. Suggested binary: working toward goal either will or will not result in cure of ennui. Implication: at no stage other than the end will she gain any idea of the probability of success. At one end is death. That is bad. At the other is life and happiness for eternity. That is good. Premise: coin flip, or similar. This is not Pascal's Wager. Conclusion: experiment until answer is reached.

#+begin_quote
  SHE HAS HAD CENTURIES TO CONSIDER ALTERNATIVES
#+end_quote

And yet not one of these alternatives were brought up. Perhaps it was because she was too deep in depression to think of them.

#+begin_quote
  The fact that a prolonged introspection about all the possible alternatives isn't happening on-page does not mean it didn't happen.
#+end_quote

Yes it does. That is a rule of writing: unless a possibility that the characters could have taken to resolve a conflict was explicitly mentioned and discarded, its existence can only mean either a plot hole or stupidity on the case of the character who didn't think of it.

#+begin_quote
  How does any of that help with "I just don't care about any new ponies I meet?"
#+end_quote

Because all of them live less than a century and Celestia has fallen into old patterns that don't bring novelty. Perhaps you are suggesting that she is too depressed for even novelty to fix her ennui, in which case she suffers from depression and should attempt to cure it.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1572490518.0
:DateShort: 2019-Oct-31
:END:

***************** u/Nimelennar:
#+begin_quote
  The latter is but a subset of the former.
#+end_quote

The latter isn't "but" anything when compared to the former.

#+begin_quote
  People donate their bodies to science, which is not seen as disrespectful.
#+end_quote

Yes. /They/ donate /their own/ bodies to science. Generally, when people ask to be buried or cremated, their relatives don't donate their bodies to science; that /is/ seen as disrespectful.

#+begin_quote
  Let's say I'm stuck in a cage. I shoot myself in the head. Are my motives irrelevant when considering whether to attempt resuscitation on my now brain-damaged body?
#+end_quote

If you're "stuck in a cage" and have received a point-blank GSW to the head, you'll almost certainly be dead (from exsanguination, if nothing else) before you can be rushed to a hospital to be resuscitated.

That said, there is a directive for first-aiders called "implied consent" stating that, if someone is in a state where they are incapable of granting or denying consent to be assisted (e.g. unconsciousness), it is assumed that they have granted consent for you to assist them. So, given that a GSW would almost certainly render you unconscious, yes, your motives are irrelevant. I'm not sure how that changes for doctors; I'm sure that I'd have to take an ethics course lasting at least one full semester to give anything resembling an educated opinion.

#+begin_quote
  If Twilight is not a fully-trained scientist
#+end_quote

You need more than one (more than two, actually).

#+begin_quote
  That is why they would use the scientific method
#+end_quote

Which includes such concepts as "independent replication." "Peer review." "Blinded studies." And so on; much of the scientific method is in place *specifically* to counteract the researcher's bias. And yet we /still/ have stuff like oil companies paying for research that undersells the impact of carbon in the atmosphere and cigarette companies paying for research that undersells the carcinogenic nature of tobacco.

If you want to get the /right/ results (/especially/ with a subject which presents an existential threat like a self-improving consciousness), your researchers can't feel pressured to come up with one set of results or another, and I can't see a way that that would be possible with either Celestia or Twilight in charge.

I can't see it safely accomplished without at least a team of dozens, all fully trained, with a support structure in place. And then there are going to be all of the other needs that those people have, and are you just going to murder the /most/ of the people you create who, through sheer probability, don't fit the mold of the kind of scientist you need for this research?

In the end, it works out to a civilization you'd need to create to do it properly, and that's exactly what Twilight /doesn't/ want to do.

#+begin_quote
  Depression literally makes the world less colorful.
#+end_quote

Well, no, not literally; visual processing is usually unaffected (unless you get into the schizophrenic variants, which are pretty rare). /Figuratively/, sure.

#+begin_quote
  It has a massive impact on the thought process, one which promotes unhelpful trains of thought and sluggishness.
#+end_quote

That's one kind of depression, yes. That is not true for all kinds of depression.

#+begin_quote

  #+begin_quote
    which has nothing whatsoever to do with intelligence or rationality
  #+end_quote

  Yes it does. Enormous debates have been had on the matter.
#+end_quote

About... anhedonia being irrational? Do you have a citation about that? I can't comprehend the idea that a lack of emotional reaction to stimulus can be irrational. Especially as emotional reaction isn't a rational thing in the first place.

#+begin_quote
  You're ignoring how common each type is.
#+end_quote

Weren't you the one who was going on and on about how we can't generalize a base rate from one example? It applies here, too: you can't assume that Twilight is representative of the most common form of depression, either.

#+begin_quote
  Subset of modifications, suggesting safe cures for depression or unsafe one if the ennui surges. If she does not know enough to perform this it is strong evidence for her possessing a type of depression which promotes stupidity.
#+end_quote

This is, in fact, suggested in the story, and rejected because artificially induced hedonism to counter anhedonia is deemed to be on a slippery slope to Cadence's condition.

And, sure, that slippery slope might be fallacious, but I submit that Twilight knows her own personality a lot better than either of use do, and is thus in a better place to make that determination.

#+begin_quote
  Mentally healthy people have jobs. Extrapolating the ability of the average individual to experience decades of routine with no noticeable increases in the average level of ennui
#+end_quote

I'm sorry, can you offer evidence to your claim that people don't get increasingly bored spending decades doing exactly the same job?

#+begin_quote
  Contrast her with Celestia for further evidence.
#+end_quote

Twilight, who is, again, in a better position to observe Celestia than we are, claims that Celestia is experiencing the same problem Twilight is, only to a lesser extent and/or is hiding it better. Celestia does not contradict this statement.

#+begin_quote
  Should mentally unwell people be allowed to commit suicide when there is a cure for the cause of their suffering?
#+end_quote

What are the other options? That they are forced to take a cure against their will (a violation of all medical ethics) or to endure suffering eternally?

#+begin_quote
  At one end is death. That is bad.
#+end_quote

Value judgement.

#+begin_quote
  At the other is life and happiness for eternity. That is good.
#+end_quote

Another value judgement. Consider that Cadence, arguably, has "life and happiness for eternity," which you say is good. Consider that you also characterize what she has as "death," which you say is bad.

#+begin_quote
  Conclusion: experiment until answer is reached.
#+end_quote

Which, again, takes /time/. Time spent suffering. Let's do some napkin math here.

Let's say that, based on the idea that "the first result you get from a process is likely to be a likely result of that process," Twilight concludes that there is a 1% chance that you can safely conduct research that will ultimately prove that safe ascension (i.e. ascension where the personality and values of the pre-ascension individual survive the process wholly intact) is possible. You're free to disagree with this next part, but from the "more and more," "less and less," I'm getting the impression that Twilight feels like her problem is getting worse over time. So, let's be really conservative. We'll say that she's maybe a hundred times as bored as she was a hundred thousand years ago, for a rate of .0046% increase of ennui every year, or ennui that doubles every 15,000 years.

Let's call the current point the point where the pleasure of just being alive is exactly balanced out with the pain of ennui, because it has /just/ gotten bad enough that she wants to end it.

If the AI research lasts 15,000 years, and leaves her with the same level of pleasure for being alive but with no pain, then she will have to live another 30,000 years to get an amount of pleasure equal to the amount of pain. Figuring in 1% probability of success in order to get the expected return, you get 3,000,000 required years to recoup the expected time spent on research, if 15,000 years are required. Which is 30 times longer than she's already been alive (and that number doubles every 15,000 years).

Factor in that I think you'll need a civilization to accomplish this, and that there have been seven Equestrias over 100,000 years, and it looks like just /setup/ for the experiment might take 14,000 years. Not to mention the time spent on the research itself (and who knows how long /that/ will take).

it's not a coin flip (two endpoints, equally probable, with one being exactly as good as the other is bad). There /are/ two endpoints, but the one data point you have is showing the good ending to very possibly be a lot less likely than the bad solution, and either ending gets exponentially worse the longer you try for the good ending.

#+begin_quote
  And yet not one of these alternatives were brought up.
#+end_quote

Did you /read/ the story? Your "remake Equestria, but make them all alicorns" alternative was brought up ("Every time we rebuild Equestria, *no matter how new and exciting we try to make it,*"), your "fix her depression" alternative was brought up ("I don't want to edit away my capacity to be bored so that I can be eternally satisfied by the raise and fall of ever new Equestrias, Princess; that just seems like a more round-about method of doing what Cadence did,") and then there was the superintelligence solution we've been arguing about.

#+begin_quote
  unless a possibility that the characters could have taken to resolve a conflict was explicitly mentioned and discarded, its existence can only mean either a plot hole or stupidity on the case of the character who didn't think of it.
#+end_quote

That sounds like the road to a very uninteresting story. If you want to write about your characters trying and failing to storm a castle, you have to write a hundred-page treatise on siege tactics in medieval warfare.

This is a /short story/. The discussion of the three alternatives presented already comprises more than 1/3 of the total word count. You couldn't go much further without making the /reader/ bored.

#+begin_quote
  Because all of them live less than a century and Celestia has fallen into old patterns that don't bring novelty.
#+end_quote

Again, they've tried novel versions of Equestria, which Twilight has stopped finding novel.

#+begin_quote
  Perhaps you are suggesting that she is too depressed for even novelty to fix her ennui,
#+end_quote

No, I'm not suggesting that at all; she herself seems to think that novelty will fix her issue (or that's how she describes the path of intelligence augmentation); she's just unconvinced that novel versions of Equestria will provide sufficient novelty, and she's not willing to risk becoming a superintelligence that doesn't retain her personality/values.
:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1572496996.0
:DateShort: 2019-Oct-31
:END:

****************** u/Lightwavers:
#+begin_quote
  The latter isn't "but" anything when compared to the former.
#+end_quote

You're free to disagree, but you have to make a counterclaim if you want to be correct.

#+begin_quote
  Generally, when people ask to be buried or cremated, their relatives don't donate their bodies to science; that is seen as disrespectful.
#+end_quote

So the wishes of a dead person trump the wellbeing of those still alive?

#+begin_quote
  If you're "stuck in a cage" and have received a point-blank GSW to the head, you'll almost certainly be dead (from exsanguination, if nothing else) before you can be rushed to a hospital to be resuscitated.
#+end_quote

Imagine a high-tech society where the premise is possible.

#+begin_quote
  it is assumed that they have granted consent for you to assist them. So, given that a GSW would almost certainly render you unconscious, yes, your motives are irrelevant.
#+end_quote

Do you not see the parallels between this situation and Cadance's?

#+begin_quote
  You need more than one (more than two, actually).
#+end_quote

Not if that scientist has been practicing her craft for actual millennia. A single person can do good science, given enough time and resources. Twilight has all the time, and all the resources.

#+begin_quote
  Which includes such concepts as "independent replication." "Peer review." "Blinded studies."
#+end_quote

Simulate things such as independent replication by changing labs and waiting a while, if you want. Else, make more people.

#+begin_quote
  I can't see it safely accomplished without at least a team of dozens, all fully trained, with a support structure in place.
#+end_quote

Try using your imagination if you can't see it.

#+begin_quote
  Well, no, not literally
#+end_quote

[[https://psychcentral.com/news/2010/07/21/decreased-perception-of-color-in-depression/15826.html][Yes, literally]].

#+begin_quote
  That is not true for all kinds of depression.
#+end_quote

Pray tell, what other type of depression do you think Twilight may be laboring under that matches the symptoms?

#+begin_quote
  About... anhedonia being irrational? Do you have a citation about that? I can't comprehend the idea that a lack of emotional reaction to stimulus can be irrational. Especially as emotional reaction isn't a rational thing in the first place
#+end_quote

Yes. There is a [[https://www.lesswrong.com/posts/SqF8cHjJv43mvJJzx/feeling-rational][Sequence article]] about it.

#+begin_quote
  When people think of “emotion” and “rationality” as opposed, I suspect that they are really thinking of System 1 and System 2 ... Conversely, an emotion that is evoked by correct beliefs or truth-conducive thinking is a “rational emotion”; and this has the advantage of letting us regard calm as an emotional state, rather than a privileged default.

  you can't assume that Twilight is representative of the most common form of depression, either.
#+end_quote

Yes you can. With Luna, we have one data point and the only basis we could use for the most common form of how intelligence increases would be how the majority of species had their intelligence emerge. There is no reason to suspect Luna went this route and has the most common form of result from increasing intelligence. This isn't the case for Twilight's depression.

#+begin_quote
  rejected because artificially induced hedonism to counter anhedonia is deemed to be on a slippery slope to Cadence's condition.
#+end_quote

Do not equate artificially induced hedonism with curing depression. They are not equivalent. That is indeed /very/ fallacious, and I submit that Twilight's mental state is not in the right place to judge that.

#+begin_quote
  I'm sorry, can you offer evidence to your claim that people don't get increasingly bored spending decades doing exactly the same job?
#+end_quote

[[https://www.reddit.com/r/AskReddit/comments/9g0ssn/redditors_who_have_opted_out_of_a_standard/e60oiey/][Yes]].

#+begin_quote
  Twilight, who is, again, in a better position to observe Celestia than we are, claims that Celestia is experiencing the same problem Twilight is, only to a lesser extent and/or is hiding it better. Celestia does not contradict this statement.
#+end_quote

Twilight is likely depressed, and depression colors your perceptions of everything. Celestia isn't going to be feeling the best because she's at the part in the cycle where literally everything except immortal beings are dead.

#+begin_quote
  What are the other options? That they are forced to take a cure against their will (a violation of all medical ethics) or to endure suffering eternally?
#+end_quote

The first one. You forcibly put people in their right mind, give them some time to think, and /then/ let them decide.

#+begin_quote
  Value judgement.
#+end_quote

If you are arguing that death is not bad then we are disagreeing /fundamentally/ and I honestly think you're either brainwashed by a toxic ideology or arguing in bad faith.

#+begin_quote
  it's not a coin flip (two endpoints, equally probable, with one being exactly as good as the other is bad). There are two endpoints, but the one data point you have is showing the good ending to very possibly be a lot less likely than the bad solution, and either ending gets exponentially worse the longer you try for the good ending.
#+end_quote

You're right. Death ends in nothing. The other is infinity experiencing everything. The second option is well worth the time.

#+begin_quote
  no matter how new and exciting we try to make it
#+end_quote

This does not at all imply mass Alicornification.

#+begin_quote
  "I don't want to edit away my capacity to be bored
#+end_quote

This does not at all imply a cure for depression.

#+begin_quote
  If you want to write about your characters trying and failing to storm a castle, you have to write a hundred-page treatise on siege tactics in medieval warfare.
#+end_quote

Three. Three alternatives. A grand total of /zero/ were brought up.

#+begin_quote
  Again, they've tried novel versions of Equestria
#+end_quote

Have they /really/? They say more interesting, but no examples have been given. I think the author just didn't give these alternatives any thought, because you can make things /really/ interesting if you try, especially with the power of an Alicorn.

#+begin_quote
  she herself seems to think that novelty will fix her issue (or that's how she describes the path of intelligence augmentation
#+end_quote

And yet she doesn't know what magic Luna used to get to the stars, she doesn't know how to instantly fix her depression, she doesn't know how brains work, she doesn't know ... a lot of things, really. How much has she tried and how much is depression talking?

The premise of the story is flawed, really. It's yet another author who hasn't thought of the alternatives to boredom. People don't work that way. I've put thousands of hours into Factorio and still haven't gotten bored. That's a game with a very small number of moving parts, especially when compared to real life. When you're not depressed, small variations on a routine seem novel. People have been playing a single instrument their entire lives and still haven't gotten bored with that.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1572499873.0
:DateShort: 2019-Oct-31
:END:

******************* u/Nimelennar:
#+begin_quote
  You're free to disagree, but you have to make a counterclaim if you want to be correct.
#+end_quote

The evolutionary acquisition of intelligence and the artificial, unfettered self-imposition of the same are /not/ equivalent. On the one hand, evolution optimizes itself towards survival and/or reproduction in a given environment. It is limited by the duration of a generation and the number of changes which can propagate from one generation to the next. Harmful variants (evolutionary dead-ends, behaviours which benefit individuals but harm the species, etc.) can be weeded out by natural selection or by other members of the community. The harm which can be done is limited.

By contrast, with a self-improving process, what you're optimizing towards can change /radically/ based on how you change yourself. There are no limits except for the laws of physics.

Nature has produced untold numbers of killers --- diseases, predators, sociopaths, dictators --- over the ages, appearing seemingly from nowhere, killing at will, until stopped by their own failures, or through intervention, or by sheer luck.

And Twilight doesn't even need to proceed /that far/ for "ascension" to be unwise: she just needs to become unrecognizable to herself, in terms of personality and values.

#+begin_quote
  So the wishes of a dead person trump the wellbeing of those still alive?
#+end_quote

In terms of the disposition of their own body? Unless they present a clear danger (e.g. plague), absolutely. Why shouldn't they?

#+begin_quote
  Do you not see the parallels between this situation and Cadance's?
#+end_quote

Not really. The parallel would be to bring Cadence out of the pleasure loop /as herself/ (imagine a high-magic society where the premise is possible), not to pour someone new into her body.

#+begin_quote
  Not if that scientist has been practicing her craft for actual millennia. A single person can do good science, given enough time and resources. Twilight has all the time, and all the resources.
#+end_quote

Give me an example of a scientist who has refined their craft to the point where they are immune to bias and I'll believe you.

#+begin_quote
  Simulate things such as independent replication by changing labs and waiting a while, if you want.
#+end_quote

"Independent" generally includes independence of /researcher/ (i.e. it's replicated not only /on/ different people but /by/ different people).

#+begin_quote
  Else, make more people.
#+end_quote

Which Twilight, for the umpteenth time, explicitly doesn't want to do.

#+begin_quote
  Try using your imagination if you can't see it.
#+end_quote

You're free to disagree, but you have to make a counterclaim if you want to be correct.

#+begin_quote
  Yes, literally.
#+end_quote

I withdraw the objection to the word choice, but now I don't see the pertinence of the example.

#+begin_quote
  Pray tell, what other type of depression do you think Twilight may be laboring under that matches the symptoms?
#+end_quote

"[[https://en.wikipedia.org/wiki/Melancholic_depression][Melancholic depression]]." The kind that messes with your thoughts is more commonly "Depression with anxious distress" (although you can have both).

#+begin_quote
  Yes. There is a Sequence article about it.
#+end_quote

That article says that /experiencing/ emotions isn't necessarily irrational. It says that emotions /can/ (and often are) be based on rational reactions to the world. It /doesn't/ say that /not/ experiencing emotions is an irrational state.

#+begin_quote
  the only basis we could use for the most common form of how intelligence increases would be how the majority of species had their intelligence emerge.
#+end_quote

See the top of my post for why I don't think that's a good example.

#+begin_quote
  There is no reason to suspect Luna went this route and has the most common form of result from increasing intelligence.
#+end_quote

You're not suggesting that Twilight go through the most common route of intelligence increase either (generations of evolution).

#+begin_quote
  Do not equate artificially induced hedonism with curing depression. They are not equivalent.
#+end_quote

I didn't; the story did.

#+begin_quote
  I submit that Twilight's mental state is not in the right place to judge that.
#+end_quote

Depression does not necessarily make you stupid.

#+begin_quote
  Yes.
#+end_quote

The singular of data is not "anecdote."

#+begin_quote
  Celestia isn't going to be feeling the best
#+end_quote

Are you arguing that /she's/ stupid, too?

#+begin_quote
  The first one.
#+end_quote

If you're arguing that people shouldn't have the ability to refuse medical treatment (that is, to decide what people do to their own body and self), I think I agree with your point coming up that we disagree fundamentally, in a way that can't be reconciled.

Speaking of which...

#+begin_quote
  If you are arguing that death is not bad
#+end_quote

Going [[https://www.reddit.com/r/rational/comments/dnijv1/rtchfthff_good_night_hello_princess_celestia_said/f5joro4/][way back in our conversation]], I've already covered this (emphasis mine-then, not mine-now):

#+begin_quote
  You consider her death evil. Which, okay, that's your value judgement. But you're imposing /your/ values on /her./ *Values are not universal constants.* If her values are such that, after many, many lifetimes of rational consideration, she has concluded that it is time for her life to end, I think that is her choice to make. /Her/ values should decide what becomes of /her/ body and /her/ consciousness (just as Cadence's values, a preference that her happiness should be maximized, determined what happened to her).
#+end_quote

My position hasn't changed one iota from there; if you think that's evidence that I'm "brainwashed by a toxic ideology or arguing in bad faith", then you should have stopped arguing with me four days ago.

I'm not arguing that death /isn't/ bad. Nor am I arguing that it /is/ bad. I'm arguing that "death is bad" is a /value judgement,/ a /human construct/ (as the very ideas of "good" and bad" are), and I'd bet if I polled all 8,000,000,000 humans, I'd get a lot more nuance beyond “death is bad,” from the Dumbledorian “death gives meaning to life,” to the Malthusian “death is necessary to prevent overpopulation,” to the historical “if people didn't die, America would still be governed by the Founding Fathers' viewpoint on slavery,” and that's not even bringing the religious/spiritual beliefs into the equation.

#+begin_quote
  You're right. Death ends in nothing. The other is infinity experiencing everything. The second option is well worth the time.
#+end_quote

And if "everything" turns out to be nothing but an eternity of boredom, as all evidence in the story itself suggests it will? Would you still say it's "worth the time?"

#+begin_quote
  no matter how new and exciting we try to make it

  This does not at all imply mass Alicornification.
#+end_quote

It implies maximum new and exciting, which /absolutely does/ imply mass Alifornication (that was a typo, but I'm leaving it because I find it amusing).

#+begin_quote
  "I don't want to edit away my capacity to be bored

  This does not at all imply a cure for depression.
#+end_quote

If being bored is the source of her depression, then it /absolutely does/ imply a cure for her depression.

#+begin_quote
  Three. Three alternatives. A grand total of /zero/ were brought up.
#+end_quote

Once again, a considerable chunk of the story is already about alternatives to dying. Let's say the start and the end remain the same, and the part after "I'm open to alternatives" and before "I suppose your decision is made, then" is expanded to your satisfaction. That's currently over one-third of the story; what percentage of the story would have to be devoted to exploring alternatives before you would be satisfied with Twilight's decision to die?

#+begin_quote
  Have they really? They say more interesting, but no examples have been given.
#+end_quote

See above.

#+begin_quote
  she doesn't know ... a lot of things, really. How much has she tried
#+end_quote

We don't know. You are insistent upon the assumption that she /hasn't/ tried sufficiently, even though she has been alive at least a hundred thousand years.

#+begin_quote
  The premise of the story is flawed, really.
#+end_quote

You're welcome to that opinion.

#+begin_quote
  People don't work that way. I've put thousands of hours into Factorio and still haven't gotten bored.
#+end_quote

Thousands, yes. Hundreds of millions? More to the point, /all/ of your hours?

#+begin_quote
  When you're not depressed, small variations on a routine seem novel.
#+end_quote

For some people, that may be true. However, if that were universally the case, boredom wouldn't exist outside of depression (spoiler: it does).

#+begin_quote
  People have been playing a single instrument their entire lives and still haven't gotten bored with that.
#+end_quote

Again, for the comparison to be apt, they would both have to be putting /all/ of their time into playing that instrument, for a thousand lifetimes. I'm pretty sure that, after two or three lifetimes at most, they'd want to at least learn a different instrument.
:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1572658558.0
:DateShort: 2019-Nov-02
:END:

******************** Alright, there are a lot of claims here and we could easily go back and forth for eternity. I could go point by point disagreeing with nearly everything you've said here in some way or another, but by now this has gotten really boring and I'd rather not. I believe you're wrong, you believe I'm wrong, and if we see each other outside this conversation we can pick up this argument again.
:PROPERTIES:
:Author: Lightwavers
:Score: 3
:DateUnix: 1572658835.0
:DateShort: 2019-Nov-02
:END:

********************* Fair enough; I hadn't yet stopped finding novelty in the small variations on this routine, but, if you have, I'm content with where we've left this.
:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1572660759.0
:DateShort: 2019-Nov-02
:END:


*** She could also, y'know, keep living normally.

I'm pretty sure I'd never get tired of immortality if I remained physically young. I think there is probably an infinite amount of things to do, and definitely enough to do till heat death. The fictional beings who get tired of immortality probably just aren't very creative.
:PROPERTIES:
:Author: eroticas
:Score: 3
:DateUnix: 1572338873.0
:DateShort: 2019-Oct-29
:END:

**** I know, right?

1. Create a society that generates works of fiction, both static (books) and interactive/self-generative (video games).
2. Influence society to generate the kinds you like.
3. Enjoy.
:PROPERTIES:
:Author: DuplexFields
:Score: 3
:DateUnix: 1572462496.0
:DateShort: 2019-Oct-30
:END:


** I mean I consider wireheading to be functionally identical to death as much as anyone but I'd still take it over plain non-existence.
:PROPERTIES:
:Author: Noir_Bass
:Score: 4
:DateUnix: 1572172348.0
:DateShort: 2019-Oct-27
:END:


** What was the prompt on this one, write the most wrong fic possible?
:PROPERTIES:
:Author: aponty
:Score: 4
:DateUnix: 1572146311.0
:DateShort: 2019-Oct-27
:END:


** Do you know if the author has done anything else?
:PROPERTIES:
:Score: 2
:DateUnix: 1572182302.0
:DateShort: 2019-Oct-27
:END:

*** He also wrote [[https://www.fanfiction.net/s/10503877/1/The-Amazing-Peter-Parker]["The Amazing Peter Parker"]] and [[https://365tomorrows.com/2015/02/27/procrastination/]["Procrastination"]].

Apparently, this guy really likes rational flash fiction.
:PROPERTIES:
:Author: erwgv3g34
:Score: 3
:DateUnix: 1572182624.0
:DateShort: 2019-Oct-27
:END:
