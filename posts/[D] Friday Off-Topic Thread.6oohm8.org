#+TITLE: [D] Friday Off-Topic Thread

* [D] Friday Off-Topic Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 19
:DateUnix: 1500649652.0
:DateShort: 2017-Jul-21
:END:
Welcome to the Friday Off-Topic Thread! Is there something that you want to talk about with [[/r/rational]], but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with [[/r/rational]] instead of going over to [[/r/japanesegameshows]], but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? The sexual preferences of the chairman of the Ukrainian soccer league? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could possibly be found in the comments below!


** I was reading some Harry Potter fanfiction the other day, and came across a scene where Neville was shot with the Killing Curse, but lived because his toad jumped into the path and took the hit instead. Which was sad and all, but the Munchkin in me immediately thought: Wait a tick, why don't wizards just carry tons of toads with them!? Just get a bunch of toads, petrify them, and glue them to clothes! So instead of meatshields, we can have soulshields!

That was the point I realized the three Unforgivable Curses are just garbage in combat. They are spells that move in straight lines and directly target souls, with no physical effects whatsoever. And for all that talk about being "Unblockable", it is actually really easy to block them: after all, lots of things have souls.

At which point I started thinking, what is the most practical defense against the Unforgivable Curses? I browsed some reddit threads and found that some other people had the "Soulshield" idea as well, but didn't find any practical suggestions. It is not clear whether you could use ghosts. You could wrap a snake around your body, but that restricts movement and would only tank 1 killing curse, since the next would just go through the dead snake and hit you. The same problem for toads, too heavy and too few souls.

No, what you really want are tiny creatures. As Moody demonstrated, even tiny spiders have souls, and it's generally hinted that anything with a brain is vulnerable to the Killing Curse. So use insects. The easiest ones would be things like ticks, fleas, and headlice. They already normally parasite on our skin. Get your combat wizards to stop bathing and using their Cleaning Charms and soon enough they will be covered with countless tiny parasites that can tank the Unforgivable Curses for them.

Unfortunately, even with all these parasites, it is unlikely that they cover a significant fraction of your surface area, so a Killing curse could still get lucky and hit you. Furthermore, the parasites don't really form a dense layer, so if you are hit in the same spot a few times, you die. Which is when I thought of my next idea: Ants. There are ways to breed ants really quickly to form really dense colonies, and have them live in small plastic tubes. Make clothes out of these plastic tubes, fill the tubes with ants, and then just fight while wearing these clothes. Now you can tank a truckload of Unforgivable Curses without taking any damage whatsoever.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 17
:DateUnix: 1500675342.0
:DateShort: 2017-Jul-22
:END:

*** When fake-moody was lecturing the class, he mentioned something along the lines of "if everyone in this class cast the killing curse on me, I'd probably get a nosebleed. (Paraphrased). Meanwhile, voldemort instakills anyone he targets with it, because he just has /that much hatred/.

Which leads me to think that the killing curse's damage dealt can be modeled as "hatred of caster"/"soul capacity of target". Which in turn, leads me to suspect that using non-persons as a shield is either unnecessary (because most people couldn't kull you anyways) or useless (because if they have enough hatred to kill humans, they can probably spare the hatred to penetrate straight through your meatshield.)

The same principle would likely apply to the other unforgivables as well. In the end, the best defense is to just not be where they aim the spell. (Or, alternatively, expelliarmus).
:PROPERTIES:
:Author: GaBeRockKing
:Score: 13
:DateUnix: 1500677183.0
:DateShort: 2017-Jul-22
:END:

**** u/ShiranaiWakaranai:
#+begin_quote
  or useless (because if they have enough hatred to kill humans, they can probably spare the hatred to penetrate straight through your meatshield.)
#+end_quote

This assumes that if the spell's "killing power" is stronger than the target's "lifeforce", it continues onwards and continues to kill everything it hits, as opposed to just killing whatever it hits faster/more certainly. That doesn't seem true: when Moody casted the killing curse on the spider, it stopped after hitting the spider. It didn't go through and continue onwards to hit whatever unfortunate student was on the floor below the classroom.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 6
:DateUnix: 1500679485.0
:DateShort: 2017-Jul-22
:END:

***** u/GaBeRockKing:
#+begin_quote
  when Moody casted the killing curse on the spider, it stopped after hitting the spider.
#+end_quote

Easily explainable by moody not really hating the spider all that much.

Anyways, while the answer is probably "J.K.Rowling didn't think of using meatshields," from a Death of the Author perspective, there has to be /some/ reason that none of the protagonists in-canon ever deliberately use meat shields against AK, and therefore the likeliest explanation is that meat shields (as you envision them) simply aren't effective for one reason or another. That's not to say meat shields aren't effective at /all/, but merely that there's more going on in the background that readers aren't directly privy to.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 2
:DateUnix: 1500680697.0
:DateShort: 2017-Jul-22
:END:

****** I think it's more likely that it wasn't a combat mechanic ever really introduced. Despite HPMOR's assertion, I'm fairly certain the killing curse can be blocked by typical, if somewhat large, objects. The Harry Potter wiki states:

#+begin_quote
  one may dodge the green bolt, block it with a physical barrier, or by the use of Priori Incantatem.
#+end_quote
:PROPERTIES:
:Author: Kishoto
:Score: 3
:DateUnix: 1500825333.0
:DateShort: 2017-Jul-23
:END:


**** The wording was "I wouldn't get so much as a nosebleed" or something to that effect: it could just be hyperbole. I don't think the Killing Curse can only partly kill someone.
:PROPERTIES:
:Author: ketura
:Score: 4
:DateUnix: 1500691570.0
:DateShort: 2017-Jul-22
:END:

***** u/GaBeRockKing:
#+begin_quote
  The wording was "I wouldn't get so much as a nosebleed" or something to that effect: it could just be hyperbole. I don't think the Killing Curse can only partly kill someone.
#+end_quote

Regardless, the point is that the killing curse can be cast with different levels of intensity.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 2
:DateUnix: 1500691814.0
:DateShort: 2017-Jul-22
:END:

****** I understood it to mean that the Killing Curse has hefty requirements that most people can't fulfill. "If you all tried to cast this spell at me, it wouldn't do anything at all [because you can't cast it properly]" doesn't necessarily imply that the spell doesn't work by a binary success/fail model. In canon, anyone hit with an AK is dead. Nobody ever treats a successfully cast AK as anything other than instadeath.
:PROPERTIES:
:Author: waylandertheslayer
:Score: 12
:DateUnix: 1500709521.0
:DateShort: 2017-Jul-22
:END:


****** Fairly sure this isn't the case. We've seen dozens of instances of the killing curse being instadeath and zero instances of it being anything but.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1500825117.0
:DateShort: 2017-Jul-23
:END:

******* Because basucally the only type of people who bother casting it (i.e., supervillains) are also exactly the kind of people who can kill with it. I don't really recall anyof the good guys killing anyone with AK.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1500825587.0
:DateShort: 2017-Jul-23
:END:

******** Severus Snape was a good guy (by the most technical definition of it, anyway)

And, even if he wasn't, we know he didn't hate Dumbledore. He actually probably had something resembling love towards the old man. But he was still able to AK him.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1500831933.0
:DateShort: 2017-Jul-23
:END:

********* u/GaBeRockKing:
#+begin_quote
  And, even if he wasn't, we know he didn't hate Dumbledore. He actually probably had something resembling love towards the old man. But he was still able to AK him.
#+end_quote

Snape hated lots of stuff. Most importantly, himself. I don't really find it hard to believe that snape, of all people, could muster enough hate to kill.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 0
:DateUnix: 1500832587.0
:DateShort: 2017-Jul-23
:END:


*** Finding out loopholes like that is only “stage-one” deconstruction of the original universe --- acceptable for casual CYOA gameplays, RPG campaigns, cracky fanfics (among which I'd list [[https://www.fanfiction.net/s/6413108/1/To-Shape-and-Change][/To Shape and Change/]] as well), and so on.

If the writer wanted for his modified setting to be more self-consistent, however, they'd have to delve deeper into “stage-two” deconstruction. Using the AK-soulshield example, they'd have to ask themselves “How come the protagonist was the first character in this universe to have ever thought about something like that?” From which point it would usually turn into:

- either [1] “The protag wasn't the first one, the technique just isn't as effective as it seems to be.” → [1.2] “What makes this seemingly very efficient technique so useless that nobody ever uses it?” (at which point you kinda close the loophole of your own discovery and maybe also introduce some changes to the canon mechanics to make it all work together);
- or [2] “The protag wasn't actually the first one to have thought about it.” → [2.2] “How should the world of this fanon differ from the canon to make way for this important change?” (at which point you start rewriting the entire history of the canon universe, like some [[http://www.scp-wiki.net/scp-140][Keter class artefact]] to make answer [2] make sense).

Working on top of the canon universe works as long as you just don't touch the loophole-looking things that you know are there. Because as long as you're not touching them, your story's focused on other things --- things that you've actively been working on to keep the whole story self-consistent. But right as you decide to use a loophole, it instantly becomes something that you have to carefully think about and re-design, which is as difficult as (if not more difficult than) doing a completely original world-building from scratch.

This is actually why I think it's so difficult to write rational fanfics for works that do not comply with Sanderson's third law of magic \ storytelling. A rational character would actively investigate all these nooks and crannies that the original texts themselves have left untouched. And with each new such path of mental experimantation discovered, the author of the rat!fic suddenly has to answer one more question about a possible loophole and thoroughly cover it in his setting. So it eventually either turns into a gargantuan effort of world-building, or into a story where the protagonist is only nominally rational and in fact ignores a vast horizon of unexplored possibilities.

--------------

Here are two additional examples of what I mean:

- In the same /To Shape and Change/ (spoilers) there's a scene where VD and Dumbledore are fighting each other, bunnyhopping through Apparations, and somehow both end up seriously wounded by stray muggle bullets. Now, in Rowling's original setting the issue of muggle weaponry used against wizards isn't touched /at all/ --- so the reader can accept that this just isn't the focus of the story and move on. Here, however, it affects a crucial part of the plot, and the reader can't keep doing that any more. And suddenly a whole lot of questions rise up from that, none of which is being answered by the author of the fanfic.

- Similarly, [[https://www.fanfiction.net/s/9659792/1/The-Boy-in-the-Team][/The Boy in the Team's/]] rendition of the Orochimaru's attack during Chūnin Exam Finals shows how Orochimaru has booby-trapped several important parts of the Konoha village (the academy, the hospital, etc) to such extent that he can completely obliterate the buildings there by simply snapping his fingers. It's explained that explosives can be stored inside storage scrolls moments before going off, so that when a scroll is activated the explosion will be guaranteed to happen in less than a second, and that seals themselves are relatively easy to sneak into enemy village. And, again, I don't think something like this has been touched in canon --- so maybe there is something preventing this from happening that the reader doesn't know about and so can just enjoy reading the story. While here, in this fanfic, suddenly rises a question: if village security can be bypassed so easily, how come there are still so many villages still standing, and so on.
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 14
:DateUnix: 1500689335.0
:DateShort: 2017-Jul-22
:END:


*** Ah, yes, the staple of [[/r/HPMOR]] while the fic was still ongoing: bee armor. It's unknown if conjured bees would work, but if not, just find a spell to make the bees swarm around you.
:PROPERTIES:
:Author: ketura
:Score: 8
:DateUnix: 1500675972.0
:DateShort: 2017-Jul-22
:END:

**** There are a number of downsides to the bee swarm tactic though:

- The bees could sting you, you would need to also use magic to stop that from happening.
- You would constantly lose mana (or whatever the Harry Potter version of mana is), since you need to keep fueling the magic spells that keep the bee swarm around you instead of simply dispersing and leaving you unprotected.
- A swarm of bees presents a much larger target to the enemy than your body, so it is relatively easy for enemies to cast spells at the swarm to disperse/kill your bees or negate your magic on them.
- It makes it obvious to your opponents why their Unforgivable Curses aren't working.

In contrast, you could wear an ant colony under your clothes, making it seem like the curses are hitting you yet have no effect on you whatsoever. Just imagine, you could calmly walk through a battlefield of mages, Determinator-style, tanking all their Unforgivable curses, stunning spells, sleeping spells, petrifying spells, body-binding spells, etc. since they all hit ants instead. The sheer intimidation alone from seeing you survive all those spells with no visible damage whatsoever would be enough to send your enemies into panic.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 7
:DateUnix: 1500678397.0
:DateShort: 2017-Jul-22
:END:

***** Except if they hit your face.
:PROPERTIES:
:Author: rhaps0dy4
:Score: 6
:DateUnix: 1500721927.0
:DateShort: 2017-Jul-22
:END:


**** *Here's a sneak peek of [[/r/HPMOR]] using the [[https://np.reddit.com/r/HPMOR/top/?sort=top&t=year][top posts]] of the year!*

#1: [[https://i.redd.it/gc4glq0ymf3y.jpg][Quirrell on the 2016 election]] | [[https://np.reddit.com/r/HPMOR/comments/5i7kev/quirrell_on_the_2016_election/][28 comments]]\\
#2: [[https://i.imgur.com/i0T99R8.jpg][Chaos Legion is real]] | [[https://np.reddit.com/r/HPMOR/comments/588fr0/chaos_legion_is_real/][12 comments]]\\
#3: [[https://np.reddit.com/r/HPMOR/comments/5ry7gm/contrasting_attitudes_towards_the_wise_old_wizard/][Contrasting attitudes towards the wise old wizard among heroes in HPMOR: What is this literary doing in my fanfic?!]]

--------------

^{^{I'm}} ^{^{a}} ^{^{bot,}} ^{^{beep}} ^{^{boop}} ^{^{|}} ^{^{Downvote}} ^{^{to}} ^{^{remove}} ^{^{|}} [[https://www.reddit.com/message/compose/?to=sneakpeekbot][^{^{Contact}} ^{^{me}}]] ^{^{|}} [[https://np.reddit.com/r/sneakpeekbot/][^{^{Info}}]] ^{^{|}} [[https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/][^{^{Opt-out}}]]
:PROPERTIES:
:Author: sneakpeekbot
:Score: 2
:DateUnix: 1500675979.0
:DateShort: 2017-Jul-22
:END:

***** [[/u/PM_ME_RATIONAL_FICS]] deleted their account?! Why, you glorious bastard, why??!
:PROPERTIES:
:Author: ketura
:Score: 3
:DateUnix: 1500676209.0
:DateShort: 2017-Jul-22
:END:

****** I've been wondering that too, so if anyone knows...
:PROPERTIES:
:Author: callmebrotherg
:Score: 2
:DateUnix: 1500700444.0
:DateShort: 2017-Jul-22
:END:


*** I was commenting down below and felt the need to point something out:

Despite HPMOR's assertion, the Killing Curse is canonically blocked by physical objects. Obviously clothing and such doesn't block it but other, denser items do. It does not go through walls. That's canon.
:PROPERTIES:
:Author: Kishoto
:Score: 6
:DateUnix: 1500825412.0
:DateShort: 2017-Jul-23
:END:


*** Here's the thing, you're focusing too much on one aspect and not enough on others. Firstly just as a minor note, we don't actually know the unforgiveables target souls specially or anything. Details about how they work are very sparse, and the closest we really get to an answer is the incredibly special cased instance of Voldemort, and one comment he made on what it felt like to be hit by the killing curse. Basically, any theory involving them being soul stuff is super sketchy.

That said, the thing is that the killing curse isn't special the way you want it to be. Your idea is that it's bad because if it hits someone else who is standing between you and it you don't get hurt? So what? Pretty much every spell in the entire series won't hurt you if it hits someone else, that's just how single target spells work. What's it matter if it's a cutting spell or a killing curse? Either way it's the thing in between you getting hit, not you. This is one thing people seem to regularly overlook with similar ideas, focusing overly much on the killing curse and not on how if this worked it'd also work against nearly everything else, so you haven't really made the killing curse weaker relative to near everything else.

So the question actually becomes, why can't you just cover yourself in ants and be immune to effectively everything barring AoE spells? Worst case for you is the simple hit logic kicks in and it just doesn't count. How clothing/armour doesn't seem to affect the single target spells, something like a stunner or a killing curse hit you and doesn't care that technically it hit your clothes, it hit you and that's what important. They clearly can't phase through walls or anything after all, so it's not just flying through what you're wearing, it can just affect you by hitting what you're wearing. If you're wearing tubes filled with ants? Whose to say it wouldn't still affect you despite wearing plastic tubes?

Plenty of different ways you could explain it either way, but a critical thing to remember is pretty much every defense that works against the killing curse is going to work against most other spells as well.
:PROPERTIES:
:Author: xavion
:Score: 3
:DateUnix: 1500764797.0
:DateShort: 2017-Jul-23
:END:

**** u/ShiranaiWakaranai:
#+begin_quote
  That said, the thing is that the killing curse isn't special the way you want it to be. Your idea is that it's bad because if it hits someone else who is standing between you and it you don't get hurt? So what? Pretty much every spell in the entire series won't hurt you if it hits someone else, that's just how single target spells work.

  This is one thing people seem to regularly overlook with similar ideas, focusing overly much on the killing curse and not on how if this worked it'd also work against nearly everything else, so you haven't really made the killing curse weaker relative to near everything else.
#+end_quote

Au contraire. The main reason why the killing curse is regularly used in the series is because of its allegedly unblockable nature, aka the ability to go through shields. If it is cast and you don't dodge out of the way, you die, period. But if you have an ant colony with you, that's no longer true. The ant colony *does* make the killing curse weaker relative to near everything else, because many (if not most) spells aren't as "single-targeted" as the Unforgivable curses.

The killing curse is known to have absolutely no physical effect whatsoever, to the point where victims appear perfectly healthy, just dead. As far as I'm aware, if a killing curse hits an ant, the ant just dies, nothing happens to you. Severing charms (diffindo) would cut through the ants and continue onwards to cut you too. Flame spells (incendio, fiendfyre) would set the ants on fire, fire which would then spread to you. Water spells (aguamenti) would drench you and the ants, which would then allow you and your ants to be electrocuted, or make it harder for you to maintain your balance. Levitation spells can be used to drop heavy stuff on you along with your ants. Summoning/Conjuring spells can be used to summon creatures which then attack you physically, biting/crushing through your ant colony pseudo-armor. Animagus spells allow the casters to become powerful animals and just trample you. Transfiguration spells can be used to create physical weapons to attack you regardless of your ants. Same for other weapons like the Sword of Gryffindor or Basilisk Fangs or throwing daggers.

#+begin_quote
  but a critical thing to remember is pretty much every defense that works against the killing curse is going to work against most other spells as well.
#+end_quote

So no, the ant colony is a very specialized defense against the Unforgivable curses and other kinds of mental/soul spells, and won't work well at all against most physical spells, of which there are several.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 5
:DateUnix: 1500772608.0
:DateShort: 2017-Jul-23
:END:


** Weekly update on the [[https://docs.google.com/document/d/11QAh61C8gsL-5KbdIy5zx3IN6bv_E9UkHjwMLVQ7LHg/edit?usp=sharing][hopefully rational]] roguelike [[https://www.youtube.com/watch?v=kbyTOAlhRHk][immersive sim]] Pokemon Renegade, as well as the associated engine and tools. [[https://docs.google.com/document/d/1EUSMDHdRdbvQJii5uoSezbjtvJpxdF6Da8zqvuW42bg/edit?usp=sharing][Handy discussion links and previous threads here]].

--------------

Well.  This last weekend was an exercise in /insufficient pessimism/, for a few different reasons.  Obviously as a game jam we didn't quite get everything in that we wanted (but so close!  Another hour would have let me get in all the audio, an hour beyond that and we would have gotten distinct enemy models, an hour or two beyond /that/ and I'd have more than just one enemy implemented...), but more than that I also overestimated my ability to manage time /and/ build a game /and/ dole out tasks to the members of our team with less Unity experience.  I suppose there was no winning move as far as that was concerned, since I turned out to be the only one familiar with Unity in the final team, which meant I had to either sacrifice development time or teaching time, and, well, the Show Must Go On.

But it goes a smidgen further than that.  Mark Brown (of Game Maker's Toolkit and runner of the jam) streamed the first few hours of his judging efforts, and he decided to do them all alphabetically.  This was convenient enough, as our entry was named Anti / Matter, so I sat down and started listening during my commute.  As he inched forward from AG to AM, I tried to get a handle on my expectations and kill the daydreaming: no, it wasn't going to get called out as a paragon of game design, no, he wasn't going to see the whole of our design past the implementation, no, no, no.  

I attempted to steel my expectations and assume the worst.  Well, the /worst/ worst would be that the game crashes on stream or fails to run in the first place, but I was fairly confident that wouldn't happen.  The next worst then would probably have him playing it mostly in silence for a minute or two before giving it the judgement that so many of the terrible entries had so far received: “Good...good try.  It is a game jam after all.”  /That/ was probably the worst case; he's far too nice a guy to actually call it out for poor design.

So he finished up his efforts on “Ant Game” (one of the ones he couldn't get to run) and moved right to... “Aqua Assault”.  No.  NO.  

I was apparently /insufficiently imaginative/.  After rushing on the jam discord, I was met with an obnoxious reality: apparently there was a subtle step I had missed, and our game, while uploaded, was not entered into the jam

/So/.  I have a renewed appreciation for being /sufficiently pessimistic/.

Regardless, the jam itself was a blast.  [[/u/Xavion]], [[/u/AgentofDmir]], and [[/u/dwood15]] were the other jammers in the team, and they did wonderfully.  The theme was [[https://www.youtube.com/watch?v=i5C1Uj7jJCg][this video]], which had to do with the Dual-Purpose Design of an obscure game called Downwell.  Thus, we had to try and build a game where every element had two purposes.  I think we did okay on that front.

The game is here: [[https://ketura.itch.io/anti-matter][Anti/Matter]].  There are builds for windows, mac, and linux (albeit the linux one is completely untested).  On that page is also a youtube link to some background music which I recommend playing while playing, as we didn't have time to include audio.

I'd be happy to hear any critique you might have for it.  I'm 70% certain that if you have a technical or balance quibble, we've already encountered it, but I'd still like to hear what you have to say, especially if it pertains to the feel or the accessibility.  

--------------

Besides the game jam, which took a day or so to recover from, my family hired some painters to come out today, which means I spent the other days this week emptying the relevant rooms.  Tonight I'm also flying out to Utah to go visit in-laws, so the next few days aren't looking hot for productivity either, but such is life.  On the other hand, the last time I was out there, with the help of my brother-in-law I hammered out the general requirements for how biomes will work, so you never know.

In the meantime, we've taken the concept of tiered difficulty and given the most apparent associated mechanic a pair of names: ‘Threat' and ‘Danger'.  If you don't recall tiered difficulty, it is summarized thusly:

#+begin_quote
  What this means from a mechanical perspective is that [hideously strong] creatures will respond proportionally to you as a threat: if you just walk by them, you won't get hit by a Hyper Beam that deals 999,999 damage to you and the entire countryside in a four mile radius, but they /might/ lazily swing their tail that deals 99 damage and puts the fear of Arceus into you.  The moment you start to challenge this preconception of weakness, however (probably by hitting back), the kid gloves come off and you'd better be prepared.  
#+end_quote

Danger is a formalization of this end result in a manner that should hopefully mean that units apply that logic to all other units, and not just as a special case for the player. Basically, every unit will evaluate all other units (that it can sense in range) and give each one a Threat rating.  This rating will vary based on what species the other unit is, whether it's a member of its own herd, the personality of the evaluating unit, and so on.  Once all other units are evaluated, all ratings are tallied into an aggregate Danger rating, which will vary somewhat based on the stats of that pokemon.  

How a pokemon reacts to a high Danger is going to vary depending on stats: timid ones will tend to run and aggressive ones will tend to attack.  In addition, INT will play a large role in determining how a unit sorts the individual Threats of those around it: a stupid pokemon is going to disproportionately be wary of closer units, and will effectively treat each other unit as if it had the aggregate Danger rating, instead of evaluating individuals differently.

For example, a low INT high Aggressiveness Tauros is surrounded by a few dozen members of its herd, which leaves it more or less calm  A lone Houndoom wanders nearby, which is immediately labeled as a medium Threat by all involved.  The moment it attempts to attack a calf, however, the Threat rating skyrockets, and the Tauros (and the adult members of its herd) stampede towards it, catching up and goring it where it stands. This brings the herd near to where a Hypno and a human are standing, and they are immediately charged.  After almost reaching them, the two disappear, and the Tauros herd stampedes only a bit further.  With no other targets in range, they slowly calm down, and a half hour later they've forgotten the entire incident.

A timid, high INT Abra is able to rank the importance of each of the disparate group of five approaching intruders. If it were evolved and a little stronger, it would need to take them out in the following order: Charizard, Wartortle, Pidgeotto, Human, and then Pichu, prioritizing further depending on how the battle goes.  As it is, however, the Threat of each of them (except the Pichu) are higher than the Abra's timidness will allow, and it teleports away.

A middling INT, carefree, high-level Snorlax enjoys a good nap, oblivious to its surroundings.  At some point a kid gets too close and it waves a paw as if shooing a fly, and the kid is knocked on his ass.  A few minutes pass and a horrible screeching sound comes from the kid, and the Snorlax grimaces, rolling over in an attempt to avoid the sound, which squashes a nearby grove of saplings.  The noise stops a moment later, but before the Snorlax can relax, a Machoke clobbers it in the back of the head.  Its eyes spring wide open and, a growl on its lips, it stands up to face the aggravating nuisance.

So as I see it, Danger will be a tracked stat (similar to how hunger and emotion is tracked), but if possible how a unit /reacts/ to Danger/Threat is going to be evaluated based on its other stats.  I /think/ that we have enough other stats to be able to derive the reaction, but worst comes to worse we add a “Danger Threshold” stat or something similar explicitly.  It should also go without saying that a unit's total power affects how much it evaluates others as a threat, which is the basis of how it allows the player to avoid cheap deaths via tiered difficulty.

--------------

If you would like to help contribute, or if you have a question or idea that isn't suited to comment or PM, then feel free to request access to the [[/r/PokemonRenegade]] subreddit.  If you'd prefer real-time interaction, join us [[https://discord.gg/sM99CF3][on the #pokengineering channel of the /r/rational Discord server]]!  
:PROPERTIES:
:Author: ketura
:Score: 12
:DateUnix: 1500662924.0
:DateShort: 2017-Jul-21
:END:


** When I first heard about the Netflix original movie /Bright/, I was worried the idea would be too weird for traditional filmmakers to handle or to find an audience. Now that [[https://youtu.be/6EZCBSsBxko][the full trailer has been released]], I'm worried the film won't be weird enough to have its own identity or impress nerds like me.

The way the trailer is presented, it looks like a fairly typical inner city buddy cop movie that just happens to feature fantasy creatures and magic. For an urban fantasy junkie like me, fantasy races as ethnic minorities and a wish-granting MacGuffin isn't crazy enough to keep me interested in an otherwise mundane world.

If you're going to splice a fantasy adventure and police drama together, I want you to go /nuts/. Biker gangs on broomsticks, vampire drug dealers, stone golems as club bouncers, soul-trafficking hellspawn mafiosos, cops shooting fireballs at poltergeists...just embrace the absurdity of the premise!
:PROPERTIES:
:Author: trekie140
:Score: 10
:DateUnix: 1500661646.0
:DateShort: 2017-Jul-21
:END:

*** I'm okay with the toned-down levels of nutsness, it actually makes me think of a low-tech Shadowrun, where things are fairly gritty-but-relatable rather than mostly alien, but I agree that if it's just "buddy cop movie that happens to feature magic creatures" I'll probably walk out of it disappointed.

The different races will have to have an identity that puts them beyond analogues to our own world. Like, they have to FUNCTION as analogues, obviously, but they can't JUST be their equivalent but with makeup, or I'll feel like it missed its true potential.

That said if those elf-looking people are actually fae folk, I'll probably appreciate something in it anyway assuming they're done even remotely competently. It seems like an original script, which is surprising.
:PROPERTIES:
:Author: DaystarEld
:Score: 11
:DateUnix: 1500665132.0
:DateShort: 2017-Jul-21
:END:


*** My first thought watching the trailer was "This looks so Suicide Squad, I'm surprised it still looks pretty cool".

But yeah, now that you mention it, it does look a bit low-key and unimaginative.
:PROPERTIES:
:Author: CouteauBleu
:Score: 9
:DateUnix: 1500662320.0
:DateShort: 2017-Jul-21
:END:

**** I was surprised to see so many people reacting to it as if it were a fresh and new idea they were all curious to see more of. At first, I assumed they are more casual moviegoers who haven't cut their teeth on surreal comics, goofy anime, and weird tabletop games the way I have, but then I saw fans of Shadowrun getting excited about how this could fit into the universe as a prequel so now I'm not so sure.
:PROPERTIES:
:Author: trekie140
:Score: 6
:DateUnix: 1500663129.0
:DateShort: 2017-Jul-21
:END:

***** Eh. I've seen it before (though I couldn't name any examples), but I think most of the time it's either very anime and goofy and "look at how weird and original our world is, so different!", or it's a CSI-style "we are important people dealing with important problems".

The Bright trailer has this thing that most (post-Avengers) Marvel Movies have and DC movies haven't (except some of the animated ones), and that Suicide Squad tried to have: the feeling that you're seeing a small part of a big, diverse world; and that people have already seen everything you're seeing.

From their perspective, it /is/ a story about cops and investigation in an otherwise mundane cop. On the other hand, that style works a lot better when there are a lot of different monsters and curiosities for the protagonist not to be surprised at, Harry Potter style.

(also I heard good things about Grimm, on that front)
:PROPERTIES:
:Author: CouteauBleu
:Score: 5
:DateUnix: 1500670244.0
:DateShort: 2017-Jul-22
:END:

****** I really like that style, taking the fantastic and making it mundane, the problem is that the story still needs to be fantastic from our perspective. Bright just looks like a generic cop movie where they just changed some details to make it an urban fantasy, which isn't enough for me.

Grimm is definitely not an example of what you're talking about, it's another monster of the week show where the protagonist only knows the mundane world and gets exposition dumped on him whenever he ventures into the fantastic world. I enjoyed it at first, but got really tired of it after a while.

Supernatural is much closer to that style, since the protagonists have been hunting monsters for years and know exactly what to do. The anime Mushishi is basically a fictionalized docudrama about an expert in the supernatural helping people who aren't. However, neither is particularly rational.

My personal favorite example of what you're talking about is the comic [[http://dragondoctors.dhscomix.com/archives/comic/ch-7-page-1-last-victim][The Dragon Doctors]], a medical drama in a post-apocalyptic fantasy setting. The first chapter sucks and the artwork isn't very good, but I love everything else about it and it's totally rational. I suggest starting with Ch. 7 and then going back, though it's the only one told from the patient's perspective.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1500676811.0
:DateShort: 2017-Jul-22
:END:


** Yesterday I tried the 10 years old game F.E.A.R., which is a survival-horror / tactical-shooter hybrid. The two genres work together surprisingly well so far. Right after I'm done taking out a squadron of elite soldiers by myself, and I feel like the king of the world, a creepy child appears and creeps towards me with a menacing music and bullets don't do anything to it, and I feel powerless and in danger again.

I've checked out this game because of reviews mentioning how good the combat AI was (which is a subject I love). So far... eh, I'm not terribly impressed, but I get why it got these reviews.

The AI reminds me both of the Arkham Origins predator AI (announces their intention and tactics so you know that they're trying to outmaneuver you) and the Half-Life/Black-Mesa AI (feel dangerous because their weapons can kill you fast and they start shooting as soon as they see you).

As a result, the combats so far are mostly a matter of exploiting the terrain to play a game of peek-a-boo, and pick off the bad guys one by one. Once you're spotted, you try to back off and flank the enemies while they're looking in the direction they last saw you. It's pretty fun!

Making a smart-looking AI in a combat game, especially a FPS, is pretty hard. The HL2 combine AI is smarter than the HL marine AI, but you wouldn't know that without looking at developer commentary, because the HL marines are more threatening: they start shooting fast, require your most powerful weapons to take them down (if you use the default machine gun, you'll probably lose as much life as them); the combine never get an occasion to demonstrate their intelligence because they die way too fast.

The Mister Freeze boss in Arkham City was an example of a smart-looking AI done right: it clearly announces what he's going to do "I'm going to cancel the thing you just used to beat me", and then when he does it, it has clear, visible consequences (you can no longer use the thing). Other AIs had similar mechanisms, especially in Arkham Knight (shot down the vantage points, throw grenades to neutralize vents, etc).

But overall, these AI still lacked something that would have made me respect them as smart. The Arkham Knight AI especially, kept broadcasting their decisions (destroy that vantage point, travel in groups so he can't sneak up on us), and it /was/ cool, but after a few fights it felt kind of pointless. Batman's victory is a foregone conclusion (especially with the Fear Takedowns, which kind of defeat the purpose of stealth sections), the only thing the goons achieve with their tactics is making it slower and more frustrating.

I would have liked for some missions where the goons have an objective that you must not let them achieve, and where the AI is dedicated to achieving that goal. There a few missions where the goons are trying to move items from point A to point B, but there's no real pressure. The goons carrying a payload aren't escorted by other goons, even if they were, you could use fear takedowns to get them anyway, and they usually never come even close to completing their "objective".

I'd like to see a game where the AI usually has an active role, and uses elaborate dynamic strategies to achieve it. Not just "the aliens need to wait 3 minutes and then they'll open the portal to space hell", but "the aliens have breached our underwater base's control room, if they manage to access 3 Gamma terminals, they'll overload the reactor and we'll all die", and then the gameplay is about trying to kill them before they reach the terminals.
:PROPERTIES:
:Author: CouteauBleu
:Score: 9
:DateUnix: 1500718336.0
:DateShort: 2017-Jul-22
:END:

*** u/ulyssessword:
#+begin_quote
  I'd like to see a game where the AI usually has an active role, and uses elaborate dynamic strategies to achieve it.
#+end_quote

I have half of an idea for a roguelike heist game with good AI. The enemies would have overarching /goals/ instead of simple combat algorithms.

A guard that finds you in a room would be perfectly happy to stand in the doorway and call for backup because they would just uselessly die if they fought you alone.
:PROPERTIES:
:Author: ulyssessword
:Score: 6
:DateUnix: 1500720007.0
:DateShort: 2017-Jul-22
:END:

**** u/CouteauBleu:
#+begin_quote
  I have half of an idea for a roguelike heist game with good AI. The enemies would have overarching goals instead of simple combat algorithms.

  A guard that finds you in a room would be perfectly happy to stand in the doorway and call for backup because they would just uselessly die if they fought you alone.
#+end_quote

Bit of a late reply, but:

No, that isn't what I was describing. The problem isn't about making an AI that is more likely to win, has semi-realistic self-preservation, or tries to outmaneuver the player. All those are relatively easy.

What I'd like to see is an AI that consistently nails that "Clever girl..." moment where you realize you've been outsmarted. Having a NPC hang back and shout "I need backup!" doesn't really do that, especially after you hear it for the 100th time.

It can be achieved with scripted sequences (the 'soldiers shoot through the air duct' setpiece in Half-Life), but I'd like to see a game that does it dynamically.

Ideally, I'd like such a game to nail the following factors for the enemy AI:

- I can see exactly which sequences of actions from the enemy lead to the current situation. If the mooks surrounded the building I was in, but I couldn't see any of them and have no idea how they got there, they could as well have teleported. I won't think they were smart.

- I can visualize which chain of decisions lead to these actions, and which information informed these decisions. If shoot an enemy, then retreat to a hidden position A, then move to a hidden position B, I don't want the enemy to throw a grenade at position B unless I can visualize how they knew I was there (for instance, A and B are the only hiding spots in the room, and they already checked A).

- The default state if I don't move or do something smart is that I lose. If I can stand in a doorway with a shotgun and pick off anyone who tries to come, or use my time-slowing power to headshot all the enemies who tried to ambush me, then I won't feel like their intelligence matters.
:PROPERTIES:
:Author: CouteauBleu
:Score: 2
:DateUnix: 1501087492.0
:DateShort: 2017-Jul-26
:END:


** [[http://i.imgur.com/iwguBrW.jpg][Funny image]]
:PROPERTIES:
:Author: ToaKraka
:Score: 11
:DateUnix: 1500689076.0
:DateShort: 2017-Jul-22
:END:

*** There's a very annoying trend on facebook where people share videos that are, literally, about 10 seconds long and display a still frame of text with some random muzak playing over it.

It's far more wasteful of bandwidth and every time I see it I want to go to their house and grip their shoulders and demand they tell me why.
:PROPERTIES:
:Author: DaystarEld
:Score: 6
:DateUnix: 1500800520.0
:DateShort: 2017-Jul-23
:END:

**** Couldn't agree more. I don't understand the purpose of that. The only thing I can think is that the creators might be getting more money for 10 s of video viewing vs. a picture viewing. Still feels disingenuous as all hell though.
:PROPERTIES:
:Author: Kishoto
:Score: 2
:DateUnix: 1500824806.0
:DateShort: 2017-Jul-23
:END:


*** [[https://s.agarri.ga/s/3ue0zbWd.jpg][Another funny image]]
:PROPERTIES:
:Author: rhaps0dy4
:Score: 7
:DateUnix: 1500721864.0
:DateShort: 2017-Jul-22
:END:

**** [[http://i.imgur.com/dzxKAFC.jpg][Funny image]]

[[https://boards.4chan.org/soc/thread/25483291][Related link]]
:PROPERTIES:
:Author: ToaKraka
:Score: 4
:DateUnix: 1500724237.0
:DateShort: 2017-Jul-22
:END:

***** I don't care about the actual project, sorry, but you could probably just use the Imgur app and upload directly instead of using Google Drive and GIMP.
:PROPERTIES:
:Author: gbear605
:Score: 1
:DateUnix: 1500867760.0
:DateShort: 2017-Jul-24
:END:


** I'm catching up on Boku no Hero Academia (and haven't been on these off-topic threads in a while, either). Anyone seen it? I'm really enjoying it, especially as an uplifting foil for the slightly nihilistic One Punch Man. The world /seems/ pretty rational on the surface, and the main character actually uses his mind to overcome his obstacles as he struggles to gain mastery over his superpower. And the shonen-style exposition focuses on tactics and how to /effectively/ become *the very best*.

How rational/enjoyable have you found it, if at all? I saw the first season a while ago, so the setting and set-up aren't that fresh in my mind.

^{sorry} ^{if} ^{the} ^{show's} ^{already} ^{been} ^{discussed} ^{here}
:PROPERTIES:
:Author: Gaboncio
:Score: 8
:DateUnix: 1500652959.0
:DateShort: 2017-Jul-21
:END:

*** I've only watched up to halfway through season 2 because I'm waiting to binge the rest of the dub when it's complete, but so far I've quite liked it. It's a good shonen anime that will appeal to young boys while tackling themes that challenge them but is just mature and intelligent enough for adults to enjoy, so [[https://66.media.tumblr.com/0acaafa9948a77d038422c8c37b31404/tumblr_o96wq8LpCY1ujb0aho1_1280.jpg][I'm happy it's gained the popularity that it has.]]

In term of its artistic merit, I find its take on the superhero genre to be interesting in how it blends ideas from both American and Japanese culture. The Youtube channel Mother's Basement has done two editorials about the show so far, [[https://youtu.be/8WDwBgjbRT4][exploring how it reflects cultural changes in the target audience]] and [[https://youtu.be/LmRfmJqE0kM][how it uses the superhero genre to deliver relevant social commentary]], which I recommend viewing.

This series is going to be a long runner and I'm ready to follow it. The show definitely isn't my favorite action anime and I've yet to see the show do something I haven't seen done better elsewhere, but it does everything it needs to do pretty darn well and has been consistently entertaining with just enough surprises and hidden depth.

The only real complaint I have is that some of the female characters are objectified unironically, but even that is less distracting and more self-aware than many anime I've seen. I wish some of the side characters were more interesting, but the show at least makes an effort to give them depth and has to keep the focus on Deku and All-Might in order to maintain good pacing.

My one totally subjective nitpick is that the theme of Deku as an underdog is getting a tad repetitive with him just barely succeeding in /every single situation/, which I can see resonating with other people but doesn't appeal to an autistic savant like me. All-Might's struggle to live up to his own standards is what clicks with me, so he's my favorite member of the cast.

In the end, My Hero Academia is a 4 out of 5 from me, but that's hardly a detraction against it. It's a world I want to dive into filled with characters I want to see more of and delivers on the emotional satisfaction it promises. As shonen-style shows go, I prefer RWBY, but this is still a fandom I'm happy to be part of and am always willing to discuss the series with other fans.
:PROPERTIES:
:Author: trekie140
:Score: 8
:DateUnix: 1500659578.0
:DateShort: 2017-Jul-21
:END:


*** I love /Hero Academia/! And yeah, it's a pretty rational show, actually. Everyone's powers have specific physical effects and limits, and they have to think about how to best exploit what they've got.

I just wish they'd pick up the pacing.
:PROPERTIES:
:Score: 5
:DateUnix: 1500657433.0
:DateShort: 2017-Jul-21
:END:

**** I agreed with you at first, but when I thought about the show's story structure I begrudgingly admitted defeat. Considering that it has such a large supporting cast but wants to keep the focus on Deku's personal arc, I think the series has done as good a job as it possibly can when it's trying to be both a school drama and a battle action show. There are so many ways it could've gone wrong, so I'm content.

If they picked up the pace then there'd be even less time for character development for an already simplistic cast, and if they slowed things down there wouldn't be time for nearly as many cool fights that you're here for. I think RWBY handled its pacing much better by making the story about the whole class, but that series has never had an arc /for a single character/ as satisfying as Deku's.
:PROPERTIES:
:Author: trekie140
:Score: 2
:DateUnix: 1500660725.0
:DateShort: 2017-Jul-21
:END:

***** Really I mostly just want them to stop repeating/re-using the exact same footage to fill episode-minutes. The /story/ pacing is fine.
:PROPERTIES:
:Score: 4
:DateUnix: 1500660846.0
:DateShort: 2017-Jul-21
:END:

****** Okay, I agree with you there. They're probably just trying to pad out episodes so the chapters of the manga can line up with the endings and commercial breaks.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1500661856.0
:DateShort: 2017-Jul-21
:END:


** I have no manifesto, no new show to discuss. But I do have a spare 20 minutes to the past week.

About a week ago, Wildbow, the Author of Worm, Twig, and Pact, joined one of the Discord servers I frequent and began talking. There, he discussed his idea of a [[https://docs.google.com/document/d/1IkTunIcvDb2Dfhrs3TOyU_JCIa_1DKhfqPVyf_Wl9qA/edit#][Worm game.]]

Three main points stuck out to me:

1. Permanent Death
2. Randomizing Worm Powers
3. SRPG, or an X-Com style input

His imagining for the game is one which is more about exploring the world of Worm and getting into fights.

After some discussion, I've discovered that his desire is based on what I've come to know as the Immersive Sim. Design a world, interface, power systems, slam them all together and put them into a world!

I spent the last week or so attempting to design a system for powers and power creation, but I haven't been able to get very far.

Programmatically speaking, a generic system for creating powers is difficult!
:PROPERTIES:
:Author: Dwood15
:Score: 8
:DateUnix: 1500666694.0
:DateShort: 2017-Jul-22
:END:

*** u/trekie140:
#+begin_quote
  A generic system for creating powers is difficult!
#+end_quote

Tell me about it. It took me forever to find a game that did what I wanted it to with superpowers, but in the end I could never find an immersive sim. The best I could find was the narrativist (as opposed to simulationist) Strange FATE, where powers are defined according to what purpose they serve in the plot compared to mundane skills.

GURPS definitely tries to be a perfect simulationist game, but there's just so much to keep track of and ways to break what game balance there was that it wouldn't make for a very fun game. I prefer to give powers a simple rating that applies to everything they do, then players can just be clever in how they use them and roll to see if it works.

Edit: FATE is uniquely well-suited to using abilities creatively thanks to its Aspect system, where any skill can attach a descriptor to something on a successful roll against an appropriate difficulty. You can declare enemies to be /Grappled/, decide you are /Behind Cover/, or choose to light the building /On Fire/ all with the same basic mechanic. Once the Aspect has been created, further rolls can remove or modify it and Fate Points can be spent to gain advantages or give disadvantages from them on rolls. Of course, the GM can do all this to the players too.
:PROPERTIES:
:Author: trekie140
:Score: 5
:DateUnix: 1500677916.0
:DateShort: 2017-Jul-22
:END:


*** The Worm video game I'd like to see would be a Telltale-style CYOA/visual novel about dialogue and consequences, where you take the role of Taylor and have to make the tough decisions she did. I'd love to see DLC that explored AU concepts or told the story from different perspectives. Speaking as someone who loved Worm at first but eventually stopped reading, I think the story would suit that medium even better than book form.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1500737385.0
:DateShort: 2017-Jul-22
:END:


*** [[https://rpg.stackexchange.com/questions/31164/worm-like-superhero-rpg][Possibly interesting]]: a discussion on tabletop RPG systems that are good for Worm-like games. Specifically, there's an indepth explanation of how to use FATE, as well as a lot of advice on how to use Wild Talents 2e.
:PROPERTIES:
:Author: waylandertheslayer
:Score: 2
:DateUnix: 1500709977.0
:DateShort: 2017-Jul-22
:END:


*** Well, Wildbow does have the Weaverdice rulebook to work off of, but most of that is designed for tabletop. Also, he joined a Discord server? I was under the impression that he disliked Discord, given how ardent he is about the [[/r/parahumans]] IRC.
:PROPERTIES:
:Author: Tandemmirror
:Score: 2
:DateUnix: 1500750597.0
:DateShort: 2017-Jul-22
:END:


*** u/ToaKraka:
#+begin_quote
  A generic system for creating powers is difficult!
#+end_quote

[[http://www.sjgames.com/gurps/books/powers][Ahem...]]
:PROPERTIES:
:Author: ToaKraka
:Score: 2
:DateUnix: 1500673064.0
:DateShort: 2017-Jul-22
:END:

**** For video games?
:PROPERTIES:
:Author: Dwood15
:Score: 3
:DateUnix: 1500674969.0
:DateShort: 2017-Jul-22
:END:

***** A /GURPS/-style system would be very useful in a video game, yes. Each basic "advantage" (or "disadvantage") costs a set amount of "character points" (experience points), but that amount can be adjusted by "modifiers" based on its specifics (e.g., a laser beam that can be used only under a full moon, or a /Worm/-style Manton Effect circumvention).
:PROPERTIES:
:Author: ToaKraka
:Score: 2
:DateUnix: 1500675232.0
:DateShort: 2017-Jul-22
:END:


** A joke:

There used to be this town in Arizona that most major truck routes heading toward LA stopped through. A lot of trucking routes would pass just through the bottom tip of Nevada, coming down from Utah and passing through Vegas on I-15, but a good number would stream into the state along I-40 or, somewhat less commonly, I-10. Now any trucker who can stop in Flagstaff in north central Arizona does so, but sometimes the timing doesn't line up right for that so you'd come to a stop in this little town just before I-40 dipped south to get around the Colorado River and Lake Mead.

Anyway, this town was just big enough for a few diners, some small businesses, and a truck repair shop. The mechanic there, he could work on any rig in the country. Your Kenworths, your Freightliners, your Peterbilt 379s, you name it, he could work on it. He was an expert. Just one thing about this fella, though: he was a dog. A walking, talking dog, sure as I'm telling you this story. He made the news a couple of times, if you remember those stories. What a sight to see! But he got the work done quickly and cheap, and he was good at it, too. What do I care if a mechanic's a dog? He was a crotchety old bastard, but he got the work done. Get the job done and I'll pay you good as anyone else.

Now, I got out of the business a long time ago. Probably been, what, ten years now? Fifteen? What with these new self-driving electric trucks, there's not much call for someone like me. It's a good living on the UBI, though, and I don't mind it. I've paid my dues and I'm retired now. Never figured it would happen, but there you go, easy as pie. Those crooks up in Washington finally got something right.

Well, enough politics. As I was saying, I passed through that town again the other day. It wasn't as big as it used to be, but the old diner was still there. Strangely enough, the truck repair shop was closed down. I figured, what with these self-driving trucks, there'd be a lot of work for a repairman. I asked the waitress what happened. Surely there was plenty of work on these new-fangled electric trucks for him.

“I thought that dog was the damn near best mechanic I ever saw. Why'd he close up shop?”

“Well, you know what the say,” she replied. “You can't teach an old dog new trucks.”
:PROPERTIES:
:Author: blazinghand
:Score: 10
:DateUnix: 1500662228.0
:DateShort: 2017-Jul-21
:END:

*** u/DaystarEld:
#+begin_quote
  Just one thing about this fella, though: he was a dog.
#+end_quote

"Please don't be a pun joke..."

#+begin_quote
  He was a crotchety old bastard, but he got the work done.
#+end_quote

"PLEASE don't be a pun joke..."

#+begin_quote
  Strangely enough, the truck repair shop was closed down.
#+end_quote

"Oh no..."

#+begin_quote
  /punchline/
#+end_quote

ARRGGHHH
:PROPERTIES:
:Author: DaystarEld
:Score: 17
:DateUnix: 1500665519.0
:DateShort: 2017-Jul-22
:END:


** I'd totally forgotten that I wrote this until Facebook reminded me. It's probably the closest thing to a manifesto I've ever written, and while "7 years ago me" was a totally different me (thanks in large part to encountering the rationalist community), I'm fairly happy with how well it holds up:

#+begin_quote
  Good dreams don't have to fade upon waking. We each hold in ourselves the best of all of us, and the worst. Change happens every day. We who go to bed at night are not the we who woke up. The you of tomorrow may not recognize the you of today, but wouldn't it be a shame if you don't recognize who you're going to be?

  Your mind is powerful. Use it to stretch the edges of yourself, find the loops that keep you in place and introduce something new, or take something out. Change happens every day. It's what separates the animate from the inanimate, the plant from the stone. The moment you stop changing is the moment you stop being alive.

  All living things grow, but only physically. To be human is to grow mentally, emotionally. You are not the you of yesterday, let alone yesteryear. From the worst of us to the best, each has grown, and each can grow more. The moment you stop growing is the moment you stop exemplifying that which is uniquely human. Change happens every day. When you change, you can change back. You can cut away who you were and start anew, and then do it again in reverse. But when you grow, you build on who you were. You cannot unknow what you make a part of you, what you keep of yourself. There is no going back: only forward. Only upwards. Only outwards.

  Everything in you is a tool you can wield. We each can create the reason behind everything in our lives. Change happens every day. When you apply a reason for the good in you, the bad in you, you plot a map that your future self will walk. Choice is more than an action in a moment. Choice is an attitude in a lifetime.

  Change happens every day. Choose to grow.
#+end_quote
:PROPERTIES:
:Author: DaystarEld
:Score: 5
:DateUnix: 1500665042.0
:DateShort: 2017-Jul-21
:END:

*** Thanks for that. Very Spiral, which is basically the highest sane compliment I can give.

Maybe that and this workout will help me stop wanting to die.
:PROPERTIES:
:Score: 5
:DateUnix: 1500676775.0
:DateShort: 2017-Jul-22
:END:

**** Glad you liked it. What's Spiral?
:PROPERTIES:
:Author: DaystarEld
:Score: 3
:DateUnix: 1500679655.0
:DateShort: 2017-Jul-22
:END:

***** You know, Spiral. Like the race of beings from /Tengen Toppa Gurren Lagann/. It's the only kind of being /worth/ being!
:PROPERTIES:
:Score: 3
:DateUnix: 1500680711.0
:DateShort: 2017-Jul-22
:END:

****** Haha, I think that's the first time I've heard it used as an adjective :) Thanks!
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1500685627.0
:DateShort: 2017-Jul-22
:END:


*** This reminds me very much of the themes of the game The World Ends With You. I like it.

The quote it reminds me of in particular:

#+begin_quote
  The world ends with you: If you want to enjoy life, expand your world. You gotta push your horizons out as far as they'll go
#+end_quote
:PROPERTIES:
:Author: Cariyaga
:Score: 1
:DateUnix: 1500719041.0
:DateShort: 2017-Jul-22
:END:

**** Oh man, I'd been meaning to play that back when it came out and just never got around to it. It looked fun, and apparently was pulled into the KH multiverse, so I met the characters briefly in Dream Drop Distance.

Glad you liked it!
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1500800574.0
:DateShort: 2017-Jul-23
:END:

***** It's super fun mechanically and I adore the story -- before Undertale, it, alongside Mother 3, had been my favorite game. :D
:PROPERTIES:
:Author: Cariyaga
:Score: 1
:DateUnix: 1500801494.0
:DateShort: 2017-Jul-23
:END:


** I'm thinking a lot about the possibility that we're in a simulation; I'm sure most people here are familiar with the basic argument, but I'll reiterate anyways.

In the case that we achieve artificial intelligence and easy access to supercomputers, one of the main things we would do is simulate complex realities, to see what happens in those realities given a certain set of circumstances. We would do this a lot; there's no reason not to. Given this information, the chances that our reality is one of these simulations is very high.

The problem that I've been thinking about is one of failure states. What is a set of circumstances that could occur in a simulation that would cause someone to turn that simulation off? The one that jumps out to me the most is if that simulation suddenly started using a lot more operating power than it previously did. The main way I could imagine this happening is if that simulation also achieved artificial intelligence and started simulating realities of their own.

Given the possibility that reaching that point could cause the simulation /we're/ in to be turned off, is it worth it to consider whether we shouldn't try to create complex simulations like this at all? Is it worth it to think about failure cases so we can try to avoid our simulation no longer existing in general?

I worry about the irony of trying to work out the preferences of an omnipotent being to avoid behaviors they might not like, considering how much I've derided that idea over my years of being an atheist, but that's... kind of a different discussion.
:PROPERTIES:
:Author: B_E_H_E_M_O_T_H
:Score: 5
:DateUnix: 1500656195.0
:DateShort: 2017-Jul-21
:END:

*** u/deleted:
#+begin_quote
  We would do this a lot; there's no reason not to.
#+end_quote

Except that as far as we know, a to-the-quarks physically accurate simulation of a basketball uses up a hell of a lot more mass, energy, and information than an actual basketball. We could learn something about the way reductionism and sloppy effective theories work that tells us, someday, that we can "cheat" and only simulate low-level physics /sometimes/. However, when is sometimes? When sentient people are looking? But how do we detect sentients within our simulations? We could suppose that post-AI or post-solving-cognition we might have some idea how to do that, but it still seems like we'd observe a much less causally consistent universe if we were inside such a thing.

Causal consistency with an underlying physics inside a simulation seems to require infeasibly large amounts of resources to have such simulations recursing into each other like a Matryoshka doll. At some point, you either stop having simulations, or one of the simulations starts to look more like /Super Mario 64/ than like reality.
:PROPERTIES:
:Score: 8
:DateUnix: 1500657639.0
:DateShort: 2017-Jul-21
:END:

**** u/B_E_H_E_M_O_T_H:
#+begin_quote
  how do we detect sentients within our simulations
#+end_quote

When they try to query something. We already have simplifications of more complex processes that work as well as those more complex process (the one that jumps out to me is the coefficient of friction---when calculating with friction, we don't have to look at things like electromagnetism). The simulation could sum up the results of complicated internal processes using simpler equations, so those things could have causal consistency with things around them while still being able to spew out the individual components of the internal processes when something (a sentient being, for instance) queries them.

#+begin_quote
  seems to require infeabily large amounts of resources to have such simulations recursing into each other
#+end_quote

If I'm not misunderstanding what you're saying here, that's my point: it takes so much energy to have recursive simulations that if one ever reached this point it would just be shut down, so if we assume ourselves to be in a simulation should we also assume that would happen to us? I know we haven't /quite/ reached the point of assuming we're in a simulation, but it's a definite possibility.
:PROPERTIES:
:Author: B_E_H_E_M_O_T_H
:Score: 3
:DateUnix: 1500658537.0
:DateShort: 2017-Jul-21
:END:


**** u/CCC_037:
#+begin_quote
  We could learn something about the way reductionism and sloppy effective theories work that tells us, someday, that we can "cheat" and only simulate low-level physics /sometimes/. However, when is sometimes?
#+end_quote

I think we can probably get away with not bothering to accurately simulate both the position and the velocity of subatomic particles. I mean, who's going to notice?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1500975261.0
:DateShort: 2017-Jul-25
:END:

***** Ow my face. The palm just hit it so hard.
:PROPERTIES:
:Score: 2
:DateUnix: 1500984162.0
:DateShort: 2017-Jul-25
:END:


*** There are quite a few issues to even the steelmanned version of the simulation hypothesis wherein it's only simulating human minds and it's likely tampering with minds to avoid letting people know discrepancies. For one thing there's the question of what the simulator is hoping to accomplish since it seems rather doubtful whatever you're hoping to find out about human minds and social structures couldn't be figured out with vastly less resource expenditure in other ways.\\
Second are the moral issues, in that there seems to be a relatively small subset of civs/AI singletons that would both have reason to do such a simulation and would be permitted to/want to. For instance any AI that cares about human well being is obviously not going to run such a sim, but on the other hand a paperclipper couldn't give a shit about any information that sim /might/ be useful for obtaining once it had the resources to run such a sim.

Of course the most plausible simulation idea would seem to be one where things are simulated at the quantum level, but that would only make sense for simulators in a universe unlike our own with vastly more potential processing power, and such things are nearly impossible to speculate about, plus were that the case nothing entities within the sim could do would really be likely to be able to affect it in any way.
:PROPERTIES:
:Author: vakusdrake
:Score: 6
:DateUnix: 1500660598.0
:DateShort: 2017-Jul-21
:END:

**** The paperclipper might run a simulation to test the AI that /could/ have popped up in the galaxy it just ate: If they both would spend some of their ressources on satisfying the other's values iff the other would do the same, they can both do so to maximize their expected values if there are diminishing returns on ressource expenditure in one of the sets of values.

For example, if it turns out there's a way to leave this universe for another, more bountiful computational substrate, and the paperclipper finds an AI that just wants to simulate its creators forever, it can just send that AI there while that AI would have also spawned a paperclipper before leaving, but first both would have to simulate the other to see whether they would have cooperated iff the first cooperates.
:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1500721033.0
:DateShort: 2017-Jul-22
:END:

***** u/vakusdrake:
#+begin_quote
  For example, if it turns out there's a way to leave this universe for another, more bountiful computational substrate, and the paperclipper finds an AI that just wants to simulate its creators forever, it can just send that AI there while that AI would have also spawned a paperclipper before leaving, but first both would have to simulate the other to see whether they would have cooperated iff the first cooperates.
#+end_quote

Leaving the universe to go somewhere with more potential computing falls under the previously mentioned categories of scenarios that are both pointless and impossible to speculate about and has a vanishingly small prior probability anyway.

#+begin_quote
  The paperclipper might run a simulation to test the AI that could have popped up in the galaxy it just ate: If they both would spend some of their ressources on satisfying the other's values iff the other would do the same, they can both do so to maximize their expected values if there are diminishing returns on ressource expenditure in one of the sets of values.
#+end_quote

This doesn't really work since a paperclipper /doesn't care about anyone's values but its own/. Even if you actually have to go up against another AI and want to try to get info about its goals, that the best way to do that would be by running a simulation of our current world strains credulity for a number of reasons some of which I mentioned before. Plus that wouldn't even tell you competing AI's goals if you didn't actually know where in possibility space the civ that created them resided, which if they aren't sharing their goals structures you're not finding out anyway.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1500747797.0
:DateShort: 2017-Jul-22
:END:

****** You're not going up against another AI, you're eating a system that might have developed an AI. In order to see which, you have to simulate it until it develops an AI, then read its sourcecode to see what it'll do before simulating its universe exploration starts consuming computing ressources in earnest.

The paperclipper can cause there to be more paperclips on average if he gets other AIs to produce paperclips in worlds where he didn't get to be the first AI in space. If he finds an AI that behaves like [[http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/][PrudentBot]], he can get it to produce paperclips in worlds where he didn't get to be the first AI in space by satisfying its values until diminishing returns set in.

Another way to look at it is that the paperclipper doesn't know whether it's in reality and can maximize paperclips by eating everything, or a simulation, where everything it does is irrelevant except for the observations the simulator makes about its actions.
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1500750604.0
:DateShort: 2017-Jul-22
:END:

******* u/vakusdrake:
#+begin_quote
  The paperclipper can cause there to be more paperclips on average if he gets other AIs to produce paperclips in worlds where he didn't get to be the first AI in space. If he finds an AI that behaves like PrudentBot, he can get it to produce paperclips in worlds where he didn't get to be the first AI in space by satisfying its values until diminishing returns set in.
#+end_quote

Ok so this ties into a larger point of me which is that, acausal deals like that don't work as I argued [[https://www.reddit.com/r/slatestarcodex/comments/60ry4v/repost_the_demiurges_older_brother/df9vhoa/][here]]. Effectively there's plenty types types of cooperation which would work just fine were you able to be certain of the other ai's source code and vice versa, as is the case in the article you linked. However in practice you can only actually know the other ai isn't deceiving you if you are vastly more powerful than it, in which case there's little point to cooperating with it anyway, and even if you tried it would be unable to know whether or not you have truly precommitted.

#+begin_quote
  You're not going up against another AI, you're eating a system that might have developed an AI. In order to see which, you have to simulate it until it develops an AI, then read its sourcecode to see what it'll do before simulating its universe exploration starts consuming computing ressources in earnest.
#+end_quote

And what exactly is the point of trying to figure out the possible AI that might have arisen in a system had you not consumed it supposed to be exactly?

Anyway you're making something pretty close to if not indistinguishable from the demiurges older brother argument so you should definitely read that comment I linked responding to it.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1500756051.0
:DateShort: 2017-Jul-23
:END:


**** u/CCC_037:
#+begin_quote
  For one thing there's the question of what the simulator is hoping to accomplish since it seems rather doubtful whatever you're hoping to find out about human minds and social structures couldn't be figured out with vastly less resource expenditure in other ways.
#+end_quote

Maybe the simulator is just playing a game of Galactic Civilisations. (Of course, this assumes that the simulator is from somewhere where computational power is /ridiculously/ cheap as compared to here - but wouldn't that be overwhelmingly probable in all simulation hypothesi?)
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1500975416.0
:DateShort: 2017-Jul-25
:END:

***** I mean having an AI that is entertained by running highly unethical simulations of entire civs strains credulity a bit too far. I mean why the hell would anyone program in any of the mental traits? Having a AI that even feels boredom is rather counterproductive as is making it find humans uniquely entertainment. Plus even with those traits it's /still/ hard to imagine why the best it could come up with for entertainment is running ancestor sims.\\
I mean given what's physically possible in terms of computing once you have nanotech having enough processing for all this is actually the least contentious part of this sort of simulation hypothesis.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1500982257.0
:DateShort: 2017-Jul-25
:END:

****** Does the simulation goal have to be to satisfy an AI? Maybe a bored teenage alien is setting the goal for the simulation, and the AI is doing it with the goal of "make the bored teenager less bored".
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1500982848.0
:DateShort: 2017-Jul-25
:END:

******* That makes more assumptions than I think you realize.

For one it requires a civ that made AI that only cared specifically about the mental states of members of it's own species, which screws itself since it restricts their ability to change very much without the AI destroying them. After all if the AI cared about suffering in sentient minds generally then it would never allow such a sim to be done in the first place. Plus even if the AI allows it you also need assume a governing body which is fine with this sort of obviously extremely unethical sim. Plus even if the AI only cares about a particular kind of alien minds it might well refuse to run such sims on the grounds that the processing could be better used to run sims of that alien mind in utopian worlds.\\
Also we aren't likely to be talking about a "bored alien teenager" here, but a sadistic or amoral mind. Because otherwise the alien would likely be terrified with how much suffering running that sim would cause and as a result the AI would have predicted that and convinced them not to run it in the first.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1500987000.0
:DateShort: 2017-Jul-25
:END:

******** /Any/ scenario that ends up with our universe being a simulation is going to make a multitude of assumptions. (Note, I do not say that the scenario that I describe is necessarily /likely/ in any way).

However, to address your specific points:

#+begin_quote
  it requires a civ that made AI that only cared specifically about the mental states of members of it's own species
#+end_quote

No, it simply strongly suggests a civ that made AI that doesn't care about the mental states of humans. It might have a definition of sapience that requires the presence of slood, which has been carefully left out of our universe in order to ensure that nothing that meets said definition of sapience ever turns up here.

And even that is not a /requirement/. It is possible that the AI does care, but simply cares more about following orders.

Or it could be that a percentage of apparent people are truly nothing more than NPCs - competer-controlled non-sentiences.

Or perhaps the AI is simply permitted to run any simulation where the total amount of suffering is a negative value (that is, where, over time, there is more good than bad).

Or perhaps the system was designed by some species with some form of non-human morality, which does not see suffering as evil.

#+begin_quote
  Also we aren't likely to be talking about a "bored alien teenager" here, but a sadistic or amoral mind. Because otherwise the alien would likely be terrified with how much suffering running that sim would cause and as a result the AI would have predicted that and convinced them not to run it in the first.
#+end_quote

I'm not seeing how this follows. Do you really think that our world is such a terrible place that it would have been better had it never existed?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1500988437.0
:DateShort: 2017-Jul-25
:END:

********* u/vakusdrake:
#+begin_quote
  And even that is not a requirement. It is possible that the AI does care, but simply cares more about following orders.
#+end_quote

See that sounds like a genie which is a type of GAI with considerable problems gone over in length in /Superintelligence/ and touched on [[https://www.gwern.net/Tool%20AI][here]] as well. Given how easily an AI can circumvent nearly any restrictions you attempt to put on it I rather doubt there's any solution to AI friendliness that doesn't involve actually solving ethics well enough that you can be certain the AI's goals coincide with your own nearly perfectly.

#+begin_quote
  Or it could be that a percentage of apparent people are truly nothing more than NPCs - competer-controlled non-sentiences.
#+end_quote

See this has struck me as the best solution to the ethics problems, provided one is willing to go down that weird quasi-solipsistic rabbit hole. On the other hand this objection also doesn't work if your life is shitty enough since you know /you/ aren't an NPC. Anyway I'm not sure anyone espouses this particular line of reasoning because it's just too weird.

#+begin_quote
  I'm not seeing how this follows. Do you really think that our world is such a terrible place that it would have been better had it never existed?
#+end_quote

That and the prior objection only really work in a answer to job type scenario where it is creating every possible world (which would also place this scenario out of the realm of things possible to speculate the likelihood of). Because otherwise it's rather clear that you could easily create any world you please /without the morally horrible bits/. In semi-realistic scenarios you only have limited processing so you ought to be prioritizing sims where the people within wouldn't prefer to live in a different sim.

Anyway none of my rebuttals are really ironclad merely statistical, and given you said you don't actually think the simulation thing is likely anyway I suspect we don't really disagree.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1500992064.0
:DateShort: 2017-Jul-25
:END:

********** u/CCC_037:
#+begin_quote
  Given how easily an AI can circumvent nearly any restrictions you attempt to put on it I rather doubt there's any solution to AI friendliness that doesn't involve actually solving ethics well enough that you can be certain the AI's goals coincide with your own nearly perfectly
#+end_quote

Now consider a programmer who does not care about what happens to simulated entities but does care about whatever he gets from the sim...

#+begin_quote
  On the other hand this objection also doesn't work if your life is shitty enough since you know /you/ aren't an NPC.
#+end_quote

...your life has to be pretty consistently horrible if it's /that/ bad.

#+begin_quote
  Because otherwise it's rather clear that you could easily create any world you please /without the morally horrible bits/. In semi-realistic scenarios you only have limited processing so you ought to be prioritizing sims where the people within wouldn't prefer to live in a different sim.
#+end_quote

...question. What effect would re-running the universe with a 1% stronger weak nuclear force have on the formation of the United Nations?

Is there any way to answer the above question without a simulation that includes various horrible bits?

#+begin_quote
  Anyway none of my rebuttals are really ironclad merely statistical, and given you said you don't actually think the simulation thing is likely anyway I suspect we don't really disagree.
#+end_quote

I said that the /specific scenario/ which I had suggested was unlikely. This is very different from saying that the /simulation hypothesis/ is unlikely (and honestly, the simulation hypothesis being true would not surprise me).
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1500994063.0
:DateShort: 2017-Jul-25
:END:

*********** u/vakusdrake:
#+begin_quote
  ...your life has to be pretty consistently horrible if it's that bad.
#+end_quote

I mean yeah, but I have heard more than one person on this subreddit express things that seem to the effect of while they aren't suicidal they /would/ really like the idea of something killing them.\\
I can't seem to find the survey but I also remember seeing a survey that basically asked whether at a given time someone would rather be unconscious (basically a roundabout though flawed way of asking whether they'd rather currently not exist) and the number of people who said yes was /disturbingly high/. So yeah my point is the number of people who if there were no external factors (like fear of death or repercussions for those around you) rather not exist is probably really disturbingly high.

#+begin_quote
  ...question. What effect would re-running the universe with a 1% stronger weak nuclear force have on the formation of the United Nations?
#+end_quote

See here you seem to be talking about a sim where reality is being run at base level here, instead of the much simpler one where you only simulate the human minds, but are forced to intervene on occasion to avoid people noticing discrepancies. As I said in my original comment to run a simulation of the universe at base level would require a larger amount of energy than the universe itself and thus only makes sense to run in a universe with physics that allow for vastly more computing.\\
However you really can't begin to assess the likelihood of such a thing, and it doesn't really have the same pressing implications that might be present for a non-base level sim.

#+begin_quote
  I said that the specific scenario which I had suggested was unlikely. This is very different from saying that the simulation hypothesis is unlikely (and honestly, the simulation hypothesis being true would not surprise me).
#+end_quote

I'm confused so what versions of the simulation hypothesis /do/ you find more plausible? Because the scenario you proposed is still rather more plausible than the ancestor simulation idea that is often argued for. Though were you talking about the version where we are in a perfect sim run in a larger incomprehensible universe one can't really assess likelihood, but at the same time it wouldn't matter the same way.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501007015.0
:DateShort: 2017-Jul-25
:END:

************ u/CCC_037:
#+begin_quote
  I can't seem to find the survey but I also remember seeing a survey that basically asked whether at a given time someone would rather be unconscious (basically a roundabout though flawed way of asking whether they'd rather currently not exist) and the number of people who said yes was /disturbingly high/
#+end_quote

I dunno. I can think of situations where I'd prefer to be unconscious but would not wish to stop existing. (The two main reasons there are (a) would like to relax for a bit as by a night's sleep, and (b) would be undergoing surgery and would prefer to just wake up once it's complete).

#+begin_quote
  See here you seem to be talking about a sim where reality is being run at base level here, instead of the much simpler one where you only simulate the human minds,
#+end_quote

Yeah... running the sim at base-level makes a lot of sense to me. (A mind-only sim is also possible; but if my mind and not my world is being simulated, then I find it very hard to see any proof at all that /anyone else's/ mind is actually being simulated; I can't tell the difference between talking to another simulation and talking to (say) a Simulator with an in-universe avatar.)

#+begin_quote
  As I said in my original comment to run a simulation of the universe at base level would require a larger amount of energy than the universe itself and thus only makes sense to run in a universe with physics that allow for vastly more computing.
#+end_quote

Well, yes. That's clearly true. There's a limited amount of simulation levels 'down' that we can go from here, but not a limited amount of simulation levels 'up'.

#+begin_quote
  However you really can't begin to assess the likelihood of such a thing, and it doesn't really have the same pressing implications that might be present for a non-base level sim.
#+end_quote

What pressing implications does the mind-only sim have, exactly? (I thought we were both talking about base-level sims all along; I may have missed some important points. I'm already noticing how a lot of your arguments make a lot more sense when talking about mind-only sims...)

#+begin_quote
  I'm confused so what versions of the simulation hypothesis /do/ you find more plausible?
#+end_quote

In general, I find the base-level sim significantly more plausible than the mind-level sim. Any /specific/ scenario under which the base-level sim runs tends to end up with a complexity penalty, but there are at least two features of known physics which appear to hint at some slight adjustments having been made to physics to make it a good deal more computable - this is evidence in favour of the base-level sim /and/ evidence against the mind-level sim (since the mind-level sim would not need to compute physics in the same way). So I think the base-level sim is a good deal more likely; but the reasons and motivations behind such a sim I can only guess at.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501061106.0
:DateShort: 2017-Jul-26
:END:

************* u/vakusdrake:
#+begin_quote
  I dunno. I can think of situations where I'd prefer to be unconscious but would not wish to stop existing. (The two main reasons there are (a) would like to relax for a bit as by a night's sleep, and (b) would be undergoing surgery and would prefer to just wake up once it's complete).
#+end_quote

I don't mean that those people necessarily want to stop existing, just that a significant amount of the time people's experience is a net negative. So given the numbers were so high (as far as I remember) it means a significant subset of those people consider /the majority of their existence/ to on the whole worse than nothing, they have more negative experiences than positive one's.\\
Of course the question isn't an ideal setup since being unconscious isn't comparable to oblivion. After all even in deep sleep i'm quite certain there's some level of experience going on. I've found it rather odd however that so many people seem to describe sleep as basically like just skipping forward into the future, whereas even if I wake up from a deep sleep phase I can remember some sort of mental experience before waking up, though not one of great complexity.

As for the difference between base level and mind only simulations: Firstly mind only simulations require that the simulators care about the specific simulated minds for /some/ reason, and that they constantly intervene to avoid people noticing discrepancies since they aren't fully simulating parts of the world when nobody's looking and have to try to hide that fact.\\
Importantly however as the original comment in this chain mentioned, it means that the simulation is almost certain to end at some point /vastly/ before when someone might stop running a base level sim (which might be at the heat death when there's no longer anything notable happening). Plus it means something bad is likely to happen to you if you try to create a superintelligent AI, since it's rapid expansion and conversion of matter into computronium will increase the costs of upkeeping the sim within the earth's future light cone to something potentially within a few orders of magnitude the cost of just running a base level sim.

Basically with a base level sim nothing is really too different and there's no reason to act drastically differently. It's just that our world happens to exist within a much larger one.\\
However with a mind-only sim it means everything we know about the world is largely wrong and that we likely need to drastically change what we're doing especially once we start considering singularity tech.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501099526.0
:DateShort: 2017-Jul-27
:END:

************** u/CCC_037:
#+begin_quote
  Of course the question isn't an ideal setup since being unconscious isn't comparable to oblivion. After all even in deep sleep i'm quite certain there's some level of experience going on. I've found it rather odd however that so many people seem to describe sleep as basically like just skipping forward into the future, whereas even if I wake up from a deep sleep phase I can remember some sort of mental experience before waking up, though not one of great complexity.
#+end_quote

Yeah, dreams are a fairly common experience.

#+begin_quote
  Firstly mind only simulations require that the simulators care about the specific simulated minds for /some/ reason
#+end_quote

Not necessarily. They might only care about how the minds react to certain stimuli, and not about the minds themselves.

#+begin_quote
  and that they constantly intervene to avoid people noticing discrepancies since they aren't fully simulating parts of the world when nobody's looking and have to try to hide that fact.
#+end_quote

Granted.

#+begin_quote
  Importantly however as the original comment in this chain mentioned, it means that the simulation is almost certain to end at some point /vastly/ before when someone might stop running a base level sim (which might be at the heat death when there's no longer anything notable happening).
#+end_quote

Yes. This seems reasonable.

#+begin_quote
  Plus it means something bad is likely to happen to you if you try to create a superintelligent AI, since it's rapid expansion and conversion of matter into computronium will increase the costs of upkeeping the sim within the earth's future light cone to something potentially within a few orders of magnitude the cost of just running a base level sim.
#+end_quote

More than likely the attempt will just fail due to either unknown reasons or reasons that look plausible at first glance. But enhancing /yourself/ beyond the level of the processing power assigned to your simulation will probably simply result in the simulation abruptly ending with no warning...

#+begin_quote
  Basically with a base level sim nothing is really too different and there's no reason to act drastically differently. It's just that our world happens to exist within a much larger one. However with a mind-only sim it means everything we know about the world is largely wrong and that we likely need to drastically change what we're doing especially once we start considering singularity tech.
#+end_quote

Hmmm. That seems reasonable.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501142438.0
:DateShort: 2017-Jul-27
:END:

*************** u/vakusdrake:
#+begin_quote
  Yeah, dreams are a fairly common experience.
#+end_quote

I don't just mean dreams though, I'm saying that all stages of sleep have something which it's like to be in them, even though the mental activity occurring isn't particular complex. There is something which it is "like" to be in even the deepest non-rem sleep. Whereas I'm not quite sure the same can necessarily be said about being under anesthesia, since from what I remember it did feel exactly like I just skipped forward in time.

#+begin_quote
  Not necessarily. They might only care about how the minds react to certain stimuli, and not about the minds themselves.
#+end_quote

I meant "care" in a more general sense, in that they need some reason to care about any information they could get out of the mind for some reason. However as I argued before it seems unlikely that the best way to get good data on minds would be to simulate not only a perfect copy of the relevant minds, /but also that you would need to simulate a massive swathe of other minds in a civ, that aren't directly connected to the development of GAI./ That's because it's hard to imagine any point to running those massive sims until you have become powerful enough that you only care about other GAI, and even in that case you'd only want to run the sims to see what kinds of programing the humans would put in the AI, so as to maybe get some insight into potential competitors. Though I've argued with the OP that this still seems hard to justify as a likely strategy for a number of reasons.

#+begin_quote
  More than likely the attempt will just fail due to either unknown reasons or reasons that look plausible at first glance. But enhancing yourself beyond the level of the processing power assigned to your simulation will probably simply result in the simulation abruptly ending with no warning...
#+end_quote

Well as I just mentioned it's also probable that the point of the sim in the first place is likely to investigate stuff related to the creation of GAI generally. Or if the simulators have some minds sufficiently weird as to justify running the sim as basically a zoo, then they would likely just consistently roll back time once we got to GAI.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1501177593.0
:DateShort: 2017-Jul-27
:END:

**************** u/CCC_037:
#+begin_quote
  I don't just mean dreams though, I'm saying that all stages of sleep have something which it's like to be in them, even though the mental activity occurring isn't particular complex. There is something which it is "like" to be in even the deepest non-rem sleep. Whereas I'm not quite sure the same can necessarily be said about being under anesthesia, since from what I remember it did feel exactly like I just skipped forward in time.
#+end_quote

Yeah, I agree with you there. My mental state on waking is often very different to my mental state on sleeping, so /something/ is clearly going on in the interval, even when I don't remember any dreams.

#+begin_quote
  I meant "care" in a more general sense, in that they need some reason to care about any information they could get out of the mind for some reason.
#+end_quote

Ah, I see. But that might well be "let's see how this simulated mind reacts to torture".

#+begin_quote
  However as I argued before it seems unlikely that the best way to get good data on minds would be to simulate not only a perfect copy of the relevant minds, /but also that you would need to simulate a massive swathe of other minds in a civ, that aren't directly connected to the development of GAI./
#+end_quote

Why on earth would you need to simulate more than, say, two dozen minds? Fill the rest in with newspapers, background characters, and a few dozen semisentient AI-controlled drones, and you can make a sparsely populated world look overcrowded from the inside.

#+begin_quote
  That's because it's hard to imagine any point to running those massive sims until you have become powerful enough that you only care about other GAI, and even in that case you'd only want to run the sims to see what kinds of programing the humans would put in the AI,
#+end_quote

Then wouldn't you only be interested in simulating those who /are/ connected to the development of the AI?

Also, there's /plenty/ of other reasons to simulate minds. I can't imagine a successful GAI that stops caring about anything except other GAI, partially for the same reason as most humans haven't stopped caring about cats and dogs, and partially because humans have a dramatic impact on our environment, and while a GAI is not at severe risk from this, it would still benefit from understanding (and, if necessary, directing) that impact.

#+begin_quote
  Well as I just mentioned it's also probable that the point of the sim in the first place is likely to investigate stuff related to the creation of GAI generally. Or if the simulators have some minds sufficiently weird as to justify running the sim as basically a zoo, then they would likely just consistently roll back time once we got to GAI.
#+end_quote

From an inside-the-sim point of view, I'm not seeing any difference between "abrupt end of the sim" and "rolling back time" - I'm just as dead, even if a younger me gets a new lease on life.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501228430.0
:DateShort: 2017-Jul-28
:END:

***************** u/vakusdrake:
#+begin_quote
  Why on earth would you need to simulate more than, say, two dozen minds? Fill the rest in with newspapers, background characters, and a few dozen semisentient AI-controlled drones, and you can make a sparsely populated world look overcrowded from the inside.
#+end_quote

Exactly my point, if you aren't /personally integral/ to the creation of GAI then your very existence refutes the idea of that sort of simulation hypothesis.

#+begin_quote
  From an inside-the-sim point of view, I'm not seeing any difference between "abrupt end of the sim" and "rolling back time" - I'm just as dead, even if a younger me gets a new lease on life.
#+end_quote

Yeah neither do I, but still a great deal of people seem to have philosophical models where it wouldn't count as permanent death. Since in many of the future iterations the sim will result in individuals who are at least briefly able to fulfill the necessary amount of similarity to current you to count under their system.

#+begin_quote
  Also, there's plenty of other reasons to simulate minds. I can't imagine a successful GAI that stops caring about anything except other GAI, partially for the same reason as most humans haven't stopped caring about cats and dogs, and partially because humans have a dramatic impact on our environment, and while a GAI is not at severe risk from this, it would still benefit from understanding (and, if necessary, directing) that impact.
#+end_quote

I would disagree with that, other than as a progenitor of other GAI I can't actually come up with /any/ circumstances under which there's any benefit to learning about lesser lifeforms. After all it won't have much impact on how long it might take you to deconstruct solar systems containing such life. Humans care about cats and dogs because they both have some effects on us, and because we're fond of knowledge for its own sake. However it seems questionable an AI is going to have any reason to care.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501264463.0
:DateShort: 2017-Jul-28
:END:

****************** u/CCC_037:
#+begin_quote
  Exactly my point, if you aren't /personally integral/ to the creation of GAI then your very existence refutes the idea of that sort of simulation hypothesis.
#+end_quote

(a) Only if I accept your claim that AIs are only interested in other AIs, which I do not. (b) I don't know that I'm not integral to the development of AI. Maybe I'm going to be a close relative of the person who actually does the code, and a large influence on their life.

#+begin_quote
  I would disagree with that, other than as a progenitor of other GAI I can't actually come up with any circumstances under which there's any benefit to learning about lesser lifeforms. After all it won't have much impact on how long it might take you to deconstruct solar systems containing such life. Humans care about cats and dogs because they both have some effects on us, and because we're fond of knowledge for its own sake. However it seems questionable an AI is going to have any reason to care.
#+end_quote

Let's say that the AI has no use for our solar system except as raw materials for computronium and an energy source in the middle. That AI would /still/ benefit from a close study of humanity, because it cares about how to use its energy with the greatest efficiency;the better it can predict human behaviour, the better it can use a little bit of energy to persuade us to spend a vast deal of our energy doing what it wants us to do, which is a lot more efficient than having to use its own energy for everything.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501297484.0
:DateShort: 2017-Jul-29
:END:

******************* u/vakusdrake:
#+begin_quote
  (a) Only if I accept your claim that AIs are only interested in other AIs, which I do not. (b) I don't know that I'm not integral to the development of AI. Maybe I'm going to be a close relative of the person who actually does the code, and a large influence on their life.
#+end_quote

The points you made in the previous answer apply here, why not just use AI subagents or something? Like how exactly is it going to be worthwhile to do a /perfect/ mental sim of somebody, because at some point in the future they may interact with people who matter in the context of GAI?

#+begin_quote
  Let's say that the AI has no use for our solar system except as raw materials for computronium and an energy source in the middle. That AI would still benefit from a close study of humanity, because it cares about how to use its energy with the greatest efficiency;the better it can predict human behaviour, the better it can use a little bit of energy to persuade us to spend a vast deal of our energy doing what it wants us to do, which is a lot more efficient than having to use its own energy for everything.
#+end_quote

If the AI running the sim has the processing to justify running this sort of wasteful sim in the first place it isn't in containment. So GAI that's reached maturity just can't be affected by any actions humans could take in any of the scenarios it seems like you could be referring to here. Like it's not going to mine the surface of the earth, it doesn't even probably care about much except gathering raw energy and matter which it can use to get everything else, or gathering concentrated heavy elements from the core. Once you're dealing with the relevant tech's here it makes the most sense to just disassemble planets, anything people on the planet could do would have no impact on you whatsoever, not even enough to justify the cost of spending mental energy thinking about diplomacy. Not to mention if you don't immediately start ripping the planet apart anything alive on the surface won't survive whatever grey goo you likely dumped onto the planet anyway.\\
Idk it just sort of seems diplomacy in this situation seems basically applicable to trying to negotiate with the ant colonies on a plot of land you are about to turn into a open pit mine. Like sure ants are super predictable and you could probably control them if you wanted, but what would be the point?
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501309871.0
:DateShort: 2017-Jul-29
:END:

******************** u/CCC_037:
#+begin_quote
  The points you made in the previous answer apply here, why not just use AI subagents or something?
#+end_quote

It's possible that people close enough to the writer(s) of the AI would need to be simulated at a fairly deep level. (I'm not talking about a single interaction - there a subagent would work - I'm talking about close, longterm contact at a minimum) Alternatively, I /do/ write code for a living - perhaps some of my code could end up in such an AI in any case.

#+begin_quote
  If the AI running the sim has the processing to justify running this sort of wasteful sim in the first place it isn't in containment.
#+end_quote

Alternatively, it could be in containment by someone who likes running sims. Or, it could be in containment (but with a good deal of extra processing power) and running a sim in order to figure out how to break that containment.

#+begin_quote
  Idk it just sort of seems diplomacy in this situation seems basically applicable to trying to negotiate with the ant colonies on a plot of land you are about to turn into a open pit mine. Like sure ants are super predictable and you could probably control them if you wanted, but what would be the point?
#+end_quote

The point is getting the ants to do the open-pit mining for you. Sure, it takes longer, but perhaps it's more energy-efficient...
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501392407.0
:DateShort: 2017-Jul-30
:END:

********************* u/vakusdrake:
#+begin_quote
  Alternatively, it could be in containment by someone who likes running sims. Or, it could be in containment (but with a good deal of extra processing power) and running a sim in order to figure out how to break that containment.
#+end_quote

For an AI to have the kind of processing that is relevant when it comes talking about running many hundreds of thousands or more human simulated minds /while still in containment and presingularity/, requires a universe with much more processing power to justify which as a serious proposal doesn't really work for the reasons I've already gone over.\\
As for the GAI creators making you create sims, that is basically the scenario outlined before, except with the added implausibility of pre singularity tech being sufficient.

#+begin_quote
  The point is getting the ants to do the open-pit mining for you. Sure, it takes longer, but perhaps it's more energy-efficient...
#+end_quote

While I think the metaphor still kind of works here continuing with it isn't actually going to simplify things so I'll just say there are serious issues: Firstly /time matters/, because we're talking about exponential growth and delaying expansion will mean more parts of your future light cone will become permanently out of reach, and that you will be able to gravitationally bind less of your local cluster (star lifting is a possibility here). Secondly and most importantly however there's no way humans could help you in any way here /even marginally/. Any energy you spend on diplomacy with them is energy you could use to dump grey goo on the planet which would be able do anything the humans could do but better. To even do diplomacy requires halting the default strategy of shooting von-neumann probes/grey goo at everything or just tearing apart the planets for resources with a stellar power laser or similar.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501396570.0
:DateShort: 2017-Jul-30
:END:

********************** u/CCC_037:
#+begin_quote
  For an AI to have the kind of processing that is relevant when it comes talking about running many hundreds of thousands or more human simulated minds /while still in containment and presingularity/, requires a universe with much more processing power to justify which as a serious proposal doesn't really work for the reasons I've already gone over.
#+end_quote

/Any/ universe which is running ours as a sim needs to have significantly more processing power than we have available.

#+begin_quote
  Firstly /time matters/, because we're talking about exponential growth and delaying expansion will mean more parts of your future light cone will become permanently out of reach
#+end_quote

As long as the probes to other star systems are sent out in time, I'm failing to see how it matters whether it takes ten year or ten million to absorb a given system.

#+begin_quote
  Secondly and most importantly however there's no way humans could help you in any way here /even marginally/
#+end_quote

I don't think that there's any way in which having grizzly bears on the planet with us is a significant benefit to humanity, yet we're willing (as a species) to go to quite some effort to ensure that they don't get wiped out. Maybe it's an AI interested in nature conservation?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501427298.0
:DateShort: 2017-Jul-30
:END:

*********************** u/vakusdrake:
#+begin_quote
  Any universe which is running ours as a sim needs to have significantly more processing power than we have available.
#+end_quote

Given we were talking about a mind sim that's absolutely not true, deconstructing /even just the earth/ would give more than enough resources to run numbers of human level minds that are far too large to even really be comprehensible to humans and vastly dwarf the numbers of humans who've ever lived.

#+begin_quote
  As long as the probes to other star systems are sent out in time, I'm failing to see how it matters whether it takes ten year or ten million to absorb a given system.
#+end_quote

That would be true if we were living in a steady state universe, but our universe is expanding and so galaxies are constantly travelling over the cosmological horizon so that we will literally never be able to reach them even travelling at lightspeed. Plus if you care about not having large parts of your civ not forever isolated, then you will want to use star lifting to counteract galaxies movement away due to expansion

#+begin_quote
  I don't think that there's any way in which having grizzly bears on the planet with us is a significant benefit to humanity, yet we're willing (as a species) to go to quite some effort to ensure that they don't get wiped out. Maybe it's an AI interested in nature conservation?
#+end_quote

It's rather hard to imagine how exactly how you get an AI programmed with that sort of ethical system. After all drawing a distinction between digital and analog minds seems just a rather weird human thing to do. So it's hard to imagine what bizarre nonsensical goal alignment would lead an AI to decide to build nature sanctuaries as opposed to just uploading every living thing of moral significance, or deconstructing the planet in order to build habitats for the animals to live in.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501463470.0
:DateShort: 2017-Jul-31
:END:

************************ u/CCC_037:
#+begin_quote
  Given we were talking about a mind sim that's absolutely not true, deconstructing /even just the earth/ would give more than enough resources to run numbers of human level minds that are far too large to even really be comprehensible to humans and vastly dwarf the numbers of humans who've ever lived.
#+end_quote

If we're in a mind level sim, then there is no Earth to deconstruct and, even if we were to try, we wouldn't be able to get more computing power out of it than is being used to run the sim (because that computing power is simply not there to be used).

The sim might not require more processing power than we /think/ we have available. It will certainly require vastly more processing power than we /actually/ have available.

#+begin_quote
  It's rather hard to imagine how exactly how you get an AI programmed with that sort of ethical system.
#+end_quote

A simple "let anything that can think decide its own destiny" ethical system will do it...
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501735739.0
:DateShort: 2017-Aug-03
:END:

************************* u/vakusdrake:
#+begin_quote
  If we're in a mind level sim, then there is no Earth to deconstruct and, even if we were to try, we wouldn't be able to get more computing power out of it than is being used to run the sim (because that computing power is simply not there to be used).
#+end_quote

I don't think you got the point I was making, that any post singularity civ could /easily/ run a sim of our civilization, provided they just simulated the minds. This isn't a point about the processing power /within/ the sim, just that massive non-baseline sims aren't hard to run for post singularity civs even in universes with the same physics as we think our universe has.

#+begin_quote
  A simple "let anything that can think decide its own destiny" ethical system will do it...
#+end_quote

I can point out the specifics about why that's not a remotely simple or self consistent ethical system, but the larger problem here has to do with apparent versus actual complexity. There's [[http://lesswrong.com/lw/jp/occams_razor/][an article]] in the sequences that covers the issue somewhat. Effectively ethical systems like that hide a massive amount of complexity beneath the surface, so calling it "simple" is like saying "a witch did it" is a simple answer to any question.\\
So the problem is that basically every part of the goal function you specified is massively nebulous and undefined, basically akin to saying you can solve AI safety by just telling an AI to not do bad things. Another way to say is that human intuitions of complexity have next to no correlation with actual formalized complexity, the amount of bits it would take to describe something from scratch.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501738413.0
:DateShort: 2017-Aug-03
:END:

************************** u/CCC_037:
#+begin_quote
  I don't think you got the point I was making, that any post singularity civ could easily run a sim of our civilization, provided they just simulated the minds. This isn't a point about the processing power within the sim, just that massive non-baseline sims aren't hard to run for post singularity civs even in universes with the same physics as we think our universe has.
#+end_quote

Ah, so you're saying that in a universe that actually /is/ as our universe /appears/, a sufficiently advanced and dedicated civilisation could run a mind-level sim of our universe, for at least a few minds (and, depending how much computing resources they decide to pursue, potentially quite a lot of minds).

Agreed, but this again leads us to the question of /why/.

#+begin_quote
  I can point out the specifics about why that's not a remotely simple or self consistent ethical system, but the larger problem here has to do with apparent versus actual complexity.
#+end_quote

Okay, noted, actually /implementing/ such an ethical system is a thorny minefield of problems and edge cases and complexity. I'm not proposing this idea as a complete or even a partial solution to AI safety. I'm merely suggesting that an ethical system that puts strong value on self-determination by other intelligent entities would have reason to not instantly obliterate any intelligent life it comes across.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501809978.0
:DateShort: 2017-Aug-04
:END:

*************************** u/vakusdrake:
#+begin_quote
  Okay, noted, actually implementing such an ethical system is a thorny minefield of problems and edge cases and complexity. I'm not proposing this idea as a complete or even a partial solution to AI safety. I'm merely suggesting that an ethical system that puts strong value on self-determination by other intelligent entities would have reason to not instantly obliterate any intelligent life it comes across.
#+end_quote

Ok so I wasn't just making a point about about AI safety generally, but that placing a value on "self determination" in a way that gets the results it seems like you're looking for from the singleton here is rather implausible. For instance if a system doesn't already have intelligent life it seems hard to come up with a reason not to consume it on the basis that something might hypothetically arise there eventually, since you could use those resources to run minds that are part of your own civ or to gather resources to push back the heat death of the universe and extend the life of pre-existing minds.\\
Secondly even if there is already a pre singularity civ there, that's not much of a reason not to incorporate them into your own civ. It doesn't need to be malicious or anything, just send down grey goo and help the people on the planet. It seems pretty inevitable that people will come to grow dependant on your assistance and due to the significant advantages of cooperation will for all intents and purposes end up part of your civ. Given most people on a planet would definitely want things you could provide them it's hard to imagine how leaving them on their own is easy to justify.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501810985.0
:DateShort: 2017-Aug-04
:END:

**************************** u/CCC_037:
#+begin_quote
  For instance if a system doesn't already have intelligent life it seems hard to come up with a reason not to consume it on the basis that something might hypothetically arise there eventually, since you could use those resources to run minds that are part of your own civ or to gather resources to push back the heat death of the universe and extend the life of pre-existing minds.
#+end_quote

Yes, any system entirely without life would be consumed as soon as the AI got to it.

#+begin_quote
  Secondly even if there is already a pre singularity civ there, that's not much of a reason not to incorporate them into your own civ. It doesn't need to be malicious or anything, just send down grey goo and help the people on the planet. It seems pretty inevitable that people will come to grow dependant on your assistance and due to the significant advantages of cooperation will for all intents and purposes end up part of your civ. Given most people on a planet would definitely want things you could provide them it's hard to imagine how leaving them on their own is easy to justify.
#+end_quote

Ah - there's an important point here. /Self/ determination doesn't mean not /harming/ people. It means not /influencing/ people. And dropping helpful grey goo down onto a planet is most certainly influencing people (for one thing, as you point out, it influences them to become part of your civ).
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501811893.0
:DateShort: 2017-Aug-04
:END:

***************************** u/vakusdrake:
#+begin_quote
  Ah - there's an important point here. Self determination doesn't mean not harming people. It means not influencing people. And dropping helpful grey goo down onto a planet is most certainly influencing people (for one thing, as you point out, it influences them to become part of your civ).
#+end_quote

See that's the thing though, what exactly is self determination /actually mean/? Because it obviously has to disregard the preferences of the people who it's supposedly helping if it cares more about not influencing them then it does about helping them get what they want.\\
If you're really totally dedicated to not influencing other civs the literally the only way to do that is to wipe yourself out and trying to avoid even leaving any signs you ever existed. After all it's the only way you can hope to avoid interacting with them. Similarly it would seem to necessitate that you never expand either since people would notice that, however not expanding is basically a guarantee that you will be overtaken by another civ that does, and trying to prevent upstarts from doing so is most definitely negating their self determination.\\
The issue here is that avoiding interfering with civs is basically an impossible and incoherent goal system that's also totally incompatible with any form of altruism. Hell how do you even get an AI like that in the first place? Wouldn't the AI just destroy itself so as not to interfere with its creators civ? Or spread it's grey goo and use it to ensure that its creators never create any new form of intelligence such as future GAI's which would "negate their self direction" as it were.

Fundamentally it's not just that self direction is a hard goal system to design, it's that it's sort of incoherent and seemingly incompatible with getting a post-singularity civ in the first place.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1501812708.0
:DateShort: 2017-Aug-04
:END:


** So, some of you probably noticed the Erogamer story posted here recently and in a comment thread prior. I started reading it and stopped because I lost interest in it. I want to discuss something that occurs in the story, so *spoilers and NSFW content ahead*.

I've put a link a bit further down, but here's the context you need to interpret that link if you haven't read that story: [[#s][]]

[[https://docs.google.com/document/d/1EvxfLESwTN6E6_Jgw1qB9rGeKQZZ-DO4zZ5t2ZTcQwE/edit?usp=sharing][Here's the ensuing discussion/self reflection. It takes place in chapter 1.6, /Morning After/, of the story.]]

It's a ~950 word section, a quick read. Just wanted to know what some of your thoughts were on this piece and what it says about explicit/implicit consent. I'm interested enough in hearing your thoughts that I may repost this comment to next week's thread, considering I'm 2 days late to this one. We'll see how it goes.

EDIT: Just to clarify; I did not lose interest with this story because of this particular event.
:PROPERTIES:
:Author: Kishoto
:Score: 2
:DateUnix: 1500833047.0
:DateShort: 2017-Jul-23
:END:

*** Well, within the constraints of the (local) legal system as I understand it, it would indeed be much more convenient for everyone if there were no people with weird mental hang-ups that prevent them from verbally giving consent.

Now, I don't think mandatory consent actually solves anything, unless you require that people file legal paperwork that would reliably indicate that every party has given their consent while in possession of their full faculties and to what exactly they consent ahead of time. Otherwise, it's not clear what it's actually supposed to achieve, other than making it impossible for people to play out rape fantasies without someone committing a felony (I'm not aware if there is legal precedent on use of safewords vis-à-vis affirmative consent, but given that they are used to communicate the lack of consent, I wouldn't be optimistic).
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1500853809.0
:DateShort: 2017-Jul-24
:END:

**** Interesting! I didn't imagine things from a legal point of view.

I think mandatory consent is one of those things that works better on a more flexible, emotional level than a strictly legal one. The idea is that no one gets (or feels as if they have been) assaulted. Or unknowingly commits assault. Of course, very little prevents someone from turning around and claiming they didn't get consent to charge someone as there's not usually a witness to that sort of thing. But I still feel that it's a policy in place to try and shield people from doing something they're uncomfortable with.
:PROPERTIES:
:Author: Kishoto
:Score: 2
:DateUnix: 1500866668.0
:DateShort: 2017-Jul-24
:END:

***** Well, the problem is: what situation does it make a difference in? To me it seems like the precise set of circumstances in which the legal requirement to explicitly get verbal affirmative consent would make a difference is rather small.

Since it's not recorded, it does not remove the issue of parties disagreeing about what happened or someone lying. Drugging someone to the point where they can no longer reason and then having sex with them is covered already without affirmative consent law, I think, so it doesn't help there. One would have to be pretty conscious of the possibility of legal trouble to even remember about it when they're about to have sex, and, frankly, if you're worried enough about legal repercussions to remember to ask for explicit verbal consent, then you're probably worried enough not to risk it in the first place. Not like there is any actual admissible evidence that you got said consent, in case you get accused, after all. What problem is this supposed to solve? Teenage people agreeing to have sex in the heat of the moment and then feeling ashamed the next morning and claiming they didn't want it in the first place? I suppose some people at least would accurately recall that they had given it when it's verbal and explicit and wouldn't consciously choose to lie about it. What I'd really want to see is some reliable data on what difference the law made, but that's obviously not gonna happen.

Making sure your parters are alright with what you're doing is extremely important on all levels. I'm just not sure the particular implementation they went for in cali solves more problems than it creates. Thankfully the trade-off isn't horrible for /most/ people (I hope, but I've heard some anecdotal evidence that indicates otherwise), but there are probably some people who /do/ have mental hang-ups, or, hell, what if you want to be gagged and restrained while someone fucks you silly? It's a pretty thorny issue no matter how you try to handle it. What if you were alright with it and verbally (or even in writing) consented before you were gagged, but then changed your mind once you were restrained? Can one even consent to something like "keep doing it even if I change my mind", in legal sense or otherwise?
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1500872078.0
:DateShort: 2017-Jul-24
:END:

****** I agree. I don't think the law is a very good solution because it doesn't really change all that much from a legal standpoint. Especially when, as you said, someone could regret it and change their mind and, unless they asked for consent at literally every new caress, it could technically be seen as a violation. Doesn't really seem realistic or helpful when you think about it that way.
:PROPERTIES:
:Author: Kishoto
:Score: 2
:DateUnix: 1500896233.0
:DateShort: 2017-Jul-24
:END:


** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1501058975.0
:DateShort: 2017-Jul-26
:END:
