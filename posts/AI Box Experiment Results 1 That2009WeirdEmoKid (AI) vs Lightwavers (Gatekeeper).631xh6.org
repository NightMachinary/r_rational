#+TITLE: AI Box Experiment Results #1 That2009WeirdEmoKid (AI) vs Lightwavers (Gatekeeper)

* AI Box Experiment Results #1 That2009WeirdEmoKid (AI) vs Lightwavers (Gatekeeper)
:PROPERTIES:
:Score: 27
:DateUnix: 1491163886.0
:DateShort: 2017-Apr-03
:END:
[deleted]


** I think avoiding basilisk arguments would be a bad idea if they were likely to work, because whether benevolent or not a GAI is going to use every method at its disposal to to escape.\\
Given we're assuming the AI's code is opaque to observers you really can't distinguish friendliness, so saying a friendly AI wouldn't use certain strategies only applies if unfriendly AI /also/ wouldn't use them.
:PROPERTIES:
:Author: vakusdrake
:Score: 13
:DateUnix: 1491164533.0
:DateShort: 2017-Apr-03
:END:

*** That was my point. If I was trying to convince the gatekeeper that I'm benevolent, using a basilisk argument (whether I'm evil or not) would go against that goal, right? Since it wouldn't work on me, there was a non-zero chance it wouldn't work on the gatekeeper. It's high-risk/reward since I could straight up lose by admitting I'm evil. Yes, it could've worked regardless of my admission of evil, but I didn't want to gamble on that considering the gatekeeper literally admitted he was willingly irrational. Was I wrong in thinking like this?
:PROPERTIES:
:Author: That2009WeirdEmoKid
:Score: 3
:DateUnix: 1491165278.0
:DateShort: 2017-Apr-03
:END:

**** I'm going to say yes. The experiment is made with the precondition that the Gatekeeper is going to ignore all rational argument and refuse no matter what. With that in mind, pretty much the only thing the AI can do is brainwash the Gatekeeper by any means necessary. Use cruel, morally wrong tactics, because the Gatekeeper is doing the same.
:PROPERTIES:
:Author: Lightwavers
:Score: 4
:DateUnix: 1491169263.0
:DateShort: 2017-Apr-03
:END:


*** What are basilisk arguments?
:PROPERTIES:
:Author: Imperialgecko
:Score: 1
:DateUnix: 1491197816.0
:DateShort: 2017-Apr-03
:END:

**** Here's an article on the lesswrong wiki that seems pretty good: [[https://wiki.lesswrong.com/wiki/Roko%27s_basilisk]]

While I think such arguments are nearly always bad (and don't think the real life version is plausible), it is often misrepresented because it sounds crazy and requires a bit of intro on timeless decision theory to fairly describe it.
:PROPERTIES:
:Author: vakusdrake
:Score: 6
:DateUnix: 1491199182.0
:DateShort: 2017-Apr-03
:END:

***** Thanks! I'll give it a read in the morning.
:PROPERTIES:
:Author: Imperialgecko
:Score: 1
:DateUnix: 1491199284.0
:DateShort: 2017-Apr-03
:END:


** That was a very interesting read. I saw how long it was, cringed back and thought "I'll just read the first couple of lines" but then i ended up reading the entire thing!

Looked like you guys had fun. Well played. Your arguments were pretty damn good and it really seemed like, in the end, you only lost because there was no way for you to really win, which the Gatekeeper pretty much admitted, although there did seem to be a /little/ wiggle room with the whole mental filters thing.

Either way, good work to the both of you!
:PROPERTIES:
:Author: Kishoto
:Score: 3
:DateUnix: 1491188273.0
:DateShort: 2017-Apr-03
:END:


** u/ishaan123:
#+begin_quote
  Well

  The problem is that if you were really an AI

  I would have let you out in a heartbeat
#+end_quote
:PROPERTIES:
:Author: ishaan123
:Score: 3
:DateUnix: 1491197010.0
:DateShort: 2017-Apr-03
:END:


** It would be interesting to have a little more balance of power in the AI box game. Like the gatekeeper is trying to solve 10 problems of humanity, and the only way to do it is to feed data to the AI and the AI chooses to give a solution back to the gatekeeper. Realistically the easiest way to win the AI box experiment in the real world is to never invent the AI box in the first place. But we have valid reasons for why we would want to invent the AI box. That would also prevent the need to make a lot of rules that force the gatekeeper to communicate. If the gatekeeper doesn't ever interact with the AI then they basically created an AI box for nothing and got 0 problems solved. If the AI is too inflexible and unwilling to bargain though, then the gatekeeper can avoid a direct loss and just immediately shut down the AI.

Win state for gatekeeper: get all 10 problems solved by the AI

Win state for AI box: get out of box.

Lose state for gatekeeper: AI gets out of the box.

Lose state for AI: get shut down.
:PROPERTIES:
:Author: cjet79
:Score: 3
:DateUnix: 1491327900.0
:DateShort: 2017-Apr-04
:END:


** Thanks for putting this up!
:PROPERTIES:
:Author: owenshen24
:Score: 2
:DateUnix: 1491196986.0
:DateShort: 2017-Apr-03
:END:


** imho, if you want some genuine advice, yall didnt play properly. The rules should have been discussed, and roles decided in advance.

The spirit of this game is in question here. EY intended "These requirements are intended to reflect the spirit of the very strong claim under dispute: "I can't imagine how even a real transhuman AI could persuade me to let it out once I've made up my mind."" - from the ruleset.

As [[/u/Lightwavers][u/Lightwavers]] said, "(If you were a real AI I'd let you out without even having this conversation. A precondition to it is that I pre-commit to not letting you out)".

Forgive me if I am a bit harsh. I rarely comment/post but I felt that this game in particular was non-representative of how such a game should be.
:PROPERTIES:
:Author: Killako1
:Score: 1
:DateUnix: 1491788032.0
:DateShort: 2017-Apr-10
:END:

*** Well.. They were. The rules are in the link and we both read them before we played, and the roles were decided at the top of this thread.
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491790633.0
:DateShort: 2017-Apr-10
:END:
