#+TITLE: The Revenant Problem - A Philosophical Doom 2 Map

* [[https://www.youtube.com/watch?v=MiEYCXPI-qY][The Revenant Problem - A Philosophical Doom 2 Map]]
:PROPERTIES:
:Author: nick012000
:Score: 17
:DateUnix: 1514625829.0
:DateShort: 2017-Dec-30
:END:

** Can I have the option of tying down everyone who thinks the Trolley Problem is interesting on one of the tracks, so we never have to deal with this again?
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 26
:DateUnix: 1514639792.0
:DateShort: 2017-Dec-30
:END:

*** [[http://existentialcomics.com/comic/106][My favorite version of the Trolley Problem]].
:PROPERTIES:
:Author: FaceDeer
:Score: 15
:DateUnix: 1514662976.0
:DateShort: 2017-Dec-30
:END:

**** The '[[https://en.wikipedia.org/wiki/Veil_of_ignorance][Veil of Ignorance]]' concept mentioned sounds curious.

If you poll people when they have established social positions, then even if you temporarily shroud their self-knowledge, they will be left with the self-serving prejudices (for their own group and against the other groups) that are inextricably(?) tied up in their understanding of social systems.

If you erase enough of their understanding of social systems such that they don't have prejudices, or likewise poll them (as high-fluid-intelligence low-crystallised-intelligence minds as though created-from-scratch AIs, rather than nearly-thought-incapable newborns of embryos) before they've lived/studied/learned and gathered knowledge/understanding about social systems, what capability do they have to make meaningful choices between options presented to them?

This seems to suggest a need for a means to either break through (and destroy) already-formed prejudices, to prevent the formation of prejudices while learning, or ideally both. After that, a veil of ignorance would be helpful in preventing people from deliberately making immoral choices for personal benefit, in contrast with making thought-to-be moral choices which are for personal benefit.

(An example that comes most quickly to mind is an actually-rich person who feels that financial support make poor people lazy and unproductive, or an actually-poor person who feels that keeping any money over certain thresholds would make rich people lazy and unproductive... though I'm leery of mentioning any examples at all, both due to (the risk of prompting an irrelevant digression) and (my own acknowledged unfamiliarity with the issues actually involved in any such example).)

...Hmm, that is to say, I'm questioning how many self-serving votes are people deliberately making unjust choices (addressable by a veil of ignorance) and how many are them making sincere choices from the perspective of a biased worldview. Though it does sound as though a veil of ignorance would only help, rather than harm (other than the specific instances of 'harm' that the voter afterwards realises they themself agreed to).

...beyond that, the concept of an analysis method that everyone can get behind... or, if not utility-function-equalising, at least accurate models of outcomes... though then we have the interesting issue of if, say, high-income people tended to end up deontological ('to watch as someone starves is tragic, to steal to prevent starvation is sin') and low-income people utilitarian ('a stolen loaf of bread to prevent starvation better than an extra loaf of bread held by one who has plenty'). Without people's ethical systems themselves being changeable, can people reach policy consensus? What new information(?), distinct from self-interest, could convince people with disparate ethical systems to unify them?

[[https://smbc-comics.com/comic/evil-ethics][Something that I sometimes ruminate on.]] ...Come to think of it, I think I've been using the term 'utility function' even when contrasting utilitarianism with other ethical systems, so I think I need a better phrase. Particularly, is there something one can call goal-directed-behaviour within an (arbitrary(/arbitrarily-designatable)) ethical framework?
:PROPERTIES:
:Author: MultipartiteMind
:Score: 6
:DateUnix: 1514708911.0
:DateShort: 2017-Dec-31
:END:

***** The Veil of Ignorance is rather straightforward in the trolley problem. You don't know whether you're pulling the switch, or potentially on the track. The idea is that this can counter those biases you mentioned by forcing you to put yourself in the shoes of everyone whom a decision effects.
:PROPERTIES:
:Author: crystal-pathway
:Score: 2
:DateUnix: 1515093281.0
:DateShort: 2018-Jan-04
:END:

****** Ahh. I was imagining it (from the article) in the form of it being a given that one would be one the track, but not knowing which track beforehand, thus shifting direct self-interest into more generalised self-interest which could be seen as a more utilitarian/ethical decision process.

If you're instead comparing the state of 'choosing a switch while on an unknown track' to 'choosing a switch while not on a track', then being on a track could be argued to be pushing people to sacrifice their own ethical system in favor of self-interest (which here has similar results to utilitarianism, even if the chooser would not normally espouse a utilitarian ethical system).

...now I'm thinking again about the relationship between 'willing to die rather than harm others' and 'willing to stain one's own hands with blood in order to save others from harm'... a matter of what one protects, whether it's one's life (kill rather than die), others, others including oneself, or one's own pride/self-image/conscience...

For the rich/poor, I'm particularly thinking of quoted cases where richer people have said 'Well, if /I/ were born poor then I would just (easily/efficiently) climb my way up by making use of {lots of things requiring foreknowledge of their existence which one would in practice never have the opportunity to even know about the existence of}'. The person thinks that life is nothing but toil for those on their track, and feather-beds for those on another track, so out of self-interest wanting to make them about equal, by improving their own and worsening the other, even if they haven't guessed that they're actually just improving things for themselves.

...or for a non-wealth instance, certain groups said to think that they're underprivileged while others are overprivileged (despite objective observations by the majority strongly suggesting the exact opposite) trying to make the tracks equal by adding privileged to the 'underprivileged' actually-overprivileged track and stripping privilege from the 'overprivileged' actually-underprivileged track... (and again running into the issues/conflicts of what different 'final-state ideal world's look like according to the valuer.)
:PROPERTIES:
:Author: MultipartiteMind
:Score: 1
:DateUnix: 1515123742.0
:DateShort: 2018-Jan-05
:END:


*** On the condition that you can let them argue which side they should be tied on.
:PROPERTIES:
:Author: Menolith
:Score: 10
:DateUnix: 1514640488.0
:DateShort: 2017-Dec-30
:END:

**** That would be cruel. In a hilariously funny "never go up against a Sicilian when death is on the line" sort of way.

"Ah, but you're from Australia, and all Australians are criminals, so clearly I can't choose to be tied down on the trolley side, because a criminal would never flip the switch..."
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 7
:DateUnix: 1514646037.0
:DateShort: 2017-Dec-30
:END:


**** But then they will all choose the same side and send the trolley the other way...
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1514641548.0
:DateShort: 2017-Dec-30
:END:

***** But they don't know which side I'm going to choose.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 5
:DateUnix: 1514646239.0
:DateShort: 2017-Dec-30
:END:


***** Sadly, they're all tied down, and no one will save them
:PROPERTIES:
:Author: TwoxMachina
:Score: 2
:DateUnix: 1514642559.0
:DateShort: 2017-Dec-30
:END:


*** Can you give some insight into why you think the trolley problem is uninteresting?
:PROPERTIES:
:Author: sparr
:Score: 4
:DateUnix: 1514688143.0
:DateShort: 2017-Dec-31
:END:

**** There's an interesting minor philosophical thought experiment in it, but it has been so overdone that people are jamming it in places where it has absolutely no applicability, to the point that it has basically no pedagogical use left.

The only people who still care about it are the people who are so defensive about philosophy they're creating a veil of ignorance where you pretend you don't know if you've heard of the trolley problem before or not, and the people who are so stupid they think a Tesla should have a literal "howmanypeoplewillIkill()" function in it.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1514725139.0
:DateShort: 2017-Dec-31
:END:

***** Why not a =howmanypeoplewillIkill()= function? Are you suggesting a smarter system that differentiates between maiming and killing and other types of injury? Maybe =howmanypeoplewillIkill()= returns a float, in death-equivalent-units?
:PROPERTIES:
:Author: sparr
:Score: 2
:DateUnix: 1514727082.0
:DateShort: 2017-Dec-31
:END:

****** u/ArgentStonecutter:
#+begin_quote
  Why not a howmanypeoplewillIkill() function?
#+end_quote

Because properly programming an autonomous vehicle has to be about avoiding getting to that point, not about driving like a human and solving the trolley problem every time your software realizes it has fucked up.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1514727268.0
:DateShort: 2017-Dec-31
:END:

******* u/sparr:
#+begin_quote
  Because properly programming an autonomous vehicle has to be about avoiding getting to that point
#+end_quote

And just give up if you encounter a situation you couldn't avoid?

#+begin_quote
  every time your software realizes it has fucked up.
#+end_quote

Ahh, spotted your mistake. You seem to be assuming that if the car needs to calculate how many people it will kill, it has [already] fucked up?
:PROPERTIES:
:Author: sparr
:Score: 2
:DateUnix: 1514727489.0
:DateShort: 2017-Dec-31
:END:

******** u/ArgentStonecutter:
#+begin_quote
  And just give up if you encounter a situation you couldn't avoid?
#+end_quote

If you get to a situation you can't avoid, you've already screwed up, and the probability that you have any better options than "brake to a stop as quickly as possible" is negligible.

#+begin_quote
  Ahh, spotted your mistake.
#+end_quote

It's not /my/ mistake.

#+begin_quote
  You seem to be assuming that if the car needs to calculate how many people it will kill, it has [already] fucked up?
#+end_quote

Of course. Anyone else who has worked on real-time safety software will tell you the same thing. Safety systems are all about keeping well away from edge cases.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 6
:DateUnix: 1514728955.0
:DateShort: 2017-Dec-31
:END:

********* u/nick012000:
#+begin_quote
  Of course.
#+end_quote

Let's take the scenario of a child running out onto the middle of the road, and the car can calculate the distance to the child, and knows it doesn't have enough space to brake. How is that the car's fault?
:PROPERTIES:
:Author: nick012000
:Score: 1
:DateUnix: 1514787440.0
:DateShort: 2018-Jan-01
:END:

********** If the sidewalk is occluded (say by parked cars) so it could not be expected to have already been tracking pedestrians before it ran onto the street (and thus slowed down as soon as it became aware of them), and the car is driving too fast to safely stop when the child runs out into the road, and there is enough oncoming traffic that it would not be able to safely swerve into the oncoming lane, then /it was driving too fast for the street/. It should have been driving slowly enough that it could stop safely for any pedestrian that might dart out from a location it was not able to observe.

This is not a trolley problem (even if there's a 90 year old man dying of cancer on a bicycle that it would hit if it had to swerve to avoid the child), this is a negligence on the part of the manufacturer problem.

I mean even if you force it into a classic trolley problem situation, it should have slowed down when the old man was detected. He might fall off his bike into the path of the autonomous vehicle!

Yes, this means narrow parking streets and other byways with limited visibility would effectively limit AV to about 5MPH. /That's OK./
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1514800971.0
:DateShort: 2018-Jan-01
:END:

*********** u/nick012000:
#+begin_quote
  it was driving too fast for the street
#+end_quote

Driving more slowly than the speed of the surrounding traffic increases the risk of collision with other cars.
:PROPERTIES:
:Author: nick012000
:Score: 2
:DateUnix: 1514816025.0
:DateShort: 2018-Jan-01
:END:

************ If it can't avoid the child by changing lanes, /and there is a trolley problem situation/, there is only one lane going each way and thus no passing traffic. If there are two lanes then it has all along been ensuring that it is not driving next to another vehicle and thus it is safe to change lanes.

If there are cars passing it in the face of oncoming traffic it will already have pulled over to let them pass until the traffic is clear. This might annoy some human drivers, but (a) it's less annoying than being behind a school bus, and (b) by the time AV are sophisticated enough to being /even potentially/ able to do something useful with a "howmanypeoplewillIkill()" function there will be very few remaining human drivers.

Coming up with a situation where a sufficiently advanced AV would potentially have to evaluate a "howmanypeoplewillIkill()" function is really really hard, and probably involves deliberate malfeasance by a human. virtually all realistic cases for /properly programmed advanced AVs/ the only rational option for black-swan events is "brakeasfastaspossible()".
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1514825338.0
:DateShort: 2018-Jan-01
:END:


********* Situations where a vehicle is physically unable to stop before colliding with something are entirely normal. Not common, but in no way ignorable. The most obvious example if if there is a collision in front of you, putting vehicles (or other objects) that you could already see into positions or with velocities that you couldn't have predicted based on what you could see. Then there's the possibility of the brakes going out. Or a person falls out of a vehicle ahead of you in traffic. Or...

These are situations that seem impossibly rare to any individual driver or car, but happen every day if you consider every car on the road.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514870778.0
:DateShort: 2018-Jan-02
:END:

********** Most situations where a collision happens in front of you, you theoretically have the ability to avoid it if you're totally aware and on task. AV will do that, and even current partially autonomous vehicles have demonstrated this ability.

And, again, we're not talking about "are AV immune to crashes", we're talking about "do AV have to have the ability to solve the trolley problem". We're not talking about just being involved in a collision, we're talking about ones where the incident requires the AV to make a decision about who to injure or kill.

My claim is that the probability of a situation that is /so out of control/ that there are no safe actions that don't involve injury, but the AV is enough in control to meaningfully make such a decision, is so low that any resources spent on it are better spent on preventing the situation in the first place.

In addition, as [[/u/stale2000]] pointed out, attempts to force some kind of "trolley problem handler" are much more likely to suffer from false positives. You suggested that it might save three lives a year? I doubt it would do that well /across the whole AV population/, let alone a single vehicle, and false triggers would cause more loss of life than that.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1514888586.0
:DateShort: 2018-Jan-02
:END:


********* u/crystal-pathway:
#+begin_quote
  If you get to a situation you can't avoid, you've already screwed up, and the probability that you have any better options than "brake to a stop as quickly as possible" is negligible.
#+end_quote

Neglible in a single instance sure, but over the all self-driving cars those probabilities add up. That's why these types of questions are relevant.
:PROPERTIES:
:Author: crystal-pathway
:Score: 1
:DateUnix: 1515093456.0
:DateShort: 2018-Jan-04
:END:

********** It is so unlikely that you are far more likely to be in a situation that is a false positive than you are actually in a real "trolley problem" scenario.

Like, the case that the other poster in this thread put forward, with someone falling out of a car in front of you and the only alternatives are to run them over or sideswipe the vehicle next to you.

First of all, identifying a human that is not upright is a hard problem. The thing that fell out of the car may be a laundry bag or a dog or it's actually a lost tire from the car in front of them.

Second, You're in heavy enough traffic that your ongoing efforts to avoid getting boxed in have failed. Sideswiping the vehicle next to you is almost certainly going to involve several other cars.

Third, you're in heavy enough traffic so the vehicle behind you is probably going to run over the obstruction anyway.

So - high false positive rate, and poor odds for a positive solution. Put cases in there to sideswipe cars to avoid running over people on the road and you're likely to /cause/ more accidents and loss of life than if you just teach it to brake and hope.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515094382.0
:DateShort: 2018-Jan-04
:END:

*********** I... think you're the one making the computers out to be human here. Machine learning is only going to get more accurate, and the ability of computers to make split second decisions is much more than that of a human. There are cases for instance where instance (highway) where the "brake instantly in a dangerous situation is an obviously bad strategy.
:PROPERTIES:
:Author: crystal-pathway
:Score: 1
:DateUnix: 1515094581.0
:DateShort: 2018-Jan-04
:END:

************ I did not suggest "brake instantly in a dangerous situation". Where did you even get that from? We're talking about a /specific/ situation where there is an obstruction on the road ahead (that the omniscient creator of the scenario has declared is a human) and you are boxed in so you can't steer around it. The person who created the situation was trying to create a situation where the trolley problem applies. I'm saying that in any such situation, where options other than "brake as fast as you can safely" are going to lead to a collision, then "brake as fast as you can safely" is the right thing to do even if that may still lead to a collision.

Obviously in any actual situation the AV will automatically be working to ensure they're not boxed in, because they /can/ safely switch lanes in situations that would be risky for a human. That's why he added the restriction that my hypothetical AV couldn't do that by specifying that it was boxed in.

If you don't like the scenario, that's fine, neither do I. Why don't you try and come up with a better one?
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515097302.0
:DateShort: 2018-Jan-04
:END:


******** No, it is because the self driving car solution to "I am in a bad situation that might kill someone" is almost always going to be "hit the breaks, and stop the car as fast as possible".

And if your software ever "thinks" that it is in a bad situation where the solution is NOT "hit the breaks", then your software is almost certainly wrong, and is in some sort of false positive situation.

Emergency car response is a problem of looking for horses, not zebras. And in this kind of situation, programming in a zebra detection algorithm will almost certainly cause more deaths than it saves in 99% of situations.
:PROPERTIES:
:Author: stale2000
:Score: 3
:DateUnix: 1514740071.0
:DateShort: 2017-Dec-31
:END:

********* u/sparr:
#+begin_quote
  if your software ever "thinks" that it is in a bad situation where the solution is NOT "hit the breaks"
#+end_quote

??!?!?

Do you realize that situations where a vehicle is physically unable to stop before colliding with something are entirely normal? Not /common/, but in no way ignorable. The most obvious example if if there is a collision in front of you, putting vehicles (or other objects) that you could already see into positions or with velocities that you couldn't have predicted based on what you could see. Then there's the possibility of the brakes going out. Or a person falls out of a vehicle ahead of you in traffic. Or...

These are situations that /seem/ impossibly rare to any individual driver or car, but happen /every day/ if you consider every car on the road.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514870590.0
:DateShort: 2018-Jan-02
:END:

********** They aren't ignorable. What I am saying is, that even in those situations, the solution 99% of the time is 'still' hit the breaks. And if you try to do something fancy that is not "hit the breaks and slow down as fast as possible" you are will probably just increase the chance of something bad happening.

Complexity kills. It leads to false positives.

Or better yet, don't get in those situations to begin with. IE, if it is possible for you to crash into something that you didn't see, then the car is moving to fast to begin with.

It should not be normal to ever be in a situation where the car can't slow down fast enough. Just never drive dangerously like that.

If the collision if front of you means that you can't slow down in time, then the car hasn't given enough space. If there is a blind spot, go slowly around the blindspot.
:PROPERTIES:
:Author: stale2000
:Score: 1
:DateUnix: 1514876736.0
:DateShort: 2018-Jan-02
:END:

*********** u/sparr:
#+begin_quote
  even in those situations, the solution 99% of the time
#+end_quote

Great. So, if those situations arise even one time per day (and they are MUCH more common than that), then this code still needs to be written to reduce the loss of life three times per year.
:PROPERTIES:
:Author: sparr
:Score: 3
:DateUnix: 1514878261.0
:DateShort: 2018-Jan-02
:END:


*********** u/sparr:
#+begin_quote
  It should not be normal to ever be in a situation where the car can't slow down fast enough. Just never drive dangerously like that.

  If the collision if front of you means that you can't slow down in time, then the car hasn't given enough space.
#+end_quote

Unfortunately our society has agreed that driving a certain degree of dangerously is not only acceptable but necessary.

Drivers, and self driving cars, are trained to follow at a safe distance assuming the car ahead might hit its brakes.

Consider the case of the car ahead of you experiencing a head-on collision, such that it stops instantly. Always leaving enough room for this unlikely situation will halve the throughput of a road/highway, which we won't tolerate.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514878422.0
:DateShort: 2018-Jan-02
:END:

************ u/ArgentStonecutter:
#+begin_quote
  Unfortunately our society has agreed that driving a certain degree of dangerously is not only acceptable but necessary.
#+end_quote

For humans, because humans actively seek a level of risk that "feels safe". AV do not have this bug.

#+begin_quote
  Consider the case of the car ahead of you experiencing a head-on collision, such that it stops instantly. Always leaving enough room for this unlikely situation will halve the throughput of a road/highway, which we won't tolerate.
#+end_quote

There are better alternatives to this, for example proper car spacing means that the AV is /always/ able to switch to an open lane in the case of an incident ahead because it's /never/ driving next to another vehicle. And in the absence of human drivers head-on collisions become black swan events.

And, again, we're not just talking about collisions, we're talking about situations where the an incident occurs too quickly to avoid, without any lead-in, /and/ safely changing lanes is not an option, /and/ braking would cause a greater loss of life than an unsafe swerve, /and/ it's plausible that the moral and ethical calculations are still potentially something an AV can be expected to know. /AND/ there is no possibility of false positives.

That's what it takes for "the trolley problem" to be relevant. It's ludicrous. You haven't even /tried/ to come up with a situation that fits.

Finally: [[https://www.youtube.com/watch?v=FadR7ETT_1k]]
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1514889280.0
:DateShort: 2018-Jan-02
:END:

************* u/sparr:
#+begin_quote
  proper car spacing

  in the absence of human drivers
#+end_quote

I see. You're trying to solve a problem that doesn't exist. I thought we were discussing the real world, where we need to take into account that AV and human drivers will coexist for years.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514911981.0
:DateShort: 2018-Jan-02
:END:

************** u/ArgentStonecutter:
#+begin_quote
  You're trying to solve a problem that doesn't exist.
#+end_quote

This whole discussion is about /a problem that doesn't exist/.

We're talking about the /trolley problem/. We're talking about a world where the cars themselves have the potential capability of making moral judgements about the worth of potential victims. That's decades in the future. It's certainly not going to happen in my lifetime.

Obviously while human drivers are common AV will be limited to things like limited access highways, low-speed pedestrian plazas, major streets without parking, and other places where it's plausible to predict what the fuck those crazy meatbags will do.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1514915169.0
:DateShort: 2018-Jan-02
:END:

*************** u/sparr:
#+begin_quote
  We're talking about a world where the cars themselves have the potential capability of making moral judgements about the worth of potential victims. That's decades in the future. It's certainly not going to happen in my lifetime.
#+end_quote

It's happening /right now/. Watch any of the publicly released tech talks on Google's self driving cars, which can identify pedestrians as a separate class of obstacle from cyclists, trees, cars, etc. Those cars /already/ have to decide whether to go straight and hit two pedestrians or swerve and hit one cyclist; they just aren't very good at it yet.

Self driving cars /today/ are smarter than most people think they will be 10-20 years from now. They are already safer than humans in clear weather. This is an industry where the state of the art is advancing so fast that it's years to decades ahead of where normal people think it is.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514916322.0
:DateShort: 2018-Jan-02
:END:

**************** u/ArgentStonecutter:
#+begin_quote
  Those cars already have to decide whether to go straight and hit two pedestrians or swerve and hit one cyclist;
#+end_quote

[citation required]

People keep making claims like this, I ask for actual situations, and get nothing.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1514917940.0
:DateShort: 2018-Jan-02
:END:

***************** I'm not sure what you mean by "actual situations". Does this need to happen, once or more than once, for you to believe it's a thing that will happen and the software will have to deal with?
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514926574.0
:DateShort: 2018-Jan-03
:END:

****************** Give me a credible scenario whereby a properly programmed AV will get in such a situation where (for example) braking or swerving will each injure someone, but they still have the luxury to choose, AND they shouldn't have avoided it by taking other actions before the critical moment.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1514935450.0
:DateShort: 2018-Jan-03
:END:

******************* I have to agree with you here. The absolutely key point is "could not have avoided it by taking other actions before."

A lot of conversation I've heard along these lines seems to presuppose that the car was driving in an unsafe manner beforehand. Your vehicle won't ever get impatient when it's driving itself, so why would it not have realized that some event has occurred which will limit its ability to react and acted to reduce the danger ahead of time?

It only seems to make sense if a human drove recklessly for some time then randomly remanded control to the autopilot seconds before some untenable situation.

Just like I learned in Driver's Ed anyway, the answer is almost always to hit the brakes. Reducing the amount of energy in the system as quickly as possible will reduce the amount of damage done to every party involved.
:PROPERTIES:
:Author: Jiopaba
:Score: 1
:DateUnix: 1514940574.0
:DateShort: 2018-Jan-03
:END:


******************* Clear lane ahead of you. Cars to your left and right, and to the left and right ahead of you. A human falls out of one of the cars ahead of you into traffic. Do you hit that human, or do you swerve into an adjacent car? Of course^{*} you start braking immediately regardless, but you're still going to hit the human if you don't swerve.

^{* actually you might not brake if you're going to swerve. That's a complicated question. Let's just assume you do.}
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514950770.0
:DateShort: 2018-Jan-03
:END:

******************** I've seen videos of what AIs "see" and even humans standing upright flicker in and out of "visibility". A human lying on the road would likely be treated as an obstruction.

If you are in a four lane boulevard you change lanes safely because you were already ensuring that there was no vehicle next to you. If you are in a two lane road, you swerve into the shoulder. If you're in a two lane road with parked cars, you brake safely because you were already driving slowly in case someone darted out from between two cars.

If Waymo is not following this protocol, they will after a few accidents, just as Tesla has had to add restrictions to their advanced cruise control. The only reason reason for an AV to drive anything but extremely conservatively is marketing, and that's not good enough for the law.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1514959992.0
:DateShort: 2018-Jan-03
:END:

********************* u/sparr:
#+begin_quote
  because you were already ensuring that there was no vehicle next to you
#+end_quote

You don't have [entire] control over this.

#+begin_quote
  you brake safely because you were already driving slowly in case someone darted out from between two cars.
#+end_quote

No, you weren't. A person can dart out arbitrarily close to you. There is no speed at which you can drive that ensures you can stop before hitting a person that darts out. 10mph is "too fast", and cars in that situation are expected to drive 20-30mph in regular conditions.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1515001110.0
:DateShort: 2018-Jan-03
:END:

********************** u/ArgentStonecutter:
#+begin_quote
  #+begin_example
    because you were already ensuring that there was no vehicle next to you
  #+end_example

  You don't have [entire] control over this.
#+end_quote

Even assuming the AV recognizes the obstacle as a human, the probability of the rare situation where /two/ asshole humans decide to box you in /at the exact moment/ someone "falls out of a car"* in front of you /and/ there is light enough traffic that the car behind you isn't going to run them over anyway, is basically so close to zero that it's ignorable. It's a black swan, or as [[/u/stale2000]] called it, a zebra. Any code that's designed to deal with situations like this is more likely to cause crashes through false positives than save lives.

EVERY actual "trolley problem" scenario anyone has EVER presented me is just total bullshit like this.

^{* And how often does /that/ happen? I've been driving for 35 years and I've yet to encounter it}
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515001695.0
:DateShort: 2018-Jan-03
:END:

*********************** u/sparr:
#+begin_quote
  And how often does that happen? I've been driving for 35 years and I've yet to encounter it
#+end_quote

This sentence makes it obvious that you don't understand how unimportant your personal experience is when dealing with hundreds of millions of other people's situations.
:PROPERTIES:
:Author: sparr
:Score: 2
:DateUnix: 1515010249.0
:DateShort: 2018-Jan-03
:END:

************************ OK, snowflake, now address the actual content without pearl-clutching over the side comment that triggered you.

#+begin_quote
  #+begin_example
    because you were already ensuring that there was no vehicle next to you
  #+end_example

  You don't have [entire] control over this.
#+end_quote

Even assuming the AV recognizes the obstacle as a human, the probability of the rare situation where /two/ asshole humans decide to box you in /at the exact moment/ someone "falls out of a car" in front of you /and/ there is light enough traffic that the car behind you isn't going to run them over anyway, is basically so close to zero that it's ignorable. It's a black swan, or as [[/u/stale2000]] called it, a zebra. Any code that's designed to deal with situations like this is more likely to cause crashes through false positives than save lives.

EVERY actual "trolley problem" scenario anyone has EVER presented me is just total bullshit like this.

Even with hundreds of millions of vehicles, this is an impossibly rare condition. Every step is rare. You need heavy enough traffic that two people box the vehicle in from either side despite software that is actively avoiding that situation. But it has to be light enough traffic that even if it avoids hitting the person they won't get killed by the vehicle behind them. AND just as this happens, someone falls out of a car in a way that is completely unpredictable (that is, for example, it's not kids in a truck bed because the AV would have avoided that as soon as it detected it). AND on top of all that, the AV has to be able to predict that the asshole human they decide to sideswipe will give way on being sideswiped instead of, for example, swerving crossways and hitting the human and a couple of other cars and creating a pileup.

No, it's not a credible scenario. It's bullshit. You know it's bullshit or you would have defended it.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515013126.0
:DateShort: 2018-Jan-04
:END:


********************** u/ArgentStonecutter:
#+begin_quote
  No, you weren't. A person can dart out arbitrarily close to you. There is no speed at which you can drive that ensures you can stop before hitting a person that darts out. 10mph is "too fast", and cars in that situation are expected to drive 20-30mph in regular conditions.
#+end_quote

Probably 5 MPH, car-park speed. Slow enough that a collision is unlikely to cause injury. /Properly programmed/ AV should generally avoid situations like that and route around them, except for the "final stretch" where they mosey on in.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515002101.0
:DateShort: 2018-Jan-03
:END:

*********************** u/sparr:
#+begin_quote
  except for the "final stretch" where they mosey on in.
#+end_quote

Any "except" means you still need to write the code in question.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1515010166.0
:DateShort: 2018-Jan-03
:END:

************************ Five. Miles. An. Hour.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515013050.0
:DateShort: 2018-Jan-04
:END:

************************* Our society has already decided that 20-30mph is an acceptable speed for that scenario, despite the risks.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1515025339.0
:DateShort: 2018-Jan-04
:END:

************************** Humans have this thing about risk. They have a level of risk they're comfortable with. If the risk level is too low they take more risks. Autonomous vehicles don't have that problem. And they don't get bored and in a hurry. So fuck society.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515028236.0
:DateShort: 2018-Jan-04
:END:

*************************** u/sparr:
#+begin_quote
  Autonomous vehicles [...] don't get bored and in a hurry.
#+end_quote

The hurry is up to the humans, not the vehicles. If you tell people they can only have AVs if they are willing to double the length of their commute, that's going to be a much harder sell than current pitches that involve reducing times.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1515028762.0
:DateShort: 2018-Jan-04
:END:

**************************** If more than a tiny fraction of their commute is down residential parking streets, sucks for them. I don't think that's going to be an issue for most people. Most people spend most of their commute at least on multi-lane surface streets, or dual carriageways and limited access freeways and tollways... and those are so easy for AV that even Tesla's misnamed "Autopilot" can handle them.

The main pitch for self driving cars, though, is not "saving time", it's "not wasting the time you spend on your commute". You can read, work, watch TV, and pretty soon you're not caring so much how long it takes.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515030617.0
:DateShort: 2018-Jan-04
:END:

***************************** u/sparr:
#+begin_quote
  If more than a tiny fraction of their commute is down residential parking streets, sucks for them
#+end_quote

You obviously don't live in a city where the majority of streets are "residential parking streets". Some/many major arterial roads in San Francisco and Chicago and various other cities have residential parking along both sides.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1515035970.0
:DateShort: 2018-Jan-04
:END:

****************************** I'm guessing that would be why Waymo isn't doing their testing in San Francisco or Chicago.

Still waiting for a vaguely credible "trolley problem" scenario.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1515056759.0
:DateShort: 2018-Jan-04
:END:


*************** u/sparr:
#+begin_quote
  Obviously while human drivers are common AV will be limited to things like limited access highways, low-speed pedestrian plazas, major streets without parking, and other places where it's plausible to predict what the fuck those crazy meatbags will do.
#+end_quote

This is not only not "obvious", it's also not true. AV are already operating on city streets with parking and pedestrians and other unpredictable situations.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514916614.0
:DateShort: 2018-Jan-02
:END:

**************** u/ArgentStonecutter:
#+begin_quote
  AV are already operating on city streets with parking and pedestrians and other unpredictable situations.
#+end_quote

[citation required]

Particularly, fully autonomous vehicles operating fully autonomously outside a test situation, without backup, on ordinary roads that haven't been preselected for safety.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1514918060.0
:DateShort: 2018-Jan-02
:END:

***************** [[https://www.wired.com/story/waymo-google-arizona-phoenix-driverless-self-driving-cars/]]

(and a hundred other articles about Waymo's operations in Arizona)

They already have fully autonomous vehicles on the road with no human backup driver, driving around cities and neighborhoods with no specific road pre-selection.

They are prepping to launch a cab-like service.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1514926537.0
:DateShort: 2018-Jan-03
:END:

****************** u/ArgentStonecutter:
#+begin_quote
  Waymo hasn't disclosed how much territory its cars will cover or what kind of hours they will run, whether it will charge passengers for rides, or the timeline for announcing or figuring out any of that.

  Starting in Arizona lets Waymo dodge some of these questions, at least for now. The weather is good, the roads aren't too crazy or complicated
#+end_quote

I've been to Phoenix. The city is a pure grid, except for one monolith-like mountain, and the residential areas all have wide streets, little or no street parking, and large yards that are not conducive to small children playing on them because they're all rocks and cactuses. The grid makes routing around schools and play areas trivial.

They picked a whole city that's "safe streets" and they're not disclosing how much of the city they're allowing the vehicles to use. They also don't mention how fast they're going to drive in obstructed areas.

The idea that they're not pre-selecting the range of the cars to avoid dodgy areas is ludicrous.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1514935279.0
:DateShort: 2018-Jan-03
:END:


** Well, obviously the most ethical choice is to kill the five imps, you can even switch the teleport and switch it back so killing the five imps was an active action on your part.

They're imps, you gotta kill 'em. He should have used NPC marines instead.
:PROPERTIES:
:Author: muns4colleg
:Score: 4
:DateUnix: 1514760888.0
:DateShort: 2018-Jan-01
:END:
