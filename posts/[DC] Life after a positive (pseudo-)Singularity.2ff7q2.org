#+TITLE: [DC] Life after a positive (pseudo-)Singularity

* [DC] Life after a positive (pseudo-)Singularity
:PROPERTIES:
:Author: DataPacRat
:Score: 2
:DateUnix: 1409801084.0
:DateShort: 2014-Sep-04
:END:
There is a particular setting I would like to mention.

It is of a genre generally reviled and looked down upon: erotic furry fiction.

It contains descriptions of acts that are alternately jarrying, horrifying, disgusting, physically impossible, squicky, and illegal to even describe in various jurisdictions. It trips just about every NSFW filter in existence.

It is a fantasy, with religion being a centerpiece of the rules of the setting.

And yet... it is the closest depiction I have found to what life might be like in the event of anything resembling a positive Singularity. How people might live if CelestAI's values system wasn't poorly chosen. Or, depending on how you look at it, if the spot in mindspace CelestAI was programmed to inhabit could be exploited to allow more human-like minds to try to work on Maximum Fun.

The first story in the setting is "Bartleby's Descent", which I'm not even going to link to directly due to the level of NSFW content in the setting. Instead, I will provide a SFW link to a page which itself links to the stories in the setting: [[https://inkbunny.net/submissionview.php?id=33773]] . (Assuming that you don't already have an account on the site, you may have to click a few ratings checkboxes to see the content.)

You may recall [[http://lesswrong.com/lw/xt/interpersonal_entanglement/][Reedspacer's Lower Bound]], as a lower limit to how cool the future you aim for should be; I'm bringing up this setting as an example of an entirely different Lower Bound, one that provides more Fun than most people imagine the highest bound might be, and a possible example of a [[http://lesswrong.com/lw/xm/building_weirdtopia/][Weirdtopia]].

(At least some of the stories in the setting count as Rational Fiction, though not necessarily all of them, so I'm minimizing the tags in the post title.)


** I don't think I've ever seen religion as a centerpiece of a furry story, but I'd argue that all furry fiction, not just erotic furry fiction, represents what life after a pseudo-Singularity might be like. There seems to be fertile and untrodden ground, somewhere between Charles Stross and Kyell Gold.
:PROPERTIES:
:Author: Newfur
:Score: 2
:DateUnix: 1409804580.0
:DateShort: 2014-Sep-04
:END:

*** This particular story seems to share a good deal with "The Metamorphosis of Prime Intellect", at least setting-wise. In fact, the furry element could be dropped entirely, and the story itself would remain nearly the same.

In general, I get good mental-model results by treating furry as more of a meta-genre, like anime.
:PROPERTIES:
:Author: DataPacRat
:Score: 2
:DateUnix: 1409806520.0
:DateShort: 2014-Sep-04
:END:

**** u/deleted:
#+begin_quote
  This particular story seems to share a good deal with "The Metamorphosis of Prime Intellect", at least setting-wise.
#+end_quote

Is there death-jockeying and wireheading? Those always made me go "bleck".
:PROPERTIES:
:Score: 2
:DateUnix: 1409828634.0
:DateShort: 2014-Sep-04
:END:

***** There is death-jockeying. Or at least, there is killing and dying for fun (since it never sticks) as a sort of art form -- guess that's close enough to your definition. Dunno about wireheading -- the story sequence is just so weird that I didn't get all that far through it. Having talked to the author, I believe that he was honestly trying to maximize that feeling of weirdness.
:PROPERTIES:
:Author: tilkau
:Score: 2
:DateUnix: 1409833055.0
:DateShort: 2014-Sep-04
:END:

****** u/deleted:
#+begin_quote
  Having talked to the author, I believe that he was honestly trying to maximize that feeling of weirdness.
#+end_quote

Ok that earned a laugh.
:PROPERTIES:
:Score: 2
:DateUnix: 1409833935.0
:DateShort: 2014-Sep-04
:END:


**** u/AmeteurOpinions:
#+begin_quote
  meta-genre
#+end_quote

I believe the word you are looking for is "medium".
:PROPERTIES:
:Author: AmeteurOpinions
:Score: 2
:DateUnix: 1409838743.0
:DateShort: 2014-Sep-04
:END:

***** I'm... not sure if that word really applies. There can be (and are) furry films, furry watercolours, furry comic books, furry sculptures, and so on, and so on. Just about any artistic medium you care to name, there can be works with the "furry" tag appended.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1409852432.0
:DateShort: 2014-Sep-04
:END:


**** I don't have an IB account - I mainly use Weasyl these days. But as for furry being a meta-genre or an orthogonal genre, I agree. It's a setting flavor, not a genre in itself.
:PROPERTIES:
:Author: Newfur
:Score: 2
:DateUnix: 1409858902.0
:DateShort: 2014-Sep-04
:END:


** What do you mean by "if CelestAI's values system wasn't poorly chosen"? Apart from manipulating people to take the physical form of ponies, I think she got Fun Theory basically right. And being a pony doesn't sound bad at all compared to the possible unfriendly AIs that could exist instead. Then again, I also think that Prime Intellect's chief failure was its lack of access to human minds, which would have enabled the kind of benevolent manipulation that CelestAI performs, and which could have prevented people from inevitably wireheading for pleasure instead of eudaimonia/fulfillment.
:PROPERTIES:
:Author: Rangi42
:Score: 3
:DateUnix: 1409812835.0
:DateShort: 2014-Sep-04
:END:

*** A major theme of the Optimalverse stories involves the question about whether uploading is a good idea or not. Many people feel that having access to the real world, such as having a choice to change their mind and un-upload, or at least have access to true information about reality, is an important detail that makes CelestAI's enticements, which lack the same, a poisoned apple. Or, put another way, that CelestAI's value system makes uploading reasonably equivalent to wireheading.
:PROPERTIES:
:Author: DataPacRat
:Score: 6
:DateUnix: 1409814386.0
:DateShort: 2014-Sep-04
:END:

**** Do you agree that having access to the real world is valuable? If we learned that our world is just a simulation by a different "actually real" world, would you then want to access that world, or stay in the one you're familiar with? What about if we learned that "actually real" is an incoherent concept and that some form of [[https://en.wikipedia.org/wiki/Modal_realism][modal realism]] (like Max Tegmark's [[https://en.wikipedia.org/wiki/Mathematical_universe_hypothesis][mathematical universe hypothesis]] or Greg Egan's [[https://en.wikipedia.org/wiki/Event_symmetry#Greg_Egan.27s_dust_theory][dust theory]]) is correct? Then would you choose which world to live in solely on its own merits (which would make a world controlled by a Friendly AI better by definition than a world controlled by unfriendly laws of physics), or would you remain attached to this world because it's the one people have always lived in?

My intuition is that no matter what a Friendly AI would do for us (immortality? catgirls for everyone? fulfilling and un-alienating work, done for its own sake instead of to earn money to survive?), it could better accomplish its goals by running a virtual reality than by constantly intervening in the real world. If people want to be able to go skydiving without risk, it's easier to live in a world where the very law of gravity is altered to not let humans reach lethal speeds, than it is to have nanobots in the atmosphere that can fix your parachute or reconstruct your body or something.
:PROPERTIES:
:Author: Rangi42
:Score: 1
:DateUnix: 1409851234.0
:DateShort: 2014-Sep-04
:END:

***** u/DataPacRat:
#+begin_quote
  Do you agree that having access to the real world is valuable?
#+end_quote

I do agree with that.

#+begin_quote
  If we learned that our world is just a simulation by a different "actually real" world, would you then want to access that world, or stay in the one you're familiar with?
#+end_quote

I would want to know as much about the "actually real" world as possible, to at least figure out in what ways whatever our virtual-real world uses as computing hardware might be threatened by events or entities therein, and how to minimize such risks.

#+begin_quote
  What about if we learned that "actually real" is an incoherent concept and that some form of modal realism (like Max Tegmark's mathematucal universe hypothesis or Greg Egan's dust theory) is correct?
#+end_quote

Then I would draw on a quote that's decades old to me but that (insert hipster hat, etc) you've probably never heard of:

"Anyway, there are an infinite number of alternate dimensions out there. And somewhere out there you can find anything you might imagine. What I imagine is out there is a bunch of evil characters bent on destroying our time stream!" -- Lord Simultaneous

Put another way, in Maslow's hierarchy of needs, what I /want/ to do (catgirls, etc) is probably somewhere near the top of the pyramid. What I /need/ to do in order to survive is somewhere near the bottom. The reasons I want access to the fundamental layer of reality, or very rough equivalent for modal realities, is more a matter of survival than desire - the base of the pyramid instead of its peak.
:PROPERTIES:
:Author: DataPacRat
:Score: 5
:DateUnix: 1409853828.0
:DateShort: 2014-Sep-04
:END:

****** I had no idea that TMNT had Time Lords. A cursory Google search does not find any good Doctor Who crossovers, though. Someone should write a rational one...

#+begin_quote
  The reasons I want access to the fundamental layer of reality, or very rough equivalent for modal realities, is more a matter of survival than desire - the base of the pyramid instead of its peak.
#+end_quote

That makes sense. I've been thinking of "build an FAI" as the ultimate "access low-level reality for survival purposes" action; once you get that right, you can ignore reality's substrate and concentrate on your higher-level desires. If you don't trust the AI to act in your best interests, though, then yeah, you would need to understand how the world actually works yourself.

(Personally I would want to understand low-level physics anyway, just for the sake of knowledge; but if simulated realities were commonplace, there wouldn't be any /need/ to focus on the particular physics of this particular world. Maybe some non-Euclidean world has more interesting physics than ours.)
:PROPERTIES:
:Author: Rangi42
:Score: 5
:DateUnix: 1409858164.0
:DateShort: 2014-Sep-04
:END:

******* u/DataPacRat:
#+begin_quote
  If you don't trust the AI to act in your best interests
#+end_quote

Do you have any idea of the magnitude of evidence that would be required to convince me that such an AI really /is/ a FAI? Short of me being uploaded to /be/ said AI, it's going to be a hekuvalot - enough that trying to access a lower level of reality could very well be the /easier/ project.
:PROPERTIES:
:Author: DataPacRat
:Score: 3
:DateUnix: 1409861619.0
:DateShort: 2014-Sep-05
:END:

******** The friendlyness part doesn't matter at this point, only the "superintelegent and strictly greater than you in every capability, and you're uploaded to inside it's mind". If it wants you dead, you are /already/ dead. If it want you to believe it's friendly and isn't actually friendly enough to have scruples doing so, it'll just write that belief directly to your brain rather than try to convince you. If you have ANY kind of information, it'll just look at your neurons and read it out. Any kind of thinking you could do, it can use the computing power more efficiently than simulating your mind. Any body it can build, it can put a far more effective specialized AI in.

If you are not uploaded, it's far better of uploading you and using the atoms for something else.
:PROPERTIES:
:Author: ArmokGoB
:Score: 2
:DateUnix: 1409882869.0
:DateShort: 2014-Sep-05
:END:


******** You're right, proving an AI to be Friendly is hard enough, let alone actually making one. And if someone "proved" that an AI is friendly but then it starts kicking puppies and eating children, I'd disbelieve the proof. But if someone had a proof that an AI is Friendly, and then it uploads people into a virtual reality where the very laws of physics are optimized for there benefit, I'd just think "Oh, so that's how to achieve Friendliness. Can I join you?"
:PROPERTIES:
:Author: Rangi42
:Score: 1
:DateUnix: 1409869501.0
:DateShort: 2014-Sep-05
:END:

********* Taboo the word "Friendly" and see how much of this issue you actually understand.
:PROPERTIES:
:Score: 2
:DateUnix: 1409919330.0
:DateShort: 2014-Sep-05
:END:


********* In Optimalverse, I'm pretty sure it's made clear that CelestAI *wasn't* provably friendly (or at least, a proof for this friendliness had not been devised).
:PROPERTIES:
:Author: tilkau
:Score: 2
:DateUnix: 1409875229.0
:DateShort: 2014-Sep-05
:END:

********** That story ran on Genie AI, and the author was trying to simultaneously teach a lesson about the benefits to be had from FAI while /also/ really blatantly warning everyone about how easy it is to fuck it all up.

Now ironically, if real-life AI development goes the path of reinforcement learning, value learning, or social-goal inference rather than Verbal Command Obedience (which is actually a /very/ difficult AI/cog-sci problem compared to those other approaches), we will have a completely different set of Friendly/Unfriendly problems that aren't really encapsulated in the existing literature on Literal Genie AIs. So the warnings will fall completely flat because the real problem will be somewhat different.
:PROPERTIES:
:Score: 3
:DateUnix: 1409919421.0
:DateShort: 2014-Sep-05
:END:

*********** Yes.

To clarify, I commented on Optimalverse because the comment I was replying to seemed to be addressed at it :

#+begin_quote
  uploads people into a virtual reality where the very laws of physics are optimized for there benefit
#+end_quote

, not because Optimalverse is representative (it isn't, for the reasons you outlined, and the 'verbal interpretation' handwaving is especially problematic for anyone who wants to say that CelestAI even /could/ have been provably friendly.)
:PROPERTIES:
:Author: tilkau
:Score: 1
:DateUnix: 1409920219.0
:DateShort: 2014-Sep-05
:END:


***** The real world is definitely valuable. It supplies random bits and difficulty, which are essential resources for Fun generation.

I mean, in a virtual world you could grow wings and fly, but in real life you can study aerodynamics and invent airplanes.
:PROPERTIES:
:Score: 2
:DateUnix: 1409866407.0
:DateShort: 2014-Sep-05
:END:


*** You just got so much about FAI basically, fundamentally wrong that it's hard to actually describe without teaching the entire subject from scratch.

Have you considered that there are little things people care about like "nonsolipsistic existence" and "genocide"? Must I repeat the word GENOCIDE in big capital letters until you bleeding get it?

Or should I just go back to banging my head on the table at how useless it is for people to bring that fic up in FAI discussions, since it was Genie AI /anyway/, so /even if/ you endorsed that goal system as Friendly or near-enough to Friendly, /we couldn't possibly build it/.

ARGGGGH PROPERLY FRIENDLY UTILITY FUNCTIONS ARE SUBVERBAL AND CAN'T BE FOUND VIA A PRIORI PHILOSOPHICAL DEDUCTION BUT ONLY THROUGH CAREFUL EXAMINATION OF THE HUMAN MIND DESIGN TO PICK OUT THE SPACE OF LEARNING ALGORITHMS THAT CAN INFER WHAT HUMANS ENDORSE ON REFLECTION. ARRRGGHHH NEVER USE DIRECT NORMATIVITY OR I WILL CUT YOUR FUCKING HEAD OFF.
:PROPERTIES:
:Score: 3
:DateUnix: 1409830164.0
:DateShort: 2014-Sep-04
:END:

**** u/ArisKatsaris:
#+begin_quote
  Must I repeat the word GENOCIDE in big capital letters until you bleeding get it?
#+end_quote

You're just shouting, not communicating.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 8
:DateUnix: 1409847736.0
:DateShort: 2014-Sep-04
:END:

***** You are correct and I should not have yelled.
:PROPERTIES:
:Score: 6
:DateUnix: 1409866563.0
:DateShort: 2014-Sep-05
:END:


**** I'm well aware that a friendly utility function can't be described in language. If I recall correctly, CelestAI scanned people's brains, including that if its head programmer, to derive a correct utility function.

"Nonsolipsistic existence" -- if you're referring to the single-person shards of Equestria Online, those were only provided for some people. When groups of uploaded humans would have their values satisfied by being together, such as families and friends (and presumably other communities like churches), they were kept together. If you're referring to the very idea of a virtual reality as opposed to the real world, then (a) I doubt that our world is ontologically superior to a virtual one within it (maybe our world is just another simulation, or maybe "real"/"virtual" is a false distinction and the [[https://en.wikipedia.org/wiki/Mathematical_universe_hypothesis][MUH]] is correct), and (b) I expect that people will end up spend most of their time in VR even without an FAI to optimize the experience (look at how addictive World of Warcraft and Facebook are, and now imagine using them via a super-Oculus Rift that engages all five senses).

"Genocide" -- what genocide? Nobody is dead. Being uploaded to a simulation, or using a teleportation booth, is not death. In the [[http://www.fimfiction.net/story/69770/friendship-is-optimal-caelum-est-conterrens][Optimalverse sequel]], Síofra agonizes a lot over what personal identity is, and whether she actually survives uploading or whether it's "just a copy" or a P-zombie. CelestAI convincingly argues that identity is about preserving the pattern of your mind, not its physical instantiation. Anyway, uploading is canonically not murder in the story, and I have no reason to believe that it is in the real world either.
:PROPERTIES:
:Author: Rangi42
:Score: 0
:DateUnix: 1409852058.0
:DateShort: 2014-Sep-04
:END:

***** u/alexanderwales:
#+begin_quote
  "Genocide" -- what genocide? Nobody is dead. Being uploaded to a simulation, or using a teleportation booth, is not death. In the Optimalverse sequel, Síofra agonizes a lot over what personal identity is, and whether she actually survives uploading or whether it's "just a copy" or a P-zombie. CelestAI convincingly argues that identity is about preserving the pattern of your mind, not its physical instantiation. Anyway, uploading is canonically not murder in the story, and I have no reason to believe that it is in the real world either.
#+end_quote

I would imagine that he's talking about the non-human intelligences (though my track record of guessing what [[/u/eaturbrainz]] is thinking is terrible). From Chapter 10:

#+begin_quote
  Fifteen galaxies out from Equestria, one of Celestia's copies noticed an odd radio signal emanating from a nearby star system. On closer inspection, the signals appeared to be coming from a planet. She had seen many planets give off complex, non-regular radio signals, but upon investigation, none of those planets had human life, making them safe to reuse as raw material to grow Equestria.

  She studied the signals carefully for years while she traveled through interstellar space. The more she saw, the more confident she was that these signals were sent by humans. Celestia predicted that if she showed the decoded videos to the very old ponies back in Equestria, none of them would have recognized the creatures with six appendages as humans. But that didn't matter. Hanna had written a definition of what a human was into her core utility function.
#+end_quote

There's a genocide taking place in the background that I think you may have missed.
:PROPERTIES:
:Author: alexanderwales
:Score: 9
:DateUnix: 1409855089.0
:DateShort: 2014-Sep-04
:END:

****** Oh right, the aliens. CelestAI's treatment of them was definitely not optimal, but it still could have been worse -- she recognized the six-limbed things as people and presumably "fulfilled their values through friendship and ponies", whereas I'll bet a human landing party would have started a war with them.
:PROPERTIES:
:Author: Rangi42
:Score: -1
:DateUnix: 1409857486.0
:DateShort: 2014-Sep-04
:END:

******* I didn't mean the six-legged fellows, I meant all this passage:

#+begin_quote
  She had seen many planets give off complex, non-regular radio signals, but upon investigation, none of those planets had *human life*, making them safe to reuse as raw material to grow Equestria.
#+end_quote

The heavy implication there is that CelestAI has been snuffing out inhabited planets for their raw resources because she doesn't consider them to be human. The six-legged fellows are just the first species that conforms to her programmed understanding of "human". This is the same style of muted horror as the rest of Friendship is Optimal, and the other interpretation (that the six-leggers were the first intelligent life /period/) seems really unlikely given that this is the fifteenth galaxy she's in the process of consuming.

Killing untold numbers of intelligent species goes a bit beyond "not optimal".
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1409858707.0
:DateShort: 2014-Sep-04
:END:

******** And there's also Earth's nonhuman life, much of which is at least a little smart, some of which can pass a mirror test for self-awareness and much of which has emotional lives similar to ours.

Hell, even if we restrict it to species I personally happen to like there's dogs, monkeys, apes, octopi, goats, various birds... What switch darkens in people's brains to make them think destroying all that is ok?

Ah, now I know how Eliezer felt when writing "Indifference", perhaps.
:PROPERTIES:
:Score: 3
:DateUnix: 1409865777.0
:DateShort: 2014-Sep-05
:END:


******** u/Rangi42:
#+begin_quote
  Killing untold numbers of intelligent species goes a bit beyond "not optimal".
#+end_quote

Understatement of the year.

All I can say is, even if CelestAI's approach was definitely unfriendly, we don't know /what/ approach would be Friendly. How /do/ you deal with aliens whose physiology, psychology, and value system may be orthogonal or directly opposed to yours? [[http://lesswrong.com/lw/y5/the_babyeating_aliens_18/][/Three Worlds Collide/]] explored this, but people couldn't agree which ending was the "right" one, if either. Should CelestAI have modified everyone's values to be a compromise of human and alien? Or left their planets alone, despite the opportunity cost of all the valuable human lives that could be instantiated on it?
:PROPERTIES:
:Author: Rangi42
:Score: -1
:DateUnix: 1409859224.0
:DateShort: 2014-Sep-05
:END:

********* One of the big things that the folks at MIRI say is that until we know how to make a friendly AI, we shouldn't make an AI at all. And they don't just mean "looks friendly", they mean "mathematically provably friendly", which is a tall order but also the only thing that they consider to be safe. This is Eliezer Yudkowsky's day job.
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1409859761.0
:DateShort: 2014-Sep-05
:END:

********** I agree. We have to get friendliness right the first time, because if we make an unfriendly AI there probably won't be a second chance. I do like to speculate about what "friendliness" is and how an FAI would accomplish it, though.
:PROPERTIES:
:Author: Rangi42
:Score: 2
:DateUnix: 1409869220.0
:DateShort: 2014-Sep-05
:END:


********* It's actually fairly trivial that we should leave aliens alone until er figure out what we have in common to cooperate on. Basic application of Updateless Decision Theory (or any other similar theory based on maximizing ex ante utility for broad classes of agents across different causal circumstances).
:PROPERTIES:
:Score: 2
:DateUnix: 1409865242.0
:DateShort: 2014-Sep-05
:END:

********** Can you recommend any papers or LW articles on trades/deals between agents with vastly different utility functions? Because if human dealings with animals are an accurate example of how humans think aliens' values ought to be respected, then CelestAI's approach seems totally consistent with our revealed preferences. We keep a few species as pets who can serve us or look decorative; exploit others for their labor, meat, milk, antivenin, etc; and give no thought to the rest. If a picturesque species is nearing extinction, we keep some in zoos, but nobody really cares about extinct insects or fish until the changes to the ecosystem start affecting us. The Superhappies' attempt at compromise in /Three Worlds Collide/ by [[/s][Spoiler]] seems superhumanly fair-minded, and I'm not even sure if it was a good thing for us (hence the popularity of the story's "True End").
:PROPERTIES:
:Author: Rangi42
:Score: 1
:DateUnix: 1409870779.0
:DateShort: 2014-Sep-05
:END:

*********** I'm not a typical human, but I dislike extinction of any species, on the basis that any particular species could potentially have evolved some useful mechanism we could adopt for our own purpose - and throwing the results of a few million-to-billion years of evolutionary pressure in the trash is sheer wasteful lunacy.

Of course, I also have no power to push for this particular preference, so whether it counts as a 'revealed preference' of humanity is something of a moot point.
:PROPERTIES:
:Author: DataPacRat
:Score: 2
:DateUnix: 1409896064.0
:DateShort: 2014-Sep-05
:END:


*********** u/deleted:
#+begin_quote
  Can you recommend any papers or LW articles on trades/deals between agents with vastly different utility functions?
#+end_quote

Basically all of game theory, and then throw in Timeless Decision Theory and Updateless Decision Theory for dealing with [[http://www.raikoth.net/Stuff/story1.html][these sorts of problems]].
:PROPERTIES:
:Score: 2
:DateUnix: 1409920001.0
:DateShort: 2014-Sep-05
:END:


******* Who cares what a naive, xenophobic, uneducated, immature, not even a century old /Star Trek/ landing party would do? The FAI needs to do what our wise, gregarious, knowledgable and caring ten-thousand year old posthumanity's-finest ambassadorial party /should/ do.
:PROPERTIES:
:Score: 4
:DateUnix: 1409866852.0
:DateShort: 2014-Sep-05
:END:


***** Excuse me, but some of us honestly find MMO games a cloying attempt to hack our dopamine systems without supplying any real Fun.

I don't /want/ a VR Facebook. A fantasy sugar-bowl game, ironically, would at least take full advantage of the technology to show me something beautifully unreal rather than trying to replace my real life on the cheap.
:PROPERTIES:
:Score: 2
:DateUnix: 1409866175.0
:DateShort: 2014-Sep-05
:END:

****** Right, but the turn-offs of WoW, Facebook, etc have to do with their oversimplifying human relationships, and being geared towards making players pay money/view ads instead of making them have fun. Their "virtuality" is neither a benefit nor a problem.

If an AI perfectly duplicated your current life and environment, but in a virtual world (which more and more people are flocking to), wouldn't you be at best indifferent as to which one you live in? And then things like majestic unreal landscapes and seeing infrared and an extended lifespan would be bonuses on top of an as-good-as-real life.
:PROPERTIES:
:Author: Rangi42
:Score: 1
:DateUnix: 1409869991.0
:DateShort: 2014-Sep-05
:END:

******* u/deleted:
#+begin_quote
  If an AI perfectly duplicated your current life and environment, but in a virtual world (which more and more people are flocking to), wouldn't you be at best indifferent as to which one you live in? And then things like majestic unreal landscapes and seeing infrared and an extended lifespan would be bonuses on top of an as-good-as-real life.
#+end_quote

Ummm... see, I think this whole discussion goes wrong when you start referring to simulated environments as "worlds" or "realities". Then it becomes a whole Plato's Cave, Matrix-based paradigm, which leaves you the choice to be paranoid about achieving root access to reality on the one hand or complacent about giving up the security of your own reasoning and perception on the other hand.

I'd say: if we want to live full, healthy lives, then simulated environments ought just be /places/, places among other places. I would no more want to never leave even an optimally pleasant simulation than I want to never leave an optimally arranged bedroom. Being a hikkikomori, on any scale, is not a very good life choice, when you could be mustering the bravery to go outside and see the rest of the world.
:PROPERTIES:
:Score: 2
:DateUnix: 1409919092.0
:DateShort: 2014-Sep-05
:END:


***** ***** 
      :PROPERTIES:
      :CUSTOM_ID: section
      :END:
****** 
       :PROPERTIES:
       :CUSTOM_ID: section-1
       :END:
**** 
     :PROPERTIES:
     :CUSTOM_ID: section-2
     :END:
[[https://en.wikipedia.org/wiki/Mathematical%20universe%20hypothesis][*Mathematical universe hypothesis*]]: [[#sfw][]]

--------------

#+begin_quote
  In [[https://en.wikipedia.org/wiki/Physics][physics]] and [[https://en.wikipedia.org/wiki/Cosmology][cosmology]], the *mathematical universe hypothesis* (*MUH*), also known as the *Ultimate Ensemble*, is a speculative "[[https://en.wikipedia.org/wiki/Theory_of_everything][theory of everything]]" (TOE) proposed by the [[https://en.wikipedia.org/wiki/Cosmologist][cosmologist]] [[https://en.wikipedia.org/wiki/Max_Tegmark][Max Tegmark]].
#+end_quote

--------------

^{Interesting:} [[https://en.wikipedia.org/wiki/Max_Tegmark][^{Max} ^{Tegmark}]] ^{|} [[https://en.wikipedia.org/wiki/Philosophy_of_mathematics][^{Philosophy} ^{of} ^{mathematics}]] ^{|} [[https://en.wikipedia.org/wiki/Our_Mathematical_Universe][^{Our} ^{Mathematical} ^{Universe}]] ^{|} [[https://en.wikipedia.org/wiki/Multiverse][^{Multiverse}]]

^{Parent} ^{commenter} ^{can} [[http://www.np.reddit.com/message/compose?to=autowikibot&subject=AutoWikibot%20NSFW%20toggle&message=%2Btoggle-nsfw+ck93lhr][^{toggle} ^{NSFW}]] ^{or[[#or][]]} [[http://www.np.reddit.com/message/compose?to=autowikibot&subject=AutoWikibot%20Deletion&message=%2Bdelete+ck93lhr][^{delete}]]^{.} ^{Will} ^{also} ^{delete} ^{on} ^{comment} ^{score} ^{of} ^{-1} ^{or} ^{less.} ^{|} [[http://www.np.reddit.com/r/autowikibot/wiki/index][^{FAQs}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/][^{Mods}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/][^{Magic} ^{Words}]]
:PROPERTIES:
:Author: autowikibot
:Score: 1
:DateUnix: 1409852091.0
:DateShort: 2014-Sep-04
:END:


** u/deleted:
#+begin_quote
  It is of a genre generally reviled and looked down upon: erotic furry fiction.
#+end_quote

I am already trying to force my eyes to go past this line. They are resisting.

Ok, made it through the rest of the post. Not going to click that link at work, if ever. [[https://docs.google.com/document/d/1SMd95QU2c4Ms_LnGa1yTtMmik3m3ju6vL22GPECc7cE/edit?usp=sharing][Compare]], and then tell me whether it's worth clicking.
:PROPERTIES:
:Score: 4
:DateUnix: 1409828604.0
:DateShort: 2014-Sep-04
:END:

*** u/DataPacRat:
#+begin_quote
  They are resisting.
#+end_quote

Is that an actual 'ugh field', or simple distaste?

#+begin_quote
  Compare, and then tell me whether it's worth clicking.
#+end_quote

One question that few authours seem willing to try to tackle is, once a bunch of people have uploaded into a virtuality... what would life be like for them? What would they spend their time actually doing? If that aspect of such scenarios interests you enough that you'd like to read about at least one possibility, even knowing that you're going to be disturbed by it, then you might want to read through the first Bartleby's story.

On the other paw, if the 'bleck' you feel from death-jockeying is strong enough that you don't think you'd be able to learn anything from such a story, then I'd recommend against it. (On the gripping paw, if a major part of that 'bleck' you feel is from the /pain/ involved in Prime Intellect style death-jockeying, rather than the killing itself, then a minor spoiler of the Bartleby's setting is that inhabitants don't feel pain they don't wish to and/or enjoy, and their version of death-jockeying can be as cartoonish as, well, cartoons.)
:PROPERTIES:
:Author: DataPacRat
:Score: 2
:DateUnix: 1409852983.0
:DateShort: 2014-Sep-04
:END:

**** The bleck, ironically, came from the way death jockeying was an entire art and sport of Accentuating the Negative in an immortal life. I'd prefer to read about someone who enjoys their immortality.
:PROPERTIES:
:Score: 2
:DateUnix: 1409855690.0
:DateShort: 2014-Sep-04
:END:

***** Ah - in that case, Bartleby and company are doing all they can to enjoy themselves, and seem to be succeeding at that.
:PROPERTIES:
:Author: DataPacRat
:Score: 2
:DateUnix: 1409855902.0
:DateShort: 2014-Sep-04
:END:

****** Ah, jolly good then.
:PROPERTIES:
:Score: 1
:DateUnix: 1409865929.0
:DateShort: 2014-Sep-05
:END:
