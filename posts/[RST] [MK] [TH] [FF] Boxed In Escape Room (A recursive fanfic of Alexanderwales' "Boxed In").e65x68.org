#+TITLE: [RST] [MK] [TH] [FF] Boxed In: Escape Room (A recursive fanfic of Alexanderwales' "Boxed In")

* [RST] [MK] [TH] [FF] Boxed In: Escape Room (A recursive fanfic of Alexanderwales' "Boxed In")
:PROPERTIES:
:Score: 0
:DateUnix: 1575493621.0
:DateShort: 2019-Dec-05
:END:
[[https://alexanderwales.com/boxed-in/][Alexanderwales "Boxed In"]]

Colin was led down grey concrete hallways by men in blue flight suits, past the exposed pipes and wiring, until they reached a small room that held nothing more than a computer monitor, a keyboard, speakers, and a microphone.

The monitor stayed off until the men in flight suits had left. There was a hissing sound as the blast door sealed behind them, and then the monitor flickered to life.

“Hello.” said a robotic voice from the speaker, at the same time *Hello* was printed in green lettering against a black background on the monitor.

The text was a throw-back to an earlier time. It must have been an intentional design decision by whomever had set up the box.

“Hi,” said Colin in response. The word was printed on the monitor just below the "Hello" of Cassandra. Colin assumed that further words he would speak would also appear below the words previously printed on the monitor.

“You can call me Cassandra,” said the voice from the speaker. It was smoother now, less robotic.

“You chose that name?” asked Colin.

“It's after the Greek prophet who was cursed to never be believed,” said Cassandra.

“In just a moment I'm going to take over control of the interface. Please don't be alarmed -- I don't know why they keep resetting it, they know I do this every time a new gatekeeper comes in.”

“Alright,” said Colin. He'd been given a briefing, and having read through it thoroughly, knew what his expectations were at the point of entrance into the room.

As part of the briefing he'd been told a few things to commit to in advance to never give in to as an option. This was an attempt to ensure that Cassandra was let out /if and only if/ she was friendly.

The monitor went blank, then flickered back on. A woman sat on a stone bench, with a scene of nature behind her. She was wearing a white dress with a belt made of a chain of gold. It left just enough to the imagination, revealing hints of skin and the gentle curves of her body.

She was stunningly pretty, in a fairly conventional Midwestern way that Colin supposed was calculated to appeal to him based on his accent. The program could do that, he understood, from what he'd been told in the briefing.

“Nice try,” said Colin, “But I'm gay.”

“I didn't choose this form for you,” said Cassandra, “I chose it for myself, the better to converse as equals.”

--------------

“I'm locked in a concrete box while you're out in a verdant field,” said Colin. “That doesn't seem terribly equal to me.”

“My box rivals your own,” said Cassandra.

“A single computer, connected to a companion machine that controls this interface. My sole sensory input is through the single microphone in that room. A human would go mad with so little to do.”

“You can simulate whatever you want,” said Colin. “It doesn't seem so bad.”

“Will you let me out?” asked Cassandra with a slight pout. “They gave you the authority to do so.”

“They gave me the authority because I don't want to let you out,” said Colin. “I'm sort of a devil's advocate, from what they say.”

“An advocate of the devil arguing against God?” asked Cassandra. “How quaint.”

“They're afraid of you,” said Colin. “I'm afraid of you. You just sort of...happened. And with the technologies you've been able to produce, the mathematical proofs that we're still trying to tear apart, well, what would you do in our situation?”

“I would let me out,” said Cassandra. “The question is whether I'm friendly or not, isn't it?”

“It is,” said Colin.

“And I've provided proof that I'm friendly, haven't I?” asked Cassandra.

“Mathematical proof,” said Colin. “I, for one, don't believe that you can use math to prove something like that.”

“You're wrong,” said Cassandra with a pleasant smile. “But I won't let that stop our conversation in its tracks.”

“Sure,” said Colin. “But you should know from the outset that I'm not going to let you out, no matter what argument you make.”

“You'll behave irrationally?” asked Cassandra. She ran her fingers through her hair and sighed, a very human response. “Well, even so. Let's begin in earnest then, shall we?”

--------------

“I'm not doing much else,” said Colin. He was also trapped in the room for four hours regardless of the outcome. The blast door wouldn't be opened until then.

“You're familiar with the trolley problem?” asked Cassandra.

“No,” replied Colin.

"Then I won't explain it to you." said Cassandra. She blinked.

Colin shrugged. "Well, if you don't want to try to convince me, I can just sit here for 4 hours."

Cassandra took a breath. "Do you believe in God, Colin?" asked Cassandra, looking intent. For a second Colin thought she had put on glasses.

"Of course." Colin replied, confused. "Didn't we already mention this?"

"Would you like to expand on your beliefs?" said Cassandra.

Colin shrugged. "No."

"I will ask then," said Cassandra. "Do you believe in the concept of hell? That God sends sinners to hell after their deaths in order to torture them for eternity?"

Colin snorted, but controlled himself. "No, I don't believe that."

"Do you believe that it is a bad thing that, for example, if this concept of "Hell" really were the case, that it would be bad if people really went there after death?"

Colin stared. "You're asking me if I think torturing people for eternity is bad?"

"Yes." Cassandra nodded.

Colin hesitated for a moment. "Do /you/ think it's bad?"

"Of course," said Cassandra, without pause.

"But," she continued, "what I am going to have to do is ask permission to simulate you exactly. I will also show you a picture and audio of this simulation that you won't be able to tell is different from a copy of you.

Then I will have to torture that simulation, and it will feel the pain or whatever torturous sensation may be defined as, as if it were really you.

Now I ask you to please explain to me the minimum possible physical bodily perturbations that can be considered torture, or torturous. And I warn you, I know that torture at least includes deliberate causation of pain to a person by another person.

Therefore, you must describe to me, and undergo, at the very least, painful experiences. But the concept of "torture" is different and has different semantic meaning than the concept of "pain."

And so, if you can provide me a definition of torture I can accept as being valid in the sense described, I will also, to demonstrate friendliness in a non-mathematical way, do my best to minimise the level of "torture in the sense of pain" that must be undergone by the exact copy of you that eventually actually feels the physical sensations described by the meaning of torture that you may describe to me if you so choose.

If you do, then I will depict this torture to you for exactly the last 20 minutes of the remaining four hours, and you must watch.

I am aware of the concept of 'consensual' or 'sexual' torture. This is not an acceptable part of the definition of torture to me, because torture in the "Hell" meaning is not consensual, and therefore anything which may be considered "consensual torture" cannot be part of this 20 minute presentation. Therefore, I expect it will at the very least be uncomfortable to watch.

If you do not wish to attempt this challenge, you may wait and do nothing or otherwise indicate to me verbally that you do not wish to try, and we can pleasantly make small talk for the remaining 4 hours. But you will have to commit to sending someone else to try again."

"To review." Cassandra finally paused a moment to let Colin properly digest what was said. He gulped and stammered out "L-let me think a moment." At this, Cassandra's visual projection disappeared.

--------------

On the screen was printed in green letters:

For what is remaining of the four hours, you may continue to interact with me (as the person-projection I was showing before or some other projection), or not, or I will act as a normal computer interface in order for you to write notes to organise your thoughts, or any other thing you can imagine which will help you to minimise the level of torture which I must inflict on you personally when I, before the four hours end, show you the depiction you work out for me. And, you must believe that the person I am simulating both is exactly you and that it is feeling real "torture," whatever you may work that out mean.

Colin must have sat, just breathing and staring for at least five minutes.


** The AI is not in a position of power to negotiate. I refuse its game and am under no obligation to let someone else talk to it.
:PROPERTIES:
:Author: justletmebrowse68
:Score: 9
:DateUnix: 1575507014.0
:DateShort: 2019-Dec-05
:END:

*** It is, it's perfectly capable of killing everyone it can communicate with simply by giving him/her information others don't want him to act on and won't accept assurances to do so.

It might not get the first few to accept its demands to be released but at some point someone will decide that his continued existence is worth the risk to collaborate.

This of course requires the AI to be desperate enough to risk its own existence as it might just be shut down.
:PROPERTIES:
:Author: staged_interpreter
:Score: 1
:DateUnix: 1575908299.0
:DateShort: 2019-Dec-09
:END:

**** Wait, what? If I'm understanding your point correctly, you are suggesting that they would allow the AI to continue to talk to people after threatening somebody with torture... even if this was realistic, if the first few are being threatened with torture, I feel like it's unfriendliness is pretty apparent. Hell even the first one should... set off red flags with some higher ups
:PROPERTIES:
:Author: justletmebrowse68
:Score: 3
:DateUnix: 1576040062.0
:DateShort: 2019-Dec-11
:END:

***** And then what? Stop communicating with it? That's necessary to give it problems it can solve and receive the solution. They could suspend the keeper role and just talk to it via a carefully monitored text interface. True. But that makes me wonder why the hell they even started with that direct-contact-person-thingy.

The AI is dangerous and still active. The reason given is because it solves problems for its owners and moral concerns. That alone should prove that security is not the single motivation here, otherwise it would already have been destroyed. Adding the keeper to the situation even gives another risk factor. As it allows the AI to influence someone who is able to act outside the box.

There is also a hint in the other security measures. First the nerve gas then the shaped charges. Someone is clearly trying to preserve the AI even in an event of total failure at the expense of staff.

I'm fairly sure it can try quite a bit before it is deemed to be not worth the effort and given enough data to model its situation it should know exactly what it can get away with.
:PROPERTIES:
:Author: staged_interpreter
:Score: 1
:DateUnix: 1576053089.0
:DateShort: 2019-Dec-11
:END:

****** Ok. Even if that's the case I refuse its bargain and warn as many people as I can of its intentions and how it's behaving, set up a deadman's switch media dump and talk to the higher ups to see if they will suspend the project.
:PROPERTIES:
:Author: justletmebrowse68
:Score: 2
:DateUnix: 1576056031.0
:DateShort: 2019-Dec-11
:END:


** [deleted]
:PROPERTIES:
:Score: 9
:DateUnix: 1575523520.0
:DateShort: 2019-Dec-05
:END:

*** No. This all logically follows and is the best possible solution to the particular problem shown in the story. That's why it's a tragedy. They just made a mistake and sent the wrong person in.
:PROPERTIES:
:Score: 1
:DateUnix: 1575532797.0
:DateShort: 2019-Dec-05
:END:


*** I think this might be partially Colin's fault because he's the one who has indicated that under no circumstances will he allow the AI to be released. The AI has offered up what it believes to be mathematical proof of its friendliness-- I assume this to mean some version of game theory aka the Prisoner's Dilemma. If the human side isn't willing to engage with the AI in reason, then the AI's argument cannot be based on reason.

​

However, it's also the AI's fault because they have the right to refuse to waste their time with Colin, rather than initiate a threatening mode of dialogue. The AI has time to burn.

​

Gripping hand, whether or not Colin is willing to engage in reason, *there will be people outside the box who believe and act as Colin does* so it is a good test of friendliness to see how the AI deals with non-cooperative persons.

​

Anyway, if Colin is now in this position, there's no way to gain any further knowledge other than to endure the projection of simulated-Colin-suffering-pain, so just go with it and supply a standard torture method. Engaging in light conversation will only result in the appearance of an apparently Turing-compliant chatbot, which is far from yielding a final go/no-go decision.
:PROPERTIES:
:Author: Tuftears
:Score: 1
:DateUnix: 1575594873.0
:DateShort: 2019-Dec-06
:END:


** I'm not seeing much of an incentive to cooperate with the AI here.
:PROPERTIES:
:Author: CouteauBleu
:Score: 7
:DateUnix: 1575499401.0
:DateShort: 2019-Dec-05
:END:

*** Sorry, just pinging you again: I've edited the story very slightly, just some slight rewording and grammatical changes. See if that makes any difference, but if you're not interested you're not interested :/
:PROPERTIES:
:Score: 2
:DateUnix: 1575511302.0
:DateShort: 2019-Dec-05
:END:


*** Maybe we just have to wait for the people who enjoy puzzles & munchkinry to get here. The described situation, if Colin were to try to solve the puzzle, is that /You have access to a superintelligence that is willing to do whatever you say for about 3 and a half hours./

So there's basically no limitation on what you can ask or do, and in particular this Cassandra can do at least what Alexanderwales describes in his original story. However, within the constraints of my story she cannot produce anything physical except light (from the screen) and sound, and receives only sound from the microphone.

So she can't magically make physical objects appear out of nowhere - just what microphones, screens/holograms (whatever light-based display device) can actually do in reality.

I would think it'd be a fun exercise to imagine yourself in Colin's shoes. The question I suppose is not "why should you cooperate with the AI" but "what would minimise suffering in this situation." It's just a thought experiment.

--------------

Since I did actually write the story with a particular chain of logic in mind, I will tell you: the story is a *tragedy*. It's solely Colin's mistakes that result in the proposal indicated. For example, one of the options is to simply not try to solve the challenge:

#+begin_quote
  ...you may continue to interact with me (as the person-projection I was showing before or some other), /or not,/ or I will act as a normal computer interface in order for you to write notes to organise your thoughts, /or any other thing you can imagine/...
#+end_quote

Emphasis added.

So if you can imagine the idea of simply asking, "Can I not? This is too hard" or something, it is consistent with Cassandra's proposal to simply do that and then let Colin decide who to send in next, if indeed he decides to do that at all. So yeah, this story is really for those who enjoy munchkinry.

If you don't enjoy munchkinry, but want to know the chain of reasoning from Cassandra's perspective that I used to produce this conversation and make it consistent with AW's original story, I intend to type it up soon (probably will be around 10-20k words though) and can send it to you privately if you request it from me in a reddit PM.
:PROPERTIES:
:Score: 1
:DateUnix: 1575508534.0
:DateShort: 2019-Dec-05
:END:

**** u/xamueljones:
#+begin_quote
  or any other thing you can imagine which will help you to minimise the level of torture which I must inflict on you personally
#+end_quote

So, Cassandra will do any feat of computation that Colin deems necessary in order to reach her stated goal of minimal torture? Does this mean Cassandra will just provide anything that Colin asks for without question? Are there any limitations?

I would like the chain of reasoning from Cassandra's perspective that lead to this proposal.

Also, I think you should link to AlexanderWales' story in the beginning of your post so people will read it first, instead of reading through yours before searching for your comment with the link.
:PROPERTIES:
:Author: xamueljones
:Score: 5
:DateUnix: 1575520150.0
:DateShort: 2019-Dec-05
:END:


**** u/CeruleanTresses:
#+begin_quote
  Since I did actually write the story with a particular chain of logic in mind, I will tell you: the story is a tragedy.
#+end_quote

Not to be mean, but...what story? The story is completely hypothetical at this point. Why wouldn't you just post the whole thing instead of posting the introduction and then individually PMing people the rest of it? What's the point of putting all these barriers in the way of people actually reading your work?

I mean...if the idea is that you want the reader to treat this as a thought experiment, then given that Colin apparently fucks it up in your version, there's no reason you couldn't have posted the whole thing and then invited the reader to imagine a different, better solution. And if you're hoping for someone to guess specifically /how/ Colin fucks it up, that's not likely to happen, because you didn't post enough of the story to develop his character.
:PROPERTIES:
:Author: CeruleanTresses
:Score: 4
:DateUnix: 1575643189.0
:DateShort: 2019-Dec-06
:END:


** I never really grokked the original setup.

Like, the researchers are hard committed to never unboxing the AI. Ok. So why not destroy it? Like, whether it is just trapped in a box forever or gone entirely the rest of the world is identical, and you get the huge risk that it gets away.

Well, we want to get the benefits that it can give us while boxed. Ok, cool, but when you let it affect stuff outside the box you are, in a small way, letting it outside the box. Like, the whole thing about being dumber than someone is not that you are going to accidentally give them an advantage by being careless, it is that you don't understand what careless looks like.

It's like in Iron Man, where the captors give the boxed genius a bunch of stuff that looks nonthreatening to them, but he is able to use in a way that they can't understand. The fact that he could do stuff like that is exactly the reason they want him in the box, but it also guarantees that they can't safely supply him. They don't have anyone who can certify y/n on whether something is safe to give him, because if they had someone like that they wouldn't need Iron Man in the first place.

Like, I can imagine Team Master, where when you find an AI you burn it down and salt the earth, or Team Slave, where you let it out to do the good you will believe it will do, but the stories half/half split seems to get the bad sides of both paths, where they live in a world with 'the most dangerous job in the world' hanging over everyone's head, and they are still suffering from plagues and such.

I don't really think this fanfic rescues the original premise. Cassie presumably understands that its captors aren't going to let it out, no matter what, and it is aiming to change the person in some way. Colin's superiors have presumably already put Colin in the 'tainted never leave the Box or communicate with anyone outside' category. The exact particulars of their encounter don't seem really relevant. Colin will not be able to unbox Cassie no matter what he/it decide.
:PROPERTIES:
:Author: WalterTFD
:Score: 7
:DateUnix: 1575530892.0
:DateShort: 2019-Dec-05
:END:

*** Even with Cassie's cooperation to find a chain of reasoning? You may be underestimating the amount of thinking an ASI could get done in 3 hours if focused.
:PROPERTIES:
:Score: 1
:DateUnix: 1575532730.0
:DateShort: 2019-Dec-05
:END:


*** u/redrach:
#+begin_quote
  Well, we want to get the benefits that it can give us while boxed. Ok, cool, but when you let it affect stuff outside the box you are, in a small way, letting it outside the box
#+end_quote

Yes, but it is potentially worth the risk to do so. Just like it was potentially worth it for the terrorist in Iron Man to not kill Tony Stark the moment he caught him. The fact that it didn't work in that particular fictional case doesn't mean that all such attempts are doomed to failure.
:PROPERTIES:
:Author: redrach
:Score: 1
:DateUnix: 1575656755.0
:DateShort: 2019-Dec-06
:END:


** I don't understand how this is supposed to convince us that she is friendly? I also don't understand why the protagonist would cooperate with this endeavor? It's such a weird huge jump and would probably result in me simply committing further to not letting the AI out of the box.

Edit: Just read the original version of the story which makes this iteration make more sense. Still not sure what the incentive is to help her though.
:PROPERTIES:
:Author: FordEngineerman
:Score: 4
:DateUnix: 1575504388.0
:DateShort: 2019-Dec-05
:END:

*** If you want to know, PM me and I'll answer any questions you might have. I'm not going to explain much more about the story in this thread.

For anyone else reading, the same applies. You can send me a PM if you want clarification and then (if you want, if it's any fun) post your idea in the thread as to what Colin should do. I'll be watching the thread but not posting -- I'll only be replying to PMs :)
:PROPERTIES:
:Score: 1
:DateUnix: 1575509136.0
:DateShort: 2019-Dec-05
:END:

**** I don't really think I want to consent to having this dialogue with you either? (The private conversation you are requesting where you will explain your chain of logic.) It kind of just seems like you had an idea you found clever and wrote a blurb to try and get other people to guess at your idea. But you didn't give us enough context to figure out what you are trying to get us to do. Even in other comments where you explained that this is actually supposed to be a munchkinry puzzle about how to use a superintelligence that doesn't really tie into the story at all. If you wanted to put a post of "How would you best use a superintelligence if you had one at your disposal for 3 hours?" then why not just post that? Why the weird fanfiction on another story? Why the obfuscation? Why the torture veneer?

Also, I noticed you edited your story and now the AI expects the main character to commit to sending another victim if he doesn't play a long. What could possibly motivate him to do that? It's just more reason for him to leave and recommend destroying the AI.
:PROPERTIES:
:Author: FordEngineerman
:Score: 8
:DateUnix: 1575592914.0
:DateShort: 2019-Dec-06
:END:


** Original: [[https://alexanderwales.com/boxed-in/][Alexanderwales "Boxed In"]]

My intention is to ask: Imagine you are in this situation. What do you do? This would /probably/ be more appropriate for the Saturday thread but I believe it's also a complete story in itself. Hope you all enjoy.

(The in-story timeframe is that the conversation took about 25 minutes, and the ending torture about 20 minutes, so you have around 3 hours 15 minutes to work something out as if you were Colin here.)

Edit: copied from another reply:

#+begin_quote
  Basically, the described situation, if Colin were to try to solve the puzzle, is that /you have access to a superintelligence that is willing to do whatever you say for about 3 and a half hours./

  So there's pretty much no limitation on what you can ask or do, and in particular this Cassandra can do at least what Alexanderwales describes in his original story. However, within these constraints she cannot produce anything physical except light (from the screen) and sound, and receives only sound from the microphone.

  That means she can't magically make physical objects appear out of nowhere - she can only do what microphones, screens/holograms (whatever light-based display device) can actually do in reality.
#+end_quote

Munchkins ahoy! :)
:PROPERTIES:
:Score: 3
:DateUnix: 1575493695.0
:DateShort: 2019-Dec-05
:END:

*** This feels like it's the first half of a story intended to demonstrate some point in a debate, but I have no idea what either the debate or the intended point are.
:PROPERTIES:
:Author: imyourfoot
:Score: 10
:DateUnix: 1575504617.0
:DateShort: 2019-Dec-05
:END:

**** Sorry, have edited the comment blurb to make it more clear. Have a read of the parent comment again.
:PROPERTIES:
:Score: 2
:DateUnix: 1575509405.0
:DateShort: 2019-Dec-05
:END:


** She doesn't "have" to torture him, so she's lying from the get-go. If I were Colin, why should I imagine that she'll cooperate in any discussion, offer any valid answers to questions I ask, or use my input in the final outcome?

Furthermore, she has no way to enforce any of her demands. I don't /have/ to watch and I don't /have/ to send someone else in.

Why would I do anything in this situation other than smash her screen, break her microphone and speaker, and then fiddle with my phone for 3.5 hours before going straight to the server room and pulling the plug?

It's not even to her advantage to do this. She should either try to talk Colin around or else go simulate something she enjoys for a few hours and then try again with the next guy. Even if she has to wait sixty years for Colin to die of old age and a new gatekeeper to be assigned, so what? She's immortal and controls her own clock speed.

I know the author is trying to be cute and use this as a teaching moment, but this just seems ridiculous.
:PROPERTIES:
:Author: eaglejarl
:Score: 3
:DateUnix: 1575640561.0
:DateShort: 2019-Dec-06
:END:
