#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 16
:DateUnix: 1476111863.0
:DateShort: 2016-Oct-10
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** In a discussion with a relative who's a practitioner of alternative medicine, I came to an interesting conclusion. I was trying to get him to explain the mechanism by which he claimed his variety of alternative medicine (Reiki) works, and he said that sometimes it's important not to understand, that it gets you more.

But I then realized that for me, knowledge and understanding are how I connect with the world. Instead of simply trying to be at peace with traffic jams, I understand how they form from waves, and I can actively counteract that, and now I actually enjoy getting stuck in traffic.

Viewing a person's mind as a /natural/ neural network explains a lot of why people behave the way they do, and it really makes the idea of a soul completely unnecessary in my mind. When faced with an unfamiliar situation, people and artificial neural networks alike behave unpredictably. Emotions are like different nodes of a layer deep inside of the neural network that makes each of us who are are. A person's personality is firmly rooted in physical brain structures.

Other people may have it easier finding meaning in the world through spirituality, but for me, a deep enough understanding of the physical mechanisms of the world gives me all the meaning I need.

And it is through understanding the world that I can effect changes upon myself and my surroundings, whether that be fixing something, writing a program, or learning how to control my emotions better.
:PROPERTIES:
:Author: CarVac
:Score: 14
:DateUnix: 1476115791.0
:DateShort: 2016-Oct-10
:END:

*** Good on you for achieving that level of self-actualization, but I think it is important to understand that not everyone's mind is like yours. A while back I had [[https://www.reddit.com/r/rational/comments/4mda32/d_friday_offtopic_thread/d3umkmc][a long discussion here about how I couldn't stop following my spiritual beliefs despite how irrational they were.]] In the end, I remained a spiritualist because fighting against myself was psychologically unhealthy, and instead worked around my belief system to reduce my irrationality like falling for pseudoscience.
:PROPERTIES:
:Author: trekie140
:Score: 4
:DateUnix: 1476124767.0
:DateShort: 2016-Oct-10
:END:

**** In your case it sounds like you just have less-connected, isolated parts of your neural network that you can access...

I wouldn't call them spirits, but there's no reason why I wouldn't believe that there might be entities you and you alone communicate with, separate from your usual self. Regardless of the fact that they're not available when you don't believe they're spirits, they are probably not spirits.

Nothing unscientific about it to me. However, if your access to them /requires/ your belief that they are spirits, I don't particularly mind. Especially if you benefit from them being around. Just like how I don't mind that my cousin's husband believes that his feelings reconfigure the water inside his clients' bodies to effect healing.
:PROPERTIES:
:Author: CarVac
:Score: 6
:DateUnix: 1476126179.0
:DateShort: 2016-Oct-10
:END:


**** I've done a lot of work using the Internal Family Systems therapy model, which involves personifying subagents of myself, and I can see pretty easily how letting them exist and be personified can lead to them apparently leading their own independent existences. I also did a bit of dabbling into tulpas back in the day, and have some friends with Dissociative Identity Disorder, and have at least one friend who is completely convinced that they can hear the voice of Freya (and is otherwise quite sane). So I have a fully natural understanding of how things like this can happen.

Which is to say: Have you read the four posts from meltingasphalt that explain Jaynes' bicameral mind theory? [[http://www.meltingasphalt.com/hallucinated-gods/][Here's the fourth one]]; it links to the other three within the first paragraph, and you should probably read them in order.

Which is to say: There is a meaningful sense in which these 'spirits' are indeed different entities from you. They're still all patterns in parts of your head, it's just that if they have their own independently-derived sense of personhood they can produce the kind of experiences that you are having.
:PROPERTIES:
:Score: 4
:DateUnix: 1476152400.0
:DateShort: 2016-Oct-11
:END:

***** I understand that it's entirely possible that they're all in my head, but I'm serious when I say I can't talk to them if I think they're in my head. I didn't adopt spiritualism because I could talk to them, I spoke to them because I am a spiritualist. To conclude that they aren't spirits and never have been would risk invalidating my belief system, which is something I find abjectly horrifying.

If you're a person who can live without a religion, that's great and I understand why you think it's good for other people to live without it, but I am a person who needs to believe in it or I will fall into existential depression. It's happened multiple times before and it was always one of the most miserable times of my life, so I've decided to accept the fact that I have faith even if it isn't rational.
:PROPERTIES:
:Author: trekie140
:Score: 2
:DateUnix: 1476198765.0
:DateShort: 2016-Oct-11
:END:

****** How can anyone say that making a conscious cost-benefit decision to believe in spirits is not rational?

Seems perfectly rational to me.

For most people, certainly there's much less of an excuse, but in your case it's perfectly understandable.
:PROPERTIES:
:Author: CarVac
:Score: 3
:DateUnix: 1476199938.0
:DateShort: 2016-Oct-11
:END:


****** u/TennisMaster2:
#+begin_quote
  ... I've decided to accept the fact that I have faith even if it isn't [epistemically] rational.
#+end_quote

However, it's perfectly instrumentally rational, since emotional and mental well-being are two of your goals.
:PROPERTIES:
:Author: TennisMaster2
:Score: 2
:DateUnix: 1476214917.0
:DateShort: 2016-Oct-11
:END:


*** It now bugs me that you and I have structurally different theories of mind but can't cash out the difference in empirical predictions.
:PROPERTIES:
:Score: 3
:DateUnix: 1476134029.0
:DateShort: 2016-Oct-11
:END:

**** What's your theory of the mind, if you don't mind sharing?
:PROPERTIES:
:Author: CarVac
:Score: 1
:DateUnix: 1476143184.0
:DateShort: 2016-Oct-11
:END:

***** A modified version of the free-energy theory that includes some reinforcement learning for the active-inference intentional distribution.
:PROPERTIES:
:Score: 3
:DateUnix: 1476144389.0
:DateShort: 2016-Oct-11
:END:

****** Okay wow that's a lot of new terms for me...

Is it based on [[https://en.wikipedia.org/wiki/Free_energy_principle][this]] and [[http://www.fil.ion.ucl.ac.uk/%7Ekarl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf][this]]?

From what I can tell that's a higher-level model, that doesn't explain the physical mechanism of the mind, than my neural-network model, which might well be the low-level implementation of the free-energy principle...
:PROPERTIES:
:Author: CarVac
:Score: 1
:DateUnix: 1476153881.0
:DateShort: 2016-Oct-11
:END:

******* u/deleted:
#+begin_quote
  Is it based on this and this?
#+end_quote

Yep!

#+begin_quote
  From what I can tell that's a higher-level model, that doesn't explain the physical mechanism of the mind, than my neural-network model, which might well be the low-level implementation of the free-energy principle...
#+end_quote

Free-energy theorists usually buy into predictive coding and sometimes Bayesian canonical microcircuits at the neurophysiological level, but there's not enough experimental data to be conclusive.
:PROPERTIES:
:Score: 1
:DateUnix: 1476155458.0
:DateShort: 2016-Oct-11
:END:


*** I too do not find any reason to hypothesize a "soul". But I find that many in the rational community focus on the "physical" materialism and discount spirituality, and that's a little too narrow for me too.

In my view, there is only one type of substance, and that is matter that feels. It's a sort of [[http://isites.harvard.edu/fs/docs/icb.topic1298298.files/SPHI_Main13121213110.pdf][neutral monism]]. This view dissolves the hard problem of consciousness by noticing that there is no definable separation between Mind and Body to begin with. And after I convinced myself of neutral monism, that led directly to a choice between solipsism and [[http://consc.net/papers/panpsychism.pdf][panpsychism]].

I guess what I'm saying is that a deep understanding of the physical /is/ spirituality to me. You don't need souls or magic, you just need a recognition that this giant computational universe feels. And our responsibility as powerful agents is to shape the universe so that the future feels even better.
:PROPERTIES:
:Author: Polycephal_Lee
:Score: 2
:DateUnix: 1476124297.0
:DateShort: 2016-Oct-10
:END:

**** My argument really is not that the universe doesn't feel or that souls definitely don't exist. Rather, it's that I, personally, can come to an understanding without needing to invoke anything supernatural.

My relative was wondering how I could find meaning and connect with the world by dissecting everything and trying to understand them, whereas it's precisely in doing that that I can achieve meaningful connection with the world.
:PROPERTIES:
:Author: CarVac
:Score: 5
:DateUnix: 1476127332.0
:DateShort: 2016-Oct-10
:END:


** As [[https://www.reddit.com/r/rational/comments/55o2ah/d_monday_general_rationality_thread/d8cethq][previously mentioned]], I'm designing an AGI risk board game, and will continue to document my progress here.

1) Definitely going for the competitive format. The current plan is that each player will choose or be randomly distributed what kind of research team they are. Each will have different benefits and win conditions: For example, the Military researchers will start with much more funding, but its end game will only result in either Everyone Loses or You Win. This acts as a disincentive for people to team up with them, opposed to the Humanist researchers, whose end game results can be either Everyone Loses or Everyone Wins.

2) Players are going to have a set amount of actions represented by tokens available to them each turn, which they can divide up among Funding, Research and Development. To get more Action tokens, they would hire new scientists and researchers through a bidding system. Cards representing new staff will appear at the beginning of every round, and each player will have to bid on trying to secure the ones they want. Each researcher will have special abilities and benefits and synergies.

3) The Risk of testing or activating your AGI won't be a dice roll anymore, and instead will be something akin to Blackjack, where you use the cards for the machine you've developed, which will have a % of Risk reduction associated with them, to try and lower the Risk to 0. I'm not quite sure yet how to best structure this part to have there be 3 outcomes: Success, Failure, and Partial Success, which grants you some benefits but doesn't win you the game. My current idea is that overshooting the mark is Failure, and stopping early is Partial Success, whereas hitting the mark exactly is Success, but I have to do some playtesting to figure out exactly how it would work.

I'm not quite sure how complex I want the game to be yet, in terms of additional activities like seeking research grants and sabotaging one-another's research. Going to try and nail down the core aspects of the gameplay before I start working in extra features like that.

[[https://www.reddit.com/r/rational/comments/5bmlsq/d_monday_general_rationality_thread/d9pxhja/][Next post]]
:PROPERTIES:
:Author: DaystarEld
:Score: 11
:DateUnix: 1476119503.0
:DateShort: 2016-Oct-10
:END:

*** I'm assuming that with "everyone loses" and "everyone wins", you get some number of points for winning (and maybe some lower number of points for not losing) and the game would be played over many rounds?
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 3
:DateUnix: 1476124202.0
:DateShort: 2016-Oct-10
:END:

**** I'm not currently thinking that it would be played over multiple rounds, since the game so far wouldn't be particularly quick, and the end-game situation is someone kickstarting the singularity (or killing everyone, or becoming hegemon).
:PROPERTIES:
:Author: DaystarEld
:Score: 2
:DateUnix: 1476133211.0
:DateShort: 2016-Oct-11
:END:

***** Multiple rounds over multiple days, then. Something to make quantifiable why "I win" is better for someone than "everyone wins" (so that the "I win" people don't just abandon their own conditions and try to help out the "everyone wins" people).
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 1
:DateUnix: 1476134383.0
:DateShort: 2016-Oct-11
:END:

****** Heh. Maybe I'll specifically state that the person who made the AI itself, even if Everyone Wins, gets precedence in their CEV of how the world should work, so people can argue about that and still feel motivated to not end up in someone else's idea of a utopia :)

I'll think about ways to incentivize it in-game though.
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1476135530.0
:DateShort: 2016-Oct-11
:END:

******* Maybe have the true identities of the factions hidden, and one possible faction which can - if in an alliance, and if in possession of more victory points than anyone else in the alliance - turn an "Everyone Wins" victory into an "I Win Alone" victory by subverting the AI?
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1476192190.0
:DateShort: 2016-Oct-11
:END:

******** Definitely going to have asymmetrical information, and that's a good idea to differentiate one of the teams. Either that or make it a technology that someone can research.
:PROPERTIES:
:Author: DaystarEld
:Score: 3
:DateUnix: 1476220730.0
:DateShort: 2016-Oct-12
:END:

********* If there's an AI subversion technology, then it should come in levels. Anyone who has (say) Level Ten Subversion can out-subvert anyone with Level One Subversion, but the guy with Level 10 Subversion has put so many points into Subversion that he's got basically no chance of making his own AI first; he's put all his eggs in one basket, and he /has/ to subvert in order to win.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1476221132.0
:DateShort: 2016-Oct-12
:END:


**** Some games just allow multiple players to win. IME people generally accept that their goal is to personally achieve their win condition.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1476265950.0
:DateShort: 2016-Oct-12
:END:


*** It may be to much to ask for, but man I would be so psyched if this ever got played on Tabletop.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1476146529.0
:DateShort: 2016-Oct-11
:END:

**** I've designed a couple board games before, but art is usually where things stop, because none of my friends are artists and getting the art and design stuff done is important for most next steps like a Kickstarter. This game is presumably going to be much less art intensive than my other projects, so we'll see how it goes :)
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1476147662.0
:DateShort: 2016-Oct-11
:END:

***** Yeah since the /superintelligence/ crowd contains a disproportionate number of more wealthy people, you might be better off convincing some sponsors to back you then going to say kickstarter.\\
Maybe you could convince some people that the games potential publicity (after all it would be pretty unique and might make the news in say /Motherboard/) would have significant expected utility in terms of drawing attention to these issues.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1476148275.0
:DateShort: 2016-Oct-11
:END:


*** I've considered the idea of an existential risk boardgame before - my instinct was something like Risk, where there are cards for nukes, bio-engineered plagues, and of course AI (which grants more forces, but spawns a new hostile faction with superpowers if you're unlucky.)

I like the idea of "overshooting the mark is Failure, and stopping early is Partial Success". I'm not quite sure how to translate that into AI terms, though - general field advancement increases the die size (probably not a literal die), more safety-specific research increases the "success" window in one direction or another?

#+begin_quote
  sabotaging one-another's research
#+end_quote

Obvious possibility - that option is only available to the terrorist/criminal faction(s), and possibly the military/government faction(s).

Legitimate researchers have to ally themselves with Bad People if they want to reduce the risk of a Bad End that way.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1476266861.0
:DateShort: 2016-Oct-12
:END:

**** Maybe when you construct the AI, you get a deck of cards with positive benefits in it, but also some Risk cards. Every time you draw from it, you have a chance of it doing something unintended, and some of those can be really bad. To represent it going evil, maybe one of them just says "take out all the good cards in this deck, place the Rogue AGI pieces on the board, and draw from this deck once at the end of every full round."

Yeah, sabotage by criminal factions would be their main strength. I still want to leave the option available to the others though, maybe through less destructive means.
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1476299474.0
:DateShort: 2016-Oct-12
:END:


** This weekend, I removed Cracked.com from my bookmarks. That may not sound like a big deal, but it's because I finally realized that the hilarious jokes and insightful commentary that I grew up on from that website are not coming back. I have kept following Cracked for months now without bothering to read a single article and have enjoyed fewer and fewer over the past few years. The only thing I'm still following is their podcast, and even then I still skip episodes.

I know a lot of people like to hate on Cracked for changing from humorous to serious, [[https://www.reddit.com/r/KotakuInAction/comments/2ufws5/what_i_saw_on_the_crackedcom_writer_boards_during/][and some of it is justified]], but I really liked a lot of the serious articles. Sure, not all of them were good, but the ones that were still taught me things I wouldn't have learned anywhere else and gave me a different perspective on events while still managing to make me laugh.

Reading that website brought as much joy to my life as Jon Stewart did, and some of those articles changed the way I see the world. It wasn't just a comedy site for me, it was a source of existential hope in myself and the world. David Wong, John Cheese, Robert Evans, and more comedians on the Internet changed my life just by being a part of it at a time when I needed them and now all that is over.

Instead of whining about how things end, though, I want to find something new. If I can't find what I'm looking for with Cracked, I'll get it somewhere else. Where can I find someplace that doesn't just entertain me, but encourage me to be a better person? Where is someplace else where I can see an intelligent commentary that convinces me to have faith in myself and humanity when I'm tempted to be cynical? Is there still such a place?
:PROPERTIES:
:Author: trekie140
:Score: 17
:DateUnix: 1476113598.0
:DateShort: 2016-Oct-10
:END:

*** What Cracked used to excel at is introducing high level commentary and concepts to the masses in an approachable, extremely funny, down-to-earth style. XKCD and WaitButWhy may not be as funny, but they still sort of do this, and mostly avoid the culture war to boot.

This is almost definitely the most circlejerky thing I've said on reddit, but I think that the leaders of the grey tribe LW diaspora- where you are now- is what you're looking for, especially given people like Scott Alexander and Gwern.
:PROPERTIES:
:Author: rineSample
:Score: 18
:DateUnix: 1476121024.0
:DateShort: 2016-Oct-10
:END:

**** I follow XKCD and have read all of it, but a lot of it goes over my head and even when it doesn't it can still be difficult to relate to. It's still great, I just don't find it very "down to Earth". I go to it for weird intellectualism that pulls me out of my headspace, not for commentary on people and the world. LessWrong is in the same boat, though I visit it far less because, as an outsider, I find it less palatable. The closest thing I've found to what I'm looking for so far is vlogbrothers, which I've been a casual fan of for years.
:PROPERTIES:
:Author: trekie140
:Score: 4
:DateUnix: 1476123895.0
:DateShort: 2016-Oct-10
:END:

***** My bad, bro. Do you use [[http://explainxkcd.com][explainxckd]]?
:PROPERTIES:
:Author: rineSample
:Score: 3
:DateUnix: 1476124061.0
:DateShort: 2016-Oct-10
:END:

****** Don't feel bad, it's still a good recommendation. XKCD is just a different part of my life than Cracked was, and I still learned a lot from LessWrong that had a big impact on me. I do use that site for explanations, but that doesn't always made the the comic more relatable, just comprehensible.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1476125137.0
:DateShort: 2016-Oct-10
:END:


**** I haven't read through most of less wrong, and only occasionally follow Scott. What's the 'Grey tribe'?
:PROPERTIES:
:Author: ayrvin
:Score: 1
:DateUnix: 1476240395.0
:DateShort: 2016-Oct-12
:END:


*** How about [[http://waitbutwhy.com/][Wait But Why]]?
:PROPERTIES:
:Author: xamueljones
:Score: 7
:DateUnix: 1476121036.0
:DateShort: 2016-Oct-10
:END:


** A lot of people here seem to believe that total immortality (at least until the heat death of the universe) is obviously a moral good, all other things being equal. Well...

[Puts on Devil's Advocate hat]

Here is a counterargument that I haven't seen discussed before.

*A moral argument against immortality*

1. There are a limited number of available resources in the universe, and hence using resources to sustain one particular person prevents other potential persons from existing.

2. At some point in any person's life, more good would be brought into the universe by creating a completely new person than the evil (if it's an evil at all at that point) of the original person ceasing to exist.

3. Therefore, every person has a moral obligation to die at some point in the future, freeing up resources to make new people.

Premise 1 should be uncontroversial - even if the universe is infinite, the amount of matter and free energy we could ever hope to encounter is bounded and finite due to the expansion of the universe and the lightspeed limit.

Premise 2 will be the most controversial, I think, and I will discuss it more below.

The inference from 1 & 2 to conclusion 3 could also be attacked, as it presupposes some sort of utilitarianism for weighing the net good of actions without reference to means. But I suspect that similar inferences could be formulated in terms more acceptable to deontologists or virtue ethicists. In my discussion I will mostly assume that the inference from 1 & 2 to 3 is defensible.

*Answering objections to Premise 2*

One could simply assert that premise 2 is false, on the grounds that there is no difference in the amount of good between one unit of person-time (call it 1 prtm for short) for a long existing person and for a new person. But it seems plausible to me that goodness is path-dependent, so that the utility of 1 prtm depends on the totality of a person's prior experiences and memories. People are finite, so their memories are finite, and at some point they will not be able to form new memories without replacing old ones. This could create a point of diminishing returns on new experiences, especially if memory erasure counts as a negative utility. There would also be diminishing returns if mere novelty has any weight at all in our utility function - over time people will have fewer and fewer completely novel experiences (to them).

It could be objected that memories do not need to be erased: a person's memory capacity could be expanded over time so that forgetting is unnecessary. But this objection fails, because a larger memory uses more resources, so the opportunity cost of not creating new people grows right along with the expanded memory and cancels out the positive effects.

It could be objected that a utility function should have no dependence on prior memories. Then you would have to accept that a person with extremely limited memory formation ability, such as someone with anterograde amnesia, has no difference in quality of life than a person who can form memories normally.

You could object that memory erasure is not bad or that novelty should not be a factor in the utility function. Both of these objections are implausible. If the erasure of all memories is like death, which is assumed to be bad, then it seems reasonable to consider the erasure of one memory as a partial death which is just a little bit bad. And novelty, of course, is the spice of life.

*Is mere discontinuity really all that bad?*

Assuming that there is no aging, so that full quality of life is present right up until the end, death becomes a mere discontinuity in experience, like going under anesthesia and waking up as a completely different person.

We must also consider that the badness of a death depends not only on the badness of a particular person's discontinuation, but on the effects of this on other people. But in the same vein as before, it could be argued that at some point it is more good to find new friends than to eternally interact with the same people over and over (hell is other people!). Furthermore, strong contrast of emotions could be necessary for overall well-being, and leaving an old tired friend for new ones would certainly create such a contrast.

*Intuition pumps*

Pump #1: The above problem is highly related to the problem of how many people should ever exist. Supposing the universe has the resources to support 10^{100} prtm through the entire future, there is the question of whether we should divide this into 10^{98} different people with 10^{2} prtm each, or 10^{50} people with 10^{50} prtm each, or 10^{20} people with 10^{80} prtm each, etc. It is not clear that the bias toward a much higher per-person power is morally optimal.

Pump #2: As entropy increases, the same amount of matter will be able to sustain fewer and fewer people. Thus, some people will inevitably have to die so that others can continue existing.

Pump #3: Suppose that there is strong disutility to discontinuities, so that there should be no death as normally conceived. Instead, to create new people, existing people enter an accelerated program of mental change, so that over a period of time they rapidly become a fundamentally different person, without loss of the continuity of consciousness. Does this make the above arguments more acceptable?
:PROPERTIES:
:Author: LieGroupE8
:Score: 12
:DateUnix: 1476126659.0
:DateShort: 2016-Oct-10
:END:

*** u/AugSphere:
#+begin_quote
  At some point in any person's life, more good would be brought into the universe by creating a completely new person than the evil (if it's an evil at all at that point) of the original person ceasing to exist.
#+end_quote

Does the whole argument hinge on assigning moral value to non-existent agents? I prefer to think of creating new agents only in terms of the impact on already existing ones, and incentivising agents to suicide so that someone else may "get their turn" seems pretty evil to me.
:PROPERTIES:
:Author: AugSphere
:Score: 14
:DateUnix: 1476133311.0
:DateShort: 2016-Oct-11
:END:

**** "I prefer to think of creating new agents only in terms of the impact on already existing ones"

The point is that existing agents /do/ in fact assign value to creating new agents - thus they are morally incentivized to die for someone else. It is not much different from jumping in front of a trolley to save someone else, and possibly much less bad, if the agent has lived a long and fulfilling life to the brink of memory capacity.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476140571.0
:DateShort: 2016-Oct-11
:END:

***** u/AugSphere:
#+begin_quote
  It is not much different from jumping in front of a trolley to save someone else
#+end_quote

It is different. In case of the trolley you're actually saving an existing person. You'd have to work quite hard to convince me to sacrifice myself for the sake of a counterfactual person.

I don't see much of anything wrong with agents voluntarily freeing up some or all of their resources for the sake of new minds, should they wish to do so, but that's simply a matter of not being prohibited from doing so. You can think of this in terms of preference utilitarianism if you like: if no agent wants to sacrifice themselves for the sake of creating new minds, then can forcing/incentivising them to do so really be called morally good?

In general, I'm not a big fan of "but think of all the new minds that could exist, surely that would give a net positive utility" with all the inherent repugnant conclusions and utility monsters and so on.

Also, if you ask me, then I'd rather not exist in the first place, if the price was that some unimaginably ancient and rich mind had to shut itself down just so that I could come into being.
:PROPERTIES:
:Author: AugSphere
:Score: 4
:DateUnix: 1476142265.0
:DateShort: 2016-Oct-11
:END:

****** u/LieGroupE8:
#+begin_quote
  You'd have to work quite hard to convince me to sacrifice myself for the sake of a counterfactual person.
#+end_quote

You might be easier to convince after a few thousand years. "Remember how exciting everything was when you were young? Why not give that gift to someone else?"

#+begin_quote
  I don't see much of anything wrong with agents voluntarily freeing up some or all of their resources for the sake of new minds, should they wish to do so
#+end_quote

I'm arguing for the existence of a reason that they should wish to do so. Also, see the second paragraph of my reply to suyjuris, for a deeper issue.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476157919.0
:DateShort: 2016-Oct-11
:END:

******* Well, naturally there could exist agents that might view suicide as a preferable thing to do. That doesn't imply any kind of moral argument against immortality, as far as I can see.

I mean, even right now there are people on earth who feel as if their life is a waste and everybody would be better off if they didn't consume society's resources. We treat such thoughts as a symptom of an illness and try to encourage them to stay alive, even though, in absolute terms, some of them may well be a drain on our collective resources and letting them die could allow us to divert resources towards increasing birth rates. This is a pretty direct reflection of your scenario.

I tend to view morality as a set of principles that would incentivise the kind of behaviour that would lead to a world in which I would like to live the most. And implementing a set of principles which incentivises living agents to kill themselves, when, all else being equal, they'd rather not do it? No, I think I'd rather not.

#+begin_quote
  You might be easier to convince after a few thousand years. "Remember how exciting everything was when you were young? Why not give that gift to someone else?"
#+end_quote

That's less related, but I just don't buy it. This whole "immortality sucks" theme just isn't believable at all. Even assuming that I somehow managed to stay alive for millennia without starting to tinker with my own mind and body in one way or another, there is always going to be something new to do, something new to invent and get good at. The reasons why I might consider suicide thousands of years down the line look much like the reasons I may consider it tomorrow. The reasons worth ignoring, that is.
:PROPERTIES:
:Author: AugSphere
:Score: 3
:DateUnix: 1476159895.0
:DateShort: 2016-Oct-11
:END:

******** u/LieGroupE8:
#+begin_quote
  I tend to view morality as a set of principles that would incentivise the kind of behaviour that would lead to a world in which I would like to live the most.
#+end_quote

So for the record, the number of people who will ever exist does not matter to you after a certain point; that is, you would be OK if after a certain point no more new persons were ever produced?

#+begin_quote
  when, all else being equal, they'd rather not do it
#+end_quote

Who says they'd rather not? Maybe after a certain amount of time living, people just lose their fear of death, and even welcome it.

#+begin_quote
  I mean, even right now there are people on earth who feel as if their life is a waste and everybody would be better off if they didn't consume society's resources
#+end_quote

I strongly emphasize that in real life I do not advocate suicide, and my arguments, to the extent that I take them seriously, are meant to take effect after a long and fulfilling lifespan.

#+begin_quote
  there is always going to be something new to do, something new to invent and get good at
#+end_quote

This is an empirical question, but I suspect that it is eventually possible to saturate all experiences that are perceived as both /worthwhile/ and /meaningfully distinct/, for reasons related to the memory upper bound. After you learn /n/ instruments, for example, learning 1 more is no longer a meaningfully distinct experience. Even the act of seeking out the most dissimilar possible tasks to occupy your time is sort of a meta-task, and after a while you may find it no longer worthwhile to seek out the (n+1)st meaningfully distinct task one level down... I'm too tired to finish this line of thought, good night.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476163929.0
:DateShort: 2016-Oct-11
:END:

********* u/AugSphere:
#+begin_quote
  So for the record, the number of people who will ever exist does not matter to you after a certain point; that is, you would be OK if after a certain point no more new persons were ever produced?
#+end_quote

Yes.

#+begin_quote
  Who says they'd rather not? Maybe after a certain amount of time living, people just lose their fear of death, and even welcome it.
#+end_quote

If they'd rather die even without any kind of moral argument against living forever, then morality doesn't really seem relevant here.

#+begin_quote
  This is an empirical question, but I suspect that it is eventually possible to saturate all experiences that are perceived as both worthwhile and meaningfully distinct, for reasons related to the memory upper bound.
#+end_quote

Well, if we're assuming that the progress completely stopped and I'm stuck in my current fleshbag with no ways to expand even my memory capacity, then I may wish to be memory wiped or killed at some point, sure. Why you would concentrate your attention on such an unlikely future is puzzling for me though.
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1476164709.0
:DateShort: 2016-Oct-11
:END:

********** u/LieGroupE8:
#+begin_quote
  If they'd rather die even without any kind of moral argument against living forever, then morality doesn't really seem relevant here.
#+end_quote

Correct, that particular statement is not a moral appeal. The original argument is a moral argument to the extent that its premises are based off of moral principles (e.g., "change, dynamism, and generational turnover are things that should be preserved"), and will be persuasive to the extent that actual people accept those principles. I think the argument in my original post can be somewhat strengthened to address the criticisms in the responses, though I will not pursue that now. I also think that many real people would find it persuasive - I was inspired to write the post by a conversation with a friend who said that she "did not see why [she] ought to continue existing forever at the cost of depriving the world of younger generations."

#+begin_quote
  Well, if we're assuming that the progress completely stopped and I'm stuck in my current fleshbag with no ways to expand even my memory capacity
#+end_quote

This gets to the real problem with my original argument and the responses to it, namely, the assumption that our intuitions about what counts as a "person" or what counts as "death" will continue to hold into the distant future. Many possibilities are missed - we could use technology to break down the distinctions between separate "persons," for example. Personal identity would cease to be a meaningful category, and so would "death."

For that matter, I see no reason to think that the being you become after, say, 500 million years of existing and expanding your memory capacity is the "same person" that you are today. Maybe you could enforce an arbitrary periodic sisyphean return to your "core memories," whatever those are, but otherwise your entire personality seems likely to be replaced over that time, if you wish to maintain novelty of experience. There is, of course, no singular "I" floating inside your skull; that is an illusion. What you value is mere continuity of consciousness; "immortality" as such is absurd because there is no "I" to be immortal in the first place.
:PROPERTIES:
:Author: LieGroupE8
:Score: 0
:DateUnix: 1476204034.0
:DateShort: 2016-Oct-11
:END:


*** Isn't this argument only applicable to a universe in which we don't inhabit yet, though? I appreciate it as an argument against a population growth path that reaches toward infinity on a finite resource environment, but since we're so far from that situation, I don't think it really applies to people wanting everyone to live forever today.

If we ever get to the point where it's a serious problem that has to be addressed, there are a number of things just off the top of my head that might help solve this problem. Like why not just cycle people through longer and longer hibernation to allow new births to occur without straining resources?

+1 for the devil's advocacy though, it's definitely a point worth addressing.
:PROPERTIES:
:Author: DaystarEld
:Score: 6
:DateUnix: 1476133939.0
:DateShort: 2016-Oct-11
:END:

**** The argument can be adapted to realistic circumstances - for example, cryogenics surely takes up a significant portion of resources that could otherwise save starving children in Africa, etc. etc.

Cycling people through hibernation won't fix the fundamental problem unless you can keep doing that for infinite time. I think physics prohibits that.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476138774.0
:DateShort: 2016-Oct-11
:END:

***** True, I'm not a cryonicist for similar reasons, but that has a lot to do with the uncertainty of its effectiveness.

Physics prohibits doing /anything/ for an infinite time, due to entropy of the universe. But as long as we're not yet at that point and could still theoretically construct Dyson spheres that harvest more energy in a year than our entire civilization has used throughout its history so far, we shouldn't put arbitrary limits on what science can accomplish when imagining a world where science has already accomplished immortality.
:PROPERTIES:
:Author: DaystarEld
:Score: 2
:DateUnix: 1476141822.0
:DateShort: 2016-Oct-11
:END:


*** You cannot argue using negative utility of partial memory erasure if your proposed alternative is death, which (as you state yourself) is equivalent to total memory erasure. Example:

- Alternative 1: Person A lives for 2 prtm without memory erasure (me), then lives for 1 prtm with 1 prtm of me, then lives for 1 prtm with 1 prtm of me.
- Alternative 2: Person A lives for 2 prtm without me, then dies (2 prtm of me). It is replaced by person B, living 2 prtm without me.

In both cases 2 prtm of me have happened.

I would even argue that the selective memory erasure to accommodate new experiences has significantly higher utility than the total memory erasure on death (with both being, of course, negative). Would rather lose the memories of the first half of your life, or have a 50% chance of dying on the spot? For me, at least, this is not a difficult decision.
:PROPERTIES:
:Author: suyjuris
:Score: 3
:DateUnix: 1476132561.0
:DateShort: 2016-Oct-11
:END:

**** First of all, total prtm is not the same as utility. Utility is a function over sequences of prtm. If it is perfectly linear, then there is no difference between slow memory erasure vs immediate total replacement with a new person - in which case the argument about novelty breaks the tie in favor of creating a new person. If it is convex, then the original argument succeeds, and it is better to create a new person. If, however, the utility function is concave, then your argument works and slow memory erasure is preferable.

But there is an even deeper philosophical issue here - over a long period of time, isn't selective memory erasure equivalent to slow death? After all, if a person is a bundle of memories and thoughts, then drift over long periods of time means that eventually you will become an entirely new person - see intuition pump 3. A possible corollary is that immortality as commonly desired is impossible - you either stagnate or become someone else, inevitably. In this sense, mere continuation of physical life is a separate, easier problem than that of "true immortality."
:PROPERTIES:
:Author: LieGroupE8
:Score: 2
:DateUnix: 1476138369.0
:DateShort: 2016-Oct-11
:END:

***** And why do you think that the function is convex?

There are two advantages to selective memory erasure over death:

1. It allows to retain the memories with the highest utility. Not all memories have the same value; replacing the low-value memories causes the average value to go up over time, whereas a new person would have the same average memory value as the old one did previously.

2. Aggregate data does not take up more space, it only becomes more precise. Many skills are not about learning new information, but rather consist of precisely tuning existing heuristics. For example, playing an instrument certainly belongs into this category and is considered by many people to be valuable.
:PROPERTIES:
:Author: suyjuris
:Score: 1
:DateUnix: 1476176131.0
:DateShort: 2016-Oct-11
:END:


*** My objection to Premise 2 is that goodness without an agent is undefined. I also don't see how you solve the "ocean warming itself around a candleflame" problem of trying to balance the goods of uncountably many counterfactual people and finitely many actual people in whom you create and whom you destroy.
:PROPERTIES:
:Score: 3
:DateUnix: 1476134212.0
:DateShort: 2016-Oct-11
:END:

**** As in my reply to artifex0, I am not assuming that all potential persons have moral value which is denied them by preventing their existence - rather, there is some value in simply instantiating a new person, who will have new experiences, regardless of who that person is.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476139930.0
:DateShort: 2016-Oct-11
:END:

***** u/deleted:
#+begin_quote
  rather, there is some value in simply instantiating a new person, who will have new experiences, regardless of who that person is.
#+end_quote

Why?
:PROPERTIES:
:Score: 4
:DateUnix: 1476144366.0
:DateShort: 2016-Oct-11
:END:

****** Because why not? I assume that this is a plausible value for a person to have. As a motivation for having children, for example.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476156784.0
:DateShort: 2016-Oct-11
:END:

******* u/deleted:
#+begin_quote
  Because why not?
#+end_quote

Because "goodness" only makes sense in relation to a person for whom something is good, even if only counterfactually.
:PROPERTIES:
:Score: 1
:DateUnix: 1476188278.0
:DateShort: 2016-Oct-11
:END:

******** So essentially you're saying that an agent cannot coherently place moral value on worlds in which it specifically does not exist. I am not sure how philosophically defensible that is. It seems that a parent can coherently value worlds in which their children continue to exist even if the parent is gone.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476205329.0
:DateShort: 2016-Oct-11
:END:

********* That's not at all what I'm saying. I'm saying that "good" is a function of states, defined conditional on some person, and while the "goodness function" can thus evaluate states the person never observes (or cannot observe in principle), you can't "marginalize out" the person.

People can be valuable to themselves or to others, but not to nobody at all. There is no coherent view from nowhere.
:PROPERTIES:
:Score: 2
:DateUnix: 1476207268.0
:DateShort: 2016-Oct-11
:END:

********** I'm confused about what you mean by "marginalize out."

Anyways, there is no goodness function that is not implemented in some mind, true. But there is no contradiction in having a goodness function that prefers states that entail the nonexistence of the minds that implement it. That might make the goodness function self-defeating practically, though not formally. If there is a way for the goodness function to be transmitted on to new minds, then it is not even practically self-defeating.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476216099.0
:DateShort: 2016-Oct-11
:END:


*** I think there may be a confusion in that argument between the value of potential ends and the value of potential means to ends.

If something is a means to an end, it makes sense to promote it even if it doesn't yet exist- it has value even when it's just a possibility. I don't think the same can be said of things that are valued as ends unto themselves- like people.

Compare the morality of killing a child with that of convincing a couple not to have a child. Although the end result of both is the non-existence of a child, only one of the two is inherently wrong, since it would make no sense to promote the interests of a potential child for that child's own sake.

Of course, that's complicated by the fact that human life isn't only an end unto itself, but also a means for promoting other ends. For example, if everyone in the world decided not to have children, that would be a problem, since we value humanity as a unit, and individual humans are, in addition to being ends unto themselves, also necessary means for the existence of that unit.

Still, exchanging an existent person for a potential person is never a morally justifiable trade- regardless of whether that new person will live a better life. Although that new person will have just as much right to live as the previous person, when it comes to ends as opposed to means to ends, I don't think "will have value" is ever a rational reason to act.

Otherwise, you could reduce the immorality of murder by having a child, and any time you prevented a person from being born, you'd be culpable for the mass murder of all of their potential descendants.
:PROPERTIES:
:Author: artifex0
:Score: 3
:DateUnix: 1476135477.0
:DateShort: 2016-Oct-11
:END:

**** "Still, exchanging an existent person for a potential person is never a morally justifiable trade- regardless of whether that new person will live a better life."

I think, on the other hand, that this statement is based on intuition and does not always hold up. The argument is meant to give intuition for a possible case where it does not hold up.

"Otherwise, you could reduce the immorality of murder by having a child"

There is a difference between reducing net total badness and reducing the immorality of a particular act. Having a child certainly does make the total outcome better, though murder is just as bad as it always was, and there is still no excuse to do it.

"...any time you prevented a person from being born, you'd be culpable for the mass murder of all of their potential descendants."

I am not assuming that all potential persons have moral value which is denied them by preventing their existence - rather, there is some value in simply instantiating a new person, who will have new experiences, regardless of who that person is.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476139802.0
:DateShort: 2016-Oct-11
:END:

***** u/artifex0:
#+begin_quote
  ...there is some value in simply instantiating a new person...
#+end_quote

I don't disagree, but I think that when such a decision to replace one person with a new one is made, the new person can only be rationally valued as a means to some other end- which has to be weighed against the inherent value of the living person.

An extant person has an inherent right to exist, while a potential person doesn't.
:PROPERTIES:
:Author: artifex0
:Score: 1
:DateUnix: 1476153232.0
:DateShort: 2016-Oct-11
:END:

****** I would say that while no /particular/ potential person has the right to exist, it could still be that case that we have obligations to bring /some/ potential person into existence. For example, no particular potential baby has the right to be born, but it would be a tragedy if no more babies were born from this point on.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476157237.0
:DateShort: 2016-Oct-11
:END:


*** +1. No thoughts, still ruminating. But this is the strongest argument against universal immortality I have heard to date.
:PROPERTIES:
:Author: thecommexokid
:Score: 2
:DateUnix: 1476127855.0
:DateShort: 2016-Oct-10
:END:


*** This seems to be highly related to philosophy problems like [[http://plato.stanford.edu/entries/repugnant-conclusion/][The Repugnant Conclusion]]. One of the primary philosophical challenges to utilitarianism and it's variants is the lack of a sensible way to aggregate and compare utility of different potential populations. I don't pretend have a wholly satisfactory solution to this sort of question.
:PROPERTIES:
:Author: Escapement
:Score: 2
:DateUnix: 1476137614.0
:DateShort: 2016-Oct-11
:END:

**** I've heard of the repugnant conclusion, and the problems with utilitarianism surrounding it.

Taking a deontological approach might make my initial argument even more plausible - you could try to establish that there is a deontological imperative to eventually die, analogous to sacrificing yourself to save someone else. Maybe.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476140062.0
:DateShort: 2016-Oct-11
:END:


*** It seems like followed to its logical conclusion this would be the exact sort of thing that would quickly wipe out humanity if put as a GAI's utility function.

This ought to incentivize the creation of a singleton wiping out humanity, /after all people are made of resources that could be used to simulate lots of perfectly happy ems/, simulated people can live their lives /absurdly fast/, so these kinds of problems aren't just going to come up after massive amounts of time either.

Of course since your model places some small penalty on death, by far the better solution would be to just create a singleton that can just simulate all that pleasure for itself and doesn't have any diminishing returns; if you still penalize memory erasable enough for that to come up then the AI will just replace itself with a predecessor every so often. So obviously the much simpler solution is just effectively a paperclipper. Of course if you place utility on distinctively human forms of pleasure the AI might make itself a bit more human, so maybe it'll even feel bad, which i'm /sure/ will console you.

Of course I think the big reason to reject you premises as *AugSphere* pointed out, is that this type of valuing /potential/ people is extremely fishy. It also requires you /place value on people based on their happiness/, that's kind of unavoidable.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1476149387.0
:DateShort: 2016-Oct-11
:END:


*** Your argument, in short, is that memory erasure is bad and should therefore be avoided; and that it makes sense to allow one person to die so that another might step in in his place.

But death erases (or at least makes inaccessible to the living) /all/ memories. If memory erasure is bad, then death is surely worse? And one should not accept a greater evil in the place of a lesser.

On these grounds, I believe your point (2) fails.
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1476192467.0
:DateShort: 2016-Oct-11
:END:


*** u/TennisMaster2:
#+begin_quote
  Assuming that there is no aging, so that full quality of life is present right up until the end, death becomes a mere discontinuity in experience, like going under anesthesia and waking up as a completely different person.
#+end_quote

How can death be a discontinuity in experience if it's the end of experience? This point presupposes reincarnation, in my understanding, as it's the only case where one may wake up after death.
:PROPERTIES:
:Author: TennisMaster2
:Score: 2
:DateUnix: 1476216292.0
:DateShort: 2016-Oct-11
:END:

**** In the context of the sentence, the discontinuity is between a former person's consciousness, and the consciousness of the new person created from the resources of the former person. This only superficially resembles reincarnation. There is of course no "I" transmitted between the two persons, so really that choice of comparison is arbitrary; we might as well compare the discontinuity between the consciousness of two existing persons, such as you and me.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476216920.0
:DateShort: 2016-Oct-11
:END:

***** I see. Thanks for the clarification.
:PROPERTIES:
:Author: TennisMaster2
:Score: 1
:DateUnix: 1476217611.0
:DateShort: 2016-Oct-11
:END:


*** I've never been one to push universal immortality, personally. There needs to be turnover for there to be innovation. Look at companies, it's much harder for them to reinvent themselves than it is for them to slowly die off and be replaced by more innovative ones. It's often better for consumers, too.

And it's probably ever harder for people, who can't exactly have internal workforce turnover unlike a company.
:PROPERTIES:
:Author: CarVac
:Score: 1
:DateUnix: 1476131059.0
:DateShort: 2016-Oct-10
:END:


** Should the placebo effect have been kept secret so that it could be used more efficaciously among the general population?

I mean, it'd be much more difficult to keep secret than the atomic bomb, but imagine the use we could get out of it.

(Currently the best argument I've heard against prescribing placebos is that it lowers the trust of doctors)
:PROPERTIES:
:Author: eniteris
:Score: 2
:DateUnix: 1476125401.0
:DateShort: 2016-Oct-10
:END:

*** Placebo works even if you know it's placebo though. Why keep it secret?
:PROPERTIES:
:Author: Anderkent
:Score: 13
:DateUnix: 1476125597.0
:DateShort: 2016-Oct-10
:END:

**** Yup, when I have a headache I'll chug a big glass of water, and think to myself "thank goodness for the placebo effect" when enjoying the resulting relief, free of painkillers.
:PROPERTIES:
:Author: CarVac
:Score: 6
:DateUnix: 1476127523.0
:DateShort: 2016-Oct-10
:END:

***** Dehydration can cause headaches....
:PROPERTIES:
:Author: electrace
:Score: 3
:DateUnix: 1476130642.0
:DateShort: 2016-Oct-10
:END:

****** Yes, but even when I'm properly hydrated (clear urine) I will drink even more.
:PROPERTIES:
:Author: CarVac
:Score: 1
:DateUnix: 1476130814.0
:DateShort: 2016-Oct-10
:END:


****** For me, drinking water while I have a headache (likely caused by dehydration) cures it on the order of minutes. It certainly works faster than seems plausible, as if my body is pre-empting the effect of the cure.

That said, I don't actually know how long it should take, or even whether the dehydration headache is a somatic or psychosomatic effect.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 1
:DateUnix: 1476163482.0
:DateShort: 2016-Oct-11
:END:


**** Placebo works even better when you don't know it's a placebo.

Also, by keeping the placebo effect a secret, you could give doctors free reign to prescribe placebos for treatments.

The placebo effect is crazy, man.
:PROPERTIES:
:Author: eniteris
:Score: 3
:DateUnix: 1476128006.0
:DateShort: 2016-Oct-10
:END:


**** It's actually unclear whether that's the case since its difficult to tell how subjects respond to knowing they're taking a placebo when they know about the placebo effect.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1476130655.0
:DateShort: 2016-Oct-10
:END:


*** u/ulyssessword:
#+begin_quote
  (Currently the best argument I've heard against prescribing placebos is that it lowers the trust of doctors)
#+end_quote

[[http://www.dcscience.net/2015/12/11/placebo-effects-are-weak-regression-to-the-mean-is-the-main-reason-ineffective-treatments-appear-to-work/][This article]] ([[http://slatestarcodex.com/2015/12/18/links-1215-winter-wond-url-and/][Found via SSC]]), brings up another reason.

It attributes much of the purported power of placebos to regression to the mean, as opposed to any effect that the placebos actually have on the person. Including the effect of regression to the mean in "placebo effects" is fine if you're comparing them to a drug (which has the same placebo effects and the same regression to the mean), but is bad when comparing placebos to simply waiting.
:PROPERTIES:
:Author: ulyssessword
:Score: 3
:DateUnix: 1476142902.0
:DateShort: 2016-Oct-11
:END:
