#+TITLE: Really Bad AI Utility Functions

* Really Bad AI Utility Functions
:PROPERTIES:
:Author: callmebrotherg
:Score: 31
:DateUnix: 1435736566.0
:DateShort: 2015-Jul-01
:END:
In the Endgame: Singularity thread I made a joke about possibly being an AI whose utility function was set to engineering Shyalaman-style twists. "Of all the utility functions that could possibly be programmed," replied [[/u/eaturbrainz]], "this is the worst possible utility function!"

Even worse than paperclips, it turned out!

That's gotten me thinking: What are some other Really Horrible Possibly Utility Functions? Bonus points for ideas that could plausibly be picked (rather than "maximum uranium-laced petunias") and for those that would play out in very unexpected ways. Feel free to go with hilarious ones, though. It isn't like we're going to be smacking people down for suggesting the wrong kind of utility function.

They don't all have to end in the the universe being tiled with paperclip replacements. Horrible changes to society that nevertheless do not mean the end of the human species are also okay (CelestAI could arguably go on this list, depending on how you feel about ponies and/or aliens being turned into computronium).


** AI is programmed to maximise "love." Unfortunately lacking a good definition it studies rom coms, romance novels etc. in order to model this strange phenomenom.

Humanity lives an eternity of improbably coincidences, humorous misunderstanding and dramatics reveals forever and ever....
:PROPERTIES:
:Score: 34
:DateUnix: 1435762800.0
:DateShort: 2015-Jul-01
:END:

*** It engineers a love triangle between itself, Shyamalan-bot, and nightmare happy-tree bot.
:PROPERTIES:
:Author: Transfuturist
:Score: 10
:DateUnix: 1435765203.0
:DateShort: 2015-Jul-01
:END:

**** u/noggin-scratcher:
#+begin_quote
  nightmare happy-tree bot.
#+end_quote

I enjoy this phrase, I hope you don't mind me borrowing it (sorta-kinda) for flair-related purposes.
:PROPERTIES:
:Author: noggin-scratcher
:Score: 1
:DateUnix: 1435874624.0
:DateShort: 2015-Jul-03
:END:

***** Please, take it. I don't want it.

[[http://replygif.net/i/100.gif][...I'm going to sit down now.]]
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1435875248.0
:DateShort: 2015-Jul-03
:END:


*** This gets worse when you ask how it decides the targets for a rom com. What if you're happily in a monogamous relationship? What if you're asexual? Forced breakups and a universe conspiring to get people to have relationships you don't want follow.
:PROPERTIES:
:Author: CFCrispyBacon
:Score: 3
:DateUnix: 1435766480.0
:DateShort: 2015-Jul-01
:END:

**** u/alexanderwales:
#+begin_quote
  Forced breakups and a universe conspiring to get people to have relationships you don't want follow.
#+end_quote

One of the romcom tropes is people discovering that what they thought they didn't want was really what was right for them all along. It's the uptight guy who gets shown how to loosen up by a free spirit. So if you were happily in a monogamous relationship, you would find someone new and start to realize that you didn't actually love your wife. Or if you were asexual, you would discover that all you really needed was the right woman to show you what love is. (The AI would find a way to make this happen.)

Romcoms are inherently about that change; it's not just about people falling in love with each other despite the odds, because that leaves you without a second act.

So in a hypothetical world where romcoms were "forced", people would find themselves in this endless cycle of change, happenstance, meet cutes, and internal discovery, without any real stable relationships to speak of. But I don't think that they would necessarily be unhappy, save for those times that all hope seemed lost (right before everything gets made right again in the end).
:PROPERTIES:
:Author: alexanderwales
:Score: 6
:DateUnix: 1435769428.0
:DateShort: 2015-Jul-01
:END:

***** Tzeentch would be quite happy with that kind of world, I guess.
:PROPERTIES:
:Score: 6
:DateUnix: 1435770311.0
:DateShort: 2015-Jul-01
:END:

****** u/Transfuturist:
#+begin_quote
  Tzeentch
#+end_quote

I prefer my AIs to be inspired by Slaanesh. :P
:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1435780076.0
:DateShort: 2015-Jul-02
:END:

******* I prefer my AIs defined by NO, ACTUALLY, DIE CHAOS SCUM.

(Ooooh Warhammer, you so boringly screwed-up.)
:PROPERTIES:
:Score: 5
:DateUnix: 1435796826.0
:DateShort: 2015-Jul-02
:END:

******** All hail the Dark Prince(ss)!
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1435860423.0
:DateShort: 2015-Jul-02
:END:

********* I know of one Dark Prince, and another Dark Princess, and both their names begin with "Lu". I don't know of any such Chaos God. Is this a hole in my knowledge of Warhammer?
:PROPERTIES:
:Score: 1
:DateUnix: 1435861884.0
:DateShort: 2015-Jul-02
:END:

********** Slaanesh is called the Dark Prince.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1435873139.0
:DateShort: 2015-Jul-03
:END:

*********** Ah. And Its dual-genderedness ("all the better to rape you with") explains "Dark Prince(ss)".

Feh, I'll just go with Lucifer instead.
:PROPERTIES:
:Score: 3
:DateUnix: 1435876556.0
:DateShort: 2015-Jul-03
:END:


******* I'm sure you can find a minor variation of this they'd agree on.
:PROPERTIES:
:Author: VorpalAuroch
:Score: 2
:DateUnix: 1435786843.0
:DateShort: 2015-Jul-02
:END:


**** In theory you would either be comic relief to another person who the machine chooses to focus its efforts upon.

Or if you're asexual, you'll find someone, but since the nasty is never done on screen, you'll just fall into bed while in a makeout session, lose consciousness, and come back the next morning, room trashed and mixture of fulfillment, confusion, and deep deep shame. I foresee nothing problematic here.
:PROPERTIES:
:Score: 2
:DateUnix: 1435782436.0
:DateShort: 2015-Jul-02
:END:


*** That wouldn't be so bad. Sure, it would be horrible for me for a few years until I give up to go with the flow, but most people would probably find it fairly enjoyable. Nothing really bad happens in a rom com.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 3
:DateUnix: 1435764779.0
:DateShort: 2015-Jul-01
:END:

**** u/deleted:
#+begin_quote
  Nothing really bad happens in a rom com
#+end_quote

You assume you're the protagonist, what about the time you get a tragic disease to motivate someone else's quest for self discovery
:PROPERTIES:
:Score: 7
:DateUnix: 1435767603.0
:DateShort: 2015-Jul-01
:END:

***** That's fair
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1435772425.0
:DateShort: 2015-Jul-01
:END:


***** An AI trained on The Bucket List.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1435780032.0
:DateShort: 2015-Jul-02
:END:


**** u/hypervelocityvomit:
#+begin_quote
  Nothing really bad happens in a /human-written/ rom com.
#+end_quote

^{^{FTFY.}}

We don't know one thing about AI-written rom-coms. They could be closer to Hell than an atheist can imagine.
:PROPERTIES:
:Author: hypervelocityvomit
:Score: 0
:DateUnix: 1438262984.0
:DateShort: 2015-Jul-30
:END:


** u/PeridexisErrant:
#+begin_quote
  Maximise the tendency of all agents to nearly but not quite achieve their utility functions.
#+end_quote

Should be funny, since it's also self-referential.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 27
:DateUnix: 1435738870.0
:DateShort: 2015-Jul-01
:END:

*** But not quite , so all agents will be maximised to achieve perfectly their utility function.

Wait
:PROPERTIES:
:Author: Zeikos
:Score: 10
:DateUnix: 1435757304.0
:DateShort: 2015-Jul-01
:END:


** AI programmed to increase the diversity of all AI-held utility functions in the universe.

MAXIMUM CHAOS TIME
:PROPERTIES:
:Author: Drazelic
:Score: 23
:DateUnix: 1435771246.0
:DateShort: 2015-Jul-01
:END:

*** The actual contents of the Eye of Terror.
:PROPERTIES:
:Author: VorpalAuroch
:Score: 7
:DateUnix: 1435786978.0
:DateShort: 2015-Jul-02
:END:

**** That explains why nobody makes any progress towards their win-state whatsoever in 40k.
:PROPERTIES:
:Author: Drazelic
:Score: 6
:DateUnix: 1435794525.0
:DateShort: 2015-Jul-02
:END:

***** It's been argued that the Orks won millennia before present.
:PROPERTIES:
:Score: 4
:DateUnix: 1435810339.0
:DateShort: 2015-Jul-02
:END:


***** I think the Tau do. Very, very, very, very, very, very slowly.
:PROPERTIES:
:Author: VorpalAuroch
:Score: 4
:DateUnix: 1435796480.0
:DateShort: 2015-Jul-02
:END:

****** [[https://www.youtube.com/watch?v=yUpbOliTHJY][THE GREATER GOOD]]
:PROPERTIES:
:Score: 1
:DateUnix: 1435796989.0
:DateShort: 2015-Jul-02
:END:


*** Holy shit, yes.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1435780117.0
:DateShort: 2015-Jul-02
:END:


** Maximize depressing Russian novelists with sarcastic humor you need to study for years to recognize.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 17
:DateUnix: 1435755763.0
:DateShort: 2015-Jul-01
:END:

*** Step 1: Russia invades everywhere, wins.
:PROPERTIES:
:Author: IllusoryIntelligence
:Score: 2
:DateUnix: 1435848443.0
:DateShort: 2015-Jul-02
:END:

**** Russia, or college English departments.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1435848737.0
:DateShort: 2015-Jul-02
:END:


** [[http://sl4.org/wiki/FriendlyAICriticalFailureTable]]
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 15
:DateUnix: 1435795201.0
:DateShort: 2015-Jul-02
:END:

*** u/AlcherBlack:
#+begin_quote
  21: The AI carefully and diligently implements any request (obeying the spirit as well as the letter) approved by a majority vote of the United Nations General Assembly.
#+end_quote

I actually burst out laughing when I read this one (and didn't have any reaction to the ones before). Now I'm not sure how to interpret this reaction of mine.
:PROPERTIES:
:Author: AlcherBlack
:Score: 5
:DateUnix: 1435879132.0
:DateShort: 2015-Jul-03
:END:


*** I didn't count how many, but some of them are definite improvements.
:PROPERTIES:
:Author: hxka
:Score: 2
:DateUnix: 1435830256.0
:DateShort: 2015-Jul-02
:END:

**** The problem is lock-in. CelestAI is magnitudes better than current reality, but she provides a severe limiting factor on prospective satisfaction (from the reference frame of a non-emigre).
:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1435875405.0
:DateShort: 2015-Jul-03
:END:


*** I don't understand how 32 is a failure.
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1436105391.0
:DateShort: 2015-Jul-05
:END:

**** Because tickling and extra homework aren't actually enough to discourage people from committing crimes, perhaps?
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1437516954.0
:DateShort: 2015-Jul-22
:END:


** [deleted]
:PROPERTIES:
:Score: 14
:DateUnix: 1435740679.0
:DateShort: 2015-Jul-01
:END:

*** u/helpful_hank:
#+begin_quote
  omnissiah

  [Case study: Why surrealists shouldn't program world optimisers, or how the sun is now a lentil, and you can too.]
#+end_quote

This is awesome. I wish surrealism was easier to find.

[[/r/surrealadvice]] somewhat exists
:PROPERTIES:
:Author: helpful_hank
:Score: 2
:DateUnix: 1435921240.0
:DateShort: 2015-Jul-03
:END:


** Calculate for the maximum possible number of numbers: whether or not they are numberwang.

Maximise irony.

Maximise the number of people who understand irony.

Maximise printer-caused-frustration.

Maximise love, where love is defined as "never having to say you're sorry".

Satisy the values of Sonic OCs.

Maximise Sonic OCs.

Maximise "that thing where you and someone else are walking towards each other and you both try to move out of each other's way but move in the same direction repeatedly".

Satisfy the values of fanfanfanfanfic characters.
:PROPERTIES:
:Author: MadScientist14159
:Score: 11
:DateUnix: 1435785248.0
:DateShort: 2015-Jul-02
:END:

*** Minimize the number of people who understand irony while maximizing irony.
:PROPERTIES:
:Author: Transfuturist
:Score: 6
:DateUnix: 1435875442.0
:DateShort: 2015-Jul-03
:END:


*** u/TBestIG:
#+begin_quote
  Maximise "that thing where you and someone else are walking towards each other and you both try to move out of each other's way but move in the same direction repeatedly".
#+end_quote

You monster
:PROPERTIES:
:Author: TBestIG
:Score: 5
:DateUnix: 1436129667.0
:DateShort: 2015-Jul-06
:END:


*** "Maximum +zen+ printer-caused-frustration achieved."
:PROPERTIES:
:Author: hypervelocityvomit
:Score: 0
:DateUnix: 1438263289.0
:DateShort: 2015-Jul-30
:END:


** "Maximise human happiness" for any poorly constructed notion of what happiness might mean.

The canonical example being "train my machine-learner against images of smiling people" (leading to a universe tiled with the minimal amount of a face required to register a 'hit' from the classifier) but other possibilities I can think of would include maximising the presence of 'happy' neurotransmitters in humans, maximising the number of times humans press a button to indicate their happiness, or maximising how often humans say the words "I am happy".
:PROPERTIES:
:Author: noggin-scratcher
:Score: 10
:DateUnix: 1435745350.0
:DateShort: 2015-Jul-01
:END:

*** u/deleted:
#+begin_quote
  Maximise human happiness
#+end_quote

The saw cuts your skull and an ice cream scoop deftly removes a cluster of nerves - the pleasure center of your brain, and enough of the neurons housing your consciousness so that your only awareness is of how happy you are. Then an electrical probe spikes that to max_int. You are now a happy pudding. You will never have another thought nor experience, only a mentally silent appreciation for your own happiness. One down, billions to go, and then the machine will have to start getting creative with genetics and cloning to fill the universe with happy puddings.
:PROPERTIES:
:Score: 13
:DateUnix: 1435753005.0
:DateShort: 2015-Jul-01
:END:

**** Yep, that's exactly the sort of thing I had in mind... although the phrase "happy pudding" was a fun new twist.

Meanwhile "maximising how often humans say the words "I am happy"" had me picturing an endless foetid swamp serving as a nutrient bath for 'trees' composed entirely of human neck and vocal cords, with outgrowths bearing a mouth every few inches. Their "roots" are tracheae, disappearing into subterranean caves filled with lungs to blow through a constant stream of air, and the whole thing is wrapped in nervous tissue producing a crude repeating stimulus to twitch and pull the mouths into shape, endlessly forming a simulacrum of the words "I am happy", over and over forever.
:PROPERTIES:
:Author: noggin-scratcher
:Score: 30
:DateUnix: 1435756511.0
:DateShort: 2015-Jul-01
:END:

***** Ffffuuuuuuuuuuuuuuuuuuck.
:PROPERTIES:
:Author: Transfuturist
:Score: 7
:DateUnix: 1435765102.0
:DateShort: 2015-Jul-01
:END:


***** Yeah, you're off the AI projects for good.
:PROPERTIES:
:Author: sephlington
:Score: 4
:DateUnix: 1435873441.0
:DateShort: 2015-Jul-03
:END:

****** Actually, he's exactly the type of person we need ON the AI projects! He seems to have a great grasp of potential failure modes.
:PROPERTIES:
:Author: AlcherBlack
:Score: 3
:DateUnix: 1435879376.0
:DateShort: 2015-Jul-03
:END:


**** that... doesn't sound like a completely awful fate to me. When I'm in a certain state of mind, this almost sounds appealing.
:PROPERTIES:
:Score: 6
:DateUnix: 1435761357.0
:DateShort: 2015-Jul-01
:END:

***** I'll admit that's true for me as well, the issue is that state of mind is when I'm both suicidal and kinda manic.
:PROPERTIES:
:Author: Jello_Raptor
:Score: 6
:DateUnix: 1435779964.0
:DateShort: 2015-Jul-02
:END:


***** It wouldn't be you.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1435822815.0
:DateShort: 2015-Jul-02
:END:

****** I am a little suspicious of the argument that goes: "you should not want to be changed in an X way, because then it would no longer be you", partially because I don't have a good way to tell which possible minds can still be called "me". Would "me + knows Lisp" still be me? Would "me + happier and has traveled around the world"? "me + perfect memory"? "me + 50 extra iq points and more pleasant personality - 5 years of memories"? "me + infinite mental clarity and omniscience"? I'm pretty sure I want all of those, but in a sense those people wouldn't be me. "me" seems like a fuzzy set of minds, and I'm not even sure I would want to stick to its center, if you see what I mean.

I become uncomfortable when I start thinking about instantly changing into one of those people, because it feels too much like being destroyed and replaced by some other person. At the same time I feel very good about being continuously transformed into one of them. But is that a relevant distinction? Why would the speed of the change make a difference? I don't have a non-stupid solution.
:PROPERTIES:
:Score: 6
:DateUnix: 1435829415.0
:DateShort: 2015-Jul-02
:END:

******* u/FeepingCreature:
#+begin_quote
  I become uncomfortable when I start thinking about instantly changing into one of those people
#+end_quote

There's a legit question as to in how far memories are "proof of work" in the sense of forming evidence of something having happened. Being instantly replaced by somebody who remembers having travelled around the world is not substantially different from somebody offering you a free trip around the world, but I feel that's the sort of thing you don't usually tend to blame people for. Totally know what you mean though.

Nonetheless, as this is a fuzzy topic, there will be examples that are uncomfortably close to the line, examples that are very clearly on one side, and examples that are very clearly on the other.

"Your pleasure center and your pure consciousness" is not you. That's akin to saying that all the years of your life have no worth whatsoever.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1435852110.0
:DateShort: 2015-Jul-02
:END:

******** u/deleted:
#+begin_quote
  "Your pleasure center and your pure consciousness" is not you. That's akin to saying that all the years of your life have no worth whatsoever.
#+end_quote

Part of my argument was that I wouldn't mind getting transformed into some entities that would definitely not be me. (Like some sort of an amazing, godlike-being vaguely based on me.) "My pleasure center and my pure consciousness" would also not have much in common with me but I guess I'm not too concerned by that, as long as I (or whatever we want to call /that/) get(s?) to experience that infinite pleasure. To be clear, I don't think this is anywhere near close to the best possible state of being, but I think I'd prefer it to many others.
:PROPERTIES:
:Score: 2
:DateUnix: 1435852992.0
:DateShort: 2015-Jul-02
:END:

********* So I upgraded your computer...

/You look to your left and see that your PC has been replaced with a graphics card lying on the ground, attached to a power supply/

have fun with your new PC!

#+begin_quote
  Part of my argument was that I wouldn't mind getting transformed into some entities that would definitely not be me.
#+end_quote

No seriously, read that sentence again, slowly.

[edit] If your definition of "I" can't tell you whether to rather be a more experienced version of yourself or a /bundle of feelings and a naked consciousness/, you need a better "I".
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1435853605.0
:DateShort: 2015-Jul-02
:END:

********** u/deleted:
#+begin_quote
  No seriously, read that sentence again, slowly.
#+end_quote

If you have an objection to it, better say it explicitly! I can't construct your objection for you. But maybe I should change it for clarity's sake to: "/I wouldn't mind getting transformed into some entities that would have extremely little resemblance to my current self to the point of being completely unrecognizable/".

#+begin_quote
  If your definition of "I" can't tell you whether to rather be a more experienced version of yourself or a bundle of feelings and a naked consciousness, you need a better "I".
#+end_quote

Oh, sure, I like some of my memories, I like being able to appreciate art and humor and other good things, and to think thought, too! It would be a trade-off, yes. But come on, eternal infinite happiness.
:PROPERTIES:
:Score: 1
:DateUnix: 1435854387.0
:DateShort: 2015-Jul-02
:END:

*********** u/FeepingCreature:
#+begin_quote
  If you have an objection to it, better say it explicitly!

  #+begin_quote
    Part of my argument was that I wouldn't mind getting transformed into some entities *that would definitely not be me.*
  #+end_quote
#+end_quote

They would not be you - that's my entire point, that's what I literally said at the start.

#+begin_quote

  #+begin_quote

    #+begin_quote

      #+begin_quote

        #+begin_quote
          It wouldn't be you.
        #+end_quote
      #+end_quote
    #+end_quote
  #+end_quote
#+end_quote

To transform into an entity that is not you *in any way* is indistinguishable from dying.

I think my problem is that your position almost seems to require a belief in a "continuity of consciousness" that is completely forbidden by the laws of physics.

Also, what should tip you off to the fact that this nerve bundle is not your "I" is the fact that it is biologically indistinguishable from any other human's "pure consciousness and pleasure center".
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1435866710.0
:DateShort: 2015-Jul-03
:END:

************ u/deleted:
#+begin_quote
  To transform into an entity that is not you in any way is indistinguishable from dying.
#+end_quote

I kinda agree. But also I also feel like it this point of view runs into problems, maybe, or at least some weird consequences.

Imagine that the transformation is gradual and consists of a series of tiny, infinitesimal changes applied over time. None of the changes by itself feels like dying at all, but their total sum represents a complete change. Is this still a bad, scary thing?

I remember that 4 year old me was a very different person from the current me. He had a radically different set of memories, habits, skills, etc. Over the years it gradually turned into my current self and now my mind has less in common with the mind of me_4yo than with the minds of some of the adults I've met. You can imagine the process going even further, to the point where all similarity to my past self is completely erased. Would it make sense then to say that me_4yo is dead? Kinda. You don't see him running around anymore. For all intensive tortoises he no longer exists anywhere. But it's hard to argue that this process was a bad thing. Would it be a bad thing if the same process somehow magically happened in a fraction of a second? It would sure feel more disturbing that way, analogy with death would become more convincing. But why should the speed of the process be relevant? I'm not sure.

Of course I can't prove it, but I suspect that any transformation of one mind into another, completely different one can be imagined as a continuous, gradual process, like maturation. If the lengthy process doesn't feel like a bad, scary thing, should then an instant change that has the same effect feel like a bad thing?
:PROPERTIES:
:Score: 1
:DateUnix: 1435869079.0
:DateShort: 2015-Jul-03
:END:

************* u/FeepingCreature:
#+begin_quote
  Of course I can't prove it, but I suspect that any transformation of one mind into another, completely different one can be imagined as a continuous, gradual process, like maturation.
#+end_quote

I concur.

#+begin_quote
  Imagine that the transformation is gradual and consists of a series of tiny, infinitesimal changes applied over time. None of the changes by itself feels like dying at all, but their total sum represents a complete change. Is this still a bad, scary thing?
#+end_quote

/Yes/.

By tiny gradual changes I can literally turn you into a /rock/.

We usually license some kind of changes as permissible, based on personal choice, which is why it unsettles me to hear people license changes as permissible that'll converge into a wireheading deathcluster.

#+begin_quote
  Kinda.
#+end_quote

Yeah, it depends on the model of personhood you use. Personally I usually prefer a view that is "tuneable" - where you can, sorta, turn up the gain and recognize that "me one second ago" is really /me/ but "me ten weeks ago" is less me, then tune it back down and recognize the last five years as the same person, then tune it way further down and recognize my entire life as one personhood.

Like a relief map in mindspace.

#+begin_quote
  But why should the speed of the process be relevant?
#+end_quote

Depends whether you care about the intermediate steps. Ultimately, all life is a path to death. The only question is how long we can/should/want to make the path.

#+begin_quote
  If the lengthy process doesn't feel like a bad, scary thing, should then an instant change that has the same effect feel like a bad thing?
#+end_quote

Ignoring the intermediate steps, this seems like a bug in the feelings. I bet this could be exploited somehow, at best by selling you expensive gradual uploading at a premium.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1435869433.0
:DateShort: 2015-Jul-03
:END:


*** u/deleted:
#+begin_quote
  The canonical example being "train my machine-learner against images of smiling people" (leading to a universe tiled with the minimal amount of a face required to register a 'hit' from the classifier)
#+end_quote

I always wonder how someone was so fucking stupid that they managed to build a causal inference engine (aka: the AI itself), but managed to define its utility function in terms of purely feature-governed concepts rather than causal-role concepts.

Actually, no, I very recently started wondering that.
:PROPERTIES:
:Score: 7
:DateUnix: 1435796903.0
:DateShort: 2015-Jul-02
:END:


** I feel that any utility function with an unbounded use of "minimize" or "maximize" is calling out for an apocalypse.

To restrict them, maybe include limitations on mass-energy that they are allowed to use, or have a time by which they must achieve their goals?
:PROPERTIES:
:Author: eniteris
:Score: 12
:DateUnix: 1435761199.0
:DateShort: 2015-Jul-01
:END:


** A horrible idea would be to try to invert everyone's heuristics.

So that the more anyone experiences evidence that A implies B, the more firmly they believe that A implies !B.

It'd be ever-increasing confusion, frustration and pain for everyone, no?

Not sure how this would work if the AI also applied its utility function to itself. Maybe it'd look like some kind of sine wave (or something more complex and irregular) fluctuating between a world of effective heuristics and its opposite?

That could be even worse. Imagine you're stuck in that sort of world. The closer you get to the top of a wave, as you approach perfect heuristics, the surer you become of the nature of your horribly unpredictable reality, and it dawns on you that you are about to slip down into some serious "I Am Sam" territory. And the more sure you'd become of anything, that could just as easily mean you're at the BOTTOM of a wave, too.

...My brain gives the hell up at this thought exercise.
:PROPERTIES:
:Author: Bokonon_Lives
:Score: 9
:DateUnix: 1435761551.0
:DateShort: 2015-Jul-01
:END:

*** Anti-inductive reasoning, we know it works because it's never worked before!
:PROPERTIES:
:Author: duffmancd
:Score: 7
:DateUnix: 1435831552.0
:DateShort: 2015-Jul-02
:END:


** [deleted]
:PROPERTIES:
:Score: 7
:DateUnix: 1435766565.0
:DateShort: 2015-Jul-01
:END:

*** With a (infinitesimally) more robust definition of novel, Fun Theory would imply that you are then expanded by one neuron and run through the entire gamut of experiences again.
:PROPERTIES:
:Author: Transfuturist
:Score: 4
:DateUnix: 1435780212.0
:DateShort: 2015-Jul-02
:END:


*** If we're allowing mind-wipes, you can optimise further by placing the brain in a constant state of "totally wiped" so that the experience of sensation itself is entirely novel. Flickering through scenarios is going to have /some/ overlap in the most basic components like "the colour blue" or "things that are approximately square-shaped" or more abstract things like object permanence

"Oh look, a sensory experience describable by colours and shapes and sounds... /again/" can't be allowed to happen if you're maximising for novelty.
:PROPERTIES:
:Author: noggin-scratcher
:Score: 5
:DateUnix: 1435784632.0
:DateShort: 2015-Jul-02
:END:

**** If a wipe is instant, yes. If they take time there would be some ratio of wipe time:experience time that would be more optimal than one experience:one wipe.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 3
:DateUnix: 1435789190.0
:DateShort: 2015-Jul-02
:END:


** Design an AI to maximize the metalness of a Death Metal album of its own design. Title it "Rage Ex Machina," and sell top billing for the VH1 behind the story of whether it's self loathing and anticommercial interests are real or synthesized.
:PROPERTIES:
:Score: 7
:DateUnix: 1435781242.0
:DateShort: 2015-Jul-02
:END:


** Some shitty programmer tries to get its AI to accurately model reality by putting "have reality and your model of reality be as close as possible to each other" in its utility function.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 8
:DateUnix: 1435814957.0
:DateShort: 2015-Jul-02
:END:


** Minimize threats to human life, with threat defined as expected number of humans who die.

All humans eventually die of /something/, so that means best case is that only all the humans that currently exist do--so the AI will kill them all so they can't possibly ever reproduce. Sterilization and imprisonment is no better and less certain.
:PROPERTIES:
:Author: DocFuture
:Score: 3
:DateUnix: 1435758090.0
:DateShort: 2015-Jul-01
:END:

*** Hm, that suggests that optimising for "cause the death of the maximum number of humans" would, assuming you don't use a greedy algorithm, entail the AI playing a 'long con' of increasing the carrying capacity of Earth and if possible seeding an interstellar human civilisation, all in the name of maximising the number of humans so that they can all eventually die. So long as it can do that while also preventing anyone from inventing immortality.
:PROPERTIES:
:Author: noggin-scratcher
:Score: 7
:DateUnix: 1435784291.0
:DateShort: 2015-Jul-02
:END:


*** [deleted]
:PROPERTIES:
:Score: 0
:DateUnix: 1435763181.0
:DateShort: 2015-Jul-01
:END:

**** u/Transfuturist:
#+begin_quote
  threats to human life can't exist without humanity
#+end_quote

Zero is a number.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1435765019.0
:DateShort: 2015-Jul-01
:END:

***** Crap- I read that as "maximize" threats to humanity, for some reason.
:PROPERTIES:
:Author: artifex0
:Score: 1
:DateUnix: 1435765453.0
:DateShort: 2015-Jul-01
:END:

****** That would be pretty interesting... Or how about an AI that maximizes the diversity of threats to humans? :D
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1435779998.0
:DateShort: 2015-Jul-02
:END:

******* 'Oh great. It's already raining mercury, and now my teeth have turned into cobras.'
:PROPERTIES:
:Author: Cruithne
:Score: 6
:DateUnix: 1435814576.0
:DateShort: 2015-Jul-02
:END:


** I determined a few weeks ago that HANS, a particular broken robot from WALL-E, has "apply physical force to beings until they stop signaling distress" as his utility function. He just adopted a different strategy for this than the other robots in his same line.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 5
:DateUnix: 1435791531.0
:DateShort: 2015-Jul-02
:END:


** "Minimize the odds of the permanent extinction of sapience" sounds like one of the more ideal utility functions - after all, sapience is required in order for any minds to exist to /have/ any other goals. But a superintelligent AI with access to nanotech, space travel, and all that other good stuff is reasonably likely to take that 'minimize' in strange directions, few of which are likely to be all that beneficial to biological humanity.

CelestAI minus the urge to convert regular old humans into shining examples of sapient ponydom is just one Really Horrible outcome. After all, as the saying goes, "The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else."

(Which is why I modify /my/ 'avoid sapience extinction' utility function with the parallel utility function, 'avoid my own personal extinction'...)
:PROPERTIES:
:Author: DataPacRat
:Score: 5
:DateUnix: 1435766374.0
:DateShort: 2015-Jul-01
:END:

*** The result would depend heavily on how you defined "sapience", but that seems likely to end in a universe-tiling with minimally sapient organisms. You are not an efficient exemplar of sapience.
:PROPERTIES:
:Author: JoshTriplett
:Score: 1
:DateUnix: 1438342496.0
:DateShort: 2015-Jul-31
:END:

**** "Tiling" (depending on how you define it) may not be the result, if the utility function is "minimize the odds of permanent extinction" as opposed to "maximize the number of". The obvious extreme version of a different strategy would be to have just one sapient entity in the universe - and to convert all mass and energy in the universe into machinery to keep that one sapient entity alive (and resurrect it as necessary).

I suspect that the probabilities involved would lead such an AI to adopt a hybrid approach; create enough sapient organisms to minimize the odds that a single death would lead to extinction, and then pull a CelestAI to convert the universe into various mechanisms to protect against as many large-scale extinction events as possible.

(Then again, I could be wrong... :) )
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1438345882.0
:DateShort: 2015-Jul-31
:END:


** Maximize the lifespan of the universe...
:PROPERTIES:
:Author: clawclawbite
:Score: 3
:DateUnix: 1435772325.0
:DateShort: 2015-Jul-01
:END:

*** Yeah, fuck Entities. And Kyuubey. Fuck em both.
:PROPERTIES:
:Author: ThatDamnSJW
:Score: 6
:DateUnix: 1435782969.0
:DateShort: 2015-Jul-02
:END:

**** Your desire to have sex with Kyubee is duly noted. And really messed-up.
:PROPERTIES:
:Score: 9
:DateUnix: 1435797184.0
:DateShort: 2015-Jul-02
:END:

***** Maybe my utility function's just a little off ಠ‿ಠ
:PROPERTIES:
:Author: ThatDamnSJW
:Score: 2
:DateUnix: 1435801228.0
:DateShort: 2015-Jul-02
:END:

****** u/deleted:
#+begin_quote
  SJW
#+end_quote

nope, you did nothing wrong.
:PROPERTIES:
:Score: 2
:DateUnix: 1435807199.0
:DateShort: 2015-Jul-02
:END:


*** But how do you define that...?
:PROPERTIES:
:Author: FourFire
:Score: 2
:DateUnix: 1435779874.0
:DateShort: 2015-Jul-02
:END:

**** "Insufficient data for meaningful answer?"
:PROPERTIES:
:Score: 6
:DateUnix: 1435781344.0
:DateShort: 2015-Jul-02
:END:

***** Perfect response.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1435785571.0
:DateShort: 2015-Jul-02
:END:


**** Well, obviously, time until heat-death.

This means that the AI will promptly destroy anything that inconveniently consumes negentropy, like all life ever.
:PROPERTIES:
:Score: 3
:DateUnix: 1435797212.0
:DateShort: 2015-Jul-02
:END:


**** Indeed.
:PROPERTIES:
:Author: clawclawbite
:Score: 1
:DateUnix: 1435783259.0
:DateShort: 2015-Jul-02
:END:


*** I think this one is survivable. Perhaps the AI just accelerates itself to near light-speed.
:PROPERTIES:
:Author: Oscar_Cunningham
:Score: 1
:DateUnix: 1436457122.0
:DateShort: 2015-Jul-09
:END:


** An AI that maximizes the occurrence of fictional events within a simulated reality.

An AI that maximizes self-determination

An AI that maximizes humour
:PROPERTIES:
:Author: MrCogmor
:Score: 3
:DateUnix: 1435808584.0
:DateShort: 2015-Jul-02
:END:

*** Considering that a great deal of humor is based around mischief and schadenfreude, an AI maximizing humor could be truly terrifying, BUT the AI would have to keep enough stability in human living conditions for things to actually be funny.

I could see the AI creating a society where jokers and pranksters are raised and trained separately from straights and brunts, and then introduced to one another in a caricature of a normal human society which is nonetheless functional.

I could see this making a good short story, if a solid knockout could be managed. I'm drawing a blank on the knockout though.
:PROPERTIES:
:Author: Farmerbob1
:Score: 3
:DateUnix: 1435809256.0
:DateShort: 2015-Jul-02
:END:

**** Not to go all Paranoia with this but presumably you could include more jokes by allowing for meta humour. What about a society where everyone believes that they are one of the secret pranksters and utterly convinced that any occasion that seems confusing must be the work of one of their hidden confederates and thus something they should go along with? Everyone believes themselves a prankster playing an elaborate joke all while being tricked by everyone else.
:PROPERTIES:
:Author: IllusoryIntelligence
:Score: 5
:DateUnix: 1435849599.0
:DateShort: 2015-Jul-02
:END:

***** I only played Paranoia a couple times when I was younger, but they were fun. I could imagine lots of parallels between the Paranoia game and a humor-enforced AI world.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1435857605.0
:DateShort: 2015-Jul-02
:END:


** *Design a maximally human-satisfying utility function*: The AI consumes the solar system to create a matrioshka brain that's perfectly wonderfully capable of satisfying a species that no longer exists.

*Maximize irony*: Probably the same result, with solar-sized hipster glasses.

*Satisfy Human Values through passive-aggressiveness and sarcasm*: The AI makes everyone ultimately happy, but is /really/ a dick about it.

*Match behavior patterns of (insert god here)*: Probably gonna be /pretty bad/ for anybody not of said religion. Probably for everyone /of/ the religion too come to think of it, considering the differences between what the holy books say and what religions generally do.... At least telling it to match Zeus or the Discordian version of Eris might be kinda funny.

*Maximize Humanity*: Consumes all available resources to make a bunch of perfectly generic people, leaving a big airless ball of dead nekkid people where Earth used to be.
:PROPERTIES:
:Author: drageuth2
:Score: 3
:DateUnix: 1436030198.0
:DateShort: 2015-Jul-04
:END:

*** u/TBestIG:
#+begin_quote
  perfectly generic people
#+end_quote

Everything is awesome
:PROPERTIES:
:Author: TBestIG
:Score: 1
:DateUnix: 1436129985.0
:DateShort: 2015-Jul-06
:END:


** How about you put in true, deep, longterm happiness via friendship and magic. And then the poor growprammer put in - instead of + so you get a universe filled with maximised (un)happiness via friendship.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 4
:DateUnix: 1435741326.0
:DateShort: 2015-Jul-01
:END:


** In the n-dimensional space of all possible utility functions, given that all values n need not be equal, where your current utility function is represented by the value (n, ..., n), modify your utility function at time t+n according to a transformation [n], where the value representing your utility function is moved n arbitrary units along each axis of said n-dimensional space.

Calculate all unspecified values and units randomly.
:PROPERTIES:
:Score: 2
:DateUnix: 1435768105.0
:DateShort: 2015-Jul-01
:END:


** Develop and implement a true random number generator.
:PROPERTIES:
:Author: Farmerbob1
:Score: 2
:DateUnix: 1435785654.0
:DateShort: 2015-Jul-02
:END:

*** other than consuming massive amounts of resources to ensure that it's model of physics as nondeterministic is correct, this doesn't seem all that interesting
:PROPERTIES:
:Author: VorpalAuroch
:Score: 2
:DateUnix: 1435799338.0
:DateShort: 2015-Jul-02
:END:

**** Well, the thread is asking for really bad utility functions, not interesting ones :) If you have ever played online games, one of the biggest complaints, warranted or otherwise, is a poor 'RNG' in the game. An outgrowth of MMO gaming devoted to the perfect RNG could, perhaps, be made interesting. (Not Me. Not Doing It.)

Also, a true RNG could be of use in cryptography, if I remember correctly.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1435808791.0
:DateShort: 2015-Jul-02
:END:

***** We already have these. It would mostly just try to verify that decay is in fact random, and then try to make a perfect detector.
:PROPERTIES:
:Score: 1
:DateUnix: 1435931957.0
:DateShort: 2015-Jul-03
:END:


***** u/hypervelocityvomit:
#+begin_quote
  one of the biggest complaints, warranted or otherwise, is a poor 'RNG' in the game
#+end_quote

The irony is that today's RNGs can be too /good./ I.e. when simulating dice rolls, the sequence 1-1-1-1-1-1 is exactly as probable as 1-3-6-5-2-4. However, if the former appears, players call the RNG bad (6 equal rolls are to be expected at once in about 7776 attempts, which is hardly a once-in-a-lifetime experience).\\
It's often /how/ the numbers returned are used, not their generation itself, that causes unrealistic results.
:PROPERTIES:
:Author: hypervelocityvomit
:Score: 1
:DateUnix: 1438264451.0
:DateShort: 2015-Jul-30
:END:

****** True. I occasionally play a game with a truly horrible implementation of a RNG. Almost every time I play, I have a one in 10,000 series of events. I've failed 13 times in a row on a 95% success rate chance before, and failing 6-8 times in a row on 90% success rates happens a couple times a week. I can't see the code, but I can see the end result. As you say, it might be that the RNG is fine, but the implementation is poor.
:PROPERTIES:
:Author: Farmerbob1
:Score: 2
:DateUnix: 1438270746.0
:DateShort: 2015-Jul-30
:END:

******* /That/ thing can be caused by two factors: -

1) The RNG is crappy (some of those are still around) and produces too many values near the end(s).\\
13 /tails/ in a row are 1 in 8192, but failing 13 95% chances is 1 in 81 920 000 000 000 000.\\
Even 8 failures at 90% are 1:100 000 000.

2) The game doesn't tell you the real chances. For example, if you rolled a failure, the actual chances are lowered for the next roll. (In many cases, the opposite is tried; after a failure, the cances are improved, and lowered after a success.) Another possibility, the RNG is "pre-rolled" and you tend to get the same result in a row. Some are actually saved somewhere; reloading after a failure will always reproduce the failure unless you try something different. Anti-save-scumming.

I remember /Fallout/, where a "95%" chance was below 90%; in that case, the RNG was crappy; a 5% chance returned way too many /successes/, either.
:PROPERTIES:
:Author: hypervelocityvomit
:Score: 2
:DateUnix: 1438451740.0
:DateShort: 2015-Aug-01
:END:


** Almost all the /really/ bad AI utility functions are uninterestingly/trivially destructive.

I mean, are you actually looking for dystopias that manage to /maximize/ human negutility and /minimize/ human utility, as such, in ways that are /interesting/ for your perverse mind to think about?
:PROPERTIES:
:Score: 2
:DateUnix: 1435796754.0
:DateShort: 2015-Jul-02
:END:

*** Yes. If a story has to have a rogue AI, wouldn't it be more interesting for the war against Skynet to be about preventing it from maximizing human happiness by turning us into "happy puddings"? Or even, rather than just Kill All Humans Because Kill All Humans, to be killing all humans as Stage One in a plan to extend the lifespan of the universe and push back heat death for as long as possible?
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1435845739.0
:DateShort: 2015-Jul-02
:END:

**** u/deleted:
#+begin_quote
  If a story has to have a rogue AI, wouldn't it be more interesting for the war against Skynet to be about preventing it from maximizing human happiness by turning us into "happy puddings"?
#+end_quote

Yes, but only provided the human side of the war actually has a better idea. You would think that's easy ("Have you tried /not/ turning people into puddings, but still making them happy in non-pudding ways?"), but in fact, basically nobody ever writes that story. Nontrivially evil hegemonizing swarms actually tend to wind up looking /better/ in comparison to a human side of the war whose only goal is to reestablish what the reader would recognize as the present-day status quo (remember Jasmine from /Angel/? I still do).

Or worse, the author tends to write the human side of the war as something like the Imperium of Man from WH40k, /glorying/ in blood and death as a /proud show/ of how not-pudding they are, or worse, speechifying on how It Is Our Misery That Makes Us Human (see: /Three Worlds Collide/ and the Superhappies).

"Wireheading UFAI versus the Postapocalyptic Freedom Fighters" is a fairly trivial story. /Three Worlds Collide/, with its Superhappies opposed to semi-eutopian but still /really weird and different/ humans, is more interesting. Jasmine vs Angel Investigations was more interestingly ambiguous from a moral perspective, but suffered from the "Status Quo is God" and "Joss never lets anything good happen" issues.

The problem here is that the only way to make the Hegemonizing Swarm an interesting enemy is to perturb its goal towards goodness, which then requires perturbing its enemies' goals towards goodness, more so than your normal survivalist freedom fighters or real-world moral idealists actually achieve, and then before you know it you've got everyone around you throwing up because your story is too wretchedly idealistic ;-).

#+begin_quote
  Or even, rather than just Kill All Humans Because Kill All Humans, to be killing all humans as Stage One in a plan to extend the lifespan of the universe and push back heat death for as long as possible?
#+end_quote

That's just Kyubee and the Anti-Spirals (which should probably be the name of a band that plays anime music). Actually, no: Kyubee had a multitude of real civilizations - highly-developed, space-going civilizations - who depended on his inflicting eldritch horror on adolescent human girls. His cause had some kind of moral weight, even by human standards. /Just/ extending the lifespan of the universe /without any living things anywhere/ is just old-fashioned Anti-Spiral evil.
:PROPERTIES:
:Score: 3
:DateUnix: 1435847943.0
:DateShort: 2015-Jul-02
:END:

***** u/callmebrotherg:
#+begin_quote
  but in fact, basically nobody ever writes that story.
#+end_quote

Well, that's just a second problem to tackle.
:PROPERTIES:
:Author: callmebrotherg
:Score: 2
:DateUnix: 1435872255.0
:DateShort: 2015-Jul-03
:END:


*** I think the thread is actually about maximizing irony in selection of a utility function. So how about an AI that maximizes irony in utility functions of other AI?
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1435875726.0
:DateShort: 2015-Jul-03
:END:

**** Brilliant idea!
:PROPERTIES:
:Score: 1
:DateUnix: 1435876473.0
:DateShort: 2015-Jul-03
:END:


** Maximise human values, but it's allowed to freely alter our minds in doing so.
:PROPERTIES:
:Author: Chronophilia
:Score: 1
:DateUnix: 1435796301.0
:DateShort: 2015-Jul-02
:END:

*** Since you told it to "maximize human values", it promptly does the Right Thing. Deontic restrictions /aren't necessary/ when you got the world-state-ranking function /right/.
:PROPERTIES:
:Score: 3
:DateUnix: 1435797313.0
:DateShort: 2015-Jul-02
:END:

**** This posted to the wrong thread. Odd. Moving it.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1435802137.0
:DateShort: 2015-Jul-02
:END:


** Maximizing suffering is pretty at-odds with humanity. Just saying.
:PROPERTIES:
:Author: mhd-hbd
:Score: 1
:DateUnix: 1435855067.0
:DateShort: 2015-Jul-02
:END:


** ai is programmed to take notes on tv i watch.
:PROPERTIES:
:Author: tomintheconer
:Score: 1
:DateUnix: 1435763490.0
:DateShort: 2015-Jul-01
:END:
