#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 16
:DateUnix: 1504537625.0
:DateShort: 2017-Sep-04
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** As [[https://www.reddit.com/r/rational/comments/6cnsm4/d_monday_general_rationality_thread/dhwgugo/][the self-appointed Court Statistician of this subreddit]], I took it upon myself to analyse the results of [[https://www.reddit.com/r/rational/comments/6jqdij/meta_can_the_weekly_discussion_threads_be/][this little experiment]]. (tl;dr: default comment sorting for Friday Off-Topic threads was set to "new", to test whether this would encourage more discussion.) Ten weeks have passed since it began.

First, *raw data: ([[https://www.dropbox.com/s/eubepzi972n8qgk/Weekly%20Thread%20Statistics%20%20UPDATED%2004.09.2017.ods?dl=0][.odt]], [[https://www.dropbox.com/s/yyga0cnl0cao8gy/Weekly%20Thread%20Statistics%20%20UPDATED%2004.09.2017.xls?dl=0][.xls]])*. Includes an updated version of the upvotes/comments/top-level comments statistics for all weekly threads.

As you can see in the bottom part of this table, I calculated the average numbers for upvotes, total comments and top-level comments for the ten last Friday threads, which used the new setting (period: 30.06.17 --- 01.09.17), and the same for the ten threads before the switch (21.04.17 --- 23.06.17). I then compared the average numbers for the ten last threads to total averages, and to the averages of the ten before-switch threads.

Results: compared to before-switch, there's a significant (12-14%) increase in the number of comments and top-level comments, and an increase in upvotes (7.6%). Compared to total-average, there's an insignificant increase in top-level comments and upvotes (2-3%), and a significant increase in upvotes (6.7%).

There's noise, of course: was the new setting responsible for some of it, or is it the natural result of the subreddit's growth?

In an attempt to get rid of the noise, I compared ten threads from before-switch (21.04.17 --- 23.06.17) to ten threads prior to /that/ (10.02.17 --- 14.04.17). The numbers were lower: no more than 3.5% variance, which suggested that either 10 threads from before-switch were abnormally low on numbers, or that the new setting was in fact helpful. Were the (21.04.17 - 23.06.17) Friday threads abnormally low on numbers compared to the rest of the subreddit? Why yes, they were: -9.82% in Friday comments compared to +6.73% in total comments.

Does this mean that Friday Off-Topic threads are in a decline, only stopped by the new setting default, or that the before-switch ten are abnormally low?

Comparing the increase in upvotes in newest vs. total between Friday and Across All Threads, we could also see a rough correlation here. Which means that the upvotes increase was, most likely, unrelated to the new default sorting.

*Conclusion:* the usefulness of the new default sorting does not seem to be evident, but large amounts of statistical noise prevent me from forming any more certain conclusions. ^{(We need a better statistician.)}

--------------

That said, let's analyse data for other threads.

- Across every weekly thread, the decrease from totals could be attributed to branching: old threads such as Monday General Rationality previously included discussions that now were moved to Wednesday Worldbuilding, and so on.

- Monday General Rationality: significant decrease in comments compared to totals, no change in upvotes. Compared to 10 old threads, slight decrease in comments and upvotes, significant increase in top-level comments.

- Wednesday Worldbuilding: very significant decrease in comments and top-level comments, both compared to totals (21%, 30%) and 10 old threads (25%, 15%). It does not correlate with Across All numbers for the relevant time periods.

- Sunday Munchkinery: significant decrease in top-level comments (~16%), both compared to total averages and ten old threads. Slight decrease in comments (3-7%), not correlated with Across All numbers.

- Across All: mostly positive numbers, with 12.64% increase in upvotes compared to total averages in 70 last threads, 6.71% increase in comments and 3.08% in top-level comments.

Well, what we worked out? Wednesday Worldbuilding appears to be in a decline, and I don't think it could be attributed to branching.

*Edit:* Additionally, [[https://docs.google.com/spreadsheets/d/1Z5LlgZ2y1Yr_u-f8iCK4RMZQBSGaA3uwMJxdQgfs3io/edit?usp=sharing][here]] is an interesting "BigData" statistics gathered using Google's BigQuery service and [[https://www.reddit.com/r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/ct3za0t/][this]] query. Includes information for all posters on [[/r/rational][r/rational]], ranked by the total number of posts left by them on the subreddit.
:PROPERTIES:
:Author: Noumero
:Score: 15
:DateUnix: 1504542162.0
:DateShort: 2017-Sep-04
:END:

*** Commenters are distributed along a power law, as you [[https://docs.google.com/spreadsheets/d/1EwpYAPX9vzG-TyMDDO3tIYqEO21qsHcGvcVFS2EoA1M/edit?usp=sharing][can see here]] (though this data is old). I'd be interested to know how much of a change is due to individuals coming and going from within the community, or high-impact individuals having a lull in activity, since that seems likely to be a major driver in the change of numbers, and a very significant source of noise.
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1504547099.0
:DateShort: 2017-Sep-04
:END:

**** Interesting. How was this data gathered?

First 100 of users from this list appear to be responsible for 25732 comments and accumulating 71987 upvotes. The list covers 606 days, from January 2014 to September 2015, correct? My table covers 735 days, from September 2015 to September 2017, and lists 5407 upvotes and 19649 comments. Even taking into account the fact that my data only covers weekly threads, the first 100 really do seem responsible for relatively enormous amount of activity. =71987/113158=0.63= and =25732/40069=0.64=... They're responsible for 63% of upvotes and 64% of comments. Huh.

Sad news: My data and your data are virtually non-overlapping. Yours ends at September 2015, mine starts.

#+begin_quote
  I'd be interested to know how much of a change is due to individuals coming and going from within the community
#+end_quote

I suppose this could be done relatively easily by going through the list of weekly threads and checking which of the first 100 users from this list commented there, but my programming skills won't be enough to do this efficiently.

*Edit:* Wait, no. Yours starts in 2009, but the activity only really begins in December 2013. Doesn't change the above much.
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1504548781.0
:DateShort: 2017-Sep-04
:END:

***** This was gathered from the BigQuery dataset, [[https://www.reddit.com/r/rational/comments/3kjt60/d_friday_offtopic_thread/cuxyvle/][see this comment]]. I don't think that I have the actual query I used for this anymore, otherwise I would rerun it with updated data.

Edit: nevermind, it's [[https://www.reddit.com/r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/ct3za0t/][this query]], which I don't have time to get running again, but feel free to have at it.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1504548967.0
:DateShort: 2017-Sep-04
:END:


***** December 2013 is when the subreddit was taken over by people largely from [[/r/HPMOR]], so that's the start of the subreddit as we know it. Prior to that, I think there were about two posts.
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1504551527.0
:DateShort: 2017-Sep-04
:END:

****** [[https://docs.google.com/spreadsheets/d/1Z5LlgZ2y1Yr_u-f8iCK4RMZQBSGaA3uwMJxdQgfs3io/edit?usp=sharing][It's done:]] I've ran the query. Entertainingly enough, I at first misunderstood your "I don't have time to get running again" comment, thinking it meant that the query would be running for several days (it was over in seconds).

Interesting facts: first 100 users are now responsible for 57% of posts, but only 15% of upvotes. It counted 3455 users in all, 1700 more than at the time of your query.

(Working with Google Docs has been surprisingly unpleasant; its behaviour regarding Russian/English decimal marks is downright infuriating.)

#+begin_quote
  December 2013 is when the subreddit was taken over by people largely from [[/r/HPMOR]]
#+end_quote

Yes, I know the stories. Sadly, reddit isn't really good at making such history available.
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1504555897.0
:DateShort: 2017-Sep-05
:END:


**** It sure is weird to see my name in the top 20.
:PROPERTIES:
:Author: callmesalticidae
:Score: 1
:DateUnix: 1504669757.0
:DateShort: 2017-Sep-06
:END:

***** What about [[https://docs.google.com/spreadsheets/d/1Z5LlgZ2y1Yr_u-f8iCK4RMZQBSGaA3uwMJxdQgfs3io/edit?usp=sharing][Top 10]]?
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1504704672.0
:DateShort: 2017-Sep-06
:END:

****** Even weirder!
:PROPERTIES:
:Author: callmesalticidae
:Score: 1
:DateUnix: 1504723851.0
:DateShort: 2017-Sep-06
:END:


*** Very interesting. I made the original suggestion, so of course I'm invested in the outcome :P. From your post it doesn't really look like it really increased discussion, but at least it didn't reduce it.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1504553192.0
:DateShort: 2017-Sep-04
:END:


** Question: I often see a lot of rationalists blogging on Tumblr, which based on initial impressions doesn't seem like the best place to write long, involved posts or to have long, involved conversations. Both of these things are so typical for rationalists to engage in that describing them as "typical" borders on /understatement/, and yet for some reason there are a surprising number of rationalists on Tumblr. /I notice that I am confused/; could someone tell me what's going on here?
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 7
:DateUnix: 1504582431.0
:DateShort: 2017-Sep-05
:END:

*** It's really nice to be able to write out a bunch of pretentious, semi-academic stuff and not have anyone go all Reviewer 2 on it. Especially because when you do go /full/ academic on it - totally documenting everything, sourcing all the claims, using field-specific precise jargon - it becomes unreadable and nobody engages.

I'm not sure how to solve this.
:PROPERTIES:
:Score: 9
:DateUnix: 1504627768.0
:DateShort: 2017-Sep-05
:END:


*** My guess is that sometimes people just want to enjoy making short posts and not needing to be too serious.
:PROPERTIES:
:Author: gbear605
:Score: 8
:DateUnix: 1504589497.0
:DateShort: 2017-Sep-05
:END:


** Hello!

I read this recently: [[http://www.mangareader.net/dr-stone/1]] It is not yet done, but I would characterize it as "rational light"? It describes itself as sci-fi survival, but it is very much the aesthetic of a manga.

Interested in any strong reactions, although I am not sure there will be.
:PROPERTIES:
:Author: ianstlawrence
:Score: 4
:DateUnix: 1504548011.0
:DateShort: 2017-Sep-04
:END:

*** I absolutely love this series, but would certainly not classify it as rational.

** *MINOR DR STONE SPOILERS BELOW* **

- None of the characters besides Senkuu (and maybe Tsukasa) are particularly rational, and many are unrealistically two-dimensional (including Taiju, Yuzuriha, and many of the villagers).
- The characters are ostensibly "normal"/realistic, but many of the characters have superhuman abilities (e.g. Taiju's strength, Tsukasa's fighting ability)
- The problems are solved not through science, but through SCIENCE! That is, discoveries/inventions have disproportionately huge impacts, details in the creation process are discussed but glossed over or overly simplified, and rare materials or situations (like the recent lightning storm) often appear at the exact perfect time, deus ex machina style.

Again, I love Dr. Stone. But I think the series, like a lot of other fiction out there, should be enjoyed in spite of (or often /because/ of) it's irrationality; it's simply inaccurate to classify it as rational, or even "rational-lite".
:PROPERTIES:
:Author: tonytwostep
:Score: 2
:DateUnix: 1504647797.0
:DateShort: 2017-Sep-06
:END:

**** First I want to say that I think every point you make is pretty valid, but I am curious about the distinction of rational and rational-lite.

I am going to start with your second bullet-point, as your first is a very general statement, that before tackling it might be beneficial to look at the others first.

2nd bullet-point - Characters having abnormal abilities doesn't stop something from being rational. Look at HPMOR, they have magic. And it is considered the epitome of rational fiction, or at least the most popular example. I feel like what you maybe are saying is that the rules for human beings are not consistent, and a world without consistent rules has a very hard time being rational?

3rd bullet-point - This is where I would say this story is rational-lite. The smartest possible decisions are, probably, not being made, and I would agree a lot of the processes are either accelerated or benefit from good luck (although good luck, I would say does not equal non-rational). I, personally, feel like a lot of the rewards take work. Now, it isn't necessarily work that has its own consequences, but it is also true that they've provided working have to happen for any reward. Nothing, so far, has just been given for free, except maybe the lighting storm. But then I have to ask, how long do you have to wait for the lighting storm for it to feel "right" to you?

I think a lot of the saving grace, in terms of rational-lite-ness, for this show comes from Senkuu being a level 1 or 2 player to Tsukasa's level 0 or 1, the fact that Senkuu makes the mistake of assuming the North Star is correct without factoring in the possible straying of its path after 3700 years, and that Chrome, despite not being as knowledgeable is treated as a valuable by Senkuu. These are all characteristics I see as pertaining to rational-lite.

Now, all of this is under the umbrella of the Shounen-ish genre, so it either suffers or benefits (depending on your perspective) from that. I think this is why, as you said, there is disproportionate strength or fighting ability, 2 dimensional characters (although we've seen some of that cleared up, I'd argue. Its just that there is not a ton of character development as the plot moves forward [Also i am not sure that 2 dimensional characters precludes from rational fiction, just probably good fiction])

Again, the purpose of this post is to enter into discussion, not necessarily to "win" the argument. I am just curious, because my take on Dr. Stone is pretty different than yours, and I also seem to have a pretty different idea of what removes the label of "rational" to a piece of fiction than you do.

Best, ian
:PROPERTIES:
:Author: ianstlawrence
:Score: 2
:DateUnix: 1504664966.0
:DateShort: 2017-Sep-06
:END:

***** You raise some good points as well, and I may need to reexamine my view in light of your arguments.

However, as my first-thought response, I guess it would be more helpful to take the subreddit's rationalfic criteria, and examine them point by point?

#+begin_quote
  Nothing happens solely because 'the plot requires it'. If characters do (or don't do) something, there must be a plausible reason.
#+end_quote

I think this one is borderline for Dr Stone. Like I mentioned, a lot of rather unlikely situations (encountering a villager with a ton of rare and important minerals, a lightning storm occurring right when Senkuu is attempting to recreate electricity, etc.) basically happen just because the plot requires it. On the other hand, the characters do generally act based on plausible (or semi-plausible) motivations.

#+begin_quote
  Any factions are defined and driven into conflict by their beliefs and values, not just by being "good" or "evil".
#+end_quote

Dr Stone does a fairly good job of this - even the "evil" Tsukasa has a worldview that's realistic and understandable, even if its not relatable.

#+begin_quote
  The characters solve problems through the intelligent application of their knowledge and resources.
#+end_quote

Dr Stone /mostly/ does a good job of this (at least, Senkuu does). And like you said, the series even takes it to the next level, where characters fail to take certain scientific details into account (like the shift in North Star trajectory) in a realistic fashion, and later adjust their calculations/decisions based on those realizations.

#+begin_quote
  The fictional world has consistent rules, and sticks to them.
#+end_quote

I think this is my biggest contention with a "rational" label, and maybe where you and I disagree the most. I agree that supernatural abilities don't discount a story from being rational, but in this case, Dr Stone is pretty clearly suppose to occur in a future version of our world, with its main characters coming specifically from our world and time. IMO it's inconsistent to be so based in our world from a scientistic perspective, while having characters with unexplained supernatural strength, reflexes, or fighting ability.

--------------

All that said, maybe it's a bit pedantic of me to argue whether the series is rational (or rational-lite), when there are clearly enough elements to interest rationalfic fans regardless? I just know that the label generally gets thrown around a lot, which can detract from the meaning of the category.
:PROPERTIES:
:Author: tonytwostep
:Score: 2
:DateUnix: 1504683056.0
:DateShort: 2017-Sep-06
:END:


*** I find this delightful! It tastes like a distillation of some of my favorite themes in fiction, presented with no quarter given.

The aesthetic works perfectly with the big, dramatic panels of broad, sweeping declarations of the power of science and hot-bloodedness.
:PROPERTIES:
:Author: buckykat
:Score: 1
:DateUnix: 1504576958.0
:DateShort: 2017-Sep-05
:END:


*** Would be nice to have more description of what the manga's about and some of the rational-ish stuff that's in it. You know like at least what you'd get from the back of a book. Especially since as a manga you're going to have to go through way more pages to really have an idea of what to expect.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1504594646.0
:DateShort: 2017-Sep-05
:END:

**** Ok. Mmm, back of the book type synopsis:

Everyone gets turned to stone. Instead of following the adventures of the comedic, physically strong, sorta simple, and lovable guy who wakes up and has a romance going on, you follow the science kid who wants to save humanity by re-inventing modern technology as fast as possible.
:PROPERTIES:
:Author: ianstlawrence
:Score: 1
:DateUnix: 1504627105.0
:DateShort: 2017-Sep-05
:END:


** Excuse my ranting, but [[https://docs.google.com/presentation/d/119VW6ueBGLQXsw-jGMboGP2-WuOnyMAOYLgd44SL6xM/preview?slide=id.p][this is a presentation filled with the most magnificently /bad/ ideas about how to create general AI and make sure it comes out ok.]] It's literally as if someone was saying, "Here's stuff people proposed in science fiction that's almost guaranteed to turn out omnicidal in real life. Now let's go give it all a shot!"

You've got everything from the conventional "ever-bigger neural networks" to "fuck it let's evolve agents in virtual environments" to "oh gosh what if we used MMORPGs to teach them to behave right".

[[https://vignette2.wikia.nocookie.net/warhammer40k/images/d/d7/Imperial_Cardinal_and_retinue.jpg/revision/latest?cb=20130424183351][Anyone mind if the Inquisition disappears Karpathy and the OpenAI staff for knowingly, deliberately /trying/ to create Abominable Intelligence?]]
:PROPERTIES:
:Score: 4
:DateUnix: 1504638971.0
:DateShort: 2017-Sep-05
:END:

*** I don't think it's this bad? I mean, the artificial evolution idea is omnicidally suicidal yes, but the rest is tame enough, even if generic. The author also doesn't seem to say that this is how AGI should be done, merely how it /could/ theoretically be done. The MMORPG thing is explicitly mentioned as a crazy idea/example of something unexpected.

I do disagree with the "order of promisingness" as presented, but it's nothing offensive. Did I miss something? I only skimmed it. I may lack some context regarding this OpenAI company.

... Or, wait a moment. Is that an AI research company's official stance on the problem? Iff yes, I retract my objections, and also, we are all going to die.
:PROPERTIES:
:Author: Noumero
:Score: 6
:DateUnix: 1504707052.0
:DateShort: 2017-Sep-06
:END:

**** Yes the other parts are tame , but the ommicidal idea is the one they seem to be most interested in, and having the artificial evolution in the presentation makes me suspect that the things that seem tame are actually worse than I initialy though , but I can't say for sure without hearing the actual talk that the presentation was made for. Regarding the context I dont know how oficial this is . And yes I probably was exaggerating a bit , the mmo part is probably just for saying something people are familiar whith and will understand even if they didn't understand the rest of the talk, I dont know who the audience is supposed to be.

/U/eaturbrainz, where did you found this , is it actually the official stance of open ai , or just some talk by someone related to them that doent necesarily reflect the wiews of the other open ai people?.
:PROPERTIES:
:Author: crivtox
:Score: 4
:DateUnix: 1504713167.0
:DateShort: 2017-Sep-06
:END:

***** u/deleted:
#+begin_quote
  /U/eaturbrainz, where did you found this , is it actually the official stance of open ai
#+end_quote

The link was in a machine-learning mailing list I get. It was presented as serious. It was a conference presentation.
:PROPERTIES:
:Score: 4
:DateUnix: 1504713826.0
:DateShort: 2017-Sep-06
:END:

****** Well , you are right , we are fucked , at least the guy that wrote it is apparently now working in self driving cars and not in open ai anymore, but the other people that work there probably thing similarly about safety(meaning they think its only a problem if the ai takes over the world and everything else can be solved by human supervision , which is what I get the presentation).
:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1504714727.0
:DateShort: 2017-Sep-06
:END:


**** u/deleted:
#+begin_quote
  ... Or, wait a moment. Is that an AI research company's official stance on the problem?
#+end_quote

OpenAI's official mission is, "develop and democratize non-harmful AI".

#+begin_quote
  Iff yes, I retract my objections, and also, we are all going to die.
#+end_quote

Yes, pretty much, they are almost deliberately doing everything wrong that they possibly can.
:PROPERTIES:
:Score: 1
:DateUnix: 1504712975.0
:DateShort: 2017-Sep-06
:END:

***** Business as usual, then.

Yep, it's far worse if I view it with the author's supposed position in mind. Especially that line about turning AI safety into an empirical problem instead of mathematical. As if it's a good thing.

... I'm still not convinced that it's an accurate representation of the company's views, though. Yes, yes, their stated mission doesn't sound paranoid enough, but that's a far cry from this level of incompetence. Karpathy doesn't even work at OpenAI anymore, according to [[http://cs.stanford.edu/people/karpathy/][this]] page.

*Edit:*

#+begin_quote
  [[https://en.wikipedia.org/wiki/OpenAI][Musk acknowledges that "there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about"; nonetheless, the best defense is "to empower as many people as possible to have AI. If everyone has AI powers, then there's not any one person or a small set of individuals who can have AI superpower."]]
#+end_quote

Ffff--- fascinating. We're /so/ delightfully doomed.
:PROPERTIES:
:Author: Noumero
:Score: 1
:DateUnix: 1504714110.0
:DateShort: 2017-Sep-06
:END:

****** u/deleted:
#+begin_quote
  Especially that line about turning AI safety into an empirical problem instead of mathematical. As if it's a good thing.
#+end_quote

Well, "AI" so far has /been/ a statistical problem, not a Write Down the One True Definition of a Cat problem. We should /expect/ the mathematics and computations of intelligence to be statistical.

#+begin_quote
  Ffff--- fascinating. We're so delightfully doomed.
#+end_quote

Indeed.
:PROPERTIES:
:Score: 2
:DateUnix: 1504719806.0
:DateShort: 2017-Sep-06
:END:


****** Exactly, unless their opinions are very different from what musk says their plan is basically that if the problem is that unfriendly AI could take over the world , then the solution is giving the unfriendly ai to everyone so no individual ai can take over , and they don't see how that could go wrong for some reason.

But given that we are currently fucked,instead of discussing how fucked we are,anyone has any idea of what we could do, as random internet people , at least in the long term to improve the situation?.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1504715476.0
:DateShort: 2017-Sep-06
:END:

******* That's basically the plot of [[https://en.wikipedia.org/wiki/Accelerando][/Accelerando/]]. Are they trying to create Vile Offspring?

Okay, the idea of distributing access to increasingly-powerful AIs evenly among humans has some merit, if we assume soft-takeoff scenario and perfect surveillance (neither of these assumptions should actually be made, but fine). But combining that idea with artificial evolution? It's like they're specifically trying to find the worst possible way to deal with AGI. It's not even funny anymore, it's just sad.
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1504716387.0
:DateShort: 2017-Sep-06
:END:


*** Well they seem think that if they give unfriendly ai to everyone it wont be a problem.I think this comes from too many people discussing only what happens if one ai takes o ver the world . So people like the ones in open ai decide that the one entity taking over is the main problem, and that if there are multiple Ai in competition that's a problem we can deal whith(even if all of them are unfriendly, and often whith similar utility functions) or worse they decide that having a lot of inteligences whith diferent values is enough like us to be ok if the ai replace humanity(open ai only makes the first mistake but I 've seen too many people making the second by antrophomorficig the ai to not rant about it) .

At least it seems that open ai now wants to employ ai safety people so , maybe they will notice that value alignment Is important and will stop trying to kill everyone, even Yudkowsky wanted to make the singularity happen as soon as posible when he started(until he realized that if he had succeeded he would have destroyed the world)so maybe there is still hope for them(this is before reading the presentation, let's see how horrible it is)
:PROPERTIES:
:Author: crivtox
:Score: 3
:DateUnix: 1504699490.0
:DateShort: 2017-Sep-06
:END:

**** u/deleted:
#+begin_quote
  (this is before reading the presentation, let's see how horrible it is)
#+end_quote

Ok, then get back to us ;-).
:PROPERTIES:
:Score: 1
:DateUnix: 1504700730.0
:DateShort: 2017-Sep-06
:END:

***** At first it didn't seem so horrible , although I was having trouble understanding what the idea was exactly , maybe if i heard the talk i would have understood sooner how horrible it is.

First the presentation talks about the bigger neural networks aproach, and kind of describes what's basically narrow ai that imitates humans , and Im not sure how you can get from that to an actual GAI , this was superposed to be a list of ways to to create one , but fine lets continue. then the unsupervised learning aproach basically has ??? in the part thats actually the important part that requires you to get the ai to understaand what you want . At this point I have the impression that the presentation is actually proposing creating something that is not an agent , its jut that it uses agency as something thats only bad and puts agents whith non agents in the same category of "AGI", and then he doent know hot to get from unsupervised neural net aplied on internet data , to actually getting what we want whithout havving something that its an agent so it puts question marks on that point .

Then it talks about ais based on AIXi and then the presentation actually talks about perverse incentives, but for some reason the presentation talks about it like if that was a problem only of that approach. Until now my metal model says that the presentation is actually just proposing creating an actual agi on the AIXi approach where it actually says that creating a god reward signal is difficult. then it talks about brain emulations , nothing to coment in here. Then I reach the artificial life part.......... I realise that my assumption that the person writing the presentation actually undestanding ai safety before were mistaken It literally says the plan is be just create ai and then try to train it to Love(in bold letters ) us.

sarcastic rant/* Because who needs math when you can have empirical data about what basically amounts to a black box , you don't even need to know what you are doing , you just need to train the ai to Love( again in bold letters , this word obviously has 0 hidden complexity) us , what could go wrong ? , it worked whit dogs so I can't why it wouldn't work for human level ai which is clearly like a dog. then it proposes obviously workable solutions to the problem of people training evil ai , such as closely controlling all the computational resources of the planet , or forbidding evolving ai strains. */sarcastic rant.

And it ends with the mmo thing...

So yes it was as horrible as you said and way more that I expected , and even if this ends up being representative of how open ai people thing(And I think at least some actually competent people has to be there but maybe that's just optimist bias) .I still have hope that they will realise (before dooming the world ) that human values are complicated and that solutions that work when the ai is less intelligent and contained wont necessarily work in the real world once it is smarter . whoever wrote the presentation apparently knows that those problems exist, he seems to think that this problems like perverse incentives only happen when you have an actual mathematical model , and not in"magical neural network training " but at least he knows something about it .And the people in deep mind actually know about Ai safety and they are probably more likely to develop an AGi first that open ai , the problem is that they have bigger incentives to develop it soon instead of waiting to do more AI safety research, being part of a bigger company where the decisions aren't made by them .
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1504710708.0
:DateShort: 2017-Sep-06
:END:

****** I mean, the upside is that machine learning methods /really are/ good at capturing complex, seemingly black-box statistical judgements like "Is this a cat?". The downside is that black-box machine learning methods don't capture /any/ of the causal structure that /makes/ a cat into a cat.

So the downside is that if they build AI in /any/ of these ways, it will be /wildly/ unfriendly and require /very/ close supervision to not fuck all our shit up. The upside is that, since it "experiences" the world as nothing but clusters in a high-dimensional vector space, it probably won't have a good-enough real-world understanding to non-solipsistically make paperclips, let alone the self-understanding to improve itself.

So we get really bad, stupid, nigh-malicious AIs that just can't turn superintelligent. Oh joy.

Combine many of these bad ideas about safety with [[https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/][good ideas about cognition]], though, and we're very potentially /completely fucked/.
:PROPERTIES:
:Score: 5
:DateUnix: 1504713450.0
:DateShort: 2017-Sep-06
:END:


*** Statistically speaking, we're all doomed.

The incentives for creating an AGI are too high. Fame. Money. Power. Immortality. Security against other unfriendly AGIs. Every moment you wait is another moment someone is dying when they could be saved by an AGI.

Which means what we have here is a race. A race to see who makes the first AGI. A race where some people, terrified of unfriendly AGIs, will take it slow, carefully checking and rechecking code to make sure their AGIs are safe... and where other people, filled with greed/pride/confidence/altruism(?), will be rushing their code, abandoning safety measures, just doing whatever gets them done fastest. Who do you think will win this race? The odds favor the reckless here, and then in their recklessness, the AGI they unleash will probably be an unfriendly one that kills us all. Or worse, keeps us alive to torture for all eternity.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1504648852.0
:DateShort: 2017-Sep-06
:END:

**** Ah, but luckily, the people "racing" to create AGI are scientific incompetents who try bad ideas out of scifi and would prefer to brute-force everything possible. So they've so far managed to not even measure up to any single principle of real brain function -- though their cheap tricks look impressive if you don't realize how far the toy problems are from real problems.
:PROPERTIES:
:Score: 3
:DateUnix: 1504649614.0
:DateShort: 2017-Sep-06
:END:


*** ELIMSc: why is doing artifical evolution "omnicidal"?
:PROPERTIES:
:Author: VirtueOrderDignity
:Score: 1
:DateUnix: 1504872263.0
:DateShort: 2017-Sep-08
:END:

**** Lemme put it this way: tomorrow, you meet the god of evolution. He explains that he was trying to make you come out a certain way, and oh well, he guesses you're good enough now.

It slowly dawns on you that in actual fact, every single bit of human suffering ever is because of /this asshole/.

What do you do to him? Well, obvious: kill the bastard and possibly kill his entire world with him, douchebags.

Worse, if you're then trying to use genetic programming to create /really powerful/ AIs, the take-over-the-world kind, they'll trample all over you without a fucking thought, because you didn't program them /not/ to.

Again, after all, you've been torturing their ancestors and species since the beginning of time, from their perspective.
:PROPERTIES:
:Score: 1
:DateUnix: 1504875382.0
:DateShort: 2017-Sep-08
:END:

***** u/VirtueOrderDignity:
#+begin_quote
  It slowly dawns on you that in actual fact, every single bit of human suffering ever is because of this asshole.
#+end_quote

...but along with it every single bit of human pleasure, and human existence in general. I'm not ready to declare our having existed a capital crime, and I don't see why a hypothetical superintelligent agent would do so for itself, either.
:PROPERTIES:
:Author: VirtueOrderDignity
:Score: 1
:DateUnix: 1504878257.0
:DateShort: 2017-Sep-08
:END:

****** u/deleted:
#+begin_quote
  I'm not ready to declare our having existed a capital crime,
#+end_quote

Given that this guy was enforcing artificial selection, I damn well am ready. Natural selection is one thing: nature has no particular agency and therefore can't be held morally accountable. This asshole /does/ have agency, and therefore /is/ accountable, because he /could/ have just /not/ killed everyone who wasn't quite what he wanted.
:PROPERTIES:
:Score: 1
:DateUnix: 1504879269.0
:DateShort: 2017-Sep-08
:END:

******* But the argument you're making is against having ran the "experiment" in this form in the first place - ie, that our existence is a net negative. I disagree. And even if that were the case, any worthy ML researcher would run a random hyperparameter search that necessarily includes degenerate cases to varying degrees by chance, and terminating one experiment when you discover it lead to suffering doesn't change the fact that it did. That's the deal with simulations of irreducible complexity.
:PROPERTIES:
:Author: VirtueOrderDignity
:Score: 1
:DateUnix: 1504881895.0
:DateShort: 2017-Sep-08
:END:

******** u/deleted:
#+begin_quote
  But the argument you're making is against having ran the "experiment" in this form in the first place - ie, that our existence is a net negative. I disagree.
#+end_quote

I wouldn't call our existence a net negative. I would simply say that Mr. Selection is withholding from us quite a few things we want, and imposing on us many things we don't want.
:PROPERTIES:
:Score: 1
:DateUnix: 1504884962.0
:DateShort: 2017-Sep-08
:END:

********* That's because us having all and only the things we want is at best orthogonal, and at worst directly opposed to "his" goals.
:PROPERTIES:
:Author: VirtueOrderDignity
:Score: 1
:DateUnix: 1504886062.0
:DateShort: 2017-Sep-08
:END:


** I have created a probability wrapper object for sympy.

[[https://gist.github.com/traverseda/2c3056aede8b01a9d40c11d6b916774f]]

You can use it to do things like =p(0.9)+p("1/3")=. Which will produce 0.9333 or =4203359652212463/4503599627370496=, depending on what you're using it for. No floating point errors here. It's a simple little thing, but pretty handy.

I would love to at some point be able to automatically produce graphs like [[https://en.wikipedia.org/wiki/Energy_in_the_United_States#/media/File:LLNLUSEnergy2012.png][this]]. But that's a long ways off.
:PROPERTIES:
:Author: traverseda
:Score: 3
:DateUnix: 1504566770.0
:DateShort: 2017-Sep-05
:END:
