#+TITLE: Passive

* [[https://www.smbc-comics.com/comic/passive][Passive]]
:PROPERTIES:
:Score: 59
:DateUnix: 1561495077.0
:DateShort: 2019-Jun-26
:END:

** There are already people who can persuade most anybody to do terribly self-destructive things, like buying a time-share in Florida or investing their life savings in that initial coin offering. That's why not having an open mind, and instead sticking to your opinions no matter what arguments your hear is a great strategy.
:PROPERTIES:
:Author: bubba3737
:Score: 32
:DateUnix: 1561513678.0
:DateShort: 2019-Jun-26
:END:

*** I've always liked the expression, "keep an open mind, but not one so open that your brains fall out"
:PROPERTIES:
:Author: iftttAcct2
:Score: 23
:DateUnix: 1561514638.0
:DateShort: 2019-Jun-26
:END:

**** "The trouble with having an open mind is that people will keep insisting to put things in it."
:PROPERTIES:
:Author: Lightwavers
:Score: 31
:DateUnix: 1561518438.0
:DateShort: 2019-Jun-26
:END:

***** I don't think that quite gets to the issue. I think something like "The trouble with having an open mind is that people mistake it for a garbage can" makes more sense.
:PROPERTIES:
:Author: appropriate-username
:Score: 2
:DateUnix: 1561905784.0
:DateShort: 2019-Jun-30
:END:


*** Clinging to no opinion but resolving not to update easily based on the words of people with motive to influence you also works.
:PROPERTIES:
:Author: raptorbarn
:Score: 3
:DateUnix: 1561574409.0
:DateShort: 2019-Jun-26
:END:


*** u/SoylentRox:
#+begin_quote
  That's why not having an open mind, and instead sticking to your opinions no matter what arguments your hear is a great strategy.
#+end_quote

The flaw with /that/ is you can't correct any errors your thinking may have in it.

​

The specific error that most adults presently alive today, but who soon will not be, are making is they trust various bullshit artists, such as the medical directors of "memory care" centers, religious priests, popular fiction - all sellers of unadulterated bullshit, even the ones licensed by the state with medical degrees.

​

So even when they are diagnosed with Alzheimers/dementia and there is a clear and logical course of action - preserve their brain before all the memories are destroyed - they instead willingly let themselves rot into a vegetable and then a corpse.
:PROPERTIES:
:Author: SoylentRox
:Score: 3
:DateUnix: 1561598488.0
:DateShort: 2019-Jun-27
:END:

**** The problem with brain preservation is suppose it's utterly impossible to revive a brain. Then they've just committed committed suicide. And like the person above said, many people are capable of tricking others. Say you're advocating to an elderly person to go for brain preservation. How do they know you're more trustworthy than the person trying to sell them a trust share in Florida?

Brain preservation is a risk. A lot of people choose to not take that risk and just stick with what they know for sure is safe, because it can be hard to know if that risk has an 80% chance of success or a 1% or a 0.000001% chance.
:PROPERTIES:
:Score: 4
:DateUnix: 1561607433.0
:DateShort: 2019-Jun-27
:END:

***** Sure. And there's a couple arguments on this that are little known.

Personally, I think the odds that brain preservation will "work" is 100%, at least assuming the frozen samples survive until a future era and a future society is interested in reviving them.

That's because I'm using a different definition for "work". Specifically, I assume the future society could generate new human brains from scratch, with random neural weights. The proof that this is feasible, is, well, nature can do it.

Then each bit of information recovered from the frozen sample is a constraint on the random samples of neural architectures and weights.

The "noodle baking" part is your own living brain is really not storing information faithfully like you think, each recall of your memory and personality is really a random sample from a possibility space, with the stored values acting as constraints.

Viewed this way, there is an enormous number of very detailed constraints you can recover even from a badly frozen mess done a day after death.

Now, sure, better preservation is much better. People preserved well will be able to accurately recall their own memories, have personalities so close to the original to be indistinguishable, etc.

But for the worst victims of their death, it'll be similar to recovering from combination of dementia and stroke. Except the difference is, unlike elderly people, patients recovering with brand new digital brains that work better than their human hardware ever did, will actually be able to recover.
:PROPERTIES:
:Author: SoylentRox
:Score: 1
:DateUnix: 1561641795.0
:DateShort: 2019-Jun-27
:END:

****** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1561667835.0
:DateShort: 2019-Jun-28
:END:

******* u/appropriate-username:
#+begin_quote
  As a "star trek teleportation isn't death," "identity is in the pattern" believer
#+end_quote

What happens if a dude is not killed before being teleported in the former scenario and is revived with their original brain in the latter? Is consciousness split among two bodies?
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1561905935.0
:DateShort: 2019-Jun-30
:END:


** I feel like this almost assumes the opposite: that a certain line of argument will persuade someone to change their views. In reality, people are stubbornly irrational about the things they think and the things they believe in.
:PROPERTIES:
:Author: iftttAcct2
:Score: 9
:DateUnix: 1561510111.0
:DateShort: 2019-Jun-26
:END:

*** The point I think is that the AI is capable of threading the single possible argument that would change someone's mind. Like a chess puzzle that has a single solution and takes 1000 moves to complete, no human is capable of solving it, but an advanced AI could.
:PROPERTIES:
:Score: 23
:DateUnix: 1561510422.0
:DateShort: 2019-Jun-26
:END:

**** Yep. The one big upcoming technology I'm most interested in/scared of is superpersuaders. There won't be any radical change-- advertising techniques have been getting better and better for decades, if not centuries. But once the most powerful governments and corporations have techniques so sophisticated as to convince individuals, not just demographics, to do what they want, I fear what will become of everything from commerce to culture.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 8
:DateUnix: 1561526540.0
:DateShort: 2019-Jun-26
:END:


**** /Maybe/ . While it makes good sci fi, present AI technology has a very key limitation.

​

/You have to be able to objectively measure a correct answer/. This has wide ranging consequences. This means present technology is capable of solving things like the movements of a robot in a warehouse or object manipulation, even in dirty and chaotic environments. This is because you can accurately simulate these environments and in the simulation, accurately score how well the AI's policy is doing against a heuristic for success. This allows the AI to continue to improve, and given enough time and an accurate enough simulation, it will eventually improve to superhuman levels of performance. (in terms of heuristic scores. Obviously a badly designed heuristic will give you an AI that is amazingly consistent at doing a shitty job)

​

Note that the simulator can use AI itself, where a different AI system is doing it's best to make the simulation better reflect the behavior observed in the real world.

​

The reason such a simulator cannot yet be used to make a super-persuader is you need an accurate simulation of a human being in order to do this. This is probably achievable, but it will take a /lot/ of data. Note that this "simulation" I am describing isn't quite what you are thinking, you wouldn't be simulating an actual human mind, just modeling the probable responses a human being will have to any line of argument you might make.
:PROPERTIES:
:Author: SoylentRox
:Score: 6
:DateUnix: 1561598800.0
:DateShort: 2019-Jun-27
:END:

***** Yeah, it is fiction, I think the author wrote it as if passive-aggressive manipulation was a game of perfect information like chess where the best possible move against a grandmaster will be the same best possible move as against a novice, so the AI just needs to play against itself to improve and never needs to simulate weak opponents.
:PROPERTIES:
:Score: 5
:DateUnix: 1561607076.0
:DateShort: 2019-Jun-27
:END:


** This is like a super power...
:PROPERTIES:
:Author: Neon_Powered
:Score: 3
:DateUnix: 1561506981.0
:DateShort: 2019-Jun-26
:END:


** So this is what happens when the jobs of diplomats and world leaders get automated.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 1
:DateUnix: 1561647248.0
:DateShort: 2019-Jun-27
:END:


** Interesting humor, an AI that can convince people disagreeing with it will cost much more that their continued existence, just by manipulating their sense of guilt. Or the sense of shame? I'm not sure, maybe combination of them. It's not realistic, of course, since people will put down their feet unwilling to entertain the thought of surrendering their interest. Especially when said interest is their own life.

Good humor nonetheless.
:PROPERTIES:
:Author: sambelulek
:Score: 1
:DateUnix: 1561518788.0
:DateShort: 2019-Jun-26
:END:

*** The point is that a sufficiently advanced persuader can convince people to be willing to entertain the thought of surrendering their interests. It's a slippery slope from there. If you precommit, they will get you to break that commitment. If you ignore, they will find a way to get your attention. If you remove your senses, they will find some other way to get their message across. A */sufficiently advanced/* persuader will turn /your/ interests into /their/ interests and there is nothing you can do to stop them.
:PROPERTIES:
:Author: sykomantis2099
:Score: 8
:DateUnix: 1561552720.0
:DateShort: 2019-Jun-26
:END:

**** This assumes something like this can exist. It assumes severe enough security flaws in human minds exist, and that they are universal enough or it is possible to find out which flaws a particular human have. This might not be correct.

​

For a toy system analogy, imagine a very simple system that will not give up information unless a particular binary number is sent to it. All it is doing is a simple while(1) get_incoming; if incoming.code.valid() break;

​

Something like that can't be cracked remotely even by a "sufficiently advanced super-hacker", assuming the length of the required code number is long enough to be un-guessable.
:PROPERTIES:
:Author: SoylentRox
:Score: 6
:DateUnix: 1561599045.0
:DateShort: 2019-Jun-27
:END:

***** [[https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml][First]], don't underestimate the lengths an AI will go to in order to achieve its objective.

[[https://en.wikipedia.org/wiki/Row_hammer][Second]], your toy system is still subject to the laws of physics and whatever physical limitations are part of the medium it runs on, and thus it is ripe for having the flaws of its medium exploited.

[[https://twitter.com/browserdotsys/status/1130909264065716224][Third]], lack of creativity on your part in regards to bypassing physical systems does not constitute lack of creativity on the part of an AI.

[[https://www.the-scientist.com/features/proprioception-the-sense-within-32940][Fourth]], our brains are also physical systems, and as such, given the right stimulus, an agent can make them do, or think, whatever it wants.

[[https://www.lesswrong.com/posts/c5GHf2kMGhA4Tsj4g/the-ai-in-a-box-boxes-you][Finally]], there are many different methods of persuasion.
:PROPERTIES:
:Author: sykomantis2099
:Score: 3
:DateUnix: 1561604209.0
:DateShort: 2019-Jun-27
:END:

****** Sure. There's another significant limitation that current AI methods are subject to: finding a good or locally optimal policy requires information. A lot of it and it needs to be accurate. So currently tractable problems generally require an accurate simulator and a way to calculate the heuristic from the sim state. Human beings are presently infeasible to simulate.
:PROPERTIES:
:Author: SoylentRox
:Score: 5
:DateUnix: 1561604447.0
:DateShort: 2019-Jun-27
:END:

******* Well you're half right. AIs do need a lot of data... the first time. Unfortunately, it was recently [[https://www.psychologytoday.com/us/blog/the-future-brain/201905/new-ai-method-cuts-deep-learning-training-69-percent?amp][discovered]] that an AI, that has already been initialized, trained, and pruned, can be easily and quickly trained on a new dataset for a new purpose, just by taking the nodes from the final version and training them on the new dataset using the corresponding starting weights from when it was trained the first time, and you'll get a newly repurposed AI with roughly the same level of accuracy as the original in a fraction of the time.

The scary thing about this is that the AI performs at the same level of accuracy /regardless/ of the type of data used for the initialization: as long as the number of inputs in the input layer remains the same, you can train the AI on something incredibly easy, then repurpose it for something incredibly hard and still get the same level of accuracy as on the easy thing!

So, all you would need is an accurate autoencoder to translate an input of arbitrary size into an input of the correct size and you could apply such an AI to practically anything, including human thought processes, as long as you have some way of encoding it, because you can use the same process outlined above to *train the autoencoder*.

Okay, I'm glad I'm arguing with you because this line of thinking is giving me all kinds of insights and I'm going to stop now because otherwise I'll never stop typing and I have a lot to think about now so thanks!
:PROPERTIES:
:Author: sykomantis2099
:Score: 2
:DateUnix: 1561651773.0
:DateShort: 2019-Jun-27
:END:

******** Disagree. Specifically, these breathless papers are producing models that work...80 or 95% of the time, depending on the specific problem.

This isn't superhuman performance, it's just human performance or very marginally better.

Problems where you have an objective measurement against your heuristic are ones where superhuman performance is possible.

Can go into more detail later.
:PROPERTIES:
:Author: SoylentRox
:Score: 2
:DateUnix: 1561659220.0
:DateShort: 2019-Jun-27
:END:


*** People do sacrifice their lives and risk them for the right cause (religion, family, a chance at great wealth) and an AI could manipulate that sense to inspire a particular ill advised behaviour.
:PROPERTIES:
:Author: Nepene
:Score: 2
:DateUnix: 1561776698.0
:DateShort: 2019-Jun-29
:END:
