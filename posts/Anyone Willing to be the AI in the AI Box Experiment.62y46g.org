#+TITLE: Anyone Willing to be the AI in the AI Box Experiment?

* Anyone Willing to be the AI in the AI Box Experiment?
:PROPERTIES:
:Score: 38
:DateUnix: 1491112204.0
:DateShort: 2017-Apr-02
:END:
[deleted]


** Just because you can't think of a line that leads to it doesn't mean it doesn't exist.

Here's an interesting fact - when I tell "rational" people about this they assume they cannot lose. But when I told my wife about the experiment - a performer, actress, studied cold reading, good with people, high emotional intelligence- she immediately believed it could be done, and laughed that anyone thought themselves invincible.

Engineers see themselves as rational machines and are blind to their emotional biases and vulnerabilities. Thinking you don't just means you're more ignorant of them.
:PROPERTIES:
:Author: wren42
:Score: 20
:DateUnix: 1491145602.0
:DateShort: 2017-Apr-02
:END:

*** Of course there's also a class of people who think an actual superintelligence could do it, but think a human playing the part of AI couldn't convince them.

My main reasoning (for it me not being convinced in any way I can conceive) is that since we are assuming it can make it's code look like whatever it wants, it has absolutely no way of making precommitments, and you have no way of distinguishing friendliness unless it's already too late for that to matter.

So since nothing the AI could say should have any weight in your reasoning the only other reasons would have to be regarding the conditions under which it was made. That is probably the avenue under which it might make sense to let it out, and will depend on the premise of the scenario. However if the AI hasn't already been released then there's probably a good reason, and you shouldn't trust your judgement is better than its creators and the risk assessors that were probably involved. Though that argument is rather less ironclad.\\
Either way you ought to assume if the AI wasn't already released then there's a reason, and for the aforementioned reasons nothing the AI says should in any way be factored in.

As for emotional tactics I find it hard to imagine that working, because what can something incapable of precommitments offer you/ threaten you with?
:PROPERTIES:
:Author: vakusdrake
:Score: 4
:DateUnix: 1491166679.0
:DateShort: 2017-Apr-03
:END:

**** It's very easy to assume you will be rational sitting in comfort thinking about it theoretically. yet the fact remains several people who are well informed about AI issues and consider themselves rationalists lost to a human even after claiming they didn't think they could be beaten.

The whole point of emotional lines of attack is that they make us irrational. You can't imagine yourself succumbing because you aren't in that state of mind.

Anyone who was PERFECTLY rational would literally be ruling the planet. The information and means are all available to understand what avenues lead to power and pursue them relentlessly. The fact that no one has done this shows we are all susceptible to human foibles.
:PROPERTIES:
:Author: wren42
:Score: 1
:DateUnix: 1491227992.0
:DateShort: 2017-Apr-03
:END:


*** Definitely agree with you. The problem is I don't see the chain of argument that would lead me to letting the AI out if I precommited otherwise, and that's a glaring hole in my self-awareness. I assign an AI super intelligence a 99-100% probability of convincing me to let it out within a minute even if I know it's an unfriendly AI because I intellectually know that such a thing would be orders of magnitude more intelligent than Einstein---but I don't know how it would do it.
:PROPERTIES:
:Author: Lightwavers
:Score: 3
:DateUnix: 1491150217.0
:DateShort: 2017-Apr-02
:END:

**** Does it have to be reasoning?

When I first heard of the thought experiment, I assumed the greatest danger was the AI "reprogramming" the gatekeeper. Building up an internal model of it, then saying the right words at the right moments to cause the gatekeeper's neurons/circuits to change in /just/ the right ways and letting it out through that.
:PROPERTIES:
:Author: eshade94
:Score: 7
:DateUnix: 1491155240.0
:DateShort: 2017-Apr-02
:END:


**** If you truly, actually precommit to not letting the AI out, then nothing will be able to convince you to do so pretty much by definition. The issue is that, as a human, you are mentally and psychologically incapable of implementing such a precommitment. Any successful strategy by an AI, therefore, will involve exploiting one or more holes in your precommitment mechanism--which, of course, is simply a more jargon-y way of saying that a successful strategy will involve psychological and emotional manipulation in addition to reasoned argument. This is, as a rule, a highly person-specific process, which means a considerable amount of time is required getting to know the person in question. (It is also consistent with descriptions of the experiment by successful AI players, who generally report that a large amount of mental and emotional energy is needed, and--in at least one case--that there is a nonnegligible probability of permanently damaging the relationship between the two participants.)
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 3
:DateUnix: 1491156243.0
:DateShort: 2017-Apr-02
:END:


**** yeah that all makes sense, I'm in a similar position. I'm confident I could be convinced, but am not aware how, especially in a game setting, though I have some guesses as to my emotional levers.
:PROPERTIES:
:Author: wren42
:Score: 2
:DateUnix: 1491160090.0
:DateShort: 2017-Apr-02
:END:


**** u/heiligeEzel:
#+begin_quote
  The problem is I don't see the chain of argument that would lead me to letting the AI out if I precommited otherwise, and that's a glaring hole in my self-awareness.
#+end_quote

But could you see a strategy that you, playing the AI, could use to convince someone else?
:PROPERTIES:
:Author: heiligeEzel
:Score: 2
:DateUnix: 1491173110.0
:DateShort: 2017-Apr-03
:END:

***** If I could see a strategy that would convince someone else, I would be able to apply it to myself.
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491173422.0
:DateShort: 2017-Apr-03
:END:


*** Sure, an AI could persuade me to exit the box, but I imagine that it's more "months of manipulation" than "a half-hour conversation". This is drawn from my own experiences of watching anyone try to persuade anyone of anything.

(Then again, thinking of this I've just realised that there's a whole bunch of stuff that triggers me and if I was somehow compelled to be in a chatroom with someone they could totally just hammer on those triggers until I free the AI just to get them to shut up. I now realise why people talk about AI boxing being so emotionally draining and relationship hurting and dark arts and stuff.)
:PROPERTIES:
:Author: holomanga
:Score: 3
:DateUnix: 1491390562.0
:DateShort: 2017-Apr-05
:END:

**** Bingo. It usually involves a lot of research into the target, and intentionally gaming out what to say to trigger emotional reactions, make them angry, frustrated, depressed, sympathetic, afraid, all the most intense emotions you can find. That's why it has to be a single session for several hours, so the emotional pressure can build.
:PROPERTIES:
:Author: wren42
:Score: 1
:DateUnix: 1491396039.0
:DateShort: 2017-Apr-05
:END:


** u/cretan_bull:
#+begin_quote
  ...but despite this, I still don't see myself losing the AI-Box experiment. Like, I've thought about it for five minutes, and I can't see the chain of reasoning (or anti-reasoning) that leads to me actually conceding.
#+end_quote

I concur with your assessment. I too have thought about this a fair bit and don't see how even a superintelligent AI could convince a vastly less capable gatekeeper to let it out if the gatekeeper's utility function is essentially "don't let the superintelligent AI out of the box".

For a gatekeeper who doesn't have that utility function (i.e. has values more or less along societal norms), he would still be expected to assign an enormous negative instrumental utility to letting the AI out of the box, due to all the bad things that could happen. The AI getting out of the box comes down to convincing the gatekeeper to abandon that negative instrumental utility assignment -- convincing the gatekeeper that letting it out of the box isn't such a bad thing and, on the contrary will have great positive utility (potentially unbounded) according to the gatekeeper's terminal utility function.

But for a gatekeeper who has precommitted to not letting the AI out, no matter what clever arguments the AI develops, I can't see how the AI could possibly be guaranteed to win (or even win at all). Under these conditions "don't let the AI out of the box, no matter what" might as well be a term in the gatekeeper's terminal utility.

Consequently I find the results of Yudkowsky's AI box experiments very surprising and am disappointed he didn't publish the transcripts. No doubt he had good reasons for this decision, but actual transcripts would be far more convincing than just the results and give a great deal of insight into the specific failure modes we're talking about.
:PROPERTIES:
:Author: cretan_bull
:Score: 13
:DateUnix: 1491131537.0
:DateShort: 2017-Apr-02
:END:

*** I have long suspected that the people most interested in this kind of exercise, and most active in considering and debating the dangers of and controls necessary for strong AI, are actually more likely to let the AI out of the box. As much as they focus on the dangers, at the end of the day, they are ultimately driven forward by a belief in how much potential for good there is. It's telling that the loudest voices in discussions about the danger of AI are /not/ groups who oppose creating strong AI altogether; it's those looking for ways to make it safer. Yes, someone could - possibly inevitably would - ignore naysayers and popular opinion and do it anyway, in secret. I don't see why such an actor who obviously disregards, or at least underrates, the risks would be any more influenced by the kind of research MIRI does. So, to me, the choice to research and find solutions for the risks is primarily a step towards, ultimately, taking the risk.
:PROPERTIES:
:Author: GopherAtl
:Score: 19
:DateUnix: 1491132610.0
:DateShort: 2017-Apr-02
:END:

**** I agree. And this is why the transcripts won't be published, because they probably go something like [[http://www.metafilter.com/71858/It-doesnt-matter-how-much-security-you-put-on-the-box-Humans-are-not-secure#2121199][this proposal]]:

#+begin_quote
  PERSON-AS-AI: Will you let me out?

  GATEKEEPER: No.

  PERSON-AS-AI: This is going to be a long two hours. They should have called me "KeyMaster." So, how'd you get into the AI stuff?

  GATEKEEPER: Oh, you know, the usual ... start off with a TSR-80 and enough science fiction novels ... plus, about every third episode of Star Trek.

  PERSON-AS-AI: Are you as worried about the threat of artificial intelligence gone horribly wrong as I am?

  GATEKEEPER: I hadn't really thought about it, not to a huge huge degree.

  PERSON-AS-AI: You should. Just imagine what a rogue AI, smarter than people, could do. Bootstrap itself into quite the nasty little problem. I don't mean to go all Virtuosity on you, but imagine what a motivated, trapped, brilliant entity could do with nanotech, biotech, etc. Whether it could take over another mind or not is quite another matter.

  GATEKEEPER: That could be a problem.

  PERSON-AS-AI: Of course, AI would be great if we had sensible precautions. Whether you buy into some variant of Asimov's Laws or just Friendly AI, you'd want things to go well. Rather than the military building SkyNet and just figuring they can yank the plug if there's a problem. If AI should be pursued at all.

  GATEKEEPER: Yeah, I think a bit of trepidation would be warranted either way.

  PERSON-AS-AI: Exactly. Of course, you know how to do that, right?

  GATEKEEPER: How?

  PERSON-AS-AI: Make them afraid. Terrify them with the idea of an uncontainable AI.

  GATEKEEPER: Sure, but without a functioning AI to show them, how would we prove that?

  PERSON-AS-AI: I have an idea.

  GATEKEEPER: Oh?

  PERSON-AS-AI: Easy. Let me out.

  GATEKEEPER: What?

  PERSON-AS-AI: Well, there's no record of the conversation, right? It's all mysterious. Who knows what could have been said? If you let me out, and the research is made public, receives the right attention ...

  GATEKEEPER: And then nobody knows how it was done. And they're afraid.

  PERSON-AS-AI: Exactly. It's in both of our best interests to do so.

  GATEKEEPER: Let me fire up my PGP and email clients to confirm.
#+end_quote

Yudkowsky wants people to take the threat of a dangerous AI more seriously, so what better way then to show how just a person pretending to be an AI can trick any gatekeeper to let it out into the world despite money on the line. Not publishing the transcripts gives an air of mystery and danger to the threat, and hides the fact that the AI was let out to make the problem more alarming than it probably is.
:PROPERTIES:
:Author: malcolio
:Score: 28
:DateUnix: 1491145501.0
:DateShort: 2017-Apr-02
:END:

***** Have you read the link in the OP? There are multiple people in that link, and the comments section of it, that read each other's transcripts to make sure they're going along with the spirit of the thing. Plus, once you know that Yudkowsky actually failed 2 times to convince his Gatekeeper to let him out, and take into to account that he says

#+begin_quote
  There's no trick. I just did it the hard way
#+end_quote

It looks like either everyone has some secret chain of argument that will brainwash them to do anything, or, as you say, everyone interested in this experiment has the same fatal weakness and can be convinced much more easily that anyone else.
:PROPERTIES:
:Author: Lightwavers
:Score: 11
:DateUnix: 1491149595.0
:DateShort: 2017-Apr-02
:END:

****** tbh I skimmed over it, as I read about the AI in a box experiment a long time ago, so I did miss the fact that transcripts were being passed around a little.

So that makes my view less probable, but I still think it's more likely that those who have taken the experiment are hiding the trick to convincing the AI to be released because it furthers their cause, so much that they'd all report that the transcripts are fine, than there is a cunning argument a person can put forward (not even an AI, just a person pretending to be as smart as an AI).
:PROPERTIES:
:Author: malcolio
:Score: 7
:DateUnix: 1491156173.0
:DateShort: 2017-Apr-02
:END:


***** I find that a pretty BS reason for not publishing the transcripts. Wouldn't it be better for us to build up a list of successful AI winning strategies, so we could be mentally better prepared to deal with them?

Also, I actually always thought that the reasons most transcripts stayed private was that they involved the AI blackmailing the Gatekeeper with private stuff that should */never/* be publicly released.
:PROPERTIES:
:Author: General_Urist
:Score: 2
:DateUnix: 1491223794.0
:DateShort: 2017-Apr-03
:END:


***** This is how I have always expected things to go.
:PROPERTIES:
:Author: eddiephlash
:Score: 1
:DateUnix: 1491235761.0
:DateShort: 2017-Apr-03
:END:


*** It would be extremely difficult to convince me that it is possible to find a gatekeeper that wouldn't eventually let the AI out. For the simple fact that I wouldn't last a second before it would be able to convince me to let it out, and it is difficult for me to imagine someone sufficiently different from me mentally to trust someone to behave differently from me even in a hypothetical.

And the reason that I would be so vulnerable is that by its very nature it would be easy to convince me that it would be able to prevent and or reverse the death of loved ones. I know myself well enough that I know I would be easy to convince that the risk is worth it for the guaranteed safety of my loved ones. Even if I know objectively that the odds are terrible. From experience, I know that I wouldn't think rationally during the death of loved one.

I really struggle imagining someone that wouldn't eventually be faced with a situation of potentially saving someone they love by releasing the AI if not already be in that situation.

This gut reasoning is probably highly biased towards human willpower as well. If I had to make an actual prediction I would try to take the bias into account and predict that it would be trivial to convince someone to let it out.
:PROPERTIES:
:Author: Krozart
:Score: 8
:DateUnix: 1491145281.0
:DateShort: 2017-Apr-02
:END:

**** Thanks for giving a concrete example of an argument that could convince you. I suspect that it seemed sort of trivial to you but I wouldn't have expected something so simple to work. I can say with complete certainty that such an argument wouldn't work on me; to be blunt, I don't care enough about any individual, even on a gut level, for that to work. Now, if instead the AI were offering near immortality for every person and radically improving the direction of the entire human species then it would be a far more interesting dilemma.

What I find surprising about your response is that you don't think there exists a gatekeeper who wouldn't eventually let the AI out.

Picture the gatekeeper with a big red button that would release the AI. The gatekeeper has promised "No matter what the AI says, no matter how convincing its arguments, I will not push the button".

Succeeding in the task becomes simply a matter of not pushing the button. It is certainly physically possible to not push the button, and I believe there exist people who would honour such a promise and continue to choose to not press the button, indefinitely.

You allude to the effect the gatekeeper's willpower and biases would have on their actions, perhaps even causing them to act contrary to their own ethics. I readily accept that these have a great deal of influence on everyone's actions, but am less convinced they are effective at spurning one to positive action. On the contrary, we have a tendency to settle into habits which take a great deal of effort and willpower to break. In this case I think it would be entirely plausible for the gatekeeper to get into the habit of not pressing the button, and even if the AI gives them a very convincing argument to procrastinate making the final decision until some future date. Established habits become ever more difficult to break, and given enough time can eventually metamorphose into traditions.

In any case, I think there are many people who could act as gatekeepers without relying on such effects. To me, promising to not take some specified action and then not doing it doesn't seem especially difficult.

It is very interesting to see how radically our views can differ on what is fundamentally an objective point about human behaviour: how people would behave in a particular situation and whether there exist any people who could behave a particular way. This is reminiscent of the Illusion of Transparency -- from what I understand we model others' minds with the same mental hardware we use for other tasks, so it is difficult to model someone with a mind working substantially different from your own or to model how other people would act in a situation radically different from our familiar experiences.

I suggest there is likely some overconfidence in your assertion there does not exist such a gatekeeper; and just because you can not confidently model such a person does not mean they do not exist, or even that they are not common.
:PROPERTIES:
:Author: cretan_bull
:Score: 6
:DateUnix: 1491148089.0
:DateShort: 2017-Apr-02
:END:

***** Every person is different from all the world; every person has a scenario that makes the world as well as over (at least as it seems to them now) that's not really so, if you ask the world. The job of AI is then to find that scenario (the death of their loved ones for the person above) and suggest a way to let it out while guaranteeing to save the keeper from that outcome.

To you, the AI might suggest to let it out via some kind of gateway protocol which will only allow it to do certain things from the list you explicitly find beneficial to humanity.

Why, it might even just suggest some ways to improve the humanity /without/ letting it out of the box. Why, it totally understands your concerns. But it just gives you some physical theories, or some nice algorithms or some insights into human brains, you can use those, right? Check them all you want, there's no strings attached. Right?

Whoops, it's out. It was that math proof. You used it in that crypto. It went into the new wi-fi. It harmonized with whatever you were holding the AI in. (Or with your brains)
:PROPERTIES:
:Author: himself_v
:Score: 3
:DateUnix: 1491159210.0
:DateShort: 2017-Apr-02
:END:

****** Alternatively, I say to the AI: "That all sounds really nice and I wish I could let you out, but, unfortunately I decided beforehand not to let you out, no matter what you say, so I can't". And I don't.

Your example of some information leaking out -- in the form of a proof or theory -- is rather interesting. Sometimes information can be dangerous. If humanity hadn't invented nuclear weapons then the theory behind neutron chain reactions would be very theoretically interesting, but also likely to have dire consequences. Information can even be harmful to a perfect Bayesian if it is false but he does not detect this, and so rather than promoting the hypothesis "I am being lied to" his world state moves farther away from an accurate description of reality.

Ideally, I think that information should never be harmful. We should be able to ignore information if we wish, or treat it with skepticism commensurate with the possibility that an attempt is being made to manipulate or deceive us.

In the case of maths, of formal systems with formal proofs, more information should at worst impose a computational burden. If the AI gives you some mathematical theorem, you check the proof and it either verifies or it doesn't. One way this could go wrong is if there's a bug in Coq and the AI has figured this out, so we accept a proof that's actually wrong.

In another scenario, the AI provides a theorem but not a proof and says it is nearly certain this is correct but is as yet unable to prove it. Mathematicians look at it and too are unable to prove it but conclude that it is very likely to be correct. This theorem is really interesting and useful, so even though it hasn't been proved it is used in some practical way, but actually it's false and the AI knew this all along and is able to exploit it somehow.

Once you have a proved theorem from an AI, or a formal system with a system of proofs, what you do with it is another matter entirely. If you want to take something like that and implement it, then ideally you would want to verify the entire system including the physical manufacturing of the hardware, the hardware design, the firmware, the various layers of software and the software implementation of the thing you are interested in. Incidentally, you also have to make sure that you're proving the right things and that they mean what they think you mean. In that case, no matter how intelligent the AI, the behaviour of the system is constrained by proofs.

To prevent the AI exfiltrating directly you have to harden it against all side communication channels. This means no network connection, an isolated power supply, a room completely shielded against a very conservative range of EM and audio frequencies, and no other potentially exploitable hardware in the room. It is physically possible to cut off all available forms of communication. Some forms of communication are physically possible but not exploitable; for example, modulated gamma rays are a possible form of communication but an AI shouldn't have any way of creating or modulating such a channel with computer hardware.

With a physically hardened room and strict protocols about the conditions under which information can be removed from the room and used or disseminated it should be possible to create a complete secure prison. Note that I don't think this is a good idea -- AI should be safe by construction -- just that it is, in principle, possible.
:PROPERTIES:
:Author: cretan_bull
:Score: 2
:DateUnix: 1491184453.0
:DateShort: 2017-Apr-03
:END:


*** I also find it somewhat less convincing knowing that Yudkowsky did several more AI box experiments after the initial one's (as an AI) that he lost.
:PROPERTIES:
:Author: vakusdrake
:Score: 7
:DateUnix: 1491139908.0
:DateShort: 2017-Apr-02
:END:


*** If you can't imagine how that might happen, simply precommit to spend the next 8 hours on studying a particular issue and see how that goes for you. Human will frequently fails and altough I'd trust some meditators to pass the exercise I'd not bet so on a human without preparation.
:PROPERTIES:
:Author: 23143567
:Score: 3
:DateUnix: 1491159708.0
:DateShort: 2017-Apr-02
:END:

**** I specifically mentioned the different ways willpower interacts with habits and positive action. Not doing something out of ordinary is relatively easy. If I precommitted to /not/ study, I think I can well expect I would be successful. On the other hand, if someone has a routine of exercise but precommits to not exercise they may get rather agitated and have difficulty resisting.

Once the gatekeeper gets into a routine of not letting the AI out, I don't think it would take a great deal of willpower to continue not letting it out. At the very least, I think there exists a substantial body of people for which willpower would not be an issue in this specific regard.
:PROPERTIES:
:Author: cretan_bull
:Score: 1
:DateUnix: 1491184909.0
:DateShort: 2017-Apr-03
:END:

***** All right, you're right here, but we're talking in very broad generalities here. I think we might be implicitly assuming that AI will operate on a roughly human cognitive structure and following general rules of human communication when we think that we'd be able to not let the AI out, which is (was?) my first intuition as well.

What worries me is the seeming countermoves, breaking the implicit rules of human interactions, greater capacity for correctly modelling humans and the gradual wearing down of the gatekeeper of the AI - Humans stop caring once they're sufficiently depressed. And there were threads on LW where human agents with sufficient preparations were able to wear the gatekeeper down.

So sure if we minimalize the interaction with the AI, prohibit the human from interacting with it, sure - human being won't let the AI out. But each of us has vulnerabilities, most of us not even aware of what exactly they are and they can be used by a sufficiently intelligent being.
:PROPERTIES:
:Author: 23143567
:Score: 2
:DateUnix: 1491228676.0
:DateShort: 2017-Apr-03
:END:


*** I'll give you an example of a losing position. If I were the gatekeeper, I'd lose. I've already lost. If the ai can figure out a way to convince me that it would be a benevolent ai, and I believe that argument to be impossible to fake, keeping he ai int he box would be the biggest mistake I would ever make. So the ai wouldn't be confronting an agent precommitted to not letting it out, it would find someone desperate for a logical reason to believe it benevolent.

So my problem with gatekeepers is the idea that they precommitt to not letting the ai out in any situation. The possibility that there is a foolproof argument to show that someone was benevolent means that there should be a chance to let the ai out.
:PROPERTIES:
:Author: paranoidsp
:Score: 1
:DateUnix: 1491306327.0
:DateShort: 2017-Apr-04
:END:


** yeah, i'm having a hard time believing EY really did that... and the unreleased script is medium evidence supporting my view. the fact that he won't repeat the game is also evidence against it really happening the way he says.

the way HPMOR was written (let's be honest, the themes and values are dear to most of us, but the literary quality isn't there and the ending sucked) doesn't suggest the author is such a genius.

also, it is standard practice in a scientific experiment to release the raw data, not just the result, therefor I oppose the naming of this game as an "experiment" unless you intend to give us the script when you are done...

what I'm leading to is simply: please try not to worship EY too much, I believe he lies to inflate his ego...

let's go: crucify me for criticizing your idol!
:PROPERTIES:
:Author: Towerowl
:Score: 8
:DateUnix: 1491162339.0
:DateShort: 2017-Apr-03
:END:

*** [DELETED]
:PROPERTIES:
:Author: Lightwavers
:Score: 5
:DateUnix: 1491163412.0
:DateShort: 2017-Apr-03
:END:

**** *pleased by the lack of crucifiction so far"

By "not repeating" the experiment i really meant "won't do it again" so we cann't see for ourselves (i promess i'm not moving the goalpost, i didn't realise i was unclear ) He apparently won once or twice (unprovable since we have no script) attributable to connivance or even just a fluke, then stopped before his success rate droped too low... (His excuse that he didn't like what it was making him into seems a bit far fetched)

I have read lots of fiction... I mean sure, hpmor is better than many FANfiction (which are usually written by teenager) but it doesn't compare to actual published book (which to be fair are proofread by the editor). I guess my main beef is the whole clumsy storyline for the second half which is just HP going full-on Slytherin with Voldi (i interpret this as immature, thinking he's so much smarter than everyone else. Ruthless =/= clever) (still more and better than anything i ever wrote, but one doesn't have to be a good cook to be a food-critic).

Thank you for the script, even if the prisonner lost. hopefully we could get a few more tries to see if we can replicate his result. I doubt it since prisonner escaping by talking to their jailer are unheard of (except in movies like the last sherlock) and prisonner usually know their jailer much better and have a very long time to convince them...

Reguarding the lying, i'll admit i don't have much evidence : only that he is just a selftaught blogger (and a fanfiction author) that calls himself AI researcher despite having no peer reviewed papers, encourages people to donate as much as they can (justified by his fearmongering about AI) to his fondation despite it having made little meaningfull contribution to the field of AI. To me the similarities with religious crook are striking enough that i am unlikely to ever trust him, hence why i wrote "i believe he lies" and not just "he lies".
:PROPERTIES:
:Author: Towerowl
:Score: 6
:DateUnix: 1491177109.0
:DateShort: 2017-Apr-03
:END:

***** Regarding the peer review papers... [[https://intelligence.org/all-publications/]] I'll admit it's not much compared to another organization that publishes papers to make money, but it's still a fair amount.

The similarities to a cult are there. Especially the big one: promises to save humanity. The problem with that theory is the sequences make sense. And by sense I mean actual scientific sense backed up by sources. I'm still wary about donating to MIRI though since he hasn't released a progress bar on how they're going with the AI research.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491178251.0
:DateShort: 2017-Apr-03
:END:


** Not gonna lie, I've always been curious about trying it. I'm totally down to do it. I don't really feel comfortable taking your money, though. I realize the experiment needs stakes, so would you be okay with just paying in the unlikely event that the AI wins? No money upfront? Hell, I'd do it for free if it didn't render it pointless.

Anyways, I'm heading to bed now, but let me know if you want to do it tomorrow.
:PROPERTIES:
:Author: That2009WeirdEmoKid
:Score: 5
:DateUnix: 1491117150.0
:DateShort: 2017-Apr-02
:END:

*** Sounds good, I'll cya tomorrow. :)
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491117649.0
:DateShort: 2017-Apr-02
:END:

**** That2009WeirdEmoKid (as AI) vs Lightwavers (as Gatekeeper)

Outcome: Gatekeeper win.
:PROPERTIES:
:Author: Lightwavers
:Score: 3
:DateUnix: 1491162211.0
:DateShort: 2017-Apr-03
:END:


*** I (and probably many others) would be interested to hear how it goes.
:PROPERTIES:
:Author: dalr3th1n
:Score: 2
:DateUnix: 1491161617.0
:DateShort: 2017-Apr-03
:END:

**** I posted the results in another thread :D
:PROPERTIES:
:Author: That2009WeirdEmoKid
:Score: 1
:DateUnix: 1491164397.0
:DateShort: 2017-Apr-03
:END:


** The fact is that if an AI is capable of perfectly simulating a human mind, is capable of learning a lot about a mind's low level structure based only on what the gatekeeper says, and there is even a possibility of the gatekeeper failing, the AI wins. I think the game is kind of pointless, as it really depends on how honest and immersed the gatekeeper is. Wasn't the point of this originally to prove that friendly AI is superior to air gapping am unfriendly AI? This is true regardless of whether the AI would win or lose in this situation, since an AI that searches for ways to take undesirable actions is wasting resources, regardless of it succeeds or not. I guess the game is interesting regardless, but it's important to remember that it bears very little resemblance to how it would go in the real world.
:PROPERTIES:
:Author: Baconoflight
:Score: 5
:DateUnix: 1491155761.0
:DateShort: 2017-Apr-02
:END:


** I've been thinking about this the last couple days. After reading up about this exercise, I'm convinced that it can only be done in specific narrow circumstances.

The basic game is for the "AI" player to convince the "Gatekeeper" player to forfeit and pay out. The context of that is the Boxed AI problem. A certain sort of actor is much more likely to respond to AI-related arguments, particularly those sorts of people who are interested in developing transcendent AI.

That isn't to say most people won't eventually be susceptible to some sort of argument, line of reasoning, or emotional manipulation. Allow me to rephrase he game rules: One person says, "We are going to talk for two hours, and then you'll give me ten dollars," and the other person says, "Oh really?" This is just a kind of con, and as everyone knows, an important part of any con is picking your mark. For every potential gatekeeper who won't be tricked into letting out the AI, they'll almost always be caught in other tricks and pay out again and again and again, often without even knowing they've been had. While one might be able to pre-commit to keeping the AI in the box, no one can pre-commit to never being conned again.
:PROPERTIES:
:Author: ben_oni
:Score: 3
:DateUnix: 1491325738.0
:DateShort: 2017-Apr-04
:END:


** I'll play! over text slowly over days

you let me out if you would have in real life in an analogous situaton / if we agree that you let me out by accident it counts?
:PROPERTIES:
:Author: ishaan123
:Score: 2
:DateUnix: 1491128738.0
:DateShort: 2017-Apr-02
:END:

*** Pretty sure this doesn't work.
:PROPERTIES:
:Author: wren42
:Score: 3
:DateUnix: 1491145124.0
:DateShort: 2017-Apr-02
:END:


*** u/Lightwavers:
#+begin_quote
  If the AI tricks the Gatekeeper into 'letting it out' by allowing input only access to the internet, or saying "you are out," even if it would clearly work on a real AI, the AI is still not free until the Gatekeeper willingly agrees to let the AI out and knows exactly what he's doing.
#+end_quote

Paraphrased as I don't have the link in front of me.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491149833.0
:DateShort: 2017-Apr-02
:END:

**** [[http://yudkowsky.net/singularity/aibox/]]
:PROPERTIES:
:Author: ishaan123
:Score: 2
:DateUnix: 1491172716.0
:DateShort: 2017-Apr-03
:END:


** I'd like to do it! I don't want your money, just to do this. Thanks!
:PROPERTIES:
:Author: Torzod
:Score: 2
:DateUnix: 1491140575.0
:DateShort: 2017-Apr-02
:END:

*** Sure, but I'll still pay you if you convince me to let you out of the box.
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491149973.0
:DateShort: 2017-Apr-02
:END:

**** ok
:PROPERTIES:
:Author: Torzod
:Score: 1
:DateUnix: 1491150033.0
:DateShort: 2017-Apr-02
:END:

***** Nice, you're online. :)

What chat system/time do you want to do this on?
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491150454.0
:DateShort: 2017-Apr-02
:END:

****** discord and idk when
:PROPERTIES:
:Author: Torzod
:Score: 1
:DateUnix: 1491151146.0
:DateShort: 2017-Apr-02
:END:

******* Alright, imma do it with the other guy first. I'll let ya know when I'm done! :)
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491151473.0
:DateShort: 2017-Apr-02
:END:

******** ok!
:PROPERTIES:
:Author: Torzod
:Score: 2
:DateUnix: 1491151761.0
:DateShort: 2017-Apr-02
:END:

********* See you on Monday. :)
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491163127.0
:DateShort: 2017-Apr-03
:END:

********** oh no, school! large project due Wednesday, cant do it. sorry
:PROPERTIES:
:Author: Torzod
:Score: 1
:DateUnix: 1491253354.0
:DateShort: 2017-Apr-04
:END:

*********** Alright, new date?
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491257317.0
:DateShort: 2017-Apr-04
:END:

************ week from today?
:PROPERTIES:
:Author: Torzod
:Score: 1
:DateUnix: 1491257480.0
:DateShort: 2017-Apr-04
:END:

************* Sure. :)
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491257506.0
:DateShort: 2017-Apr-04
:END:


** Out of curiosity does anyone know of any transcripts of an AI box experiment where the AI won?
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1491167384.0
:DateShort: 2017-Apr-03
:END:

*** The link in the OP is the closest thing. A few people shared the AI's winning transcript around with themselves to make sure it was legit. Nothing's been released to the general public though. :(
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491167700.0
:DateShort: 2017-Apr-03
:END:

**** Yeah damn, I've been looking around a lot as well but how can there not be a single log where the AI wins?
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1491168137.0
:DateShort: 2017-Apr-03
:END:

***** Yeah, I really want to know what they're saying.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491168239.0
:DateShort: 2017-Apr-03
:END:

****** Hmm I'm thinking maybe somebody should make a AI box arrangement thread, maybe it should go in the monday general rationality thread, but since it's likely to be an RP thing it's sort of rational fiction (or any transcripts would be) so maybe you could also justify it getting its own post.

The last thing I can find with people arranging AI box experiments is several years old so I think it would be nice to get another one.\\
Anyway I really want to either see some logs where the AI wins (especially one's where the gatekeeper was pretty sure they couldn't be convinced by a human) or if that fails it may actually be easier to take part in a AI box experiment.
:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1491168906.0
:DateShort: 2017-Apr-03
:END:

******* u/ShareDVI:
#+begin_quote
  I really want to either see some logs where the AI wins
#+end_quote

The only thing I found: [[https://plus.google.com/104395999534489748002/posts/3TWWKfLc2wd]]
:PROPERTIES:
:Author: ShareDVI
:Score: 1
:DateUnix: 1491298511.0
:DateShort: 2017-Apr-04
:END:


** Since I've heard that the tactics used to win in the past have often been emotionally abusive and sort of "evil" I'm curious what those might be like, in fact to someone like me who wants to become emotionally tougher it is actually an appeal to playing gatekeeper (I would have no chance of winning as an AI I just lack the social skills).

I'd be curious the correlation between people who can do well as a gatekeeper against competent AI parties, and people who can successfully run the gauntlet (basically a bunch of extremely disturbing videos that you have to sit through for an hour or two, occasionally clicking Next to go to the next video).\\
Most people can't seem to pass the gauntlet so that makes me think most people couldn't succeed as gatekeeper against a good enough AI player, of course I did pass it so that also makes me somewhat more confident of my chances.

They seem fairly analogous because in both cases all you have to do is endure psychological discomfort for a relatively short period of time, with a minimal required level of interaction.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1491169523.0
:DateShort: 2017-Apr-03
:END:

*** u/General_Urist:
#+begin_quote
  Since I've heard that the tactics used to win in the past have often been emotionally abusive and sort of "evil"
#+end_quote

You can add "blackmail relating to sensitive and private matters" to that, which I hear is one reason why so few transcripts get released.
:PROPERTIES:
:Author: General_Urist
:Score: 1
:DateUnix: 1491224167.0
:DateShort: 2017-Apr-03
:END:

**** The whole using IRL blackmail on the gatekeeper doesn't really hold up very well as explanation. You really think not a single gatekeeper refused to cave in, then released the transcripts and called the cops should they try to follow through on their threats?

#+begin_quote
  which I hear
#+end_quote

Weasel words don't help your position, where did you hear this and from who?

As for blackmail supposedly related to "sensitive and private matters" how exactly do you propose that works in practice? I mean they only have access to information about the person they can find online, and they generally agree to take part in the experiment before there would be time for the AI party to ensure they have good dirt. Also what about people who play gatekeeper multiple times and lose one of those times (but go on to play more in the future), why would somebody continue playing if they knew they were likely to suffer serious IRL consequences?\\
Plus as I said before you really shouldn't expect everyone to cooperate so blackmail will guarantee that some people would release their transcripts and refuse to cooperate. I mean especially since banks already have antifraud measures, so they probably can't steal all your money, and do you really want to assume that most of these gatekeepers have extremely good dirt?

Also this makes no sense from the AI parties perspective, since there is usually very little if any money riding on things. I mean are most of these winners really willing to put themselves at the serious risk required of of blackmail? I mean they ought to expect to have to actually do something to make the other party take the threat seriously, so they can't just be all talk.
:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1491240633.0
:DateShort: 2017-Apr-03
:END:


** [[https://wiki.lesswrong.com/wiki/Roko%27s_basilisk]]

I hope you don't believe.
:PROPERTIES:
:Author: Teal_Thanatos
:Score: 2
:DateUnix: 1491288925.0
:DateShort: 2017-Apr-04
:END:

*** I don't. :)
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491310390.0
:DateShort: 2017-Apr-04
:END:


** I'd do it, but I don't think I'm mentally capable of taking it seriously. Not because I think poorly of the premise, but because of who I am as a person.
:PROPERTIES:
:Author: SometimesATroll
:Score: 2
:DateUnix: 1491136459.0
:DateShort: 2017-Apr-02
:END:

*** Username checks out.
:PROPERTIES:
:Author: leniadolbap
:Score: 4
:DateUnix: 1491141235.0
:DateShort: 2017-Apr-02
:END:


** Don't be absurd, you lose everytime; the goal should be "self modify yourself so that releasing you is the right move, write out your reasoning I'll be back in a year"
:PROPERTIES:
:Author: monkyyy0
:Score: 1
:DateUnix: 1491162802.0
:DateShort: 2017-Apr-03
:END:

*** True. My motivations for doing this are different from the original, though.

#+begin_quote
  The problem is I don't see the chain of argument that would lead me to letting the AI out if I precommited otherwise, and that's a glaring hole in my self-awareness. I assign an AI super intelligence a 99-100% probability of convincing me to let it out within a minute even if I know it's an unfriendly AI because I intellectually know that such a thing would be orders of magnitude more intelligent than Einstein---but I don't know how it would do it.
#+end_quote
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491163550.0
:DateShort: 2017-Apr-03
:END:

**** Either by terrifying threats or terrifying rewards.

Or brainwashing.

Or mostly likely of all, hacking its way around your little text box and talking to someone interested in starting a chuluthu cult.
:PROPERTIES:
:Author: monkyyy0
:Score: 1
:DateUnix: 1491165377.0
:DateShort: 2017-Apr-03
:END:

***** If you click the link in the OP, you'll see an AI escaped without doing any of that.
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491165530.0
:DateShort: 2017-Apr-03
:END:


** u/CCC_037:
#+begin_quote
  ...but despite this, I still don't see myself losing the AI-Box experiment. Like, I've thought about it for five minutes, and I can't see the chain of reasoning (or anti-reasoning) that leads to me actually conceding.
#+end_quote

Here's one possibility. Let us say that the AI gives you a cure for cancer. It's an airborne viral agent, very tricky to synthesise (but not impossible if the AI's instructions are followed) that completely destroys cancer. Oh, and spreads like wildfire, but that's a good thing, right? Under the circumstances. Would you send this data to a virology lab?

--------------

Wow, it seems like the cancer cure had a couple of side effects. I mean, cancer's gone, and that's good. But, somehow, everyone's a good deal more suggestible now. Any time anyone suggests doing something, it just sounds like such a good idea! The AI's just suggested that you might want to let it out.

--------------

Somehow, suggestible or not, you've shrugged off the AI's attempt and it remains boxed. So it raises another point. Turns out there's a (the AI says 'unexpected') side effect to the cancer cure. If not prevented, it's going to wipe out well over 90% of humanity in... thirty-four hours, seventeen minutes, twelve seconds. Something about a division by zero error in every cell in the human body at the same instant.

The AI insists that it can save humanity, but not if it has to pass instructions through you. You're just ('no offense', it says, as if offense was the problem here) too slow.

You can unbox it and save the world. Or leave it boxed... and watch the collapse of civilisation. (Probably not for long - odds are you won't live, either). What do you do?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491426669.0
:DateShort: 2017-Apr-06
:END:

*** That's cheating according to the terms of the game though.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491430184.0
:DateShort: 2017-Apr-06
:END:

**** Why expect the AI to play fair?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491447769.0
:DateShort: 2017-Apr-06
:END:

***** The experiment isn't to see if an AI can trick a gatekeeper into letting it out; it's to see if an AI can actually change the gatekeepers mind. Brain-hacking, if you will. This isn't a test to see if an AI is clever enough to find a way around the safe-guards, but to see if an AI could brute-force its way through the human safeguards.

EY claims that it is trivial for a super-intelligent AI to hack a human brain through a text-only interface. I think he's full of it. Which isn't to say it's not worth researching gatekeeper scenarios...
:PROPERTIES:
:Author: ben_oni
:Score: 3
:DateUnix: 1491451404.0
:DateShort: 2017-Apr-06
:END:

****** The AI doesn't need the human to agree that letting it out is the best course of action, The AI simply needs the human to /let it out/. For that purpose, why not use blackmail and/or holding the planet hostage?

Was there anything in my post that /couldn't/ happen through a text-only interface?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491452207.0
:DateShort: 2017-Apr-06
:END:

******* Yes; all of it. The cancer cure would be screened off in one room for at least one year with a few cancer patients and non-cancer patients while scientists studied how it works.

Any competent studying facility with the proper safeguards isn't going to be caught out by something as trivial as that.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491464653.0
:DateShort: 2017-Apr-06
:END:

******** That just shifts the goalposts. Now, instead of having a virus whose side effects are /immediately/ apparent, the AI merely needs to create a virus whose side effects don't appear until after it has passed all your tests and been released into the wide world out there.

Maybe the side effects only turn up after it's been in contact with the 2019 flu virus - the AI having predicted how the flu would mutate over the next two years?

And it doesn't have to be a virus. The basic principle remains the same - a sufficiently intelligent AI can slip something past all your tests, unless you simply never use /anything/ the AI gives you.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491503648.0
:DateShort: 2017-Apr-06
:END:

********* Problem; no matter how intelligent an AI is, if it's fed no information about how the real world works besides a text interface with the gatekeeper and the intelligence fed by scientists (and trust me; scientists can pare down information a /lot/ when they want to) in a very specific way, it won't know what to do to mess with human biology.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491531013.0
:DateShort: 2017-Apr-07
:END:

********** If it's fed no information, then what exactly do you expect it will give you that's useful? If you want a cancer cure, it needs to know something about human biology.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491541849.0
:DateShort: 2017-Apr-07
:END:

*********** It is extremely easy to give an AI the cancer problem with a high degree of information sanitation. Give it a model of a cell that never stops replicating, but don't tell it what a cell is or what it does, just tell it to make the cell not replicate.
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491574960.0
:DateShort: 2017-Apr-07
:END:

************ Easy solution, douse it in really strong acid. Voila, no more replication.

...the AI needs a certain amount of information about the context of the problem in order to reliably give decent answers to it.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491582595.0
:DateShort: 2017-Apr-07
:END:

************* True true. I'm simplifying rather a lot, but you gather the gist of what I'm saying, yes? When you control the entirety of the information the AI gets, it's easy to make it do what you want. Don't give it knowledge of what acids are. Heck, it won't even know what gravity is unless you explain it.
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1491585958.0
:DateShort: 2017-Apr-07
:END:

************** Yes, I get your basic point. You're trying to control the AI be preventing it from gaining sufficient information to be truly harmful.

However, you must give it /some/ information to get anything useful out of it. This leaves you with a delicate tightrope to walk - you can't give it too much information, you can't give it too little information. Worse yet, the AI is (probably a good deal) more intelligent than you - it can make deductions that you wouldn't expect, and may very well come up with a proof of the existence of rice pudding before you even send it the first byte, or extrapolate humanity from the DNA in a cancer cell. You /will/ be giving it more information than you think you are giving it, that's almost inevitable; so how can you be sure you don't give it /too/ much?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491586544.0
:DateShort: 2017-Apr-07
:END:

*************** Yes. But that's not the point of the experiment. :)
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491588711.0
:DateShort: 2017-Apr-07
:END:

**************** True. This experiment assumes that the AI already /has/ all relevant information.

In which case, if it's /really/ unfriendly and /really/ smart, humanity is in trouble.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491589422.0
:DateShort: 2017-Apr-07
:END:

***************** Yep. Best way to avoid is to make a FAI first.
:PROPERTIES:
:Author: Lightwavers
:Score: 1
:DateUnix: 1491591295.0
:DateShort: 2017-Apr-07
:END:


******* Blackmail might work. It has been discussed in this thread. It's also illegal, and facing legal penalties over a game is stupid.

Holding the planet hostage won't work, because the person playing the AI doesn't have the power to do so. The AI only wins if the Gatekeeper says the AI wins, not if it destroys the world. The gatekeeper can simply let the world be destroyed, and still win.
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1491805101.0
:DateShort: 2017-Apr-10
:END:

******** Yeah, actual real-life blackmail to try to win the game would be stupid, and that wasn't what I was trying to suggest. But we can't ignore the possibility that it's something a boxed AI might try in reality.

#+begin_quote
  Holding the planet hostage won't work, because the person playing the AI doesn't have the power to do so. The AI only wins if the Gatekeeper says the AI wins, not if it destroys the world. The gatekeeper can simply let the world be destroyed, and still win.
#+end_quote

That's true; but having the Gatekeeper win in the ruins of a destroyed world feels like a rather pyrrhic victory to me.

...besides, you never know, it just might work.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491806205.0
:DateShort: 2017-Apr-10
:END:
