#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 19
:DateUnix: 1471273498.0
:DateShort: 2016-Aug-15
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** I've noticed something interesting about human awareness. Or my awareness, at least.

It might be hard to explain, so bear with me.

Did you ever noticed that you could live without actually having complex thoughts, only by... /reacting/, for lack of a better word? I'm not talking about routinely performed actions, such as to walking to work or changing your clothes. I'm talking about all actions and events.

In this thoughtless state, if anything at all happens, you don't /think/ about what you should do, don't formulate the situation as a problem in order to analyze and solve it; instead, a list of possible courses of actions flashes through your mind, and then you /feel/ which one you like the most, and go with that. This list consist of various cached thoughts, instinctive reactions and socially expected behaviour.

It dulls situational awareness, creative problem-solving and empathy-related abilities. As example, if you argue in this state with someone, you don't truly think about the opponent's arguments, don't contemplate his/her state of mind in order to tailor your counterarguments; you just answer with what first comes to mind. Or if you need to solve a mathematical problem, you look at it, try to solve by systematically using algorithms you've used in the past on similar-looking problems; if all fail, you stare at the problem blankly, and then go seek help. You don't experiment with it, don't try to understand it.

Now that I wrote it, it sounds similiar to [[https://en.wikipedia.org/wiki/Flow_%28psychology%29#Components][The Flow]] in a few ways: concentration on the present, check; merging of action and awareness, check; loss of reflective self-consciousness, check. Only instead of increasing performance, it decreases it, and instead of giving sense of control, it takes it away.

And I have a suspicion that many people live their lives almost perpetually stuck in that state.

Did that made sense? Have you ever experinced it/heard about it? Does it have a name?
:PROPERTIES:
:Author: Noumero
:Score: 21
:DateUnix: 1471282109.0
:DateShort: 2016-Aug-15
:END:

*** I just call that living \ functioning on autopilot (if I'm understanding you correctly, of course). When your brain thinks it has seen the same situation often enough to not waste additional energy on analysing it. In simplest (and more acceptable, IMO) cases this can be about things like walking down a familiar street, cleaning the house, etc. But it can also spread onto things like [[https://www.youtube.com/watch?v=eVtCO84MDj8][learning new material]] or supposedly putting your beliefs and opinions to test in a debate with another person. “Supposedly” because you're not really testing anything, if you're debating on autopilot --- you're just engaged in the debate for the sake of engagement itself (maybe it makes you feel nice and clever).

Strangely enough, this state is not the same as what Alan Watts (Zen?) advises as “living in the present” (correct me for a better term if you know one), because even though in both cases you're maintaining a non-verbal state of mind, in one case you're still deliberately keeping your mind actively investigating your surroundings while in another your present just gets lost from your life in the [[http://elderscrolls.wikia.com/wiki/Fast_Travel_%28Skyrim%29][“fast travel mode”.]]
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 11
:DateUnix: 1471283859.0
:DateShort: 2016-Aug-15
:END:

**** 'Autopilot'. Yes, that term fits. Thank you.

#+begin_quote
  “living in the present” (correct me for a better term if you know one)
#+end_quote

[[https://en.wikipedia.org/wiki/Mindfulness][Mindfulness]], perhaps?
:PROPERTIES:
:Author: Noumero
:Score: 4
:DateUnix: 1471285440.0
:DateShort: 2016-Aug-15
:END:


*** .
:PROPERTIES:
:Score: 3
:DateUnix: 1471287974.0
:DateShort: 2016-Aug-15
:END:

**** Nope, as OutOfNiceUsernames pointed out above, it's two quite different albeit similarly-looking things: autopilot (what I was talking about) and mindfulness (what you're talking about).

In fact, autopilot is an opposite of mindfulness, of sorts. Mindfulness is about self-control and absence of regrets, while living on autopilot assures that you /will/ feel miserable/break to pieces if it seems like you're supposed to, and it won't even occur to you to do anything about that.
:PROPERTIES:
:Author: Noumero
:Score: 9
:DateUnix: 1471289751.0
:DateShort: 2016-Aug-16
:END:

***** .
:PROPERTIES:
:Score: 3
:DateUnix: 1471294045.0
:DateShort: 2016-Aug-16
:END:


*** I would just call that a low-attention or low-precision state. I can enjoy it a bit when I'm pleasantly drunk, but over a long period or involuntarily it's completely awful.
:PROPERTIES:
:Score: 3
:DateUnix: 1471292549.0
:DateShort: 2016-Aug-16
:END:


*** u/scruiser:
#+begin_quote
  Now that I wrote it, it sounds similiar to The Flow in a few ways: concentration on the present, check; merging of action and awareness, check; loss of reflective self-consciousness, check. Only instead of increasing performance, it decreases it, and instead of giving sense of control, it takes it away.
#+end_quote

I think they are the same state of mind, just applied in very different ways... playing sports or an playing instrument, or doing martial art all involve applying skills that have been worked into muscle reflex and rote memory. Thus entering into a reactionary state can be beneficial and useful because it lets you react faster, without thoughts or hesitation slowing you down...

I don't have any particular evidence for this, but for my own anecdote... I played piano from 3rd grade to my Freshman year of high school, and for recitals, or the Christmas music that I played yearly, I would have the song down well enough to automatically play it. I did Karate throughout high school... overall, doing a Kata, I might think about specific portions that I need to get right, but overall I would be in a automatic state... for sparring, at least as I got better, I would react automatically, and only occasionally think to practice or plan a specific technique/move/thing I needed to work on.
:PROPERTIES:
:Author: scruiser
:Score: 2
:DateUnix: 1471301425.0
:DateShort: 2016-Aug-16
:END:


*** It may be related to [[https://en.wikipedia.org/wiki/Bicameralism_(psychology)][bicameralism]], originated from the book [[http://selfdefinition.org/psychology/Julian-Jaynes-Origin-of-Consciousness-Breakdown-of-Bicameral-Mind.pdf][The Origin of Consciousness in the Breakdown of the Bicameral Mind (PDF)]].
:PROPERTIES:
:Author: eusx
:Score: 1
:DateUnix: 1471358823.0
:DateShort: 2016-Aug-16
:END:


*** [deleted]\\

#+begin_quote
  [[https://pastebin.com/64GuVi2F/08923][What is this?]]
#+end_quote
:PROPERTIES:
:Author: the_steroider
:Score: -4
:DateUnix: 1471342304.0
:DateShort: 2016-Aug-16
:END:


** My gaming group managed to take down one of the "big bosses" in our campaign. Well, we managed to take down a little demon creature that was well beyond our level, and the DM told us afterwords it was basically like "Kicking down the big bosses door and killing them, at level 3".

So the boss is a little flying demon-like caster. It summoned minions, and those were what we were supposed to deal with. Or we were supposed to run away, I'm unclear.

Instead, we /grabbed/ the flying guy, while our cleric cast luck on the one holding it. In pathfinder, grappling is based on CMB, which is entirely class-defined. Being small and agile makes you harder to grab the first time, but doesn't give you any advantages when someone's actually got you in their grip. So we did that for a while, tying it up.

Then we used the coup de gras rules, which says something like "You get an automatic critical hit, if attack doesn't kill opponent, opponent, opponent must make fortitude roll of higher then damage or die instantly".

All in all, a pretty good execution of tactics. Just grab the tiny flying caster, hold it down, and kill it.
:PROPERTIES:
:Author: traverseda
:Score: 12
:DateUnix: 1471296898.0
:DateShort: 2016-Aug-16
:END:

*** Not sure if it's in Pathfinder, but this is why Permeable Form is such an invaluable spell.
:PROPERTIES:
:Author: Iconochasm
:Score: 4
:DateUnix: 1471299761.0
:DateShort: 2016-Aug-16
:END:


** As I mentioned on Friday, I've been using Chrome extension Crackbook [[https://chrome.google.com/webstore/detail/crackbook/nbgjmohekjolcgemlolblankocjlgalf?hl=en][(link)]] [[http://github.com/gintas/crackbook][(source)]] which provides a splash page to websites of your choosing.

I've applied it to Facebook, Reddit, Sufficient Velocity, SpaceBattles, and The Old Reader. Overall, I've found that the trivial inconvenience involved in visiting my favorite time-wasting websites has been a good help. Spending a few seconds on the splash screen has caused me to think "should I really be checking facebook right now?" and close the tab. Although I'd prefer being forced to press a button, this does a good job of keeping me more on task. It was especially helpful in the morning when I traditionally have trouble getting into my working flow.

I also like that if I really want to, I can still unwind and take a break; now, though, it's always a deliberate action. Also, since it's just a timer, I don't have to disable it to take a break. This extension remains enabled the whole time, so it's unlikely I'll turn it off then forget to turn it on. I recommend it.
:PROPERTIES:
:Author: blazinghand
:Score: 11
:DateUnix: 1471291930.0
:DateShort: 2016-Aug-16
:END:


** In [[http://kissmanga.com/Manga/Hi-no-Tori/Vol-002?id=196537][one of the books in Osamu Tezuka's /Phoenix/ manga]], he posits a future in which humanity has declined to the point where there are five cities left on Earth, each with about a million citizens.

Each is managed by a "supercomputer" (which seems to be a hollywood version of an FAI, I think) which acts as an executive, has final say in any new laws proposed, decides the fate and life choices of each citizen, etc. Needless to say, this is not remotely rational.

However, the biggest problem is that when the protagonist and his girlfriend are escaping from one city (Yamato) to another (Lengud), the Yamato AI contacts the Lengud AI. The Yamato AI demands that they be extradited from Lengud, but the Lengud AI disagrees.

The two AI then agree to nuclear war, and annihilate each other (the other three cities also explode, but this is never explained, and probably only happens to advance the plot).

There are many, many irrational things in this work, but I wanted to concentrate on this specific thing. Why would or wouldn't this happen?

Edit: "from" Lengud, not "to" Lengud.
:PROPERTIES:
:Author: rineSample
:Score: 6
:DateUnix: 1471281580.0
:DateShort: 2016-Aug-15
:END:

*** Is... is there a point to all this? Is it supposed to be a tragedy or something?

Anyway... sure I guess it could happen. It's imaginable that the right combination of bad programming and bad choices could result in that particular result. Your utility function could be optimizing for something other than human well being. Or the Supercomputers could have bad prediction/learning algorithms, and therefore make bad choices in a game of nuclear chicken/prisoner's dilemma.

But who decided the AI's were ready to be in charge of a city in the first place? Let alone the nuke buttons... Seems like they didn't quite test things enough. Then again the AI's could have tricked their handlers into thinking they were stable. That scenario is more likely if they had bad utility functions and less likely if they had bad predictive algorithms.
:PROPERTIES:
:Author: gabbalis
:Score: 12
:DateUnix: 1471283990.0
:DateShort: 2016-Aug-15
:END:


*** Why would AIs decide that the best course of action is to destroy one another?

The easy and boring answer is that because they are buggy/quirky.

Why would a pair of rational agents, let's name them A and B, decide to do that? If continued existence of A is more harmful for B than nonexistence of B, and vice versa, and there's no other course of actions.

As example, if A values paperclips (1 paperclip = 1 utilon) and greatly values nonexistence of pens (1 pen = -100 utilons), while B values pens (1 pen = 1 utilon) and greatly values nonexistence of paperclips (1 paperclip = -100 utilons), then productive existence of either agent is harmful for another one. If they have an equal amount of resources and neither of them can destroy another one and survive, then killing each other is a net gain for both.

I think. I'm not an expert.
:PROPERTIES:
:Author: Noumero
:Score: 5
:DateUnix: 1471284112.0
:DateShort: 2016-Aug-15
:END:

**** AUnless the actors are inherently irrational.
:PROPERTIES:
:Author: Dwood15
:Score: 1
:DateUnix: 1471316019.0
:DateShort: 2016-Aug-16
:END:


** [deleted]
:PROPERTIES:
:Score: 3
:DateUnix: 1471276200.0
:DateShort: 2016-Aug-15
:END:

*** Nope. I hold my breath instead. Haven't brought it up with other people because I suspect they'll get jealous if it doesn't work for them.
:PROPERTIES:
:Author: Chronophilia
:Score: 5
:DateUnix: 1471277087.0
:DateShort: 2016-Aug-15
:END:

**** I only rarely get hiccoughs. Whenever it came up in conversation, I used to mention that I had a trick to curing it that worked ~50% of the time and had never failed to work within 3 attempts.

That trick was to hold my breath and do some sort of calculations to distract myself. Usually I'd just enumerate the members of an arithmetic or geometric progression (e.g. 7, 14, 21, 28, ...; or 4, 16, 64, 256, ...), but really any task that requires actual thought would work.

In one of these conversations, a friend told me that I was overcomplicating it. What mattered was that you take a deep breath, and then take a further small breath (the latter should be uncomfortable, since your lungs are already mostly full).

I've only had one opportunity to test it since then, and it worked perfectly. I was actually somewhat surprised. Obviously, the sample size is poor even for an anecdote, but I'd still recommend trying that specific variant of the "hold your breath" cure, at least once.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 3
:DateUnix: 1471282591.0
:DateShort: 2016-Aug-15
:END:


**** The trick to it isn't just holding your breath, it's deliberately holding your lungs/diaphragm as still as possible. Basically, overriding the involuntary reaction by taking active control of a normally automated process.
:PROPERTIES:
:Author: Iconochasm
:Score: 3
:DateUnix: 1471299568.0
:DateShort: 2016-Aug-16
:END:


*** I used to call it "willing it". But my understanding is it's actually tensing some muscles in the back of the throat.

But yes, after a short period of concentration no hiccups.
:PROPERTIES:
:Author: traverseda
:Score: 3
:DateUnix: 1471295099.0
:DateShort: 2016-Aug-16
:END:


*** A drink is usually enough to get rid of them for me. Don't have to do anything special with it.
:PROPERTIES:
:Author: Cariyaga
:Score: 2
:DateUnix: 1471279499.0
:DateShort: 2016-Aug-15
:END:


*** I have literally never had hiccups happen to me in the last ~10 years at least. It's just never been a problem for me or come up. I had them when I was a kid, I know, but at some point they just went away and never came back. I don't know why.
:PROPERTIES:
:Author: Escapement
:Score: 1
:DateUnix: 1471289007.0
:DateShort: 2016-Aug-15
:END:


*** For me, I experience the hiccups as a sort of knot in my chest. My response is to have a drink of water and try to undo that knot. Not really a miracle cure, but that's my personal experience.
:PROPERTIES:
:Author: _Zero12_
:Score: 1
:DateUnix: 1471291798.0
:DateShort: 2016-Aug-16
:END:


*** When I was a kid, something that worked for me to eliminate hiccups was drinking water from the far side of the water glass, bending over it. Then I realized I could do the same without the glass of water or the bending over - just breathing the same way.
:PROPERTIES:
:Author: Charlie___
:Score: 1
:DateUnix: 1471458318.0
:DateShort: 2016-Aug-17
:END:


** How long do you think it will take for machine learning algorithms to take over the regular diagnosis that physicians do? Obviously it's in its infancy currently, but the core of diagnosis is matching up a bunch of symptoms (inputs) with diagnosis (outputs). The problem is the sheer volume of possible inputs and outputs. But, given the exponential nature of computing advancements, could this outcome be closer than you'd think it to be?
:PROPERTIES:
:Author: jkkmilkman
:Score: 3
:DateUnix: 1471312332.0
:DateShort: 2016-Aug-16
:END:

*** "A rash" is a symptom of many ailments, but a good doctor can look at the characteristics of your particular rash and provide a specific diagnosis. I think the hard part will be specifying symptoms---whether with language or with images---using enough precision to convey the information.
:PROPERTIES:
:Author: thecommexokid
:Score: 3
:DateUnix: 1471334617.0
:DateShort: 2016-Aug-16
:END:


*** [deleted]\\

#+begin_quote
  [[https://pastebin.com/64GuVi2F/58524][What is this?]]
#+end_quote
:PROPERTIES:
:Author: the_steroider
:Score: 2
:DateUnix: 1471342819.0
:DateShort: 2016-Aug-16
:END:


*** I think cultural change will be slower than technological change, so by the time we see complete takovers (instead of humans babysitting machines) we're already pretty close to GAI anyways, so probably around the 2045-2060.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1471317516.0
:DateShort: 2016-Aug-16
:END:

**** But aren't we alway's 30 years away from GAI? (not sarcasm)
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 3
:DateUnix: 1471317956.0
:DateShort: 2016-Aug-16
:END:

***** I read something by DataPacRat (I believe) where he showed off a table that claimed, following current trends, we'd see computer chips reaching about the computational density as the human brain in 2042, with similar numbers for hard drives. I figured that made as good of a "best case" scenario as any, so I tacked on another 17 years to my estimate to account for optimism bias and because 2060 is a nice round number. And we'd be getting GAI around that timeframe one way or another, because if all else failed we'd be simulating the human brain.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 3
:DateUnix: 1471318659.0
:DateShort: 2016-Aug-16
:END:

****** u/Chronophilia:
#+begin_quote
  if all else failed we'd be simulating the human brain.
#+end_quote

If it were that simple we'd be doing it already.

From [[http://hplusmagazine.com/2009/04/07/brain-chip/][this article]], a computer that simulates the human brain would need 3.2 petabytes of memory. To run in real-time, it would need a speed of 38 petaflops. Now, that's from h-plus magazine, which is notoriously optimistic, but even so.

The current world's fastest computer has a speed of 93 petaflops. And petabyte-sized datasets are practically routine in Big Data circles - Google's largest data centres push into the exabytes.

Yet the most complex brain we've simulated is a nematode worm's. Clearly, raw computer power isn't the only factor.
:PROPERTIES:
:Author: Chronophilia
:Score: 1
:DateUnix: 1471387919.0
:DateShort: 2016-Aug-17
:END:

******* We /have/ simulated the human brain, to an extent. Only parts of it, only at reduced speeds, and only at degraded resolutions, but it's not something that's completely unattainable. And thirty to forty years is a /lot/ of time for computers.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1471388217.0
:DateShort: 2016-Aug-17
:END:

******** Unless we manage to hurt the speed of tech research too much.
:PROPERTIES:
:Author: VivaLaPandaReddit
:Score: 1
:DateUnix: 1471458758.0
:DateShort: 2016-Aug-17
:END:


***** That's what people always say, except for the experts. A Google Brain dude on an AMA recently broke it down:

- If by "AI" you mean scifi AI, the answer is never.

- If instead you mean "any task with a differentiable/sub-differentiable loss function to act as a supervision signal in a continuous space of function approximators", which applies to tasks like object recognition and machine translation and so forth, the answer is, "Yesterday".

How long before I can phrase the designs for something fucking dangerous? Ehhhh, I can already imagine them now, but give it 8-10 years to have all the major conceptual issues worked out.
:PROPERTIES:
:Score: 2
:DateUnix: 1471385311.0
:DateShort: 2016-Aug-17
:END:
