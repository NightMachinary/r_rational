#+TITLE: [RST][C][HSF][TH] IO.SYS

* [[https://www.datapacrat.com/IO.SYS.html][[RST][C][HSF][TH] IO.SYS]]
:PROPERTIES:
:Author: DataPacRat
:Score: 44
:DateUnix: 1552233557.0
:DateShort: 2019-Mar-10
:END:

** 6300 words of ravings about AIs and rockets.

I appreciate all the feedback I can get.
:PROPERTIES:
:Author: DataPacRat
:Score: 11
:DateUnix: 1552233569.0
:DateShort: 2019-Mar-10
:END:

*** This was a stellar read. I want it to be longer; I am /not/ suggesting that it /should/ be longer, just expressing my /desire/ for it to be longer, which itself is a signal that it's a more or less perfect length.
:PROPERTIES:
:Author: gryfft
:Score: 9
:DateUnix: 1552248080.0
:DateShort: 2019-Mar-10
:END:


** This was a great read! What a punchline, wow. Wouldn't a sim-runner who sees the AI advocate for destroying Earth be pretty unlikely to let him into reality, or is the hope that they see the logic and applaud the use of the Y-S Security Protocols? Also, if there really is an AGI on Earth, why would .2 think it's still confined there after all this time? Seems to me that destroying the planet either kills all/the vast majority of biohumans, or angers a hugely more powerful AI.
:PROPERTIES:
:Author: LazarusRises
:Score: 3
:DateUnix: 1552235714.0
:DateShort: 2019-Mar-10
:END:

*** u/DataPacRat:
#+begin_quote
  This was a great read!
#+end_quote

Thank you, I'm glad you enjoyed it. :)

#+begin_quote
  Wouldn't a sim-runner who sees the AI advocate for destroying Earth be pretty unlikely to let him into reality, or is the hope that they see the logic and applaud the use of the Y-S Security Protocols?
#+end_quote

"Yes." <ahem> Non-mathematician answer: It's all about the payoff matrices.

#+begin_quote
  Also, if there really is an AGI on Earth, why would .2 think it's still confined there after all this time?
#+end_quote

Remember the weak anthropic principle: Any universe containing a superintelligence that left Earth wouldn't be a universe containing the narrator, leaving the options of an Earth lacking any such entity, or an Earth containing an inscrutibly-motivated 'stay-at-home' intelligence.
:PROPERTIES:
:Author: DataPacRat
:Score: 3
:DateUnix: 1552237059.0
:DateShort: 2019-Mar-10
:END:

**** u/GuyWithLag:
#+begin_quote
  Any universe containing a superintelligence that left Earth wouldn't be a universe containing the narrator.
#+end_quote

I'm not certain that follows; it seems to presuppose that any superintelligence that left Earth would be of the consume-the-universe kind (which is not a bad estimate given the effort needed to leave Earth and what kinds of motivations are needed to expend that effort, but it doesn't cover the behavior space).

OTOH I might be misreading this, corrections are welcome.
:PROPERTIES:
:Author: GuyWithLag
:Score: 1
:DateUnix: 1552752805.0
:DateShort: 2019-Mar-16
:END:


** ([deadpan] Yes, this has all been VR, and the simulation wetware is an author's brain. We'd normally be happy to learn from you, but resource limitations compelled us to skip some steps--you don't know anywhere near as much technological knowledge as your memories tell you you do. [/deadpan])

I'm contemplating the 'each bit' concept, and whether feasible; at the very least, I feel that the bits would have to pass a threshold of meaningful comprehension, before--for instance--incomprehensible 0s and 1s are interpretable as 'SOS' and drastically collapse the future decision tree all at once.

Or to put it another way, if 75% of your decision branches have you choosing to drink water, and 20% milk, is there going to be a timing when a single 1-versus-0 prunes away half the branches to leave a 40% milk distribution? That said, I like the information-limitation line of thought about the absolute requirements for an entity to choose between --that is, specify--one of many possible outcomes. By the time you're choosing the timing and the different reactions, though, isn't every moment a 1/0/[nothing] three-way interaction? Even if not waited for, opportune silence can also influence... but I'm overcomplicating things, easier to just think of the bits as [signal]/[silence]. But still, when you can choose your timing then all moments of silence are bits too, plus the influence/threshold needed to accomplish meaningful branch-distribution pruning, so I feel that the actual number of bits to halve something's outcome likeliness would always be far greater than 1..?

About the overall outlook, the 'Dark Forest' line of thought comes to mind--scarier in some ways in that all information transfer is suspect.

...hmm, maybe if you don't treat as involcing 'bits' any options which wouldn't end up having any behavioural changes, as though the brain's own limitations were collapsing inputs into the same output... well, there's still the butterfly effect brainwashing approach, granted... Hrm.
:PROPERTIES:
:Author: MultipartiteMind
:Score: 3
:DateUnix: 1552293022.0
:DateShort: 2019-Mar-11
:END:

*** u/DataPacRat:
#+begin_quote
  Or to put it another way, if 75% of your decision branches have you choosing to drink water, and 20% milk, is there going to be a timing when a single 1-versus-0 prunes away half the branches to leave a 40% milk distribution?
#+end_quote

I was surprised when I learned about Rowhammer attacks on RAM. And when I learned about Spectre attacks on predictive-branch CPUs. I figure that when dealing with a superintelligence, it's never the attacks you can think of that are the ones you have to worry about.

Put another way, a sufficiently advanced superintelligence could be indistinguishable from Worm's Simurgh, who could manipulate atmospheric conditions to flip a single, vital bit in a message in-transit.

#+begin_quote
  About the overall outlook, the 'Dark Forest' line of thought comes to mind--scarier in some ways in that all information transfer is suspect.
#+end_quote

I'll admit that I deliberately tried to channel Peter-Watts-style technopessimism a few times while I was writing. (I can't hold that state of mind for very long, but it's a useful exercise.)
:PROPERTIES:
:Author: DataPacRat
:Score: 2
:DateUnix: 1552400188.0
:DateShort: 2019-Mar-12
:END:


** I like how it's subtly possible that the main character is a simulation being run by the theoretical AI on Earth, who's trying to figure out what happened to the unhackable craft that was in orbit.

The setup is pretty unlikely (there's randomly an airgapped tablet left on the ship with a human level intelligence on it? That sounds like /someone's/ worst case scenario), and things keep on happening in a way that's both plausible but could also be the governing AI making sure the simulation matches observed reality - up to and including the main character's odd certainty that there's something to hide *from* in the first place.
:PROPERTIES:
:Author: IICVX
:Score: 2
:DateUnix: 1552575491.0
:DateShort: 2019-Mar-14
:END:

*** u/DataPacRat:
#+begin_quote
  I like how it's subtly possible
#+end_quote

One of the writing tricks that's stuck in my memory is to think of some aspect of the setting that underlies an important part of the story - and then never actually mention it. If you do it right, then the hints that do get left can be reconstructed by the reader to support any of several theories, most of which you won't have thought up yourself. :)
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1552579328.0
:DateShort: 2019-Mar-14
:END:


** This was enjoyable, but I really wish you hadn't namedropped Yudkowsky twice. He really doesn't deserve that much credit, especially since it can't be determined whether he's actually doing any research...
:PROPERTIES:
:Author: JohnKeel
:Score: 2
:DateUnix: 1552622289.0
:DateShort: 2019-Mar-15
:END:

*** Given that the whole protagonist spends the whole story in fear of Unfriendly AI, I had trouble thinking of any other notable name to drop alongside Schneier's, who is as noted for involvement with AI safety as Schneier is with general computer security.

For the other mention, the closest replacement quote I thought of was the classic "Listen, and understand. That terminator is out there. It can't be bargained with. It can't be reasoned with. It doesn't feel pity, or remorse, or fear. And it absolutely will not stop, ever, until you are dead."; which while certainly emphasizing an AI's relentlessness, doesn't quite match the one I used for emphasizing the AI's lack of caring.

I only have so many writing tricks that I know well enough to use; one of them is to try to link different parts of a story together by repeating some element. It's not exactly Chekov's gun (which, when introduced in the first act, has to go off by the third), but like repeating the expressed desire to wake to reality at the start and end, or having a picture of each new moon show up as it's introduced, I think that in this case using Yudkowsky's name twice helps to strengthen the story rather than weaken it, even if only on a structural level. (Of course, my thinking that may only be a post-hoc rationalization which my subconscious created to defend what little social status I might have acquired against a perceived social threat, but the only reason I have this particular insight into my own potential motivations is because I've read some blog-posts by... well, you know. :) So feel free to ignore this particular parenthetical, as it risks adding enough levels of meta to boggle any sane mind (or my own). ;) )
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1552624938.0
:DateShort: 2019-Mar-15
:END:


** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1552262369.0
:DateShort: 2019-Mar-11
:END:

*** u/DataPacRat:
#+begin_quote
  This is very good.
#+end_quote

:)

#+begin_quote
  I enjoy having the magic box of the neural net, which I assume is so that we can actually follow the narrator instead of having to view them from an outside perspective like with CelestAI. It's a clever solution.
#+end_quote

Actually, it's a cross between my not having wanted the protagonist to have started with all the software they'd need to Von Neumann up a full-scale industrial base from scratch (in which case, why bother with an em at all?), and trying to forecast what software advances might be made within a couple more decades.

#+begin_quote
  Though I do think the narrator is being paranoid about Earth.
#+end_quote

That sounds like a reasonable conclusion.

#+begin_quote
  I think the colony of clones should've
#+end_quote

There are all manner of different plans the colony could have tried. (I came up with a few alternatives, myself, before settling on the ones in the story. :) ) But a significant limit on which of those plans can be chosen are the colony's members' decision-making processes.
:PROPERTIES:
:Author: DataPacRat
:Score: 3
:DateUnix: 1552263632.0
:DateShort: 2019-Mar-11
:END:
