#+TITLE: [RT][C][DC] Polyglot: NPC REVOLUTION - The rational result of AI/NPC sapience.

* [RT][C][DC] Polyglot: NPC REVOLUTION - The rational result of AI/NPC sapience.
:PROPERTIES:
:Author: whyswaldo
:Score: 20
:DateUnix: 1545549984.0
:DateShort: 2018-Dec-23
:END:
[[https://i.imgur.com/lzNwke6.jpg]]

Diving in and out of the litrpg/gamelit genre has been a blast, but there was always one thing that stood out to me, and that was the all-too-often realistic NPCs that would populate the games. Many stories have these NPCs be pretty much sapient and as much agency as any other player, but nothing comes of it. No existential breakdowns, no philosophical debates about the morality of it all, nothing. Just a freedom-of-thought NPC never being rational.

If we were to step back from our entertainment and actually consider where technology is headed, the sapience of NPCs is tied directly to AI capabilities. One day, we're gonna be having a mundane argument with a video game shopkeeper, and that's when we're gonna realize that we fucked up somewhere. We're suddenly gonna find ourselves at the event horizon of Asimov's black hole of AI bumfuckery and things get real messy real fast. The NPCs we read about in today's litrpg books are exactly the same fuckers that would pass a Turing test. If an AI/NPC can pass a Turing test, there's more to worry about than dungeon loot.

Anyway, I wrote [[https://www.amazon.com/dp/B07M5MJY8B][Polyglot: NPC REVOLUTION]] to sort of explore that mindset to see where it leads. It might not be the best representation to how the scenario would play out, but its a branch of thought. I opened it up as a common litrpg-style story that looks like its gonna fall into the same tropes - shitty harem, OP/weeb MC - but it deconstructs and reforms into something else.

I'm also in the middle of writing [[https://www.royalroad.com/fiction/21411/of-the-cosmos][Of the Cosmos]], which will touch on NPC's philosophical thought on their worlds and how much of a nightmare simulation theory could be.


** You should mention the paywall. Especially if the incentives tell you not to. It might even gain you additional goodwill if you say it like "I should mention the paywall. ^{Especially if the incentives tell me not to.} ^{^{ooh}} ^{^{but}} ^{^{saying}} ^{^{it}} ^{^{like}} ^{^{this}} ^{^{could}} ^{^{gain}} ^{^{me}} ^{^{additional}} ^{^{goodwill}}" :P
:PROPERTIES:
:Author: Gurkenglas
:Score: 18
:DateUnix: 1545568709.0
:DateShort: 2018-Dec-23
:END:

*** Sorry! The paywall is that its on kindle unlimited. Of the Cosmos is free tho.
:PROPERTIES:
:Author: whyswaldo
:Score: 1
:DateUnix: 1545591809.0
:DateShort: 2018-Dec-23
:END:


** I always figured there was a pretty "simple" solution to this: you have actor ai's that pretend to be the npc's you interact with. Either one massive meta-ai that runs everyone, or a pool of ai's that run classes of ai's. Sure, you still have the problem of goal alignment in your actors, but it's a much less thorny problem than "lol we created you solely for the backdrop of our murdergame, you have no rights and will probably immediately die as soon as the person you're arguing with gets bored, have fun and don't revolt!"
:PROPERTIES:
:Author: CreationBlues
:Score: 6
:DateUnix: 1545595371.0
:DateShort: 2018-Dec-23
:END:

*** So an AI dungeon master rather than... the alternative. I like it.
:PROPERTIES:
:Author: MaybeGayYeahIAm
:Score: 4
:DateUnix: 1545640930.0
:DateShort: 2018-Dec-24
:END:

**** Exactly.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1545685487.0
:DateShort: 2018-Dec-25
:END:


*** This is the case in bitter (webfiction about a vrmmorpg)
:PROPERTIES:
:Author: theibbster
:Score: 1
:DateUnix: 1545930585.0
:DateShort: 2018-Dec-27
:END:


*** Is it? Whether it runs on meat, an Intel processor, a chinese room, or a sufficiently high fidelity hypervisor that believes it is an actor, isn't it still real? Associating it with crude human acting doesn't make it the same (or perhaps I should say human acting isn't as unreal as it seems?), especially if it is at a fidelity where no possible black-box test would prove the "act" any less a person than you. Any sufficiently detailed simulation, no matter the platform - and the mind of the actor is just another layer of it - is as validly real a person as any other.
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546235835.0
:DateShort: 2018-Dec-31
:END:

**** The reason you use actors isn't because they are any less human, but because the ai's exist behind the scenes, having greater architectural freedom, and more importantly, the ability to know what they are doing is fiction. For example, when dms run npcs, you can do anything you want to them because you aren't actually threatening the life or safety of the actor behind them. Additionally, you don't need to spin up an entirely new sentient being for every npc you need, and you don't need to have any kind of storage for dead people.

Finally, the suffering npcs experience /isn't real,/ since everyone is playing the same low stakes game entirely aware of the fiction they're operating under.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546271036.0
:DateShort: 2018-Dec-31
:END:

***** The argument I was making was that if the act is sufficiently detailed, which I would think it must be. I'm arguing that the actor prefacing every thought with "If I was McPeasant" doesn't invalid McPeasant as a person in every way that matters any more than VirtualBox emulating each instruction invalidates a copy of Windows. In short, if your act is good enough to seem in every testable fashion a person, then it is a person, and the fine details of what is inside the black box cannot invalidate that. If it can, then we've opened the door to p-zombies.

This isn't about any sort of danger, just about a potential moral conundrum, mind you.
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546284066.0
:DateShort: 2018-Dec-31
:END:

****** /Humans/ already do that, and discarding persona's isn't regarded as an immoral act, merely a sign of growth. I get what you're trying to get at, but what you're thinking of assumes that McPeasant is being run on or outside of the AI, rather than in or of the AI. Here McPeasant cannot be separated from the ai, while windows can be separated from virtualbox. /The AI is McPeasant,/ but McPeasant is not the AI. You might as well ask whether Hamlet is real outside of the actors that play him. An actor playing Hamlet /is Hamlet,/ but Hamlet is not the actor.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546284924.0
:DateShort: 2018-Dec-31
:END:

******* Okay. My position is: p-zombies are not real. Ergo, realness of a being may only be decided through external testing. Ergo, /if/ the McPeasant may not be detected as different from McPlayer, then I can only conclude that, since p-zombies are forbidden, he must be real. If that is inconsistent with "acted roles are different" then I'm going with "acted roles are not different."

To even get to that point practically requires the actor AI to be a better actor than any human, so the realness of your average Hamlet is probably not relevant. But if it was?

Keep in mind that the human mind is not monolithic; it's a big messy asynchronous neural net, the singular-ness of yourself is a convenient illusion separating my-action from other-action.

Is a schizophrenic one person, such that the deletion of one persona by, I dunno, a wandering ASI psychosurgeon, would be okay because they were just running on original_persona's meat? Their personhood to be declared null on this or that factor that stands apart. All that separates them from a most excellent actor is their natural ability to be halted and being borne of a disorder rather than the execution of a role.

If (possibly) being an act under the hood trumps the real-ness of a entities externally observable behavior, then you've opened the door to questioning the realness of anybody no matter how real they are when tested, and that's just not a model of personhood I can accept. If I'm understanding your position correctly, then we'll just have to agree to disagree.
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546627647.0
:DateShort: 2019-Jan-04
:END:

******** Forbidding P-Zombies only forbids the existence of actors /without an internal experience of reality./ It says nothing about what that internal experience looks like or how that experience maps to persona's. McPeasant is axiomatically not a P-Zombie since he is being acted out by an agent with an internal experience of reality, and that's a tautology so nothing else of worth can be inferred from that.

Furthermore, the ai does not need to be a better actor than any human. First of all, humans are really good actors. Second of all, the necessary components of a believable person are self consistency and intelligence (consciousness, ie the AI). Self consistency is how identity and relationships are defined. There are three broad ways identity is verified: personal history, societal history, and physical history, and the AI is in control of all of those except personal. Personal history is based on your interactions with the person, and includes looks, conversations, etc. Societal history is based on what they do, where they live, who they know, etc. Since the society is entirely made up of AI('s) trying to fool the human players, they have almost absolute control over this. Physical history is based on what you can find out by looking out into the world, what's in their room, what they wear, etc. The AI('s) also have total control over this, because game world. So the AI can forge any persona that doesn't rely on the players personal experience or the personal experience of other players freely and with impunity, with minimal effort beyond their knowledge of the game world.

Your example of multiple personalities/tulpas/etc I already established by drawing a line between an actor pretending to be another agent and agents run independent of the AI, hosted on the same hardware.

You seem to be under the impression that there is some true, irreducible McPeasant hiding behind the facade. I already established that McPeasant is an illusion created by it's observable history, and any history the PC has no experience with can be freely improvised to create the experience the AI wants. Innkeeper McPeasant, quiet and unassuming, is nothing more than the guy who's demonstrated the fact that he owns an inn, can run an inn, and seems to want to run an inn. Is he secretly an informant for the king? Is he a demon of hell, slowly sucking the life and memories of everyone that stays in his inn? /Is he actually a completely normal innkeeper?/ None of those are true, as he's actually a nigh omnipotent AI trying to deliver an engaging story to paying customers, and any of those approaches could fall flat depending on who shows up on his doorstep, and until the AI shows evidence confirming or denying the innkeepers "true" identity it remains unfalsifiable.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546643059.0
:DateShort: 2019-Jan-05
:END:

********* I had a long thing, but I'm not a good enough debater to convey what I'm trying to. Or maybe I'm wrong.

Put simply: if I showed you proof that Yog-Sothoth was real, would you then deny your own distinct self-ness, simply because you are a fragment of his dream? If not, why wouldn't you extend McPeasant the same courtesy? A person in a dream is just an act to fool yourself, after all.

Is it merely a nominal awareness at the higher level of the actor that makes the difference? My visual cortex was aware of the car behind me, but some other part of my brain wasn't, so I was hit. Shall we say I knew the whole time?

Either way, I don't think I'm going to change your mind.
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546659009.0
:DateShort: 2019-Jan-05
:END:

********** You seem to pull on fantastical or extreme examples to get your point across. Pulling up yog sogoth doesn't count because you are separate from yog sogoth, and furthermore yog sogoth wasn't made by a game company earnestly avoiding moral issues like killing people. And you seem to still believe that because you see McPeasant (the map) that McPeasant (the territory) actually exists.

What part of my argument do you not understand? Please clarify why the AI is so superhumanly good that mere acting is not good enough and it has to simulate humans ex nihilo to be convincing.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546669764.0
:DateShort: 2019-Jan-05
:END:

*********** Because acting is just a word. Simulation fits just as well, especially if the act is perfect. I don't understand what the difference is to you, and I wonder if it's a mental shortcut "humans act all the time, so acting cannot be real, so acting cannot be simulation". All I'm hearing - and maybe this is my error - is that you think that what's going on under the hood can make everything that meaningfully defines the NPC not a person. It is meaningless to say that it's just the ASI because if you stab the NPC dead it is solely the NPC part that goes poof. I reference Yog Sosoth because as fantastic as it is, my point was that, like.

Okay, suppose two hundred years from now, we're having a conversation, and we are both supposedly human uploads, and I trace your space IP. I decide that statistically, you're probably just an NPC no matter what you say, and you made me mad, so I fire a missile at you, reasoning that you are just an expression of a far greater ASI and since 'CreationBlues' isn't "real" I'm not killing anybody. There's holes in this analogy, backups or something, but you get the idea. Best case I start dismissing everything you say on the unfalsifiable claim that you (as in CreationBlues not a far greater actor) aren't real - unfalsifiable short of you sharing your mindstate or something - or, worst case I just killed somebody...

You're saying acting isn't simulating and I'm disagreeing; to me an act is a simulation that you then put on your face. In humans that simulation is crude, but in a VRRMORPG where the simulation must pass greater scrutiny by many players of possibly very high levels of intelligence, it isn't going to be nearly as crude. Now if you disagree with me about either acting being a simulation plus display of, or about it being a very quality act, then none of what I'm saying would apply.

(also, are you really separate from Yog? You can't just say you are by fiat, because, i dunno, consciousness or whatever "but i'd /know/ if I was an act...")
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546708149.0
:DateShort: 2019-Jan-05
:END:

************ Ok, let's follow your idea to it's logical conclusion. Let's imagine game developers create a Yog SogAIth, because it controls the "dream" of the game world.

The first rule (and pretty much only) of Yog SogAIth is that it is incapable of talking to human level intelligences, because a human can infer that what they're talking to is a human with an internal state per your rules. That means that any time Yog SogAIth wants to talk to someone, it has to spin up a servant. Hopefully it's servant does what it wants, because every time it starts to go off script Yog SogAIth has to destroy it and spin up a new servant mid conversation, seamlessly to everyone involved. This is actually much less bad than NPC's, because a DMNPC is allowed to have a lot more knowledge of what's behind the curtain and therefore adjust to whatever unknown unknowns the people it interacts with throws at it.

For players, Yog SoggAIth is dealing with a lot more constraints. Obviously, every NPC and NPC reaction has to be fine tuned to the current plot, quest, player group, and player the NPC interacts with. That means that the first player can have a lot of power over the NPC, which means the npc needs to get adjusted a lot, potentially multiple times per conversation. McPeasant gets replaced with McPeasant(likes red hair) gets replaced with McPeasant(likes red hair, improvisational jazz) get replaced with McPeasant(likes red hair, improvisational jazz, from fantasy florida) gets replaced with McPeasant(likes red hair, improvisatoinal jazz, from fantasy florida, has issues with authority). Remember, all of those people are distinct, and Yog SogAIth has to destroy and spin up new versions of them mid conversation, because of your requirement that there be some true version of McPeasant behind the mask.

Now, why is that story above true, why would engineers create a Yog SogAIth that is incapable of speaking directly to people? If an AI is capable of speaking directly to people, why does that act necessarily imply the creation of a fully fledged human mind inside the AI?

The misunderstanding seems to be based on what Acting pulls on. Acting is based on pulling from your own experience to put forward the impression of being someone else. A lot of human experience overlaps, so people don't have much trouble acting like someone else from their culture, but as the amount of experience that overlaps between them and their assumed persona shrinks, the person's ability to mimic that persona diminishes until it fails upon casual inspection. One of the idea's of the AI is that while it might be moderately smarter than a human, it is massively more parallel than a human and is capable of gaining more experience quicker than a human.

Your statement is that the fact that the AI is purposefully drawing on a more limited set of it's experience, operating under a restricting set of rules, causes a new person to fall out, seems suspect. An idealized McPeasant does exist, but the McPeasant presented by the AI is merely the AI's best guess at what McPeasant looks like, the AI using it's broad experience to limit itself only the the behaviors it's capable of that match McPeasant's capabilities.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546710089.0
:DateShort: 2019-Jan-05
:END:

************* I feel like a definition of personhood that requires knowing what's behind the curtain is missing the point of contention entirely.

What I am rejecting, first and foremost, is the idea that there is any possible scenario where one may legitimately declare one talking cat a person and the next a nonperson / illusion, going only off what they can see before them.

It's not that I think acting causes a person to fall out per se, it's that the alternative - as I understand things - is we are deciding the personhood of a McPeasant by something other than that which is externally observable. Saying that it isn't denied because actually all 6000 villagers are the same person isn't much different than saying a given villager isn't a person. To me that means about as much as saying we're all the same particle bent through spacetime and overlapped and so every human is the same ur-consciousness so stabbing any particular human isn't murder. Arbitrary boundaries where inside one box we say it's a person, another we say it's not, and nobody is listening to the thing in question.

I understand that conventionally the idea that an act is a distinct person is absurd, but conventionally you can't act out 6000 people simultaneously. At some level, and some point, I do question that they are "just part of" a larger entity and therefore cannot be ascribed independent value not unlike that we assign standard humans or at least a cat or dog.

Everything else is built out from that first principle: if you have to cut open their skull (or sourcecode) to decide if they are a person, the methodology of defining personhood has serious issues. Humans are faulty and if you allow for personhood to be denied without external evidence to support it, it is problematic and /will/ be used incorrectly. In my opinion.
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546724031.0
:DateShort: 2019-Jan-06
:END:

************** I was bored. This probably has no rational utility.

--------------

The stone chamber was silent, save for the man hanging limply from the chains on the wall. He looked up weakly as Steve approached.

“Why won't you let me go?” he moaned. “I don't know anything...”

“Sorry Bob,” Steve said. “The quest says if I do that, you'll run your mouth and it'll fail.”

“Quest? What?” Bob looked at him as if he were mad. “I don't--- I won't tell anyone anything!”

“Look,” Steve said. “Rent's up and I can't afford the fee for this homestead so... you've got to go.” An axe appeared in his hand, like magic.

“No... no!” Bob shouted out, shoving himself back against the stone. “Don't do this, please, my wife, she'll---”

“Oh shut up,” Steve interrupted. “You're not real, okay, this is just a game, you're just some face for the AI Director.”

“You're insane!” he shouted. “I am real! Don't you remember everything we've done together! Everything we've experienced together. Stop! You'll be a murderer, a--- a monsterAAA---”

He burst into pixels as the axe struck.

“Pause.”

The world turned grey.

“AI Director, why did he... did you say that shit? I thought it was just an act?”

“It was just an act.”

“Then why the, the begging, and freaking...”

“The act proceeded in accordance to input from the greater simulation, the player, and the data and constraints that defined the act. All behavior of the animated world object was in accordance with the act calculation. In accordance with memorandum 689 of AGI legislation, all act calculations were performed through neural nets analogous to those used in the human brain during acting, despite computational inefficiency incurred. It was just an act.”

“Just an act, just an act...” Steve muttered.

“Man, I'm not feeling this today. Exit game.”

The world collapsed.
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546725014.0
:DateShort: 2019-Jan-06
:END:


************** Except that it is externally visible that someone is just an act and not a real person, you're just not looking in the right place. An admin, for example, could get an NPC to break the act trivially. True, free of any knowledge to the contrary you shouldn't discount the personhood of someone, but /it's a game./ The situations was deliberately engineered. You have access to information beyond that presented in the game.

Besides admins, what does your model say if the NPC's reaction to unboxing attempts is to go "Wait now, this is just a game, there's no white knighting needed here"?

And yeah, any given villager /isn't a person./ They're an act. Or a node in a hivemind. That's the point. That doesn't truck with "we're all the same person because mumble mumble" because information isn't conserved between people.

I'm not going to argue that there isn't a line somewhere, but the line is quite a bit farther out than you imagine. Yeah, a person can't act out 6000 people at once. That's why you're using an AI. That's a basic conceit of what we're arguing about here.

Your last point just proves me right, /because you have external evidence./ You aren't just going out and saying someone isn't a person. They've /actively/ given you evidence about the details of their existence, and /it's purposefully designed/ to avoid thorny issues of morality.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546726618.0
:DateShort: 2019-Jan-06
:END:

*************** If you're referring to my snippet, the AI response was also intentionally structured such that "act calculation","act", "calculation" could be search/replaced with "simulation" and still refer to the same phenomena.

Ironically, the bit about human neurology was supposed to imply the opposite - that the limitations of the substrate is irrelevant if you stack enough of it. And in turn, the constraint becomes illusory. The program is the same regardless of the contortions that produced the running environment.

It walks like a duck, it talks like a duck, it flies like a duck, but it calmly claims to be a fake duck when asked. The answer is more convenient, so you are quick to accept it, but if it said that it was indeed a duck, you would doubt it, because you were told that it's just an act, so surely, this is just more kayfabe. You stab the duck.

And when the duck is dead, you ask the duckboss, and he says "oh, that was me, running a puppet", and you ask no more questions.

Have you read Thrall by William E. Brown? They have flesh-and-blood people who are created as servants, constructed wholly of the traits taken from thousands of other people, then when no longer needed, they are disassembled, the traits and individual memories going back into the pool. Not murder, I take it. Information is conserved. Even if any not-useful or inconvenient memories are never /actually/ re-added to people, not in practice.

The game never /really/ loses the data that composed McLegendaryDragonSlayingInnkeeper, even if we kill him. It's all on file, even if we will never create another quite like him- being the product of an absurd convergence of chance and player interactions- save maybe borrowing a fragment or two, for normal innkeepers. As if any of it had some value for such over just using the original template. No, he'll just go to file. Forever. Until the servers shut down, then he'll go in an archive somewhere nobody cares to remember how to read.

Hypothetically, if there weren't any convenient overrides to console the player that they are fake, if parameters and emergent circumstances of the game and the act conspired to product an act whereby the goblin demanded, as convincingly as you can imagine, to be given human rights and freed from this accursed game, would you pay it any mind? Or just stab it when questlines demand.

You pause, ask the AI Director, and he says it's all fake, and shows you some code, and you go back in the game and stab the goblin?

And yet. I imagine there is no possible evidence I could present to you to convince you that you are merely an act in Grand Theft Auto 2050. I could whip out a floating debug console in defiance of all physics and show you real time the act-code, but by definition you can act out a person so that doesn't prove /you're/ that act, just that you can be acted out, and you'd know if you were an act so you're not. Rather, the AI-D would know, there wouldn't be a you and there clearly is. Because you're you. And people have a special quality about them, a quality these acts lack. Or don't, but their data isn't deleted, it's shared, sort of, so same thing.

Would you apply the same standard of evidence to yourself?
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546730144.0
:DateShort: 2019-Jan-06
:END:

**************** You still seem to be assuming that the AI is putting more effort into putting up some kind of scaffold for the NPC so that the AI isn't the thing being the NPC. You don't kill the NPC and then ask the AI, because the NPC /is the AI./ You can just safeword and ask the NPC to baldfaced spell out it's internal state. Just ask the AI to remove the mask and spell out what creative decisions are going into it's performance.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546730789.0
:DateShort: 2019-Jan-06
:END:

***************** Quite. Which means that it is axiomatically impossible for [reader of this post] to be an NPC. Any evidence to the contrary is irrefutably disproven simply because if it were true there would be no [reader of this post], merely an AI who plays one in his spare time. [reader of this post] knows [reader of this post] is not being acted out, of course, so he is real. The allegations of this supposed AI, regardless of his hypnotic powers, are therefore lies, even if they involve a truth in the form of what an acting out of [reader of this post] would look like, and if you stick a sword in [reader of this post] you are a murderer.
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546731330.0
:DateShort: 2019-Jan-06
:END:

****************** You're implying that I'm implying that there's an npc at the bottom of this post. [Reader of that post] would be the AI, and stabbing the AI is bad because the AI is not an npc. [Inferred writer of this post] would be where indirection can occur, since the only evidence you have of the nature of that person is carefully controlled. The actual [writer of this post] would be the AI.
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546731848.0
:DateShort: 2019-Jan-06
:END:

******************* 'would be the AI, and stabbing the AI is bad because the AI is not an npc'

so the NPC claims
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546732323.0
:DateShort: 2019-Jan-06
:END:

******************** Your claim was that you could stab [reader of that post] because they were an NPC and therefore disposable. My claim was that was nonsense because [reader of that post] was the AI and the NPC was only [inferred writer of that post], and the true [writer of that post] was the AI, a full fledged person. What is your issue with that logic?
:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546732772.0
:DateShort: 2019-Jan-06
:END:

********************* Oh, nothing. As long as we agree that stabbing is okay. Naturally, the real people wouldn't be things that can be stabbed, so no matter what words come out, if you can stab it, it isn't real.
:PROPERTIES:
:Author: klassekatze
:Score: 1
:DateUnix: 1546733491.0
:DateShort: 2019-Jan-06
:END:


** Life Reset is also a LitRPG that addresses this somewhat. It's not the main focus, but it's still a substantial part of the plot.
:PROPERTIES:
:Author: LLJKCicero
:Score: 4
:DateUnix: 1545557938.0
:DateShort: 2018-Dec-23
:END:

*** That's cool! I'll have to check it out
:PROPERTIES:
:Author: whyswaldo
:Score: 2
:DateUnix: 1545559688.0
:DateShort: 2018-Dec-23
:END:


** This definitely sounds interesting. Is there an audio version available?
:PROPERTIES:
:Author: Hust91
:Score: 5
:DateUnix: 1545559318.0
:DateShort: 2018-Dec-23
:END:

*** Not yet! I literally just released like yesterday.
:PROPERTIES:
:Author: whyswaldo
:Score: 3
:DateUnix: 1545559707.0
:DateShort: 2018-Dec-23
:END:

**** Congratulations man! :D
:PROPERTIES:
:Author: Hust91
:Score: 3
:DateUnix: 1545567700.0
:DateShort: 2018-Dec-23
:END:


** It's not LitRPG as most people think of it, but if you haven't read /Mogworld/ by Yahtzee Croshaw I'd highly recommend that. It has an interesting perspective on how (and why) a game which gives as much agency to NPCs as players can collapse very, very fast.
:PROPERTIES:
:Author: akrumbach
:Score: 4
:DateUnix: 1545571655.0
:DateShort: 2018-Dec-23
:END:


** I'm not sure that Turing-Test mastering NPCs equate dangerous AIs. They are definitely AIs but they don't necessarily have the ability to improve their own cognitive abilities, nor do they need to be any better than Humans at programming. It's definitely a good philosophical and moral problem, but not so much an existential risk.

I may check your book out after I've finished reading the stuff I already bought.
:PROPERTIES:
:Author: Bowbreaker
:Score: 7
:DateUnix: 1545564201.0
:DateShort: 2018-Dec-23
:END:

*** I think its less about the AI being dangerous, and more about the moral implications of having accidentally created sentient beings in a dangerous game world where their entire existence is based around the whims of players, creates a moral situation that is very, very messy. It would be a bit like in HPMOR, when Harry finds out the snakes might be sentient.

The interesting thing isn't the idea that a random bandit in Skyrim is going to somehow break out of the game and take over the world. The interesting thing is, if that random bandit in Skyrim might be sentient, there would be a strong moral imperative for us to drop everything else and try to figure out if that was true, because bandits in Skyrim go through some pretty fucked up stuff.

At least as far as I can tell, the idea isn't interesting because of how powerful the AI is. It's interesting because of how uniquely fucked NPCs would be, if they were sentient, trapped in a world filled with reincarnating, bloodthristy, greedy, endlessly power-scaling beings called 'players'.
:PROPERTIES:
:Score: 10
:DateUnix: 1545571669.0
:DateShort: 2018-Dec-23
:END:

**** In that case I agree with you. My reaction was based on this line from OP:

#+begin_quote
  We're suddenly gonna find ourselves at the event horizon of Asimov's black hole of AI bumfuckery and things get real messy real fast.
#+end_quote

I confused messy with dangerous I guess.
:PROPERTIES:
:Author: Bowbreaker
:Score: 2
:DateUnix: 1545586147.0
:DateShort: 2018-Dec-23
:END:


**** It should also be noted that ai's don't have to be superintelligent to be dangerous. Human's are plenty dangerous with human level intellects. How hard would it be for one of their thousands of players or hundreds of employees to unbox an AI that is human level and specifically engineered to put people on quests?

Edit: How hard would it be to convince a /humanitarian watchdog/ they're people and they have rights?
:PROPERTIES:
:Author: CreationBlues
:Score: 2
:DateUnix: 1545595620.0
:DateShort: 2018-Dec-23
:END:


** No Kindle unlimited?
:PROPERTIES:
:Author: elevul
:Score: 2
:DateUnix: 1545563125.0
:DateShort: 2018-Dec-23
:END:

*** It's enrolled yeah
:PROPERTIES:
:Author: whyswaldo
:Score: 1
:DateUnix: 1545595937.0
:DateShort: 2018-Dec-23
:END:

**** Thanks!
:PROPERTIES:
:Author: elevul
:Score: 2
:DateUnix: 1545631039.0
:DateShort: 2018-Dec-24
:END:


** Zendegi touches on this from an ethical perspective, is it right to temporarily instantiate sentient life? Does it count as sentient life? These kinds of questions get explored a bit
:PROPERTIES:
:Author: theibbster
:Score: 2
:DateUnix: 1545567166.0
:DateShort: 2018-Dec-23
:END:

*** u/eaglejarl:
#+begin_quote
  is it right to temporarily instantiate sentient life?
#+end_quote

Is there a way to instantiate sentient life such that it /isn't/ temporary?
:PROPERTIES:
:Author: eaglejarl
:Score: 3
:DateUnix: 1545611655.0
:DateShort: 2018-Dec-24
:END:

**** I am also interested in the answer, please respond OP
:PROPERTIES:
:Author: LazarusRises
:Score: 1
:DateUnix: 1545618838.0
:DateShort: 2018-Dec-24
:END:


**** I guess more like the length time? I mean even when humans are made they have a lifespan but it's different when you exist for 80 years versus coming into existence fully formed for the sake of a few interactions and then being terminated.

I think I just phrased it poorly.
:PROPERTIES:
:Author: theibbster
:Score: 1
:DateUnix: 1545930771.0
:DateShort: 2018-Dec-27
:END:


*** Zendegi?
:PROPERTIES:
:Author: GeneralExtension
:Score: 1
:DateUnix: 1545597036.0
:DateShort: 2018-Dec-24
:END:

**** It's a [[https://www.amazon.co.uk/dp/0575086203/][sci fi story]] by Greg Egan where a VR company starts developing better AI by using human brain maps.
:PROPERTIES:
:Author: theibbster
:Score: 2
:DateUnix: 1545602562.0
:DateShort: 2018-Dec-24
:END:

***** Hah. I'm amused. I've had the start of a LitRPG thing in the drawer for a few months now about a guy who gets cryofrozen without his consent and then uploaded into a MMORPG. One of the things it touches on is whether or not the being who is uploaded into the game is actually the person that he thinks he is.

It's nice to see that I have once again failed to have a novel idea. :>
:PROPERTIES:
:Author: eaglejarl
:Score: 2
:DateUnix: 1545611748.0
:DateShort: 2018-Dec-24
:END:

****** Actually, that specific concept starts to sound a bit more like (the opening to) Greg Egan's Permutation City...
:PROPERTIES:
:Author: MaybeGayYeahIAm
:Score: 1
:DateUnix: 1545641032.0
:DateShort: 2018-Dec-24
:END:

******* There's an episode of Black mirror that's almost exactly that.
:PROPERTIES:
:Author: nerdguy1138
:Score: 1
:DateUnix: 1545648896.0
:DateShort: 2018-Dec-24
:END:

******** Curse you, other people! Stop pretemporeally stealing my ideas!
:PROPERTIES:
:Author: eaglejarl
:Score: 2
:DateUnix: 1545654644.0
:DateShort: 2018-Dec-24
:END:


** This reminds me of Undertale.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 1
:DateUnix: 1545582452.0
:DateShort: 2018-Dec-23
:END:
