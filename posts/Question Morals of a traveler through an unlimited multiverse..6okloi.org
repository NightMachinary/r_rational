#+TITLE: Question: Morals of a traveler through an unlimited multiverse.

* Question: Morals of a traveler through an unlimited multiverse.
:PROPERTIES:
:Author: ViceroyChobani
:Score: 18
:DateUnix: 1500598241.0
:DateShort: 2017-Jul-21
:END:
Preface: I'm not sure how to tag this, so if someone could let me know, I'll edit the post.

A short time ago I was reading an SI fic on SpaceBattles about a guy who is given an alien device that allows him to travel through the complete multiverse - everything that was ever dreamt of in man's imagination and beyond, etc. He spends his time creating an Empire of Man, pulling tech from all over the place - Star Wars, Star Trek, BioShock, etc, and sets himself up as Emperor, Warhammer 40k style.

Thing is, it's specified by the author many times that his device has access to /everything/. Every possible universe ever. All of them. And this bothered me, because I am convinced he didn't think of the implications. His Empire only ever accesses one iteration of these universes, and if he messes up contact, oh well, on to the next fiction.\\
But there should be infinite variations of each one of these universes, created when some schmuck decides to tie her left shoe first instead of her right, when some other random person scratches their nose or doesn't, when a swallow beats its wings 7 times or 8 during a landing...and every other possible variation brought about by quite literally anything.

So anyways, I have a lot of problems with is implementation, but this led me to the following moral question: In a multiverse where you can easily and quickly jump to the next universe over literally any time you want at zero cost, to what extent are the current set of human morals still relevant? In one universe, a woman crosses the street and is hit by a bus. Would a hypothetical traveler be morally obligated to perform a jump and save her in the next 'verse over?\\
When there are an infinite number of copies of people both dying and not dying, succeeding and failing, experiencing net-happiness or net-sadness, what obligation, if any, can there be on an individual able to travel between these?

Link to story, for those interested: [[https://forums.spacebattles.com/threads/cruel-to-be-kind-si-multicross.259260/]]

EDIT:

Reading replies and thinking about it more, I think this question isn't actually a unique scenario in terms of the ethics involved, and can be debated using the exact same arguments one uses for non-fictional situations as well, rendering the subject, interesting as it is, largely moot.


** A particular universe in such a multiverse would be defined by its complete history. Given that travel between universes is possible and that every possible universe exists, it follows that universes with every possible intervention (including none) at every possible moment of time already exist.

The woman is going to be saved in various universes by every possible combination of people, and pushed in front of the bus to begin with by every possible combination of people, and you will and won't decide to save her anyway.

In short, no such obligation exists unless you feel it does. Do whatever you think is in your enlightened self interest however you define that, which is really what we try to do anyway.
:PROPERTIES:
:Author: imyourfoot
:Score: 16
:DateUnix: 1500602039.0
:DateShort: 2017-Jul-21
:END:

*** If you want morality in a known infinite multiverse, you have to abandon consequentialism. Virtue Ethics and deontological systems work just fine, even better if your infinite multiverse includes libertarian free will. Consequentialism loses its intuitive appeal when all possible worlds necessarily obtain, though.
:PROPERTIES:
:Author: Oh_Hi_Mark_
:Score: 9
:DateUnix: 1500634441.0
:DateShort: 2017-Jul-21
:END:


*** Interesting idea. I guess I would ask: is it your responsibility to ensure that the universe(s) you exist in are the more moral ones?
:PROPERTIES:
:Author: ViceroyChobani
:Score: 1
:DateUnix: 1500606644.0
:DateShort: 2017-Jul-21
:END:

**** Perhaps an egoist sort of responsibility. Whatever your degree of attachment to the universe of your residence, you do live there.
:PROPERTIES:
:Author: Oh_Hi_Mark_
:Score: 3
:DateUnix: 1500634510.0
:DateShort: 2017-Jul-21
:END:


** u/MrCogmor:
#+begin_quote
  Thing is, it's specified by the author many times that his device has access to everything. Every possible universe ever. All of them. And this bothered me, because I am convinced he didn't think of the implications. His Empire only ever accesses one iteration of these universes, and if he messes up contact, oh well, on to the next fiction.
#+end_quote

No it isn't. Alex is limited to fictional worlds he knows about that had mass appeal before he got the trans-dimensional technology. Visiting a particular world also prevents him from accessing other worlds that are too similar.

#+begin_quote
  So anyways, I have a lot of problems with is implementation, but this led me to the following moral question: In a multiverse where you can easily and quickly jump to the next universe over literally any time you want at zero cost, to what extent are the current set of human morals still relevant? In one universe, a woman crosses the street and is hit by a bus. Would a hypothetical traveler be morally obligated to perform a jump and save her in the next 'verse over?
#+end_quote

How do you distinguish between a travel method that takes you to already existing worlds or generates them on demand? You can't and each have diametrically opposed moral imperatives. One wants you to travel to utopian universes and avoid bad ones, the other tells you to try and fix as many hell universes as you can.
:PROPERTIES:
:Author: MrCogmor
:Score: 3
:DateUnix: 1500601453.0
:DateShort: 2017-Jul-21
:END:

*** You are correct. I misremembered the details of the story. I suppose I would edit my question to a hypothetical scenario that fits the details I described. (I still don't think he uses his abilities efficiently, though.)

The question, though, assumes as given the principle that these worlds all exist already. It's a hypothetical, since that cannot actually be proven.\\
In that instance, is he really obligated to fix as much as he can? Any more than us creatures stuck on the "default world" setting are obligated to ease the suffering of others? Or does his superior ability imply superior responsibility?
:PROPERTIES:
:Author: ViceroyChobani
:Score: 2
:DateUnix: 1500606553.0
:DateShort: 2017-Jul-21
:END:

**** I already answered that in my previous post. If you assume that the myriad worlds exist already (and there aren't infinite worlds for every possible intervention) then both the Golden Rule and plain Utilitarianism would dictate you maximise positive intervention.

(Possible counter: You may not be required to save people from death who have a living identical replica depending on how closely they diverge. You would still be morally obligated to prevent suffering however)

Most hardcore utilitarian approach would be to get 'identity optimal' universes filled with computation running simulations of all possible Boltzmann brains with non-negative mental states and destroy all non-optimal universes.

Enlightened Self-Interest wouldn't really obligate you to anything since you would be basically untouchable.

Basic human psychology wouldn't really obligate you to anything because of the scope insensitivity bias.
:PROPERTIES:
:Author: MrCogmor
:Score: 2
:DateUnix: 1500627929.0
:DateShort: 2017-Jul-21
:END:


**** I would question the assumption that humans on the "default world" are obliged to reduce the suffering of others. I might be convinced that we are obliged to reduce the suffering of others when it would not be too inconvenient for us, but I won't accept that we need to reduce suffering as much as we can.

If I came upon a man drowning in a lake, I would save him. If, however, I came upon a few million men drowning in lakes (under circumstances where I could save them one at a time), I would stop saving them at some point, because the cumulative inconvenience to me was too large. The fact that the men gain more utility from being saved than I lose by spending the rest of my life saving them doesn't put any obligation on me to spend the rest of my life saving them.

I don't actually need to travel to alternative universes to be in such a situation; I can save a lot of Africans from malaria if I use my entire wealth to do so, and I refuse, even though each individual African wouldn't cost much to save.
:PROPERTIES:
:Author: Jiro_T
:Score: 1
:DateUnix: 1500788999.0
:DateShort: 2017-Jul-23
:END:


** u/ElizabethRobinThales:
#+begin_quote
  But there should be infinite variations of each one of these universes, created when some schmuck decides to tie her left shoe first instead of her right, when some other random person scratches their nose or doesn't, when a swallow beats its wings 7 times or 8 during a landing... and every other possible variation brought about by quite literally anything.
#+end_quote

I think you might enjoy the second half of Harry Potter and the Irrational Odyssey (specifically an arc which starts somewhere between Ch. 25 and Ch. 30 (I'm not sure because they haven't been written yet, and there are three arcs before that which might end up getting "tightened up" and losing a chapter or two each)).

EDIT:

As far as your moral dilemma, I think each "instance" of a person is separate. There's an infinite number of "me" typing these exact words, but they're all discrete entities. If the girl from The Ring crawled through my computer screen as I was L:Ksdj;lakfsjgasfklbvgaskjvg

Just kidding, universes like that are exceedingly rare, but if I /did/ get killed to death, I would take no solace in my dying moments from the thought that an exact replica of me in a nearby universe was still alive and happy. You can't jump in front of an infinite number of buses, but that doesn't mean that any pain/suffering you inflict is morally acceptable just because a copy of the suffering person is alive and well in a neighboring universe.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 3
:DateUnix: 1500600110.0
:DateShort: 2017-Jul-21
:END:

*** I agree with your point, and accept your conclusion vis a vis inflicted harm, but the question still remains - to what length does a reasonable expectation of moral interference extend to when the person in question has access to extraordinary means?

Now that I'm thinking about it more, I realize that the fantastical details of "universe hopping" and "infinite realities" aren't really relevant. The question is essentially the same for any individual in real life that has access to means for creating well-being far beyond that of an ordinary individual. Say, someone who has access to the cure for every form of cancer.

#+begin_quote
  I think you might enjoy the second half of Harry Potter and the Irrational Odyssey (specifically an arc which starts somewhere between Ch. 25 and Ch. 30 (I'm not sure because they haven't been written yet, and there are three arcs before that which might end up getting "tightened up" and losing a chapter or two each)).
#+end_quote

I look forward to that vaguely implied point in spacetime when that becomes available for viewing, then.
:PROPERTIES:
:Author: ViceroyChobani
:Score: 3
:DateUnix: 1500606357.0
:DateShort: 2017-Jul-21
:END:

**** u/ElizabethRobinThales:
#+begin_quote
  to what length does a reasonable expectation of moral interference extend to when the person in question has access to extraordinary means?
#+end_quote

That's just the thing, innit? You're looking at a screen right now. There's a universe nearby where I crawled out of that screen like the girl from The Ring and you never got to finish reading this sentence because I killed you to death. The Ring girl is my go-to, because that's exceptionally implausible and also ridiculous; I can think of many more graphic and gruesome things that could be said here, but it feels very wrong to say them. The point is that Infinity means there's a version of you travelling through the multiverse doing things you would never do. There's a "me" in a universe that /exactly/ resembles this universe who stood up and left this comment unfinished and got in "my" car and got on the nearest highway and drove for 12 hours straight then veered into oncoming traffic at high speed causing 12 fatalities and 37 serious injuries. Infinity is terrifying. But the question was "In a multiverse where *you* can easily and quickly jump to the next universe over literally any time you want at zero cost, to what extent are the current set of human morals still relevant?" If I could snap my fingers and land in Narnia, I wouldn't immediately strangle Mr. and Mrs. Beaver just because there's an infinite number of identical Mr. and Mrs. Beavers in an infinite number of identical Narnias. The current set of human morals are relevant to the extent that you yourself are a moral person.

That wasn't as polished as it could've been. Eh. Point was made.

#+begin_quote
  I look forward to that vaguely implied point in spacetime when that becomes available for viewing, then.
#+end_quote

A draft of a portion of the first chapter exists right now. The first four chapters might drop on Halloween or Thanksgiving (if I can manage to hold on to them that long), but if not then the first 10 chapters will all release on Christmas of this year. I assume the vaguely implied point in spacetime will occur some time after /next/ Christmas.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 3
:DateUnix: 1500616725.0
:DateShort: 2017-Jul-21
:END:


*** u/Oh_Hi_Mark_:
#+begin_quote
  You can't jump in front of an infinite number of buses, but that doesn't mean that any pain/suffering you inflict is morally acceptable just because a copy of the suffering person is alive and well in a neighboring universe.
#+end_quote

It does, though, as long as we're working in a consequentialist framework. Whether you choose to inflict suffering or not on a given timeline, the same amount of suffering occurs. All actions are morally neutral, since every course of action produces the same consequences.

You've really got to adopt some form of virtue ethics or deontological system if you want morality in an infinite multiverse.
:PROPERTIES:
:Author: Oh_Hi_Mark_
:Score: 1
:DateUnix: 1500634758.0
:DateShort: 2017-Jul-21
:END:

**** u/ElizabethRobinThales:
#+begin_quote
  Whether you choose to inflict suffering or not on a given timeline, the same amount of suffering occurs.
#+end_quote

I disagree. I don't share the common opinion that all the copies of you are "you." Suffering doesn't aggregate, it doesn't matter if 10,000 different individuals get a spec of dust in their eyes and experience 0.1 units of suffering each, you can't collect all of their suffering into a collection plate and offer it up as 1,000 units of suffering because suffering is an experience and no one experienced 1,000 units of suffering in that scenario.

If I were to crawl through the screen you're currently looking at like the girl from The Ring and then inflict 1,000 units of suffering upon you, the consequence of that would be that you experienced 1,000 units of suffering that you would not otherwise have experienced. If we're taking this seriously, then I /did/ crawl through your screen in a nearby universe (except that's humanizing it, I crawled through your screen in an infinite number of universes). How, exactly, are infinite copies of "me" and infinite copies of "you" relevant to the "me" and the "you" who are currently communicating with each other? They aren't.

If I stab you in the neck then I've stabbed you in the neck, and that's the thatness of it.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 2
:DateUnix: 1500921200.0
:DateShort: 2017-Jul-24
:END:

***** Exactly, and a virtuous person would strive not to stab others in the neck. Stabbing me in the neck isn't treating people as ends in themselves. Stabbing me in the neck violates ideals of justice and fairness. Stabbing me in the neck violates my natural rights. Stabbing me in the neck is against God's will, who has perfect knowledge of morality.

What stabbing me in the neck emphatically does not do is add one universe to the pile of universes where I (or someone very like me) gets stabbed in the neck. That pile is static. You can't constrain the probability of people getting stabbed in the neck by any choice you make, so you cannot therefore claim consequential responsibility for my stabbing. By stabbing me you have revealed about your character that you are a stabby sort of guy, but you did not produce the consequence of more people getting stabbed than would otherwise have gotten stabbed.

#+begin_quote
  How, exactly, are infinite copies of "me" and infinite copies of "you" relevant to the "me" and the "you" who are currently communicating with each other?
#+end_quote

Since my neck has not yet been stabbed, I assume we are talking about future permutations of this universe, in which case all stabbed and unstabbed branches of me are "me", though they may not be each other. All stabbing and unstabbing versions of you likewise.

#+begin_quote
  you can't collect all of their suffering into a collection plate and offer it up as 1,000 units of suffering because suffering is an experience and no one experienced 1,000 units of suffering in that scenario.
#+end_quote

If this is the case, then I would say that your units are improperly calibrated. If you mean that literally no amount of small harms can equal a large one, though, I would tell you that this idea is not popular in circles of moral philosophy because it is so messy as to be useless.

Where is the line between a large harm and a small one? Can large harms be added together to outweigh even larger harms? How many instances of 9000 units of suffering must be aggregated to be worth preventing by one instance of just over 9000 units of suffering? Should we purely disregard the small harms that result from our actions, no matter how far reaching or long-lasting those small harms may be?

Or is there an infinite series of harms, where the larger individual harm always outweighs any number of incrementally smaller harms? Is painlessly killing all life in the universe preferable to painfully killing one person?

More importantly, what are we meant to do with this information? The logical conclusion would seem to be that we should locate the harm of greatest magnitude, duration, and fecundity in the universe and by any means stop it, then work our way down the list. That might make some kind of intuitive sense, but it simply isn't a morality that can be practically applied to human life.
:PROPERTIES:
:Author: Oh_Hi_Mark_
:Score: 1
:DateUnix: 1500923917.0
:DateShort: 2017-Jul-24
:END:


** [[http://slatestarcodex.com/2015/03/15/answer-to-job/][Answer to Job]] has a pretty good discussion on multiverses, ethics, and suffering.
:PROPERTIES:
:Author: Predictablicious
:Score: 3
:DateUnix: 1500650441.0
:DateShort: 2017-Jul-21
:END:

*** [[#s][spoiler]]
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 3
:DateUnix: 1500651848.0
:DateShort: 2017-Jul-21
:END:


** u/Jakkubus:
#+begin_quote
  But there should be infinite variations of each one of these universes, created when some schmuck decides to tie her left shoe first instead of her right, when some other random person scratches their nose or doesn't, when a swallow beats its wings 7 times or 8 during a landing...and every other possible variation brought about by quite literally anything.
#+end_quote

Did the author of this work stated anywhere that the setting of his fanfic adheres to Everett's many-worlds interpretation? Because if not, then it doesn't have to be the case. And it's just a theory after all, not a scientifically proven fact.

#+begin_quote
  So anyways, I have a lot of problems with is implementation, but this led me to the following moral question: In a multiverse where you can easily and quickly jump to the next universe over literally any time you want at zero cost, to what extent are the current set of human morals still relevant? In one universe, a woman crosses the street and is hit by a bus. Would a hypothetical traveler be morally obligated to perform a jump and save her in the next 'verse over?
#+end_quote

Assuming that MWI is true, it would be pointless, because there will be nearly infinite amount of similar scenarios and the multiversal voyager wont be able to save all such people by himself. Also he wouldn't really save anyone, but just cause another branching of the universe into one where he saved the woman and one where he didn't.
:PROPERTIES:
:Author: Jakkubus
:Score: 2
:DateUnix: 1500812023.0
:DateShort: 2017-Jul-23
:END:


** Question: Can the MC actually choose which universe to go to? Like can he say, send me to the universe where an actually friendly AI has been created? If so, just that that AI to copy itself to some small trinket, and start distributing that AI to every single world.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1500607810.0
:DateShort: 2017-Jul-21
:END:

*** Hmm. Interesting. Based on the parameters, yes, but if so then there also exists an infinite number of evil AI's that have been let loose on the multiverse, and by extension an infinite number of multiversal battle between Friendly and Paperclip AI's...

Actually, that'd be interesting.
:PROPERTIES:
:Author: ViceroyChobani
:Score: 1
:DateUnix: 1500611108.0
:DateShort: 2017-Jul-21
:END:

**** I was tempted to say, for a moment, that a battle between a Friendly AI and a Paperclip AI would probably resolve very quickly, multiversal or not. But then I realized that if there's the slightest chance they could reach stalemate, there are an infinite number of universes where a Friendly AI and a Paperclip AI are warring against each other and have reached a stalemate.

Interestingly, there would also be an infinite number of such conflicts in which a single human can break the balance and bring victory for the Friendly AI. It's highly unlikely, but that term is less than meaningless here.
:PROPERTIES:
:Author: InfernoVulpix
:Score: 2
:DateUnix: 1500648518.0
:DateShort: 2017-Jul-21
:END:


**** u/ShiranaiWakaranai:
#+begin_quote
  infinite number of multiversal battle between Friendly and Paperclip AI's...
#+end_quote

I'm assuming the MC is the only multiverse traveller here, so he can give the friendly AIs an extra edge. For example, consider a universe where it just so happens that some basic resource X is incredibly useful, yet very scarce and cannot be produced. The MC could ferry over resource X from other universes and give it to the friendly AI in this universe, allowing it to construct more technologies that competing AIs do not have the resources to construct.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1500617948.0
:DateShort: 2017-Jul-21
:END:
