#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 20
:DateUnix: 1519657634.0
:DateShort: 2018-Feb-26
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** Do humans have any axiomatic beliefs? An axiomatic belief it one that is inherently true; you can never argue yourself out of that belief, nor be argued from it. Some things seem extremely difficult to be convinced otherwise of, like the fact I am alive (conditional on me being able to think it), but... not impossible.

If there are no axiomatic beliefs, how far could you take this? Could you change their mind on every belief simultaneously? Could you turn a person into another preexisting model, solely through sensory hacks? I'm tempted to say no, not least for physical structure-of-the-brain reasons.

This is a silly question, but it's one of those silly questions that's endured casual prodding pretty well.
:PROPERTIES:
:Author: Veedrac
:Score: 7
:DateUnix: 1519685059.0
:DateShort: 2018-Feb-27
:END:

*** u/MagicWeasel:
#+begin_quote
  Some things seem extremely difficult to be convinced otherwise of, like the fact I am alive (conditional on me being able to think it), but... not impossible.
#+end_quote

Yeah, I think when you allow for anomalous psychology, you end up with axiomatic beliefs to be impossible.

See the cotard delusion for examples of people who believe they are not alive: [[https://en.wikipedia.org/wiki/Cotard_delusion]]
:PROPERTIES:
:Author: MagicWeasel
:Score: 10
:DateUnix: 1519686988.0
:DateShort: 2018-Feb-27
:END:

**** Talking about Cotard delusion recently was actually what prompted me to post this, though I've had the question longer than I've known of the illness. One issue with the analogy is that Cotard seems to be a physical illness, more like snapping a computer in two than hacking it.
:PROPERTIES:
:Author: Veedrac
:Score: 5
:DateUnix: 1519687647.0
:DateShort: 2018-Feb-27
:END:


*** Axiomatic belief: I exist.

Not "I exist in reality", that's different. "I exist" in the sense that I am a thing. In the sense that Frodo Baggins exists, not in reality, but in a fictional story.

Without some kind of mind control, I cannot be argued out of that belief. I could be convinced that I don't exist in the real world, that I'm a fictional character of a story written by a simulated person in a virtual reality maintained by aliens who are simulated by super-intelligent robots who are being dreamed of by a mental patient in a hypothetical of a god, but at the end of the day, I still exist.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 8
:DateUnix: 1519688491.0
:DateShort: 2018-Feb-27
:END:

**** This seems like a pretty strong contender for needs-superhuman-effort, but for reasons I'm not sure I can explain concisely I'm not sold on that really being the case.

There are conceivable paths I see that lead to beliefs like "we don't have evidence for a continuum of time" and then to "everything exists only in as much as it is does from its own perspective", to which there are paths to beliefs like "everything exists to an equal extent", after which pointing to something that doesn't exist at least shakes the belief in self-existence.

I'm not saying these are /correct/ arguments, but I don't need to do so; they only need to be convincing when given by its most effective advocate, however theoretical.
:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1519694582.0
:DateShort: 2018-Feb-27
:END:

***** When I say "I exist", I don't mean I have to exist in any kind of time continuum, or make any kind of logical sense.

For example, I could say "squares that are circles". They don't exist in reality. They don't even exist theoretically, since squares are by definition, not circles. But since I have mentioned them, they are a thing now. Which means they exist, even if only in this hypothetical illogical paragraph.

So when I say "I exist", I mean it in the same sense. You could convince me that the entire world is an illusion, that there is no real world, that time and space don't exist, that there are no beings that can simulate or create hypotheticals for other things to exist in, that the basic axiom of mathematics reach a contradiction rendering the entire thing meaningless, and it still wouldn't change my belief that "I exist".
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1519695402.0
:DateShort: 2018-Feb-27
:END:

****** You seem to be arguing that you wouldn't change your belief /because it is correct/; this doesn't hold. You can be convinced of false things. I'll go out on a limb and even suggest you already have been about something.

#+begin_quote
  For example, I could say "squares that are circles". They don't exist in reality. They don't even exist theoretically, since squares are by definition, not circles.
#+end_quote

Except, you know, in Manhattan space.
:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1519724055.0
:DateShort: 2018-Feb-27
:END:

******* It isn't because it is correct, but because it is so ridiculously weak that I don't see how you could convince me that it's wrong.

Most beliefs are like towers: the belief sits at the top, and the rest of the tower are the premises and assumptions that are necessary for that belief. If you knock out the assumptions, you can topple the tower. For example, the belief that "there are no squares that are circles", relies on assumptions like squares have 4 sides and circles are round. You comment about Manhattan space is an attempt to knock out my assumption that circles are round, which would indeed topple my tower of belief that "there are no squares that are circles".

The belief that "I exist" /(in the weakest possible sense of the word)/ is like a single block. There aren't any other assumptions necessary for it as far as I can tell. That's why I was listing so many examples of assumptions you could knock out without having any effect on that belief. The existence of the world isn't part of the tower. The existence of time isn't part of the tower. The existence of other beings isn't part of the tower. You remove them from my belief space, and the single block "I exist" will still be standing there by itself.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 3
:DateUnix: 1519726130.0
:DateShort: 2018-Feb-27
:END:

******** u/Veedrac:
#+begin_quote
  It isn't because it is correct, but because it is so ridiculously weak that I don't see how you could convince me that it's wrong.
#+end_quote

This reminds me a lot of [[http://lesswrong.com/lw/up/shut_up_and_do_the_impossible/][the AI box experiment]]. First someone said "a superintelligence can't possibly convince me of X, no matter how smart it is", then Eliezer (not superintelligent) convinced him. Then an onlooker said "I know you just convinced someone who was convinced he couldn't be convinced even by a superintelligence, but I'm still convinced a superintelligence can't convince me of X", then Eliezer (still not superintelligent) did it again.

Not seeing an argument doesn't mean there isn't one.

#+begin_quote
  The belief that "I exist" (in the weakest possible sense of the word) is like a single block. There aren't any other assumptions necessary for it as far as I can tell.
#+end_quote

I've already said why I disagree with this. I can certainly imagine myself not believing I exist.
:PROPERTIES:
:Author: Veedrac
:Score: 4
:DateUnix: 1519727887.0
:DateShort: 2018-Feb-27
:END:

********* If you don't believe you exist in any sense then what is doing the disbelieving? A super intelligence can convince people of things they thought they would never believe but there are limits. It isn't going to make a convincing argument that 1+1=99 and it isn't going to be capable of convincing people that their senses don't exist barring neurological dysfunction.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1519804498.0
:DateShort: 2018-Feb-28
:END:

********** u/Veedrac:
#+begin_quote
  If you don't believe you exist in any sense then what is doing the disbelieving?
#+end_quote

I am. Reality doesn't care that I'm wrong.

#+begin_quote
  A super intelligence can convince people of things they thought they would never believe but there are limits.
#+end_quote

Yes, my point is you don't see those limits by making conservative guesses. You can't get anywhere by just restating that it can't do things, because that isn't evidence of anything. It's not even evidence that a human wouldn't convince you in a spare hour!

When you're talking about a brain a billion times faster and a trillion times larger, these limits start looking more like the physical limits on what one /can/ believe, because it is smarter than you and you can only say with confidence what it can do. There are /many/ neurologically healthy people who believe they don't exist. That's real evidence. There is at least one that believes 1+1 is not 2, so I wouldn't even rule that one out.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1519810315.0
:DateShort: 2018-Feb-28
:END:

*********** u/MrCogmor:
#+begin_quote
  There are many neurologically healthy people who believe they don't exist. That's real evidence. There is at least one that believes 1+1 is not 2, so I wouldn't even rule that one out.
#+end_quote

Who are these people and what do they mean by that they don't exist? They might believe that reality is an illusion, their mind is a perceptual theatre of ideas that doesn't actually think for itself or have complicated ideas of person hood that are expressed imperfectly (probably involving P-Zombies Edit:(Different meanings for 'I') ) but it takes mental dysfunction to believe you don't actually exist in some form. It is like a sight capable person looking out at the world and believing that he can't see. You might believe that your senses are feeding you an illusion but the sense data itself acts as incontrovertible proof that it exists.

Edit:

#+begin_quote
  There is at least one that believes 1+1 is not 2, so I wouldn't even rule that one out.
#+end_quote

Conservation of number is a skill that is learned in childhood. If an adult is incapable of it then they have stunted or impaired brain functions. [[https://en.wikipedia.org/wiki/Conservation_(psychology)]]
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1519815108.0
:DateShort: 2018-Feb-28
:END:

************ u/Veedrac:
#+begin_quote
  Who are these people and what do they mean by that they don't exist?
#+end_quote

I'll get back to you on this when I have time.

#+begin_quote
  it takes mental dysfunction to believe you don't actually exist in some form.
#+end_quote

Again, this is an assertion that isn't grounded. People believe all sorts of stupid nonsense with healthy brains; we aren't built to be SMT solvers, so it's really odd to keep modelling us as one. Logic is something we've built /on top/ of our fuzzy, pseudo-bayesian brains, not something intrinsically hardwired into them. Saying someone can't believe something /because it is false/ is not only dismissive of how many people profess to, but of the basic architecture of our minds.

#+begin_quote
  If an adult is incapable of it then they have stunted or impaired brain functions.
#+end_quote

I didn't say they were incapable of it.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1519816475.0
:DateShort: 2018-Feb-28
:END:

************* u/MrCogmor:
#+begin_quote
  I didn't say they were incapable of it.
#+end_quote

Then I fail to understand what you mean. If this person believes adding a marble into a box and then adding another marble into the box results in the box having more or less marbles than they started with then they lack conservation of number.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1519818517.0
:DateShort: 2018-Feb-28
:END:

************** I'm certain I could convince someone that some 8 digit number plus some other 8 digit number equals something it does not; this does not mean they believe that quantities appear and disappear, just that that person is confused.

Similarly, someone can believe 1 + 1 is not 2 without appreciating the implications; perhaps they simply don't believe there is a useful projection from the naturals onto reality, though they probably wouldn't have the background needed to say it that way.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1519819992.0
:DateShort: 2018-Feb-28
:END:

*************** u/MrCogmor:
#+begin_quote
  Similarly, someone can believe 1 + 1 is not 2 without appreciating the implications; perhaps they simply don't believe there is a useful projection from the naturals onto reality, though they probably wouldn't have the background needed to say it that way.
#+end_quote

In which case they don't believe that 1 + 1 is not 2. They just don't understand what they are saying and mathematical notation is a foreign language for them.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1519820908.0
:DateShort: 2018-Feb-28
:END:

**************** I don't see how that follows.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1519821123.0
:DateShort: 2018-Feb-28
:END:

***************** If a person says that 1+1=5 but still believes that if you put one thing and another thing together you have two things then they don't understand what they are saying.

Edit: To further clarify.

#+begin_quote
  They simply don't believe there is a useful projection from the naturals onto reality.
#+end_quote

This is like saying that they can believe the statement "The sky is green" because they don't believe there is a useful projection from words onto reality. The natural number system is used because it is descriptive of reality (hence 'natural'), if reality followed different rules then our standard arithmetic would be different.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1519822175.0
:DateShort: 2018-Feb-28
:END:

****************** Before we continue, could I ask you to put a probability on that claim?
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1519830619.0
:DateShort: 2018-Feb-28
:END:

******************* Which one?

#+begin_quote
  If a person says that 1+1=5 but still believes that if you put one thing and another thing together you have two things then they don't understand what they are saying.
#+end_quote

They could also be trolling or crazy but with those possibilites included I would say with extremely high probability >95%.

#+begin_quote
  if reality followed different rules then our standard arithmetic would be different.
#+end_quote

This isn't strictly true. If reality followed different rules of arithmetic then we wouldn't have standard arithmetic because we wouldn't exist. My point is that mathematics was made to model the natural world. When accountants in ancient Babylon were summing mathematical figures on a stone tablet to work out how many barrels of grain they had they weren't trying to figure out how many barrels they had in some imaginary system that had no bearing on reality, they were trying to figure out how many barrels they actually had.

There are mathematical models and formalisms of the natural numbers and arithmetic that aren't directly dependant on reality and use axioms to prove statements but the ones we generally use and refer to when we say things like 3+6=9 use axioms developed from observing reality. If you believe 1+1 does not = 2 in Peano arithmetic then you don't understand Peano arithmetic. There are formalisms that don't reflect nature but when just use normal notation without qualifications then you are implicitly referring to the normal formalisms which reflect reality.

That '1+1=2' corresponds to 'one thing and another put together results in two things' is extremely basic mathematics and I believe with extremely high probability >95% that if you can't follow that then either you misunderstand the meaning of mathematical notation or are being deliberately obtuse.

Edit: fixed a missing word
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1519853689.0
:DateShort: 2018-Mar-01
:END:

******************** My response was prior to seeing your edit, which actually answered a few things on its own.

#+begin_quote
  If reality followed different rules of arithmetic then we wouldn't have standard arithmetic because we wouldn't exist. My point is that mathematics was made to model the natural world.
#+end_quote

This is true and useful information, but it's possible for (a) this to be misunderstood, and (b) the model to not satisfy the thing it is applied to.

For the first case, I can point to a precedent among extremely smart Cambridge students (likely top percentile of global population) that both misunderstood the interaction between math and reality as you presented it here, and misunderstood some basic mathematical claims (eg. there are an infinity of reals between 1 and 2). This does not give credence to the idea that one has to be neurologically defective to be wrong about the meaning of addition.

For the second, it's easy to find places where you can't just apply the naturals; two puddles squished together does not make two puddles.

#+begin_quote
  If you believe 1+1 does not = 2 in Peano arithmetic then you don't understand Peano arithmetic.
#+end_quote

Yes, this is true, but we should distinguish this from the ability to hold opinions on the topic, and understand what the topic is. You could, after all, make the same claim about being wrong about the sum of two eight-digit numbers, but there it is clear that this is a /legitimate/ kind of incorrectness for the purpose of our argument.

#+begin_quote
  They could also be trolling or crazy but with those possibilites included I would say with extremely high probability >95%.
#+end_quote

The rest of your reply actually voids the reason I initially asked for a probability estimate, but note that what you have given is a measure of the evidence you would need to be convinced. A superintelligence would laugh at the challenge of providing evidence with the power you say is needed.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1519945213.0
:DateShort: 2018-Mar-02
:END:

********************* You still misunderstand. I'm not talking about misunderstood mathematical models. I'm talking about the fundamental life skill mentally healthy people learn between the ages of 2 and 7 regardless if they have any formal mathematical notation or understand mathematical notation.

My earlier post was meant to clarify that. A person can claim that '1+1=3' but if they know that 'one thing and one thing and another thing is two things' then they still actually believe that '1+1=2' but don't understand mathematical notation and are claiming something they don't actually believe. E.g someone can honestly claim that "The north pole is salty" if they think 'salty' means 'cold' and that doesn't mean they actually believe "The north pole is salty".

A super intelligence is not going to convince a mentally healthy and sober adult that there aren't the same number of circles on the left and right side of the line in this [[https://imgur.com/tWIQ2gP]] through logical argument. Likewise they aren't going to convince an experienced bike rider that the safest and most comfortable way to ride a standard bike is with their head upside down on the seat. A super intelligence could still convince people of these things using basilisk hacks, coercion and so on but not through logical argument.

#+begin_quote
  For the second, it's easy to find places where you can't just apply the naturals; two puddles squished together does not make two puddles.
#+end_quote

You still get more of a puddle. Adding sets is different from adding quantities.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1519955801.0
:DateShort: 2018-Mar-02
:END:

********************** I am finding this conversation frustrating at times because I don't feel it is really getting through that I am objecting to your claims.

It is not enough to convince me that superintelligences cannot convince someone of something by stating that they cannot do so, because my belief is that they normally can. I have been trying to give evidence for why I think this, giving examples of precedents, trying to prise apart where our opinions diverge, talking about the structure of the brain.

In contrast, I cannot point to anything in your most recent post which is an /argument/ rather than a /statement of opinion/. This makes it very hard to understand what I need to do to understand your point of view, which means you are probably never going to convince me and means that I am struggling to figure out how to convince you.

I understand that you think a superintelligence cannot convince you that riding upside down is safer, or that the number of circles is different on the different sides. Rather than telling me this, please try to tell my /why you believe it to be true/. That way we stand a chance of getting to the [[https://lesswrong.com/lw/o6p/double_crux_a_strategy_for_resolving_disagreement/][crux]] of the matter.

E: After [[http://lesswrong.com/lw/hu/the_third_alternative/][5 minutes]] in the shower, it occurs to me that there is a fairly simple approach a superintelligence could use to convince me that there aren't the same number of circle on each side of that diagram, and a generalization of the idea that also works for the cycling example. It might be instructive to go over this, but I'm worried that this will end up in no true Scotsman territory, rather than you updating your meta-belief about people's ability to be convinced. I especially don't want the limits of my ability to [[https://www.lesserwrong.com/posts/aYX6s8SYuTNaM2jh3/idea-inoculation-inferential-distance][inoculate you with regards to the abilities of the superinteligent]] ([[https://en.wikipedia.org/wiki/Inoculation_theory][see also]]).
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1519986413.0
:DateShort: 2018-Mar-02
:END:

*********************** u/MrCogmor:
#+begin_quote
  In contrast, I cannot point to anything in your most recent post which is an argument rather than a statement of opinion. This makes it very hard to understand what I need to do to understand your point of view, which means you are probably never going to convince me and means that I am struggling to figure out how to convince you.
#+end_quote

Okay to convince someone of a false conclusion through logical argument you need to get someone to accept a false premise that is not obviously contradictory to their experience. For example if you are carrot farmer has lived his life out in the sun you are not going to convince him through just words and logical argument that it is and has always been impossible to grow carrots in soil because it is so obviously inconsistent with prior evidence. To do so you would first have to make a complex explanation for why the farmer's memories are incorrect and get the farmer to believe your explanation is more likely than this 'This wacko is lying to me'. The more a lie diverges from a person's understanding of reality (and the prior evidence they have already received) the more credible evidence is needed to support the lie. People assign the words of their conversation partners a very limited amount of credibility, an amount that quickly runs out when they start stating absurdities.

To convince someone that they can't count and have never been able to count requires the person to the trust the computer more than they trust themselves at which point the computer has already won. (A A.I could stick into you a simulation and use gaslighting techniques to convince you that you can't count or work as a perfect ruler for centuries to attain a massive reputation for never making a mistake before recommending that people ride their bikes upside down but that is outside of the scope here)

#+begin_quote
  E: After 5 minutes in the shower, it occurs to me that there is a fairly simple approach a superintelligence could use to convince me that there aren't the same number of circle on each side of that diagram, and a generalization of the idea that also works for the cycling example. It might be instructive to go over this, but I'm worried that this will end up in no true Scotsman territory, rather than you updating your meta-belief about people's ability to be convinced. I especially don't want the limits of my ability to inoculate you with regards to the abilities of the superinteligent (see also).
#+end_quote

I'm extremely doubtful that you have a convincing logical argument that two circles are not two circles or so on considering that you don't currently believe that two circles are not two circles. I think trying to come up with a super intelligent false argument that way is a doomed enterprise.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1519999377.0
:DateShort: 2018-Mar-02
:END:

************************ Thanks, this response is exactly what I was hoping for. I don't have time for a detailed reply, but one thing stood out.

#+begin_quote
  I'm extremely doubtful that you have a convincing logical argument that two circles are not two circles or so on considering that you don't currently believe that two circles are not two circles. I think trying to come up with a super intelligent false argument that way is a doomed enterprise.
#+end_quote

It seems to me that this argument [[https://en.m.wikipedia.org/wiki/Proving_too_much][proves too much]]; it would equally predict Eliezer's failure in the AI box experiment.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1520001420.0
:DateShort: 2018-Mar-02
:END:

************************* Non-Mobile link: [[https://en.wikipedia.org/wiki/Proving_too_much]]

--------------

^{HelperBot} ^{v1.1} ^{[[/r/HelperBot_]]} ^{I} ^{am} ^{a} ^{bot.} ^{Please} ^{message} ^{[[/u/swim1929]]} ^{with} ^{any} ^{feedback} ^{and/or} ^{hate.} ^{Counter:} ^{155219}
:PROPERTIES:
:Author: HelperBot_
:Score: 1
:DateUnix: 1520001424.0
:DateShort: 2018-Mar-02
:END:


************************* No it is saying that the A.I box experiment is not a accurate simulation of an super-intelligence because it is involves two humans. Elizier has hidden what actually went on in the experiment because he believes the results would be disputed and they would be. Humans cannot create a false argument that is irrefutable to humans because the person making the false argument is human and not convinced by their own argument. If he actually actually released the information there would be hordes of people pointing out the stupid mistakes on the part of his opponent. I doubt he used purely rational argument (see [[http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/][here]]) and convincing a gatekeeper to let you out of a box is not the problem we are discussing. Emotional manipulation can get you to take an action on impulse but it generally takes time or a receptive subject to change longstanding beliefs and even when it works you can get people that 'Believe in belief' without actually believing. You might be able to convince people that 1+1 is not 2 with a whole 1984esque apparatus but not through just rhetoric.

Edit: expanded on last sentence.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1520003636.0
:DateShort: 2018-Mar-02
:END:


************************* To be more specific. The A.I box experiment doesn't prove or disprove that a super intelligent actor can convince anybody of anything. At best it proves that some people can manipulate some other people into typing "I let you out" into a chatbox.

Edit: fixed typo
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1520046638.0
:DateShort: 2018-Mar-03
:END:


**** There are idealists who would extend their doctrine of non-continuity/ non-existence of the physical realm to the observer, leading them to doubt the notion of self, that represented by "I".

Idealism is weird.
:PROPERTIES:
:Author: Roneitis
:Score: 3
:DateUnix: 1519730804.0
:DateShort: 2018-Feb-27
:END:


**** Excuse me while I go double check the literature on self-modeling and figure out precisely what I'll have to knock out in your nervous system to lesion out that belief.
:PROPERTIES:
:Score: 3
:DateUnix: 1519782293.0
:DateShort: 2018-Feb-28
:END:

***** Hey, you're supposed to convince, not mind control X_x.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1519794315.0
:DateShort: 2018-Feb-28
:END:

****** I didn't say anything about /necessarily/ having to physically alter or hack your nervous system, though it's extremely likely that /would/ be necessary, and thus that "arguing away" your belief in your own existence /should/ be impossible.

But I'm not /sure/. If the Rubber Hand Illusion doesn't require surgery, I find it hard to be completely certain that more extensive illusions of selfhood or nonselfhood /don't/ require surgery.
:PROPERTIES:
:Score: 1
:DateUnix: 1519827031.0
:DateShort: 2018-Feb-28
:END:

******* Are you sure? I remember something about using post hypnotic suggestions to temporarily remove ideas about the past, present and future
:PROPERTIES:
:Author: kingofthenerdz3
:Score: 1
:DateUnix: 1519889161.0
:DateShort: 2018-Mar-01
:END:


****** And if you find all that incredibly disturbing, well, I assure you it runs on the most elegant probabilistic and information-theoretic principles, and while it undermines many of the philosophical intuitions people typically hold, it has better mathematical and scientific support than those intuitions ever did.
:PROPERTIES:
:Score: 1
:DateUnix: 1519827097.0
:DateShort: 2018-Feb-28
:END:


**** I can see how that could get bogged down to arguing over definitions of “I” and “to exist”.
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 2
:DateUnix: 1519779969.0
:DateShort: 2018-Feb-28
:END:


**** See: the Buddhist principle of no-self. The mind is an illusion, the brain is made of atoms, there are no ghosts in the machine, and it's possible to understand this on a gut level given enough effort. I'd recommend Mastering the Core Teachings of the Buddha for an expert low-woo explanation.

As a point of actual fact, though, "I" don't exist, and neither do "you". Deterministic events are happening in the universe, and it's computationally convenient to pretend that some of them have identities. No one is a thing, especially not Frodo Baggins.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1519872887.0
:DateShort: 2018-Mar-01
:END:

***** I'm so tired of re-iterating this point: I mean existence in the weakest possible meaning of the term. Every one of these posts saying X doesn't exist is clearly using a different definition of exist than the one I'm using, and I'm not sure how to explain what I mean any further. I literally said in the first post, that I don't mean exists /in reality/. So telling me that nothing exists in the universe illustrates that you completely missed the point. Under the weakest definition of existence that I'm using here, you can't say X doesn't exist, because simply saying that means that X now exists in your statement. That is how weak the definition of exist I'm using here is.

It doesn't matter if everything is an illusion. They are still things. Illusory things. Paradoxical things. Nonsensical things. Hypothetical things. Unreal things. Contradictory things. All. Still. Things.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1519874701.0
:DateShort: 2018-Mar-01
:END:

****** Those "things" are computational conveniences, which means that you're using a personal definition of "exists." If a real thing doesn't exist more than an illusion does, then the state of existing or not existing conveys no information, which means that the claim "I exist" isn't really any more true than it is false.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1519886395.0
:DateShort: 2018-Mar-01
:END:


****** Since I suspect there is confusion, I want to make it clear that I believe I understand what you mean when you say that you exist, I agree that it is true, and I agree that your reasoning is correct. What I disagree on is whether this is a belief we can be argued out of.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1519945977.0
:DateShort: 2018-Mar-02
:END:


*** Yes; at least some humans have at least some beliefs which are true by definition. I believe there is no such thing as a married bachelor, since bachelor implies unmarried, by definition of bachelor. Thus, I poses an axiomatic belief that is not subject to change.
:PROPERTIES:
:Author: 1337_w0n
:Score: 1
:DateUnix: 1519686763.0
:DateShort: 2018-Feb-27
:END:

**** And you think no argument would change your mind? I'm not restricting this to standard arguments and standard efforts.
:PROPERTIES:
:Author: Veedrac
:Score: 5
:DateUnix: 1519686925.0
:DateShort: 2018-Feb-27
:END:

***** This is interesting. There exist certain arguments, such as appeal to violence, which are not logically valid that will cause me to state that my belief has changed.

However, there exist no arguments, be they sound, cogent, or otherwise, which would cause me to be less convinced that there do not exist married bachelors.

Do you think some argument could convince you that there exist married bachelors?
:PROPERTIES:
:Author: 1337_w0n
:Score: 1
:DateUnix: 1519689600.0
:DateShort: 2018-Feb-27
:END:

****** u/ulyssessword:
#+begin_quote
  I believe there is no such thing as a married bachelor, since bachelor implies unmarried, by definition of bachelor.
#+end_quote

"Married" is a legal state, while "bachelor" is a social one. A hypothetical friend of mine is in the last stages of his (long, drawn out) divorce while he's taking the first steps towards finding a new girlfriend.

He's a married bachelor.
:PROPERTIES:
:Author: ulyssessword
:Score: 6
:DateUnix: 1519691962.0
:DateShort: 2018-Feb-27
:END:

******* That's certainly the same series of phonemes, but conceptually, it's not the same.

I was using the definition of "unmarried male of marital age". The definition you used had (Hypothetical) cases such that they do not count as a bachelor as I define it, despite the fact that both of our definitions were fair representations of the common concept of what makes a bachelor.

Therefore your argument to convince me relies on an equivocation fallacy, and so I find it unconvincing.

It was a good attempt, though.
:PROPERTIES:
:Author: 1337_w0n
:Score: 5
:DateUnix: 1519693274.0
:DateShort: 2018-Feb-27
:END:


****** It seems very likely to me, yes, though I don't know what that argument is else I would believe it. I think this might even be in the realm of what a very prepared, very smart person could do.

Certainly I have made mistakes about (obvious) logical truths in the past, flipped flopped on issues I thought myself certain of, and those terms are sufficiently vague and steeped in law that it doesn't seem even particularly hard to trick me somewhere.

When you get to more fundamental beliefs like Modus Ponens, it's more likely that extraordinary, potentially superhuman, effort comes into discussion.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1519693318.0
:DateShort: 2018-Feb-27
:END:

******* Alright, let me reduce this to base logic, then.

Let q(x)="X is both male and of marital age." Let M(x)="X is married."

BACHELOR(x)=q(x) ^ ~M(x) (by definition)

So, a married Bachelor would be:

BACHELOR(x) ^ M(x)=q(x) ^ [M(x) ^ ~M(x)]

Through logical simplification, we find that BACHELOR(x) ^ M(x) implies [M(x) ^ ~M(x)].

We know that for all p, p ^ ~p=F. So,

BACHELOR(x) ^ M(x) implies F.

Modus tolens, BACHELOR(x) ^ M(x)=F for all X.

Therefore, there does not exist a married Bachelor.

Therefore, any argument to the contrary is flawed.
:PROPERTIES:
:Author: 1337_w0n
:Score: 1
:DateUnix: 1519697878.0
:DateShort: 2018-Feb-27
:END:

******** I don't think you're engaging with this question in (what I would consider to be) the right mindset. I certainly agree that logic is injective onto reality, and I'll even take your definition of BACHELOR(x), and I certainly agree with your conclusion, but these are not beliefs that I was born with, they are not beliefs that no amount of forgeable evidence could dissuade me of.

It would be hard, /very/ hard, to show me enough seeming counterexamples of the map between FOL and reality that I don't allow its usage as you did, but I can certainly imagine there being some argument that convinces me to discard non-Bayesian arguments, and I've seen enough stupid arguments from philosophers to know that getting muddled up in this respect is something that /does/ regularly happen.

It would be less hard to convince me that BACHELOR(x) is not, in fact, by definition, something I expect I would be a lot less surprised about than, say, the sun not rising tomorrow (a fact I can certainly be convinced of).
:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1519724861.0
:DateShort: 2018-Feb-27
:END:

********* Are we working under a definition of axiomatic beliefs in a global sense or an individual sense? Also, why would one need to be born with this belief?

If we are working on the definition of axiomatic belief that requires all persons to share this belief and for it to be unshakable, then I am entirely unconvinced that such beliefs exist.

If we're using the definition that I thought we were using, then I as an example have many specific beliefs that derive from axiomatic logic and definitions that I cannot be convinced away from.
:PROPERTIES:
:Author: 1337_w0n
:Score: 1
:DateUnix: 1519832175.0
:DateShort: 2018-Feb-28
:END:

********** An individual sense.

#+begin_quote
  If we're using the definition that I thought we were using, then I as an example have many specific beliefs that derive from axiomatic logic and definitions that I cannot be convinced away from.
#+end_quote

Why do you believe this? Not why are they /true/, but why you believe that your belief is unshakeable.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1519833877.0
:DateShort: 2018-Feb-28
:END:

*********** Because logic is the way I make sense of things. I have a profound trust in how logic works.
:PROPERTIES:
:Author: 1337_w0n
:Score: 1
:DateUnix: 1520042138.0
:DateShort: 2018-Mar-03
:END:


******** I believe there is a way to convince you otherwise, but it requires a mind far smarter than I, and our assumptions of nearly everything to be horribly horribly wrong.

What this would take would be an elegant thoroughly checked proof, showing that from basic logical axioms, we can derive a contradiction. Logically then, either everything follows, or one of the basic logical axioms is wrong. And if the basic logical axioms that we base our logic on are wrong, then any of our beliefs that rely on logical arguments would be weakened.

Now, you might think, that this is impossible. That there's no way we could be mistaken in our logical thoughts. That this particular event will never happen, so you can never be argued out of your belief. But to that I point out the [[https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect][Dunning-Kruger effect]]: a well known phenomenon where people who are more ignorant think that they know more instead, simply because they are so ignorant that they do know not how to correctly assess their own ignorance. Is it not then possible, that the entire human species is actually incredibly stupid about logic, so stupid that we can't even tell that we are stupid?
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1519727254.0
:DateShort: 2018-Feb-27
:END:

********* Yes, demonstrating a contradiction arising from axiomatic logic would necessarily be step 1. However, once this is done you would need to establish a new system for deriving statements from premises and convince me that it's at minimum workable.

However, given how good logic is at producing results, it is unlikely that there is some contradiction that results from the emergent properties of axiomatic logic.
:PROPERTIES:
:Author: 1337_w0n
:Score: 1
:DateUnix: 1519739942.0
:DateShort: 2018-Feb-27
:END:


**** A possible counter-argument example: You are now suddenly in a country in which there are only two judges who have the authority to solve cases regarding marriage problems. Their verdicts are always final, and even they themselves can't change them once they are declared.

In this country bachelor Bob has signed a dubious marriage contract with Alice, and now Bob says that this contract is invalid while Alice says it is valid. Bob takes his copy of the contract to Judge A, while Alice takes hers to judge B. Judge A rules out that the contract is invalid, while judge B rules that it is valid. Thus, Bob becomes trapped in a sort of legal purgatory --- he is both an unmarried bachelor able to commence with his first proper marriage with whomever he likes, /and/ a person married to Alice who would get jailed for polygamy if he tried marrying someone else as well. He is a married bachelor.
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 1
:DateUnix: 1519828689.0
:DateShort: 2018-Feb-28
:END:

***** This is not bad. It is true that if Married(x) isn't well-defined, that is to say the some entry has output T and F, then the proof fails. However, prepositional logic in general fails for these cases, which is why there's an axiom to prevent that (in english):

An open statement with a decided variable is always a statement. (Part of the definition of open statements).

All statement are true or false, and no statement is simultaneously true and false. (Definition of a statement).

Now I admit that I did not consider an exact definition of marriage, but I am still convinced that there exist no x such that x is both married and unmarried.
:PROPERTIES:
:Author: 1337_w0n
:Score: 1
:DateUnix: 1519831609.0
:DateShort: 2018-Feb-28
:END:


*** I think belief in the existence of free will is one of them. I don't think it's possible for a human being to function psychologically if they do not believe they possess some degree of autonomy that is intrinsically separate from external influence.

Even philosophies like Buddhism that believe the “self” is an illusion still believe that humans have the ability to choose to disassociate from the self to become free of attachments that hold a person back from reaching a better state of existence.

It's one thing to believe in fatalism or nihilism where your life doesn't matter, but to believe that you have no control over your existence at all is schizophrenic. If you don't think that you can think, then you would either continue thinking or cease to be capable of living as an organism with a brain.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1519693430.0
:DateShort: 2018-Feb-27
:END:

**** This strikes me as way too easy, and you're vastly underestimating the size and scope of arguments out there. Have you ever changed your mind on free will? If so, was it more surprising than learning the sun wouldn't rise tomorrow would be?
:PROPERTIES:
:Author: Veedrac
:Score: 4
:DateUnix: 1519693704.0
:DateShort: 2018-Feb-27
:END:

***** I have never changed my mind on it because I literally cannot conceive of myself existing as a conscious entity without free will, despite knowing everything I do about implicit bias, cultural pressures, and psychological disorders.

I was also born with autism and have developed anxiety and depression, so it's kind of essential to my mental health that I believe there is a “higher me” capable of controlling the rest of myself. Otherwise, I'd rationalize my self destructive thoughts even more.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1519703689.0
:DateShort: 2018-Feb-27
:END:

****** u/Veedrac:
#+begin_quote
  I was also born with autism and have developed anxiety and depression, so it's kind of essential to my mental health that I believe there is a “higher me” capable of controlling the rest of myself.
#+end_quote

This kind of justification is something that you can almost certainly be convinced otherwise of, and it's the kind of thing that suggests to me your opinions here are less rigorously based than you think.

We're talking about the kind of adversary who, on hearing that, would immediately start planning your next 10 years of (non-contact) mental health treatment, just in order to, in the end, change your mind on free will.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1519725419.0
:DateShort: 2018-Feb-27
:END:

******* I think changing my mind on free will would utterly destroy me if it was even possible. What reason would I have to live if I think I have no control over myself and neither does anyone else? It would mean convincing me that consciousness is just an illusion that perpetuates itself, so giving value to human life means accepting a falsehood.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1519736195.0
:DateShort: 2018-Feb-27
:END:

******** There are lots of people who don't believe in free will who get by just as well as those who do. This response sounds very similar to Christians who say they would murder if not for their faith; in practice many people convert without turning psychopathic. It makes sense that they would believe that about themselves, but it's rather unlikely to actually be true.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1519736338.0
:DateShort: 2018-Feb-27
:END:


**** Err, not true. There's plenty of people who don't believe in free will, the theory even has a name: [[https://en.wikipedia.org/wiki/Determinism][Determinism]]. One can be convinced to believe that everything in the universe is made out of uncaring asentient particles moving according to static rules, and that free will is merely an illusion from highly complex interactions between countless particles.

It doesn't even have to be sciency, it can be a religious belief in something like fate. Plenty of people believe in fate, and believe it is unchangeable. If fate is unchangeable, then free will is clearly a lie, since you are already fated to will whatever you would will, with no freedom to do otherwise.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1519693864.0
:DateShort: 2018-Feb-27
:END:

***** Determinism is /not/ just the absence of free will, as shown by the existence of compatibilist philosophies.
:PROPERTIES:
:Author: 3combined
:Score: 2
:DateUnix: 1519712198.0
:DateShort: 2018-Feb-27
:END:

****** Huh, I was not aware of such philosophies. But still, the very fact that they had to call it "compatibilist philosophies" indicates that plenty of people do not think that free will and determinism are compatible, which means that people can be argued out of believing in free will.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1519713011.0
:DateShort: 2018-Feb-27
:END:


***** I've never seen people discuss determinism in the context of how they live and act, only as an interpretation of reality beyond themselves. The possibility that my decisions are preordained does not concern me since I still view my actions from the perspective of a person making a choice without knowledge of my destiny.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1519704002.0
:DateShort: 2018-Feb-27
:END:

****** Ah but that presents an avenue for attacking your belief doesn't it?

Imagine an omniscient being came to you and told you about your entire destiny in extreme precision. Would you still believe in free will then? When you know your destiny, and see all your actions match exactly what you now know they were destined to be all along?

This isn't a likely event of course, but if it does convince you that free will isn't real, then your belief in free will isn't an axiomatic belief.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1519726514.0
:DateShort: 2018-Feb-27
:END:

******* Well, from my perspective, the choices I am told I will make would still be choices I feel like I am making at the time that I make them. Even if I was told that I would make them, that wouldn't make my decisions or anyone else's less real.

Non-linear experiences and knowledge of the future do not undercut my belief in free will, it just means events can cause themselves to occur. What happens just happens because that's the way it happened based on decisions made.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1519736715.0
:DateShort: 2018-Feb-27
:END:


**** u/MrCogmor:
#+begin_quote
  I think belief in the existence of free will is one of them. I don't think it's possible for a human being to function psychologically if they do not believe they possess some degree of autonomy that is intrinsically separate from external influence.
#+end_quote

Free will is an incoherent concept. You make choices on the basis on external circumstances, who you are and possibly some random element inherent in the process. Who you are is the result of external circumstances and possibly some random elements that lead to your birth, upbringing and prior experience. Everything ultimately arises from external circumstances and possibly some randomness inherent in the universe.

What is free will? Where is the autonomy in making decisions in ways the universe has shaped you to make them? Where is the autonomy in making decisions on the basis of random quantum fluctuations? You can choose, not because you are intrinsically separate from the universe but because you are part of the universe and the universe decides everything.
:PROPERTIES:
:Author: MrCogmor
:Score: 0
:DateUnix: 1519805613.0
:DateShort: 2018-Feb-28
:END:


*** - (0) Things, the counter-arguments to which would instantly be proven false the moment I tried considering the accuracy of those counter-arguments that the world has presented to me:

  - “I believe I can believe.”;
  - “I believe I can change my beliefs.”; (?)
  - “I believe in my ability to think.”;
  - “I believe in my ability to understand what a belief is.”; “... what believing is.”; etc;
  - “I believe currently the flow of time (laws of physics, etc) around me is such that it makes it possible for my mind to continue being functional.”;
  - “I believe at least some sort of consciousness exists inside of what I am used to think of as my mind.”

- tautological statements:

  - 1) I believe statement X is True OR False OR Invalid. Example statement X: the pen is blue. Normal world: the pen continues to be blue (statement True). Stress-test world: the pen suddenly turns out to be red for whatever reason (statement False). Stress-test world: turns out there is no pen at all, there's no me, there's no colour blue, etc (statement Invalid). In all possible cases, however, the higher-level statement still continues to hold true.
  - 2) Building (or acquainting oneself with) a logical system and then believing in a property of the said logical system. E.g. with binary counting system taken as the logical system, and the statement 1+1=10 taken as the belief, I think there'd be no way to convince me that this statement will not hold true inside that system. And even if I did somehow get convinced that there is some way for 1+1=10 to hold false inside binary counting system, I can create an even more minimalistic logical system which only states that inside this system, 1+1 is equal to 10, and then I can say that I have absolute belief that inside this system 1+1 will always be equal to 10. I think [[/u/1337_w0n]]'s example was an imperfect example of this.

- “I believe at least some of my beliefs are not axiomatic beliefs.”
- “I believe belief X can't be an axiomatic belief.”
- “I believe there is a chance --- however small --- for X to be true.”; “... for X to be false.”
- “I believe at least something exists.”
- “I believe at least something is possible.”

Possible candidates:

- “On the relatively same intensity scales, last time I checked pain was more difficult to tolerate than pleasure.”;
- Find such a state of being X that 1) it would be impossible for my current self to turn into that state, no matter how many incremental changes happened between now and that final state (to deny the Sorites paradox) and 2) I define my “I” of the current state in such a way that it would be incompatible with being state X. In other words, if it turned out that I was in state X, my image of I would collapse instead. Then: “I believe I am not in state X.”;

  - possible examples: “I believe I am not omnipotent.”, “I believe I can not comprehend the world in its entirety.”

--------------

I'd also like to point out a certain difference. Compare: (1) “I believe that I axiomatically believe in X” and (2) “I axiomatically believe that I axiomatically believe in X.”

Since we are talking about beliefs being changed through /arguments/, the changes happening to the world should stay limited to the domain of the axiomatical belief that's part of the statement. That is, if I said (3) “I believe that I axiomatically believe in X” and the world suddenly changed in such a way that I developed a very specific kind of brain tumour that made me stop believing in X, that wouldn't count as a failed stress-test against statement #3, because mind-controlling me into shifting my belief is not the same as arguing me into changing it.

This is not to say, however, that mind-controlling me would never be a part of a stress-test world. For instance, if my statement was (4) “I believe that I have memories of X”, then since my mind itself becomes the domain of the axiomatic belief, for the world to mind-control me into forgetting that memory would indeed become a valid counter argument against statement #4.

This is why I think axiomatic beliefs would by their nature mostly be limited to relatively pure, abstract logical statements, or to such properties of the believer's mind that they don't additionally make the believer's mind itself part of the domain of the axiomatic belief (with the #0 group of bullet-point statements being exceptions to this due to the “paradox immunity”, so to speak).

p.s. This could be a fun game to play at parties once or twice!
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 1
:DateUnix: 1519828978.0
:DateShort: 2018-Feb-28
:END:


** So I'm taking a class mixing psychologists, electrical engineers, computer scientists and neuroscientists. We're supposed to be building a /lingua franca/ amongst each other, to conduct interdisciplinary work...

It's depressing how much of what we're really doing amounts to very basic "rationalist-type", "read the Sequences lol" stuff. One of today's engineering lessons was that the map is not the territory. Actually, that's a big lesson from the whole class, since the entire history of cognitive psychology and neuroscience often looks like one long string of mind-projection fallacies.

Such is life.
:PROPERTIES:
:Score: 5
:DateUnix: 1519782493.0
:DateShort: 2018-Feb-28
:END:


** (This comment was thoroughly edited approximately six hours after its original posting. The original version can be read [[https://toakraka.nfshost.com/000021.html][here]].)

--------------

[[http://np.reddit.com/r/rational/comments/7xzb1r/d_friday_offtopic_thread/duge4g0/?context=1][An entertaining argument]] recently reminded me that the proper matching of payments to goods and services can be impossible. For example:

I probably would pay fifteen or twenty dollars to [[https://www.fanfiction.net/u/1960462][ShaperV]] to reward him for writing [[https://www.fanfiction.net/s/5193644][/Time Braid/]] and to encourage him to finish [[https://www.fanfiction.net/s/5207262][/Indomitable/]]. However, copyright laws forbid me from doing so (or, at least, forbid ShaperV from accepting such money). Instead, if I want to buy anything from ShaperV, it must be one of [[https://www.goodreads.com/author/show/8314513][his original works]]. However, I don't find his original works to be worth rewarding or encouraging (based on several chapters of [[https://www.fictionpress.com/s/3198066][/Fimbulwinter/]] and several summaries of his other works, at which I glanced years ago). I therefore find myself in a dilemma: I must, either buy ShaperV's original work and run the risk that he'll be encouraged to keep writing books that I don't like, or not buy it and run the risk of his being discouraged from ALL writing.

Likewise, shortly after the completion of [[https://www.fanfiction.net/s/5782108][/Harry Potter and the Methods of Rationality/]] (for which I probably would pay ten dollars if I could), the organization that employed Prophet Yudkowsky (pbuh) saw fit to publish (on a "pay what you want" basis) another, nonfiction work of his, [[https://intelligence.org/rationality-ai-zombies/][/Rationality: From AI to Zombies/]]. I was forced to confront a similar problem: Should I pay an extra sum of ten dollars (above the five-dollar suggested price, which I found reasonable for the nonfiction book on its own merits) and risk sending the wrong message, or should I refrain from paying that premium and risk damaging the author's future willingness/ability to entertain me? I eventually chose a middle course of paying only a two-dollar premium. (Alternatively, did I actually consider /From AI to Zombies/ valueless and intend the whole seven dollars for /HPMoR/? At this late date, I am unable to remember.)

A third example is FilthyRobot. After watching hundreds of this Twitch streamer's videos [[https://www.youtube.com/user/FilthyRobotChannel/playlists][on YouTube]], I subscribed to [[https://www.patreon.com/FilthyRobot][his Patreon]] for five dollars per month. However, he produces /both/ videos that I /watch/ (/e. g./, of [[https://www.youtube.com/playlist?list=PLQFX9B_9L4-lP-878kjh86uT0BQ1xdrvK][/Battle Brothers/]], [[https://www.youtube.com/playlist?list=PLQFX9B_9L4-nbnLVxYieErwNWGCFZ_ILh][/XCOM 2/]], and [[https://www.youtube.com/playlist?list=PLQFX9B_9L4-lrBij8Tnj_mmD34jhjhh81][/Darkest Dungeon/]]) /and/ videos that I /don't/ watch (/e. g./, of [[https://www.youtube.com/playlist?list=PLQFX9B_9L4-lmo-10hR87jtS7jPXw1gdp][/Northgard/]], [[https://www.youtube.com/playlist?list=PLQFX9B_9L4-lKYf-lJ1Xuuzoq6Wd0zQiq][/Mordheim: City of the Damned/]], and [[https://www.youtube.com/playlist?list=PLQFX9B_9L4-k_sCZZZN3vK_baLXXIePpq][/They Are Billions/]]). I can't mark my Patreon subscription "Do not interpret as supporting /Mordheim/ content", any more than I can mark my Amazon purchase of a ShaperV book as "Do not interpret as supporting the /Daniel Black/ series" or my MIRI purchase of /From AI to Zombies/ as "Past five dollars, do not interpret as supporting /From AI to Zombies/"---and, even if I could, I would refrain from setting such a precedent because it would be ridiculous to expect a content creator to read and interpret all the hundreds or thousands of messages that he would get. So, my monetary support of FilthyRobot is on very unstable footing.

The conclusions of this random comment: (1) Bundle deals that force people to buy what they don't want are bad (see also ESPN's [[https://www.techdirt.com/blog/?tag=espn][problems]] with [[/r/cordcutters][r/cordcutters]]); (2) [[https://en.wikipedia.org/wiki/Derivative_work#The_fair_use_defense_in_derivative_work_cases][as applied to the sale of derivative works]] (leaving aside the argument linked above, which was about unauthorized distribution), copyright laws are bad (see also [[https://en.wikipedia.org/wiki/Doujinshi#Copyright_issues][openly-sold Japanese doujinshi]] and the open proliferation of commissioned fanfiction stories on [[https://www.fimfiction.net/][FIMFiction]]).
:PROPERTIES:
:Author: ToaKraka
:Score: 8
:DateUnix: 1519665785.0
:DateShort: 2018-Feb-26
:END:

*** u/appropriate-username:
#+begin_quote
  However, copyright laws forbid me from doing so.
#+end_quote

There are laws against making donations to people? Also, isn't there a [[https://en.wikipedia.org/wiki/Transformation_(law)][specific thing]] that legalizes works where effort was put into the end product even if it's not 100% original?

#+begin_quote
  Likewise, shortly after the completion of HPMoR (for which I probably would pay ten dollars if I could), the org. that employed Prophet Yud (pbuh) saw fit to publish another, non-fiction work of his. I was forced to confront a similar problem: Should I pay an extra sum of ten dollars (above the five-buck suggested price, which I found reasonable for the nonfic book on its own merits) and risk sending the wrong message, or should I refrain from paying that premium and risk damaging the author's future willingness/ability to entertain me? I chose the middle course of paying a two-dollar premium.
#+end_quote

Send Yud a $10 donation with a message that mentions HPMOR and then pay whatever you want for the nonfiction book.

#+begin_quote
  or, if I could, it would be ridiculous to expect the content creator to read and interpret all those hundreds or thousands of messages
#+end_quote

Yeah which is why petabytes of data get interpreted daily with 0 human intervention. I'm not expecting the author to spin up a database and learn SQL for this sort of thing but a ctrl+f with a few names of their works or just something that makes a word cloud out of all the messages should be enough.
:PROPERTIES:
:Author: appropriate-username
:Score: 3
:DateUnix: 1519669878.0
:DateShort: 2018-Feb-26
:END:

**** Donation is often used as a thin fig leaf of deniability in a number of circumstances, but the problem is that the law often cares most about /intent/. This comes up most often not in the realm of copyright infringing prose fiction, but prostitution, and the primary reason that all prostitution isn't run on a donation-based model is that even if you call it a donation, the courts will still say "if there had been no donation, there would be no sex, ergo it is paying a fee for sex, and therefore, prostitution as defined by the legal code".

I'm not aware of any actual legal test of this with regards to prose fiction, and it would probably come down to a question of intent; are people donating in order to signal, in order to show appreciation, or because if they don't donate, no work will be created? Is any of this actually provable to the level of burden required by the courts?

Except it won't actually come to that, because there are very, very few fanfic authors that can withstand a legal battle in terms of money, and very few legal organizations that would take on such a case pro bono (the [[http://www.transformativeworks.org/][Organization for Transformative Works]] might be one, but it would probably have to be a very solid case that would set good precedent).

Regarding transformation, it's not always enough, and in many of the cases ToaKraka listed, the works /aren't/ sufficiently transformative, at least as far as my understanding of the law goes (copyright law is a hobby of mine). Writing a sequel to the Harry Potter series is an infringement of copyright, at least as far as the law goes, because you're taking the bones of the original series and using them in the same way they were intended to be used. Most of the successful uses of fair use that lean on "transformation" are about parody, critique, or social commentary of the original work for this reason, and there are a slew of failed cases where someone tried to defend a derivative work as transformative because while it created something new, that new thing wasn't actually transforming the original.
:PROPERTIES:
:Author: alexanderwales
:Score: 10
:DateUnix: 1519671079.0
:DateShort: 2018-Feb-26
:END:

***** Why do you think sending donations to authors would result in a legal battle? How would the OC creators find out about the donations?

#+begin_quote
  Regarding transformation, it's not always enough, and in many of the cases ToaKraka listed, the works aren't sufficiently transformative, at least as far as my understanding of the law goes (copyright law is a hobby of mine). Writing a sequel to the Harry Potter series is an infringement of copyright, at least as far as the law goes, because you're taking the bones of the original series and using them in the same way they were intended to be used. Most of the successful uses of fair use that lean on "transformation" are about parody, critique, or social commentary of the original work for this reason, and there are a slew of failed cases where someone tried to defend a derivative work as transformative because while it created something new, that new thing wasn't actually transforming the original.
#+end_quote

That's a fair point. Do you think a law should be made that protects works that aren't blatant ripoffs but also aren't 99% new content, like parody critique and social commentary works tend to be?
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1519671732.0
:DateShort: 2018-Feb-26
:END:

****** The large rights-holding corporations already trawl the internet looking for rights-violations. All it would really take is for one of them to get a bug up their butt about fanfiction, /probably/ as a result of a wildly successful fanfic that was perceived to be taking sales from the original series, /probably/ through a somewhat flagrant violation (e.g. someone who finishes every chapter with 'support me on Patreon if you want more chapters!').

Except that it probably wouldn't actually come to a legal battle, because the monied rights-holder would instead come after the services used for hosting and/or payment. I'm pretty sure that fanfiction.net already caves immediately to any legal gesture whatsoever, or even a polite request, given that there are a list of fanfics not allowed on the site. C&Ds would get sent to ISPs, hosting services, payment processors ... and most of them would instantly cave, because there's very little profit involved in providing legal defense for someone writing fanfic, even a popular one.

And yeah, I think copyright law is in a horrible state and in need of reform. I'm not really totally on-board with everyone being able to make sequels of whatever they want, whenever they want, because I think that would accelerate the culturally destructive nostalgia mining we see all around us ... but yeah, I'd like some kind of change.
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1519672944.0
:DateShort: 2018-Feb-26
:END:

******* u/ToaKraka:
#+begin_quote
  I'm not really totally on-board with everyone being able to make sequels of whatever they want, whenever they want, because I think that would accelerate the culturally destructive nostalgia mining we see all around us
#+end_quote

I frown sternly on your [[https://plato.stanford.edu/entries/freedom-speech/#PatJusForLimSpe][paternalistic]] view of free speech. The solution to bad movies that exploit nostalgia is /not/ restrictions that prevent the production of such movies. Rather, a loosening of copyright would allow consumers the freedom to choose between /bad/ movies that /exploit/ nostalgia and /good/ movies that /expand on/ old material, because in such an environment movies in /both/ categories would be able to proliferate.
:PROPERTIES:
:Author: ToaKraka
:Score: 7
:DateUnix: 1519688092.0
:DateShort: 2018-Feb-27
:END:


******* u/appropriate-username:
#+begin_quote
  e.g. someone who finishes every chapter with 'support me on Patreon if you want more chapters!'
#+end_quote

I was talking more about OP sending money on their own initiative without being asked. Doesn't seem like something that could be made illegal without Orwellian measures. Though I guess that's kind of beside the point if OP hadn't intended to talk about that one situation in particular but rather money making for authors in general.

#+begin_quote
  culturally destructive
#+end_quote

Even with regards to fanfiction? Why? Media like a movie can get pretty trashy and [[/r/hailcorporate]] but there's only so much of that that can go over well in a book in my experience.
:PROPERTIES:
:Author: appropriate-username
:Score: 3
:DateUnix: 1519688519.0
:DateShort: 2018-Feb-27
:END:

******** u/alexanderwales:
#+begin_quote
  Even with regards to fanfiction? Why?
#+end_quote

Fanfiction would become commercial fiction; even though fans would still write it, there would be people writing derivative works purely as a money-making enterprise. My worry/prediction is that the market would be flooded with "sequels" to popular books, in the same way that the market gets flooded with imitators already, except that we'd be even more locked into rehashing and regurgitating the same old shit, mostly because derivative works often ride the goodwill, characterization, investment, etc. of original works, and are often read because of risk aversion on the part of the readers (and written/produced because of risk aversion on the part of writers).

People trying to write original fiction are /already/ in competition with established franchises, and that problem would only get worse if the monetary incentive starts going toward fanfic as well.

(I think the arguable point here is that more fanfic and less original fiction because of a change in incentives is a bad thing. I generally think that fanfic has advantages that aren't artistically or culturally good, but that's probably up for debate. Write a million words, and people will want you to write four million, and fanfic fills the role of expanding a universe indefinitely, which for /most of them/ is where the appeal comes from, which I think leads to this incestuousness that's already a part of modern culture that I really dislike on both aesthetic grounds and on cultural health. Culture can't survive or thrive when everything is just a remix of a remix, and putting more fuel on that fire seems bad to me. What would really help is lowering the copyright length to something like 14 years, which would promote originality while allowing free expression on cultural touchstones.)
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1519707394.0
:DateShort: 2018-Feb-27
:END:

********* u/appropriate-username:
#+begin_quote
  My worry/prediction is that the market would be flooded with "sequels" to popular books,
#+end_quote

Why would more books than the [[https://www.fanfiction.net/book/Twilight/?&srt=1&r=10][~214,000 there are for, e.g. twilight,]] significantly change the situation?
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1519831246.0
:DateShort: 2018-Feb-28
:END:

********** Incentives for authors, especially authors good enough that people would be willing to pay them?
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1519831323.0
:DateShort: 2018-Feb-28
:END:

*********** There would be more books made, sure, but I was more asking what would change in a parallel universe where there were 300k stories or 500k stories instead of the 200k in this universe. The number is already too big to read all of them without financial incentives for authors.
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1519831543.0
:DateShort: 2018-Feb-28
:END:

************ In that parallel universe, those extra 300k books wouldn't come from nowhere. It's not like a bunch of people who don't write stories would suddenly write stories; most of the talent would be pulled from people exploring their own ideas, characters, and settings (because that's currently where the financial incentive is).

If the argument is that you can't actually read all of the books anyway so it doesn't matter how many are written, I'd disagree with that; recommendation algorithms and/or recommenders are, IMO, good enough that I actually /can/ read the top whatever fraction of books that perfectly align with my interests, especially since a good amount of work goes into ensuring that books find the right audience. Cutting the number of works of original fiction written per year in half actually does impact me then, in that case.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1519831957.0
:DateShort: 2018-Feb-28
:END:

************* Also a fair point. Do you think good authors write worse fanfictions than they do original stories?
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1521777424.0
:DateShort: 2018-Mar-23
:END:


** [[https://www.reddit.com/r/truegaming/comments/80ia6h/which_games_carry_over_to_either_real_life_or_to/][Maximizing utility of one's time spent gaming.]]
:PROPERTIES:
:Author: appropriate-username
:Score: 2
:DateUnix: 1519763014.0
:DateShort: 2018-Feb-27
:END:


** Is it possible to come up with a constitution of sorts that would allow for every "right" be granted for all technological improvements. One of the key failings of the american constitution is not knowing how privacy can be manipulated with tech and how vehicles would transform work. Same with how guns and warfare might evolve.

I would think that we would only need to make laws and rights up and to the point that our technology makes each of us into some sort of god. (at least comparable to what we are now and how we can perceive how our tech may evolve.)
:PROPERTIES:
:Author: I_Hump_Rainbowz
:Score: 1
:DateUnix: 1519732883.0
:DateShort: 2018-Feb-27
:END:

*** [Group] may (not) [perform action] such that [qualifier] using any technology extatant, conceived, or is yet to be imagined. So long as [caveats].

First attempt, probably has holes.
:PROPERTIES:
:Author: 1337_w0n
:Score: 1
:DateUnix: 1519782480.0
:DateShort: 2018-Feb-28
:END:
