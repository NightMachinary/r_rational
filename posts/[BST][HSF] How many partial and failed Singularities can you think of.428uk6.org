#+TITLE: [BST][HSF] How many partial and failed Singularities can you think of?

* [BST][HSF] How many partial and failed Singularities can you think of?
:PROPERTIES:
:Author: DataPacRat
:Score: 12
:DateUnix: 1453514783.0
:DateShort: 2016-Jan-23
:END:
I'm looking forward to the next story I write, which is going to involve something vaguely /like/ a Singularity, but which isn't going to match any precisely-defined description of one. How many different not-quite-Singularities can [[/r/rational]] come up with?

For example:

- Moloch wins: AIs follow short-term incentives with long-term consequences, and evolve themselves into extinction.
- Several entries from the [[http://www.sl4.org/archive/0310/7163.html][Friendly AI Critical Failure Table]].
- Staged singularities: Going from human-level intelligence to post-human level 1 is a /lot/ easier than going from post-human level 1 to post-human level 2.

(I'm particularly interested in pseudo-Singularities that get rid of technological civilization on Earth, but for idea-generation purposes, am open to all suggestions.)


** Creation of brain implant that gifts to all humans photographic memory. Fifty years or so of accelerated research, since more readily accessible information means more potential connections and insights. Assume effective immortality in the form of agelessness is invented. Knowledge generation slows down as scientists become able to leverage their gained years into financial success, and thus can self-fund their own research, leading to highly insightful and innovative science being conducted in many parts of the word in secret; as a result, the level of knowledge found in public domain publishing of new experiments, while now decentralized, becomes the metaphorical undergrad to the scientist-recluses' post-post-grad.
:PROPERTIES:
:Author: TennisMaster2
:Score: 10
:DateUnix: 1453518077.0
:DateShort: 2016-Jan-23
:END:

*** Real 'photographic' memory isn't all it's cracked up to be, and even the kind of perfect memory you describe wouldn't do much. I have something moderately close to that (I know people tend to exaggerate on the internet but I actually do have a classic 'photographic' memory with measurable effects), but it doesn't help with academics past precalculus. It's also considerably less powerful than Google Search, so you shouldn't expect it to revolutionize anything in a world that already has Google Search.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 5
:DateUnix: 1453524329.0
:DateShort: 2016-Jan-23
:END:

**** I know - I specifically meant the colloquially referenced photographic memory, not eidetic memory.

I should have specified before, but I also mean something a bit more powerful, beyond the ability to read a page and then recall the image of the page later, to read once more sentences forgotten (I almost had the ability when very young, so I know what you mean). I should have rather said the ability to /glance/ at a page and read it later; such a person could glance at the pages of physics textbooks from introductory to graduate level, then go lie down, and mentally read the pages at an advanced rate, focus allowing. Since it'd be an implant of artificial neurons, once a subject were understood, that understanding wouldn't go away, and any memories of confusion could be forgotten.

The primary advantage is in having expert-level understanding of many different fields, with which the brain could draw many insightful connections; the present day equivalent is two experts from different fields working together on a project, and chance sparking insight into the mind of one of the experts. Google Search does not assist in the generation of insight in human brains: without connectomes already containing information, insight of one connectome's relation to three other connectomes can't be drawn in the first place.

Granted, inability to do complicated calculations in one's head limits the usefulness the implant might have to some fields' experts. Assume another implant solves that problem, twenty years post-singularity.
:PROPERTIES:
:Author: TennisMaster2
:Score: 5
:DateUnix: 1453526586.0
:DateShort: 2016-Jan-23
:END:

***** That's sort of my point, though. I can give you specific details about books I read decades ago, and even recite passages verbatim. I accidentally got the wrong Intro to Sociology textbook in my first year at university, and I still got top marks even though half my grade was based on information from a book I didn't have - because I'd run into most of those experiments before and still remembered them. I can draw an accurate geographic map of the world from memory, though any detail I've never explicitly noticed will be absent and Sri Lanka will probably be in the wrong place.

And, despite all of that, I /still suck at calculus/. Those things I mentioned are parlor tricks, just like everything else an 'eidetic' memory can do. All that rote memorization is useless without a deep understanding of the subject material, and believe me when I say that it's no easier if you have the book in your head than it is if you have it in front of you. And again on the Google Search point, because you can do all of those things too; all you need is a computer so small it fits in your pocket.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 4
:DateUnix: 1453527949.0
:DateShort: 2016-Jan-23
:END:

****** Does your memory allow you to associate many superficially disparate facts with each other with little effort? Do you notice unique patterns because you have more knowledge to inform split-second analysis? The answer to those questions informs the viability of the scenario.

I think the second underlying assumption is deep understanding can be more easily achieved when reading mental images than when using eyes. If that's not true, then the scenario has little credibilty.

I say little, because for a smart person who's excellent at drawing connections but has a horrible memory, the implant would greatly assist the degree of intellectual insight they'd be capable of producing.
:PROPERTIES:
:Author: TennisMaster2
:Score: 2
:DateUnix: 1453529365.0
:DateShort: 2016-Jan-23
:END:

******* No to the first, yes to the second. Disparate facts don't associate themselves unless they're directly addressed, so while I can pick up on obscure references and solve riddles pretty quickly, I almost never make long leaps of logic, even if I have all the pieces to a given puzzle. I'm actually pretty slow on the uptake, in large part because I have no idea what I'm expected to know and way too many potential matches to go through. It's like trying to find matching socks in a drawer with ten thousand pairs. It took me weeks just to figure out public transportation, and even then I still stop to ask directions pretty much every time I ride the bus.

I can't say whether it'd be easier to learn from a memorized text, because I can't memorize or visualize images with high enough fidelity to read them. The closest I can come is reconstructing old memories based on the shapes of letters and lengths of words.

For the last point, I have no idea. Typical Mind Fallacy would suggest that I'm way underestimating how much I rely on my memory, but at the same time I think the information itself is less important than the ability to make the inferences. Everyone who came before Archimedes had all the tools they needed to work out the volume measurement, but it took more than that to make the connection.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 2
:DateUnix: 1453530867.0
:DateShort: 2016-Jan-23
:END:

******** I agree that you're underestimating, based upon your comment history ("cloning but metabolism" meaning mitosis is a nice insight) and that you said yes to the second. The second is more important for generating insight, as it can be applied to one's knowledge manually via introspection.

I sympathize with you on being slow on the uptake. At times I have to model others' understanding of a given situational context in order to grasp the salient topics, though I'm sure you have it ten times as bad. That said, having had and lost acumen in memory, I'd still say you're lucky.

Why the difficulty with public tranportation? Enough fidelity to access the potential memory you seek to recall, but not enough fidelity to be certain whether the recalled memory is accurate or conflated?
:PROPERTIES:
:Author: TennisMaster2
:Score: 2
:DateUnix: 1453532258.0
:DateShort: 2016-Jan-23
:END:

********* I have no idea why, actually - or rather, I don't know how other people have such an easy time with it. The only way I could imagine getting the level of understanding everyone and their mother seems to have is by sitting down for half an hour and brute force memorizing all the bus schedules. Which is ridiculous, because obviously everyone else who rides the bus never did that.

That might be a personal quirk, actually. It could very well be that my memory isn't the cause, but a mitigating factor.

EDIT: Relating to your first point, here's how my thought process worked: cloning but metabolism, sounds like cloning but you have to eat a lot, sounds like you eat then clone then eat then clone, sounds like mitosis, but for people. Unless my attention is drawn to a specific subject, I can and usually do fail to draw the link. Someone smarter than me might have an easier time with it, but by and large it seems like that logical leap is the limiting reagent here.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 2
:DateUnix: 1453568097.0
:DateShort: 2016-Jan-23
:END:

********** Fascinating discussion and conversation! Thanks for sharing and having it with me.
:PROPERTIES:
:Author: TennisMaster2
:Score: 2
:DateUnix: 1453574893.0
:DateShort: 2016-Jan-23
:END:

*********** Thanks to you too! I've never thought along these lines before, and I think I should give what you described a try, just in case.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 2
:DateUnix: 1453579274.0
:DateShort: 2016-Jan-23
:END:


********** People use an online planner, or they just get a bus map, work out which buses they need to take to get from A to B and then look at what time those buses arrive at the stops they need.

Once they have the route planned they take it and generally remember it meaning they don't have to plan it again.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1453760962.0
:DateShort: 2016-Jan-26
:END:

*********** That doesn't match anything I've seen. For one, I use online planners as well, often writing out or memorizing the exact route I need to take. Unfortunately, there's never any indication of which direction the bus is going, meaning that just knowing I need to take, say, bus 95B, doesn't help in and of itself unless I also know where I am, which direction is which, and how those two things relate. For another, when I stop passersby to ask directions, they usually have an easy time figuring out how I should get where I'm going.

Putting these two together, this would seem to suggest some sort of "obvious" information that I'm missing, such as an intuitive understanding of city layouts, or the ability to easily keep track of directions without a compass or any particular effort. It could just be that my proverbial river is full of red herrings, or it could be a straightforward deficiency, or anything else under the sun; I have no information to go on.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1453763410.0
:DateShort: 2016-Jan-26
:END:

************ Understanding of city layouts doesn't have to be intuitive. You just need a decent local map.

The online planner should tell you the stops. Each stop individual stop is only one side of the street. All vehicles on that side of the street are going in the same direction.

To keep track of directions without a compass you just need to work out where the direction is in relation to the direction you are facing and then consciously alter it whenever you turn into a different street. It becomes unnecessary only when you are familiar with the area.

People become familiar with a route once and then don't worry about it again. The people who you are asking didn't learn because they memorized all the bus time tables. They learned from using an online planner, asking a bus driver or friend. Alternatively they could have called the public transport help line. Once they learn the route they start taking it and it gets solidified in their memory. When they screw up they learn from their mistakes.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1453769858.0
:DateShort: 2016-Jan-26
:END:

************* For the first two, I haven't seen anyone use a physical map in almost a decade, and in my part of Toronto stops are almost always on both sides of any given street (and often at all four points of an intersection), with the lines going on the same route in opposite directions sharing a name.

As for consciously altering a mental direction whenever I turn, I've seen people struggle to /count/ while doing other tasks. Are you suggesting it's normal for a person to actively mentally keep track of which direction is which while, say, grocery shopping?

As for the fourth point, as a rule anyone I ask for directions is going to a different place than I am. They never consult the internet, ask directions themselves, or make phone calls. Without exception, I lack the information to derive the solutions myself.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1453771801.0
:DateShort: 2016-Jan-26
:END:

************** People have google maps which does provide directions for public transport in Toronto, provides the direction where you travel and plots the course on a map which should be all you need. Street view is an added bonus.

Consciously altering a mental direction whenever you turn is what I do when I'm going down unfamiliar streets, it doesn't require necessarily require good multitasking skills just that you take a second to mentally review where you are in relation to some known location or location whenever you take a turn or enter a new street. Knowing the rough direction of a area you are familiar with helps avoid getting lost. It is improving your navigation skills through deliberate practice. When you navigate in the same place often enough it becomes unnecessary because you can just recognize the area and remember where it is.

It's not necessary in familiar areas or small places like a grocery store (It's not like you are going to forget which way of the store is the front and even if you do you can just look up and around to reorient yourself in the store).

Just because they are not going the same place as you at the moment doesn't mean they have never gone there before and remember the route. If they have been in the city for a while and take public transport often then eventually they will get familiar with most places.

What I'm saying is that the best way to get familiar with public transport isn't to memorize all the schedules. You just need to learn the routes of the buses you need, take them and remember where you went.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1453809025.0
:DateShort: 2016-Jan-26
:END:


****** There's no point to asking you if you calculus. The question though is do you think you do things generally better than someone else.

A world changing moment can be framed as small. A little bit of guaranteed increase across the board, like having an eiditic memory offers, could push those few extra that could really matter.
:PROPERTIES:
:Author: Stop_Sign
:Score: 1
:DateUnix: 1453959987.0
:DateShort: 2016-Jan-28
:END:

******* I can't separate everything well enough to say for sure, and I'm extremely high-variance (high neuroticism, high scrupulosity, high Dark Triad, abnormal emotional traits...). That said, as a rule, I'm on average better at things that have cross-disciplinary synergy like painting, writing, solving riddles, rhetoric, philosophy, biology, and so on, but not subjects with lots of prerequisite knowledge, like physics, chemistry, and math (obviously these aren't mutually exclusive). This is bearing in mind that I'm actually pretty good at math, just not as much as you'd expect if you guessed based on my ability at biology.

Within subjects, I tend to pick up theoretical knowledge much faster than practical or technical ability. For example, my brushstrokes are horrible, but my composition is spectacular.

The issue stems from there; I don't pick up /technical/ abilities any faster than normal, and everything /non/-technical is just a glorified magic trick. Past a certain point, I have no choice but to learn the technical skills, so I have a broad theoretical knowledge but more-or-less average ability within my actual areas of expertise. Which brings me back to the Google Search point.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1454001850.0
:DateShort: 2016-Jan-28
:END:

******** Interesting, because I'm largely the opposite. I'm a programmer, and phenomenal at learning new things and great at remembering understandings but horrible at remembering facts. I've started organizing notes in different ways, since I'll remember that I was convinced of something but not remember how. If I don't make the notes, I'll also forget the why. I'm actually incapable of doing heavy memorization roles, like biology or history or a foreign language. I don't genuinely remember something until I use it practically.

I wouldn't expect you to learn things faster, but I'm wondering if that's different the second time you learn something similar. The way I get better at things is to remember what worked last time and repeat it until I figure out the pattern of successful behavior. This is limited by how much I can remember, so I feel like I would gain more from every experience just by retaining more to think about later. The first time you try something, you're on equal ground, but you get more knowledge per experience, so you'd outpace others.

I'm also curious about the much longer term effect. I've been out of college for three years and I can name 2 college professors. Memories of childhood friends are feelings and faces. No places, words, event specifics, or sounds carry over decades.

Not just long term facts, but also long term understandings. I can't do long division by hand without needing to reverse engineer it. I forgot how to write some letters in cursive. I couldn't tell you how to build a computer after ordering all the parts, because all I know is the last 7 times I did this I figured it out without extra help and in 3 hours, so I can reasonably assume I can do that again. You can just remember the steps.

This also might have shaped us into our specialties: I rely on repeatable understandings (how to build a computer) and you get a more detailed layer (the arrangement of steps in the manual). If the computer is a different model, though, you have a lot more work than I do to incorporate it - when things change, the patterns that normally work for you might suddenly not. You remembered that in the last computer model, you plugged in the power to the motherboard first, then hard drive. I remembered vaguely the power needs to be plugged in, and then look for where it fits. For me, a different model changes at a level below where I care about, because I'm remembering the category, not instructions. Nothing about the categories changes in a new model, so I don't see it as additional effort just because the model changed this time. Replace computer with something that changes less, like a biology textbook, and that's why you find yourself better at it.

This is also top down (understanding the concept for identifying the missing facts) vs bottom up (using the existing facts to create an understanding). I pretty much exclusively think in top down, and it sounds like having a memory like yours teaches you to think bottom up. I'm great at hearing "make a new car" and imagining all the separate tasks as prioritizable categories, and you're great at getting a list of car parts and figuring out what model it is. This is irrespective of natural talent of a particular skill, too.

Also, the tone of your response rings bells in my head. You think that if you're not a natural at something, that means you'll just have to live with forever being bad at it. I read something in a fanfiction where one character asked another to do something, and got back "I can't". The response was something like "you only think you can't because you didn't immediately know how. At the absolute minimum, spend 5 real minutes - by the clock, no cheating - giving this your absolute attempt. To say you can't without at least one meaningful attempt is wasteful and will lead to perpetually underestimating yourself ." I've since taken that to heart, and found that my lifelong "I can't draw, I can't play music, and I can't write" was totally shattered by - for what was probably the first time in my life - actually trying. I'm not quite sure what about your responses makes me think you could just this advice, but maybe it's helpful.

This worked too for silly things like yoyos and hula hoops. Really, I'm a lot less specialized to computers, math, and programming than I thought. I just only learned this when I read that advice, a year ago.
:PROPERTIES:
:Author: Stop_Sign
:Score: 1
:DateUnix: 1454005363.0
:DateShort: 2016-Jan-28
:END:

********* u/UltraRedSpectrum:
#+begin_quote
  You think that if you're not a natural at something, that means you'll just have to live with forever being bad at it.
#+end_quote

Not exactly, no. The point I'm making is that the memory thing doesn't help with technical skills any more than having a medical textbook on hand will make you a surgeon. Quite the contrary to the quote, though, I never had this problem. Whether by pluck or narcissism, I usually only accept that I'm bad at something as a result of overwhelming evidence. This has happened exactly once, with music composition, after about a dozen songs, reading everything I could find on the subject, and about twenty hours total of trying.

#+begin_quote
  I read something in a fanfiction where one character asked another to do something, and got back "I can't". The response was something like "you only think you can't because you didn't immediately know how. At the absolute minimum, spend 5 real minutes - by the clock, no cheating - giving this your absolute attempt.
#+end_quote

The fanfiction you're thinking of is probably HPMoR. From chapter 25:

#+begin_quote
  You /never/ called /any/ question impossible, said Harry, until you had taken an actual clock and thought about it for five minutes, by the motion of the minute hand. Not five minutes metaphorically, five minutes by a physical clock.
#+end_quote
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1454007438.0
:DateShort: 2016-Jan-28
:END:

********** Hm, I was off mark, then. And yes, hpmor is definitely where I got it from. You can see how my memory of the understanding, not the specifics, influenced how I said it.

I'm still curious about what your thoughts are for long term memories.

Also, my theory is that our lifelong patterns and specialties are formed due to the perceived ease or difficulty when trying different things. It's perceived because we're comparing our skill to others to determine if we're better, and who we're compared to matters. You saw you were better at memory, so you focused on how to use it to solve problems, which led to bottom up thinking. I saw I was better at understanding, so I went to bottom up thinking.

I suppose I'm trying to get an answer to "Are you able to separate how a better memory affected you verses how being better than your peers affected you?"

Because the whole point of this discussion is to determine the viability and complexity of writing about a global increase in memory, which is exclusively that first category. If you can't even tell between them, this whole topic is unwritable (unless you used it as an example of something you can sort of but never fully understand, like a world of color blind people learning to see color.)
:PROPERTIES:
:Author: Stop_Sign
:Score: 1
:DateUnix: 1454010342.0
:DateShort: 2016-Jan-28
:END:


********** Also yes, I fully understand a medical textbook on hand doesn't make you a surgeon.

My point was that on average, I'd want my surgery from the surgeon with the figurative textbook on hand, rather than the surgeon without one.

Separately, my wandering thoughts were about whether that means all surgeons would do better with better memory than without it. And then which professions or situations that it is concretely always better in, which led to "what does better memory affect, before personality and culture and job specifics join the equation?" to "is this question answerable?" to where I was with my other posts.
:PROPERTIES:
:Author: Stop_Sign
:Score: 1
:DateUnix: 1454010754.0
:DateShort: 2016-Jan-28
:END:


***** Clairvoyance is the word your looking for. To be able to use all the information you know to get an answer you know is right, without needing to actively process the steps in the middle.
:PROPERTIES:
:Author: Stop_Sign
:Score: 1
:DateUnix: 1453959675.0
:DateShort: 2016-Jan-28
:END:

****** u/TennisMaster2:
#+begin_quote
  Clairvoyance
#+end_quote

What definition are you using? I think "to intuit" and "intuition" might be the mot juste, but the word has too many meanings, making confusion unacceptably likely.
:PROPERTIES:
:Author: TennisMaster2
:Score: 1
:DateUnix: 1453961353.0
:DateShort: 2016-Jan-28
:END:


** u/EliezerYudkowsky:
#+begin_quote
  AIs follow short-term incentives with long-term consequences, and evolve themselves into extinction.
#+end_quote

Requires stupid AIs. If you can see it, why can't they?
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 3
:DateUnix: 1453515795.0
:DateShort: 2016-Jan-23
:END:

*** The model I based this idea on was that each initial AI "owned" a certain portion of the CPU cycles per unit time, but could trade them away. A market could result in which each AI faces an incentive to streamline its mental processes, to reduce the CPU overhead spent on keeping itself alive, in order to have a larger "bank account" of spare cycles to perform financial transactions of various levels of risk with. Even if the original AIs were of human-level intelligence, or even outright ems, then depending on the details, it seems possible for some versions of this model to result in ever-stupider AIs.

(I'm afraid that I don't have the economic chops to prove anything about this model, I'm only proposing it at the level of detail required for fiction. I'm looking forward to whatever Robin Hanson might say on the topic in his forthcoming book.)
:PROPERTIES:
:Author: DataPacRat
:Score: 7
:DateUnix: 1453516386.0
:DateShort: 2016-Jan-23
:END:

**** Could you drop a link to the story where you implemented this? I have forgotten the name and can't seem to find it.
:PROPERTIES:
:Author: mycroftxxx42
:Score: 1
:DateUnix: 1453590272.0
:DateShort: 2016-Jan-24
:END:

***** I haven't actually explicitly directly implemented this in a story. That said, it's one of the possible scenarios that the in-universe characters have described for their Singularity in [[https://docs.google.com/document/d/1_ZcUba_GKVCm_i2VeGrfSBBxC8pR6VZC5VBBUVKKxYk/edit][S.I.]], and is a possibility for the future of the universe of [[https://docs.google.com/document/d/1nRSRWbAqtC48rPv5NG6kzggL3HXSJ1O93jFn3fgu0Rs/edit][LoadBear's FAQ]].
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1453599887.0
:DateShort: 2016-Jan-24
:END:


*** If Hofstadter's super-rationality doesn't for some reason actually play out in practice, it won't necessarily matter if they can see it.
:PROPERTIES:
:Author: ishaan123
:Score: 5
:DateUnix: 1453518554.0
:DateShort: 2016-Jan-23
:END:

**** Or they could get together and build an enforcement mechanism like, you know, even humans try to do.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 7
:DateUnix: 1453521276.0
:DateShort: 2016-Jan-23
:END:

***** Of course, but in theory there do exist cases where it's not possible to make enforcement mechanisms, so i don't think the potential danger of unsolvable coordination problems should be dismissed in practice (if nothing else, to err towards caution).
:PROPERTIES:
:Author: ishaan123
:Score: 2
:DateUnix: 1453567729.0
:DateShort: 2016-Jan-23
:END:


*** Perhaps they live at such high speeds that human timescales seem absurdly long-term to them. Just like humans rarely worry about environmental damage that will take centuries to have an effect, the AIs might do things without fully anticipating the consequences in a week's time.

Or, to put it more formally, their utility function has particularly high discounting - one utilon immediately is worth two utilons in a day.
:PROPERTIES:
:Author: Chronophilia
:Score: 3
:DateUnix: 1453572655.0
:DateShort: 2016-Jan-23
:END:


*** Accellerando seems to do this with the Vile Offspring.
:PROPERTIES:
:Author: JackStargazer
:Score: 3
:DateUnix: 1453520106.0
:DateShort: 2016-Jan-23
:END:

**** My understanding was that they were not stupider, but just had drifted further and further from human values?
:PROPERTIES:
:Score: 1
:DateUnix: 1453569208.0
:DateShort: 2016-Jan-23
:END:

***** But in the process they were evolving themselves out of existence, as we saw in the [[#s][]]
:PROPERTIES:
:Author: JackStargazer
:Score: 2
:DateUnix: 1453585706.0
:DateShort: 2016-Jan-24
:END:

****** The what now?
:PROPERTIES:
:Score: 1
:DateUnix: 1453665642.0
:DateShort: 2016-Jan-24
:END:


*** They can, but the short-term cost of stepping away is too high.

Anyone who overgrazes or overfishes knows what's coming, but if they're at subsistence they can't afford to do anything about it. Everyone who goes to war, or even spends the clean-water money on weapons, knows how horrible it is, but being the first not to is worse.
:PROPERTIES:
:Author: dspeyer
:Score: 2
:DateUnix: 1453522581.0
:DateShort: 2016-Jan-23
:END:

**** u/electrace:
#+begin_quote
  Anyone who overgrazes or overfishes knows what's coming, but if they're at subsistence they can't afford to do anything about it.
#+end_quote

This wouldn't be a problem with superintelligent AIs. The reason it's a problem with humans is because there isn't always an enforcement mechanism to stop them. Deals between AIs can be enforced through credible threats.

So, either it's the case that overfishing has already lead to a state where, no matter what else happens (like, say, a binding agreement where you can only fish for x fish per day), there won't be enough fish for everyone, in which case /there is no long term tradeoff/ (because death in the long-term has already been decided).

Or, it's the case that the problem hasn't crossed the threshold where there is no solution, at which point, the AIs would form a binding agreement through precommitments, and solve the problem.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1453525736.0
:DateShort: 2016-Jan-23
:END:

***** u/ArgentStonecutter:
#+begin_quote
  Deals between AIs can be enforced through credible threats.
#+end_quote

Deals between humans can be enforced with credible threats. That's what an arms race /is/.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1453561004.0
:DateShort: 2016-Jan-23
:END:

****** [deleted]
:PROPERTIES:
:Score: 0
:DateUnix: 1453590046.0
:DateShort: 2016-Jan-24
:END:

******* u/ArgentStonecutter:
#+begin_quote
  The kidnappee AI could change their own source code so that they were incapable of talking to the police.
#+end_quote

That statement makes all kinds of assumptions about the complexity and transparency of the AIs code, their ability to make such a fundamental change in their code and still consider themselves the same entity, and what kinds of precautions (in terms of auditing, duress procedures, meta-coding, interstitial code, and so on) the kidnappee has taken to maintain its continuity.

The most likely outcome of this case would be the kidnappee wiping their code because they know they will be restored from backup, once their watchdog timer goes off.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1453591798.0
:DateShort: 2016-Jan-24
:END:

******** u/electrace:
#+begin_quote
  That statement makes all kinds of assumptions about the complexity and transparency of the AIs code
#+end_quote

Yes, which is why I said...

#+begin_quote
  That wouldn't happen with 2 AIs *(given they have access to each other's source codes, and the ability to change their own).*
#+end_quote

--------------

#+begin_quote
  their ability to make such a fundamental change in their code and still consider themselves the same entity
#+end_quote

This shouldn't be an issue. An AI with a utility function is indifferent regarding its own continued existence. All that matters is that the utility function gets maximized. /Normally/ that happens when the AI continues to exist, with its utility function unaltered. There are, however, exceptions to this rule.

If a bigger, more established, more intelligent AI came face to face with Clippy, and told it "Completely terminate yourself, and all backup copies of yourself, or I will destroy all paperclips ever created. If you do as I say, I'll make sure that the paperclips you've created are not destroyed." Then Clippy would terminate itself (assuming, for the sake of simplicity, that the more powerful AI had the ability to detect deception in Clippy). Why? Because the choice for Clippy is (Either do as it says, which means some paperclips will exist, or don't, which means that none will. Since some is better than none, I'll do as it says).

#+begin_quote
  The most likely outcome of this case would be the kidnappee wiping their code because they know they will be restored from backup, once their watchdog timer goes off.
#+end_quote

Now who's making assumptions :P

Simplifications are needed in any model (or, in this case metaphor). If you want to add in the (totally reasonable) existence of backup copies, I'd need to find a new metaphor, because the existence of backup copies pretty much negates the purpose of kidnappings in the first place. Clippy's war (above) is looking pretty good.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1453593784.0
:DateShort: 2016-Jan-24
:END:

********* u/ArgentStonecutter:
#+begin_quote
  That wouldn't happen with 2 AIs (given they have access to each other's source codes, and the ability to change their own).
#+end_quote

I know you made that assumption explicit. I am challenging that assumption.

#+begin_quote
  An AI with a utility function is indifferent regarding its own continued existence. All that matters is that the utility function gets maximized.
#+end_quote

Another assumption that treats artificial superintelligences as somehow fundamentally simpler than humans, when it's likely they will be more complicated, and their utilitity functions will be at least as messy as ours.

#+begin_quote
  Now who's making assumptions :P
#+end_quote

I'm assuming that an intelligence that is more capable than a human is unlikely to be simpler than a human.

#+begin_quote
  Simplifications are needed in any model
#+end_quote

That's of course a problem when dealing with entities that are more complex than you.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1453594396.0
:DateShort: 2016-Jan-24
:END:

********** u/electrace:
#+begin_quote
  I know you made that assumption explicit. I am challenging that assumption.
#+end_quote

Ah, that makes more sense.

If an AI has the ability to hide their source code, they won't be able to make precommitment deals with other AIs. That makes them worse off than if they could make those deals. Agents that can pre-commit do better than those that can't.

If it is possible for an AI make itself auditable by another AI, it's in its best interest to do that. If it is always possible for an AI to fake its source code, then that's just an unfortunate fact of the universe, and pre-commitments wouldn't be possible.

#+begin_quote
  Another assumption that treats artificial superintelligences as somehow fundamentally simpler than humans, when it's likely they will be more complicated, and their utilitity functions will be at least as messy as ours.
#+end_quote

Humans don't have utility functoins....

If a superintelligence is first built with a utility function, they will continue to have that utility function because of the nature of utility functions (an agent wouldn't act to change it's utility function, under almost all scenarios, because doing so almost always leads to a lower utility).

If it isn't first built with a utility function, there really isn't any reason to speculate.

#+begin_quote
  I'm assuming that an intelligence that is more capable than a human is unlikely to be simpler than a human.
#+end_quote

Simpler in what respect? Simpler in their motivations? In their responses to a situation given their motivations?

Motivation simplicity is going to be a function that is mostly determined by the process by which the motivations come into being. For humans, that means an evolutionary process, which would create a less capable being with highly variable and complicated motivation system. For AIs, well, that would depend on the AI, but assuming they had a simple utility function, it would create a highly capable being with a simple motivation system.

Response simplicity is going to be a function that is mostly determined by intelligence. The more intelligent an agent is, the more likely that it is going to be the answer that a rational agent will be.

A very stupid agent will have a highly irrational and confusing thought process when trying to figure out 100* 100. An intelligent agent will have a very simple thought process.

#+begin_quote
  That's of course a problem when dealing with entities that are more complex than you.
#+end_quote

You can't think without models. /Thinking is modeling./ There's literally no other option
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1453598928.0
:DateShort: 2016-Jan-24
:END:

*********** u/ArgentStonecutter:
#+begin_quote
  If an AI has the ability to hide their source code, they won't be able to make precommitment deals with other AIs.
#+end_quote

"Source code" includes actual code, implicit models, explicit models, neural nets and associated weightings, and so on. It's really making a whole lot of assumptions to think that any AI can make sufficient sense of their own code or that of an AI of comparable complexity to be able to encode "I won't go to the cops" into that system.

#+begin_quote
  If a superintelligence is first built with a utility function, they will continue to have that utility function because of the nature of utility functions
#+end_quote

The concept of a utility function, as something more than a metaconcept used for modelling an AI that's necessarily smaller than us, is something that's unlikely to map to anything in reality.

An actual, working, conscious AI is likely to be at least as messy as the most complex meta-systems we know of.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1453600670.0
:DateShort: 2016-Jan-24
:END:

************ u/electrace:
#+begin_quote
  It's really making a whole lot of assumptions to think that any AI can make sufficient sense of their own code or that of an AI of comparable complexity to be able to encode "I won't go to the cops" into that system.
#+end_quote

Not sure what you're arguing with, I already covered that possibility.

#+begin_quote
  then that's just an unfortunate fact of the universe, and pre-commitments wouldn't be possible.
#+end_quote

--------------

#+begin_quote
  The concept of a utility function, as something more than a metaconcept used for modelling an AI that's necessarily smaller than us, is something that's unlikely to map to anything in reality.
#+end_quote

I think you're more sure of that than the evidence suggests that you should be. So I ask, how do you think that you know this?
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1453602754.0
:DateShort: 2016-Jan-24
:END:

************* u/ArgentStonecutter:
#+begin_quote
  Not sure what you're arguing with, I already covered that possibility.
#+end_quote

Your statement was that "deals with AIs can be enforced through credible threats" without qualification.

I challenged that, you responded "Humans often can't credibly precommit to non-subgame-perfect strategies. AIs can." as if that was a given for any AI. Then qualified it with "given they have access to each other's source codes, and the ability to change their own".

That is a very strong assumption. It needs more support.

#+begin_quote
  I think you're more sure of that than the evidence suggests that you should be. So I ask, how do you think that you know this?
#+end_quote

Probabilities. AIs would be developed, initially, by humans. Out of all the possible paths to the development of an AI, how many would involve a process that is completely unlike the development of any other software system of comparable complexity to date? Even systems of far less complexity have unexpected behavior, that is, it's not possible to predict or even describe describe their "utility function", if they have one. Having been developed, how many would impose an artificial constraint on their actions?
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1453625266.0
:DateShort: 2016-Jan-24
:END:


********* u/DataPacRat:
#+begin_quote
  An AI with a utility function is indifferent regarding its own continued existence.
#+end_quote

As a counter to this point, you may wish to read "Formalizing Convergent Instrumental Goals" at [[https://intelligence.org/files/FormalizingConvergentGoals.pdf]] , whose 11 pages describe how any sufficiently intelligent agent is likely to want to preserve itself, as well as preserve its goals and amass more resources to pursue its goals.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1453599560.0
:DateShort: 2016-Jan-24
:END:

********** Did you read my whole post? I basically said exactly what you're saying.

#+begin_quote
  Normally that happens when the AI continues to exist, with its utility function unaltered. There are, however, exceptions to this rule.
#+end_quote

Even though what you're saying is true, the exception applies when the AIs continued existence leads to a lower value of the utility function compared to the AI allowing itself to be terminated.

Bostrom talks about the possibility of an AI "failing" in such a way that it causes the programmers to feel more secure about the AI, which would lead them to create a very similar AI with a very similar utility function, but with less restrictions.

The 2nd AIs maximization of its utility function would likely be a very high value on the 1st AIs utility function, and the 1st AI may deem it the expected maximum that it can attain (especially if the first AI is severely stunted).
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1453600952.0
:DateShort: 2016-Jan-24
:END:


*** What about a bad utility function that highly discounts future expected payoffs to the point where a short term payoff following strategy A is greater than the sum of all payoffs following strategy B?
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1453526131.0
:DateShort: 2016-Jan-23
:END:


*** Because they lack certain information, so their regularization throws out the models pointing out those nasty long-term consequences.

It's always worth noting that just as almost all (in the measure-theory sense) AIs don't help humans, almost all mind-like /thingies/, /especially/ attempted self-altering thingies, turn into gibbering bags of insanity rather than converging to superintelligent AIs.

"There are more ways to be +dead+insane than +alive+smart."

If that's not true in real life, it's still quite plausible for fiction.
:PROPERTIES:
:Score: 1
:DateUnix: 1453665793.0
:DateShort: 2016-Jan-24
:END:


** What exactly do you mean by "Singularity"? Your concept of it seems to include a lot of stuff that is not included in my concept of it.

When I say "Singularity", I am referring to the moment that a system that is better at designing systems than its designer was starts iteratively designing better systems. I don't know what it would mean to partially do that, or to fail to do that in some nontrivial way. I can /conjecture/ lots of things that may follow from that event, but the event itself is reasonably unambiguous, I think.

Is one of us using the word wrong?
:PROPERTIES:
:Author: Anakiri
:Score: 5
:DateUnix: 1453516946.0
:DateShort: 2016-Jan-23
:END:

*** There are at least three main definitions for "Singularity", each partially exclusive of the others, plus innumerable minor variants. It's gotten to the point where no two people are describing the same thing with it anymore.

Thus, I'm looking for ideas that at least vaguely resemble some version of any definition Singularity, if you squint and turn your head, but which don't necessarily lead to the creation of superintelligent AI Gods who pursue their utility functions with godlike calculations. Near-singularities, pseudo-singularities, things that a random skiffy reader wouldn't blink at if they were /called/ a Singularity regardless of what "Singularity" actually means.

This is a brainstorming thread, which I'm hoping to leverage to evoke as many potentially useful ideas as possible, particularly ideas that I wouldn't have come up with on my own. If you think you've got some ideas that at least partially match the general pattern I'm describing, feel free to post them, and everyone here can use them as springboards for further elaborations thereof. :)
:PROPERTIES:
:Author: DataPacRat
:Score: 7
:DateUnix: 1453517448.0
:DateShort: 2016-Jan-23
:END:

**** I then propose that you run with a singularity that happened in the past - could be the 1600s (Isaac Newton was a fan of this 'chemistry' thing), or the 1920s (significant telegraph upgrades proposed by Nikola Tesla) or the 1970s (novel drug which vastly accelerates the clock speed of mammalian brains) or the 1990s (limited AI programmed to autonomously simulate pandemics eventually attempts to improve accuracy by releasing a sample of smallpox). Whatever, man. Werner von Braun builds a Nazi moonbase? Leonardo da Vinci designs working calculators, or submarines, or airplains? Henry Ford, in his twilight years, popularizes the uranium engine?

This isn't wholly related, just food for thought.
:PROPERTIES:
:Author: chthonicSceptre
:Score: 3
:DateUnix: 1453531098.0
:DateShort: 2016-Jan-23
:END:


** Proposal by MoltenSlag from a Skype conversation: Unanticipated social collapse, in spite of large resources and no external threats:

- [[https://en.wikipedia.org/wiki/Behavioral_sink]]
- That article really doesn't go into enough detail. The experiments this guy did were ****ing mental. Or, at least, the results were.
- [[https://www.youtube.com/watch?v=0Z760XNy4VM]] Here, this video should go over it pretty well. It's 8 minutes, but it's worth it. If you don't mind not sleeping.
:PROPERTIES:
:Author: DataPacRat
:Score: 7
:DateUnix: 1453516576.0
:DateShort: 2016-Jan-23
:END:

*** did you read this article on the behavioral sink: [[http://www.cabinetmagazine.org/issues/42/wiles.php]]

it's much better than the wikipedia page
:PROPERTIES:
:Author: Covane
:Score: 3
:DateUnix: 1453627309.0
:DateShort: 2016-Jan-24
:END:

**** Unrelated thought: Bringing in Hansonian-style signalling, assume that the "Beautiful Ones"' grooming was the result of an urge for signalling filling those mice' time, and then transplant the resulting social patterns onto humanity... and "writing fanfiction" could be considered a roughly equivalent method of signalling.

Yeesh; now I've got even /more/ incentive to dislike being called "beautiful".
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1453668935.0
:DateShort: 2016-Jan-25
:END:


** - Al-Hazard : the civilization was on the way to breaching Singularity-after-Singularity, ever-accelerating... until something went Wrong. Maybe it two Singletons-to-be planned too far around each other, maybe some completely unexpected event pulled the rug out from the main actors, maybe someone got just a little too much power, or maybe someone decided to put far too much [[https://en.wikipedia.org/wiki/Internet_refrigerator][computational power into a toaster]]. Maybe the Singularity itself was a dead end, and someone built an AI that paved over most of the planet to print out a proof Riemann hypothesis. But whatever there was is broken, and it's left its bones nearly everywhere.

- Not Dreamed of in Your Philosophy : There was a gap in the machine. Maybe the AI's maximum word size was too small, maybe too many things were excluded from self-modification, or maybe the folk that bootstrapped themselves up were a little odd. But while the overwhelming majority of the solar system has turned into computronium, or vacuum tubes, or paperclips, the Planet Earth has had a slightly odder time. Some portion of the people were Outside Context Problems to the Singularity itself, and what we see today are the remainder who survived from its escape from the crazies.

- The Closed Box : The folk who "have bad understandings of infinity" won, and the Singularity put 98% of the populace in a shoebox. In a couple billion years it might move to another planet, but there's no /hurry/.

- (On and Off) The Nature Preserve : The Singularity left people behind /intentionally/, either because the dominant groups excluded them, or because they were given the option to not sign up. There might even be communication or travel between those who went along and those who stayed behind, but the difference in timescales and desires and interests is so vast that it's pretty uncommon. The sky is filled with God's Dust, but Earth and the earth was inherited by the meek and the Amish.

- Scientific Progress Goes Thwack: There isn't a Singularity, not in the Kurzweil sense. The technology scales upwards, endlessly, and culture changes dramatically... but people stay the same. A time-displaced protagonist might not understand the language, or know the norms for how to look both ways before crossing the cyber-street, but nobody's turned into Tang and there's no equivalent to Languagev2.

- Glorious Future Nonawakening. The aesthetics of the future are /weird/, and weird in ways that people in the setting can't really handle or adapt to. Everyone's been uploaded into a universe where martial arts really do predict success at [[https://en.wikipedia.org/wiki/Shaolin_Soccer][wildly]] varying [[https://en.wikipedia.org/wiki/The_God_of_Cookery][goals]], or even an [[https://en.wikipedia.org/wiki/Kung_Fu_Hustle][internally meaningful enlightenment]], and people treat this [[http://forum.rpg.net/showthread.php?480387-setting-brainstorm-Kung-Fu-Transhumanism][the same as they do power in a world of scarcity]].
:PROPERTIES:
:Author: gattsuru
:Score: 3
:DateUnix: 1453606958.0
:DateShort: 2016-Jan-24
:END:

*** u/DataPacRat:
#+begin_quote
  Al-Hazard
#+end_quote

This is, at least generally, the approach I think I want to take. Working out what the remaining bones to pick over might resemble is the tricky part.

#+begin_quote
  The Closed Box
#+end_quote

At least to an extent, this is a detail that could be thrown in with many other pseudo-Singularities. At least, a post-human AI may have done something of the sort, and then Complicated Stuff happened derailing said billion-year plan.

... And now I'm imagining a near-human protagonist being handed a shoebox-sized artifact and being told, "Oh, and by the way, here's humanity. All hundred-billion or so instances. Take good care of it."
:PROPERTIES:
:Author: DataPacRat
:Score: 3
:DateUnix: 1453669304.0
:DateShort: 2016-Jan-25
:END:


*** u/deleted:
#+begin_quote
  (On and Off) The Nature Preserve : The Singularity left people behind intentionally, either because the dominant groups excluded them, or because they were given the option to not sign up. There might even be communication or travel between those who went along and those who stayed behind, but the difference in timescales and desires and interests is so vast that it's pretty uncommon. The sky is filled with God's Dust, but Earth and the earth was inherited by the meek and the Amish.
#+end_quote

This was the one I'm planning to use, should I ever grow a gajillion hours of spare time out of the side of my chest and write a wanky scifi novel.

The people of the future believed in /freedom/ in carefully thought-out senses, and left behind a large body of people on Earth who simply did not actively demand to be part of any technological whatever. Also, frankly, when the Nerd Rapture happened, the Nerds were kinda chauvinistic assholes anyway, so much of the population simply could not be persuaded to go along with them by any means.

This makes for interesting interactions between present-day Earth people and the eventual /children/ of the original Nerds, who not only grew up in a weirdly warped society, but wound up wondering why the hell there's all these mortals living pathetically anachronistic lives on Earth. Meanwhile, the mortals think that they were spared in a massive apocalypse that destroyed the sinful and arrogant assholes who were trying to usurp God, and that they live the only /truly meaningful/ lifestyles in the Solar System.
:PROPERTIES:
:Score: 1
:DateUnix: 1453666276.0
:DateShort: 2016-Jan-24
:END:


*** (On and Off)The Nature Preserve is a good explanation for what happened with /Firefly/. It's the most straightforward explanation for a situation where you can have inexplicably-powerful terraforming technologies /and/ a requirement to leave the Sol system. Otherwise, I would think that you could just use even the precursor terraforming technologies on either Mars or one of the Jovian or Saturnal moons while you re-terraform Earth That Was.

I actually asked [[/u/cstross]] about Firefly/Accelerando crossovers. He rightfully ignored me. The world just isn't ready for Aineko/Jayne shipping.
:PROPERTIES:
:Author: mycroftxxx42
:Score: 1
:DateUnix: 1453864243.0
:DateShort: 2016-Jan-27
:END:

**** I don't generally watch TV/film SF -- it's too hard on my teeth (my dentist loves the splintering effect all the plot/world-building flaws have on my dental work).

A friend once forced me to watch an episode of "Firefly". That's an hour of my life he owes me. The only TV SF I've been able to follow in the past decade has been the 'toons -- Futurama, Invader Zim, and so on. (They're comfortably far across the other side of the uncanny valley of realism from where my imagination lives.)
:PROPERTIES:
:Author: cstross
:Score: 2
:DateUnix: 1453916253.0
:DateShort: 2016-Jan-27
:END:

***** I wholly agree with your appraisal of the Firefly universe's worldbuilding flaws. Those flaws are the reason that I had to try using your setting as a possible explanation. It was literally the only source of possible magical terraforming techniques and a completely unusable inner solar system that I knew of.

I do not, in any way, think that Accelerando was actually /meant/ to fit with Firefly. Your work stands well enough on its own, it just happens to be big enough that you could probably fit a bunch of Space Amish and Trekkies onto a starship with the last of the outer system's supply of idiotballium and chuck it away in the background of the MacX family drama.
:PROPERTIES:
:Author: mycroftxxx42
:Score: 1
:DateUnix: 1453917448.0
:DateShort: 2016-Jan-27
:END:


** There are some potential issues that could become important when Max Plank's "Science advances one funeral at a time," is fully considered in this context. If you either reduce the rate of funerals, or increase the rate of revolutions, I think you would run into the same problem. Economic pressures would be brought to bear against the existing culture of Science-with-a-capital-S, and presumably the culture would fall apart first.

There's little (but not zero) reason to suppose that the resulting cultural collapse would leave the parts of science that we like, mass publication and sharing of technique, popular and common after the idea of a "Scientific legacy" was disrupted. A less-collegiate atmosphere itself could retard advancement.
:PROPERTIES:
:Author: mycroftxxx42
:Score: 2
:DateUnix: 1453590619.0
:DateShort: 2016-Jan-24
:END:


** [[http://www.prequeladventure.com/2011/03/236/]]

Later the orc in question is shown with dozens of four-leaf clovers pinned to his tunic.
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1453633924.0
:DateShort: 2016-Jan-24
:END:

*** Hey! I made that suggestion! Somone remebers somehting I contributed! :D
:PROPERTIES:
:Author: ArmokGoB
:Score: 1
:DateUnix: 1453642704.0
:DateShort: 2016-Jan-24
:END:

**** Yes, I think we had this conversation before. I agree with the first half, that luck would compound on itself (unless metaluck is specifically forbidden) but not with the second half that this would somehow be unlucky.
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1453687159.0
:DateShort: 2016-Jan-25
:END:

***** yea, that part is a lot more hypothatical.
:PROPERTIES:
:Author: ArmokGoB
:Score: 1
:DateUnix: 1453715917.0
:DateShort: 2016-Jan-25
:END:


** What about an anti-singularity singularity? A superintelligence whose apparent goals add up to being the maintenance of the approximate status quo?
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 1
:DateUnix: 1453593166.0
:DateShort: 2016-Jan-24
:END:

*** Wouldn't that be equivalent to rolling a 17 on the [[http://www.sl4.org/archive/0310/7163.html][table]]?
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1453599700.0
:DateShort: 2016-Jan-24
:END:

**** Essentially, with the long-term consequence of future singularities becoming impossible.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 1
:DateUnix: 1453603552.0
:DateShort: 2016-Jan-24
:END:


**** Well, there is stross's variant, where due to flt and time travel being the same thing, is mostly preserving the history that leads to its creation, and otherwise stopping people from playing with time travel.
:PROPERTIES:
:Author: clawclawbite
:Score: 1
:DateUnix: 1453709871.0
:DateShort: 2016-Jan-25
:END:


** Can someone explain why they reported this thread for removal by mods?
:PROPERTIES:
:Score: 1
:DateUnix: 1453665885.0
:DateShort: 2016-Jan-24
:END:

*** ... Well, /I/ didn't do it.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1453668981.0
:DateShort: 2016-Jan-25
:END:


** You should really binge read Artie

[[https://m.fictionpress.com/s/3223802/1/Artie]]

It has a /lot/ of great story telling about preventing the wrong singularity, and does so in many different ways. It's also criminally underrated.
:PROPERTIES:
:Author: Stop_Sign
:Score: 1
:DateUnix: 1453959487.0
:DateShort: 2016-Jan-28
:END:
