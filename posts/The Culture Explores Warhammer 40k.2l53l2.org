#+TITLE: The Culture Explores Warhammer 40k

* [[http://archiveofourown.org/works/649448/chapters/1181375][The Culture Explores Warhammer 40k]]
:PROPERTIES:
:Author: andor3333
:Score: 14
:DateUnix: 1415001201.0
:DateShort: 2014-Nov-03
:END:

** Is any of this going to be fun for someone (i.e., me) who knows The Culture but is entirely unfamiliar with Warhammer 40K?
:PROPERTIES:
:Author: MoralRelativity
:Score: 3
:DateUnix: 1415011556.0
:DateShort: 2014-Nov-03
:END:

*** As someone familiar with both, I tried this a while back and gave up several chapters in - it wasn't compellingly written or interesting, I didn't care about any of the characters so it was really tough to care about anything that was happening. In Iain Banks' Culture books, I always cared about the main characters. Even if I did not agree with their goals, methods, etc - as in, for example Use of Weapons - I still felt they were fully realized characters and was interested in how their stories played out. In that portion of the Warhammer fiction that's I've read that is not terrible, the same has also been true. However, in what I read of this fic before giving up I didn't care about basically any of the characters at all so the fic as a whole failed to engage me.
:PROPERTIES:
:Author: Escapement
:Score: 6
:DateUnix: 1415021648.0
:DateShort: 2014-Nov-03
:END:

**** It's really more of a thought experiment than a story, but it's a fun one.

There's an assumption in the WH40K universe that a culture as utopian as the Culture could never arise- the universe is presumed to be so cruel that any civilization would be crushed into a state of dystopia. To me, a fully formed Culture struggling against that assumption is an interesting enough premise that it works even in the absence of actual characters or human drama, though I can see how some would feel differently.
:PROPERTIES:
:Author: artifex0
:Score: 4
:DateUnix: 1415035653.0
:DateShort: 2014-Nov-03
:END:

***** Thanks.
:PROPERTIES:
:Author: MoralRelativity
:Score: 1
:DateUnix: 1415049697.0
:DateShort: 2014-Nov-04
:END:


**** Thanks, that's very helpful.

Is there any The Culture fanfic which you would recommend? (I haven't found anything good yet, but then this was only the second I had come across.)
:PROPERTIES:
:Author: MoralRelativity
:Score: 2
:DateUnix: 1415049779.0
:DateShort: 2014-Nov-04
:END:

***** The harry potter cross [[https://www.fanfiction.net/s/3983128/1/Culture-Shock][Culture Shock]] came closest to not sucking of the culture fanfic I've read.

Spoilers for the fic follow hereafter:

.

.

.

.

.

.

.

.

Problems with it:

- Diziet Sma feels a bit Mary-Sue-ish, as does Harry. Dumbledore got run over a bit in the fic.

- The story was just getting to interesting bits when it stopped updating. At this point, six years since the last update, it's probably deader than Iain Banks and is unlikely to ever be continued.

- The canon plot was about to get hit by a truck - at the end of the last posted bit, Minds were about to absorb all magical knowledge meaning they would be likely to be uber-Hermiones and get all the plot details that are in any magic (or other) textbooks and, and they were also Uber-Harrys and get all the plot details that can be heard by being invisible (or operating flea-sized surveillance drones) and also have infinite carrots (in terms of wealth and the like) and also have infinite sticks (in terms of military power). So any continuation of the plot would have to cope with the fact that any villains are about as badly overmatched as it's possible to be - and also the principal characters (Harry, Diziet, Dumbledore, et al) are basically irrelevant in comparison the the Culture's temporal power.

- Why did the Culture stopped surveilling the Earth apparently after finding Harry there and seeing the whole flying motorcycle thing? I would think there would be 10+ major ships around Earth pretty fast and for the duration "until they worked out what was going on down there".

Good bits:

- Harry felt genuinely different as raised by the Culture, and I liked the Diziet/Harry relationship in the fic - it generated and used characters I cared about well.

- Dumbledore's (and others') perplexity was hilarious at times

- Writing quality was reasonably high / technically proficient.

- I liked the Troll 'fight' scene (if one can call it that) a lot
:PROPERTIES:
:Author: Escapement
:Score: 4
:DateUnix: 1415051688.0
:DateShort: 2014-Nov-04
:END:

****** Thanks so much for taking the time to share in such detail. I really appreciate it. I'll give it a go.
:PROPERTIES:
:Author: MoralRelativity
:Score: 1
:DateUnix: 1415094088.0
:DateShort: 2014-Nov-04
:END:


** I went into this with no prior knowledge about The Culture and some about the WH40K and so far it seems ok. Not terribly engaging, but a decent procrastination aid.

I have a question though. How come the citizens of the Culture are so dumb? What in-universe explanation is given for this? How have they managed to develop that level of technology when they constantly make blunders a modern human of average intelligence could avoid?

From what I read so far the Culture seems to be a bit of a dystopia in its own right. An eternity of wondering the stars with ships full of stagnant human level intellects enforcing moral preference of majority on anything in sight with magic tech. Quite chilling.

And no matter how much you capitalise the M in Mind, they do not display any cognitive heavy lifting in story. Some good processing power is shown, but they apparently have about the same level of cognitive ability as a human (so basically they are dumb really fast). This scale of intelligence thingy that pops up from time to time is confusing as well.

Should I even try reading the source material or am I just going to be pissed off at how nothing ever went FOOM, even though the tech is apparently there?
:PROPERTIES:
:Author: AugSphere
:Score: 6
:DateUnix: 1415049634.0
:DateShort: 2014-Nov-04
:END:

*** They consciously avoid going full FOOM, in that the noted endpoint in universe is a process called 'Subliming' which is essential Ascension. There are extant Sublimed beings, group minds, and in some cases species which exceed the power of the Culture in their own home universe.

They have numerous other cultural hangups. There is a general distaste on living for over about 500 years, though some choose to live forever and are not prevented from doing so. Small groups have chosen to Sublime or to come together in group minds.

It's actually pretty complicated.
:PROPERTIES:
:Author: JackStargazer
:Score: 3
:DateUnix: 1415051875.0
:DateShort: 2014-Nov-04
:END:

**** So, basically, it's a story about technophobic retrogrades left behind by the ones that did go FOOM. I guess that explains it.

It just seems silly that they are still on modern real world human level intelligence. I could understand the adaptive use of different levels of processing power and differently optimised cognitive systems in post-singularity society, but staying a baseline human just seems comical.

Thanks for the reply, though. It clarified the situation a bit.
:PROPERTIES:
:Author: AugSphere
:Score: 4
:DateUnix: 1415052578.0
:DateShort: 2014-Nov-04
:END:

***** Not exactly. When people Sublime they enter some sort of extradimensional space and rarely interact with the physical world afterwards. Most of them leave the material world behind forever. The Minds are worried that Sublimation will somehow strip someone of their identity so they are leery of it.

They also view Sublimation as the easy way out and selfish. By leaving the material universe, you are leaving the younger inexperienced civilizations to fend for thenselves.

Also Culture biological citizens can enhance and change themselves however they want. I think genius level intellect is standard
:PROPERTIES:
:Author: okaycat
:Score: 3
:DateUnix: 1415055740.0
:DateShort: 2014-Nov-04
:END:

****** I can understand the reasons for avoiding this nebulous "Sublimation". But why not just self improve without subliming? What bugs me, is that the default setting for citizen intelligence is "human level" instead of something better. Supposedly they know how to reach higher, and yet they don't.

It's a tricky subject, but my position would be this: first uplift the intelligence in question to your best capabilities (without Subliming), while preserving their values, then let the newly enhanced individual work out their best course of action themselves. If they decide that they are better served by being as dumb as a human, they can self-modify accordingly. Just leaving them at the default level seems a bit cruel and callous. It's like leaving an addict to his own devices. Sure from his current point of view, he may not even want to be cured, but it's not like it would be impossible to go back if you magically cured him. It's pretty fucking tough to read about the enhanced individuals (Minds) lording it over the whole society of average humans without uplifting them.

Are the Minds in source material actually smart though? Or do they just have a bunch of processing power strapped to human-level cognitive processes? I've been going on the assumption that the Minds are in fact more intelligent, and not just faster.

Edit:

#+begin_quote
  Also Culture biological citizens can enhance and change themselves however they want. I think genius level intellect is standard
#+end_quote

Somehow missed this part of the post. Still, current human level of genius is a pretty low bar to aim for. Compared to what they could achieve it's like changing the setting from "monkey" to "smart monkey" on the scale of human intelligence, sure it's higher, but not at all a significant improvement. My point about going as smart as possible at least for once in their lives still stands, I think.

Edit2:

#+begin_quote
  Culture biological citizens
#+end_quote

Wait, what about the rest of them? I hope this specifier is there because non-biological citizens self-modify as a matter of course, and not because only the biological ones are allowed to self-modify.
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1415092277.0
:DateShort: 2014-Nov-04
:END:

******* Even if you have no interest in the books, you might be well-served by reading the wiki page for [[http://en.wikipedia.org/wiki/The_Culture#Citizens][the Culture]]. It goes into some detail about what the Culture actually comprises.

Citizens (biological or artificial) are free to modify themselves pretty much as much as they want - but the Culture is hedonist, and so more intelligence is not really the be-all end-all of their lives.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1415134698.0
:DateShort: 2014-Nov-05
:END:


******* u/VorpalAuroch:
#+begin_quote
  first uplift the intelligence in question to your best capabilities (without Subliming), while preserving their values,
#+end_quote

This is considered impossible. In-universe, a "Mind" is an entity capable of Subliming by itself. The Culture makes at least one Mind per ship, and of the Minds created, nearly all Sublime on the spot. All those who remain are in some way slightly deranged; it is believed that any psychologically stable entity capable of Subliming will inevitably do so.

And all Culture citizens can modify their own intelligence however they like; most choose to stay at the same level as the rest, but this, like most other self-modification consensuses in the Culture, has long-term fads, and it may go up and down from century to century.
:PROPERTIES:
:Author: VorpalAuroch
:Score: 2
:DateUnix: 1415229893.0
:DateShort: 2014-Nov-06
:END:

******** I see. There should still be some level of intelligence above that of a human to which one can safely raise it, right? So you raise it to that level instead then.

My gripe is not that it's prohibited or impossible to enhance yourself, it is the fact that superintelligence is not the norm.

In this fanfic, in particular, everyone is somehow really stupid. We get a peek into the thought processes of the Minds and it's all standard human-level dumbassery in there. I won't even go into the minds of standard citizens, since it looks like they are supposed to be human-level for some reason. Whether this is a fault of this fanfic, or the Culture canon I don't know yet, but the enhanced intellect is not shown at all. We are told they are smart, but shown how they act in all the stupid ways one would expect superintelligence to avoid.

Seriously, they are capable of intergalactic travel, but the median (demonstrated) intelligence level in their society is that of a 20 century Earth human. Am I the only one, who is weirded out by that?
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1415269580.0
:DateShort: 2014-Nov-06
:END:

********* u/VorpalAuroch:
#+begin_quote
  I see. There should still be some level of intelligence above that of a human to which one can safely raise it, right? So you raise it to that level instead then.
#+end_quote

As I said:

#+begin_quote
  And all Culture citizens can modify their own intelligence however they like; most choose to stay at the same level as the rest, but this, like most other self-modification consensuses in the Culture, has long-term fads, and it may go up and down from century to century.
#+end_quote

Banks clearly does not agree with EY's notion in Fun Theory that the amount of fun available increases proportional to intelligence, and the humanoids of the Culture are looking for fun, not effectiveness. Superintelligence to a level significantly below that of the Minds would not particularly help them achieve their goals.

#+begin_quote
  Whether this is a fault of this fanfic, or the Culture canon I don't know yet, but the enhanced intellect is not shown at all. We are told they are smart, but shown how they act in all the stupid ways one would expect superintelligence to avoid.
#+end_quote

I disagree. Examples?
:PROPERTIES:
:Author: VorpalAuroch
:Score: 1
:DateUnix: 1415301716.0
:DateShort: 2014-Nov-06
:END:

********** u/AugSphere:
#+begin_quote
  I disagree. Examples?
#+end_quote

The Minds are surprised way, way to often. Presumably, they experience surprise when some prediction they were fairly confident in turns out to have been wrong. And they are constantly surprised by things a human could have seen coming. Just one concrete example for now:

#+begin_quote
  We have informed the Orks on the world of this matter and advised them to move to a different planet, even offering transportation assistance. *Unexpectedly, the Orks instead appear to be preparing for battle.*
#+end_quote

This is after several weeks (if not months) of studying the society and culture of Orks. A random modern human could have predicted this with ease and the damn superintelligence is surprised!

There are many examples of the short-sightedness and bad planning from the Minds in this story. It's almost as if they are doing whatever pops into their minds at any moment without any coherent unifying vision or strategy in mind. I could supply some more examples, of course, though I don't see much point. Your reading may well be different from mine and I don't see any particular value in convincing you that the Minds are stupid in this fanfic. It may diminish your enjoyment of the story and this would be pointless and mean. I have nothing to gain from it.
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1415346511.0
:DateShort: 2014-Nov-07
:END:


******* u/JusticeSlut:
#+begin_quote
  It's a tricky subject, but my position would be this: first uplift the intelligence in question to your best capabilities (without Subliming), while preserving their values, then let the newly enhanced individual work out their best course of action themselves.
#+end_quote

if you get a doctor that says it's all in your head then they're moralist or RWA. if they give up because they traditionally don't care about what's in your head, they're RWA. if they sneer at you, narcissist. and if they pat you on the head then moralist
:PROPERTIES:
:Author: JusticeSlut
:Score: 1
:DateUnix: 1417036972.0
:DateShort: 2014-Nov-27
:END:


***** Hmm, I think if you actually read the books (not fanfiction) you'll see that "technophobic" is the wrong way to describe them. The reason there's lots of real-world level humans (besides the fact that real-world level humans usually make for better stories) is that life is already good enough that any sort of personal upgrade won't make it any better. Their level of technology is so high that they just don't see any point in further advancement so a lot of the focus (of the society and of the novels) is on philosophy and ethics and social interaction and technology is just a facilitator. Their technology isn't unique or even the close to being the best in their universe but what sets them apart is the stability and longevity of their society.
:PROPERTIES:
:Author: starfries
:Score: 3
:DateUnix: 1415069363.0
:DateShort: 2014-Nov-04
:END:

****** The out-of-universe reasons for human level protagonists are obvious. I just can't help but hope to find a good story about a truly superhuman protagonist. It's near impossible to write for a human author, but the hope remains.

As for their level of tech, I'm not conserned about technological advancement that much, what bugs me is the intelligence level inequality (see my reply to [[/u/okaycat]])
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1415092663.0
:DateShort: 2014-Nov-04
:END:

******* I kind of see the concern but I think you're overvaluing intelligence, treating it as an end in itself. The Culture is all about quality of life and personal freedom and raw intellect doesn't have much bearing on that. Arguably being a Mind is actually something of a burden because of the increased responsibility. In a society like ours we have all sorts of concerns like how to survive and grow and become successful and we need as much intelligence as we can get but the Culture has already solved most of these and the only real question left is how not to get bored.

As for uplifting humans: first of all, personal choice is a big part of Culture society so uplifting someone (at least a sentient being) without their permission is very poor form, even if you think you're doing them a favour. Unless they're a complete degenerate, any sort of mind alteration (and even probing, in most cases) is only done with consent. Also, I'm not sure the uplift you describe is even possible, any more than you could make a human brain out of an earthworm's while "preserving its values". The vast majority of the new brain would be something fabricated during the uplift process, and going back down to its original level would entail destroying a huge amount of (newly-created) personality. Still, humans always have the option of having their consciousness integrated into an existing Mind's and a lot of them have done just that (or transferred to artificial bodies, or moved into a virtual realm, or any number of other things. The stories just focus on people with human bodies and human capabilities.) Or you could just ask a Mind to lend you some perspective and enough processing power to understand Mind-level philosophy.

That said, why don't you just read the books? All this stuff is covered and I think your misunderstandings are a result of trying to comprehend the world solely through second-hand accounts. They're good books and any explanation I give you will be imperfect and filtered through my own opinion so if you're as interested as you seem to be, just get it from Banks himself.
:PROPERTIES:
:Author: starfries
:Score: 2
:DateUnix: 1415095666.0
:DateShort: 2014-Nov-04
:END:

******** u/AugSphere:
#+begin_quote
  I think you're overvaluing intelligence, treating it as an end in itself.
#+end_quote

I do value intelligence in itself a great deal, it's true. But I think this is a healthy position, the alternative is refusing to improve just because you are happy at your level and that is akin to wireheading for me. I do not condone it.

#+begin_quote
  Arguably being a Mind is actually something of a burden because of the increased responsibility.
#+end_quote

It would not be such a burden if everyone was that smart. The question is, of course, what would a bunch of superintelligencies do with their time and resources, and I would rather like to read a story exploring that.

#+begin_quote
  As for uplifting humans: first of all, personal choice is a big part of Culture society so uplifting someone (at least a sentient being) without their permission is very poor form, even if you think you're doing them a favour.
#+end_quote

As I said, a tricky subject. I fall on the side of doing a reversible action so that the mind in question can decide for itself using optimal decision algorithms. The Culture seemingly falls on the side of letting the addict drug himself into a coma as long as it's his personal choice. I think that kind of freedom works only when you have a society of truly rational agents, otherwise it really is sometimes better to force things on individuals for their own good.

#+begin_quote
  Also, I'm not sure the uplift you describe is even possible, any more than you could make a human brain out of an earthworm's while "preserving its values".
#+end_quote

Hmm. Sure, it would be a challenge, but it's not like there is scarcity in The Culture, so someone could work out a way to do this, unless there is some fundamental reason why this cannot be done, and I don't see that.

#+begin_quote
  That said, why don't you just read the books? All this stuff is covered and I think your misunderstandings are a result of trying to comprehend the world solely through second-hand accounts. They're good books and any explanation I give you will be imperfect and filtered through my own opinion so if you're as interested as you seem to be, just get it from Banks himself.
#+end_quote

Hey, It's not like the opinion of Banks is somehow privileged compared to your own, death of the author and all that. But you're right, I'll give his books a go myself after all. I just hope they don't hand wave all the potential away for the sake of having relatable protagonists.
:PROPERTIES:
:Author: AugSphere
:Score: 2
:DateUnix: 1415097632.0
:DateShort: 2014-Nov-04
:END:

********* On a personal level I do agree with you and I'd like to be as smart as I can be, but I'm not sure it's self-evident that this is a good thing or a meaningful pursuit. Even if you look at our society, one that emphasizes growth a lot more than the Culture, you find that a lot of people value things like friends, family, shared experiences and those sorts of things above pure mental self-improvement and studying by yourself in the library. And who's to say that's a bad thing? I think it's a false dichotomy to say that you must engage in exponential self-improvement to lead a meaningful existence and labeling the alternative as solipsistic masturbation. It's very telling that death is an accepted part of Culture society even though immortality is easily attained because that sort of thing would be unthinkable to a society that values progress and improvement above all.

To me, self-improvement is more of a means to an end, and you hope that the increased understanding you attain by becoming more intelligent helps you find a pursuit that /is/ meaningful. Otherwise, what's the point? You convert all matter in the universe into computing hardware for your ginormous brain and then what? It's just as meaningless as the human who lives and dies a human. I think you'd end up a lot like a Culture Mind, and most of your vast intelligence goes towards thinking up things to do to occupy your vast intelligence.

Whether or not you buy that, let's pretend for a moment you did and set aside the goal of intelligence for its own sake. What value would a Culture citizen derive from massively upgrading his or her mind? Any question that requires superhuman brainpower to solve can be answered immediately through neural lace by a Mind. Personal danger is immediately recognized and deflected or undone by the Mind. You already get a vote, worth as much as a Mind's, so what else is there to be gained? On the other hand, your personality and everything that's "you" is nearly indestructible because it's small enough that you can have nigh-unlimited backups and versatile because you can insert it into just about any sort of physical form and you can indulge in very human things like falling completely in love and not have to worry about the effect that might have on your sanity. As a Mind the stakes are much higher because if you mess up, there's no one to rescue you. And people might die, or worse. To me, it's the difference between being an adult and being a kid... and I bet there are a lot of adults who wish they could be kids again.

Regarding the earthworm thing: it's not a technical challenge, it's the fact that in order to make a human brain you need to fill it up with /something/. There just isn't enough /stuff/ in an earthworm's brain to fill a human brain, so the majority of this human's personality will have to be created on the spot or derived from some other source. I mean, what kind of movies does an earthworm like? Who does this earthworm vote for? And at this point the brain is that of a human, even if you remove the original, tiny earthworm bit from it, so if you downgrade by discarding the stuff you added you are essentially killing a human.

#+begin_quote
  The Culture seemingly falls on the side of letting the addict drug himself into a coma as long as it's his personal choice. I think that kind of freedom works only when you have a society of truly rational agents, otherwise it really is sometimes better to force things on individuals for their own good.
#+end_quote

Ooh boy... not sure I wanna touch this one. Although... if you're going to force him to decide according to an algorithm, why don't you just run the algorithm for him and tell him the answer?

Lastly, I'm as interested as you in reading a book that does superhuman intelligence well. Unfortunately it might be asking the impossible. Many people have tried (and not just with AI, but with superheroes, aliens, etc.) and I don't know if I've ever seen one that's really convincing because, well, human writers. And would you really recognize it when you see it?

I hope you do read the books. I'm glad you value my opinion but I make no promises that anything I wrote even resembles anything written by Banks and even if it does, he's a far better writer than I am.
:PROPERTIES:
:Author: starfries
:Score: 2
:DateUnix: 1415103451.0
:DateShort: 2014-Nov-04
:END:

********** u/AugSphere:
#+begin_quote
  What value would a Culture citizen derive from massively upgrading his or her mind?
#+end_quote

That is the pertinent question, is it not? I think it should be answered by the most capable mind that has an optimal representation of the subject's values as opposed to the default intelligence the subject has at the moment. My key point here is that, if it's possible to tune yourself down in intelligence in case you decide it's the best choice, then you ought to consider this choice while employing the most powerful feasible cognitive system available. The choice of what to do with oneself is of immense importance in post-scarcity society after all. If it's not possible to fluidly tune your own cognitive power then I would consider the matter more carefully, but as it is, with the magical level of tech in The Culture, I just don't see any reasons not to do this.

#+begin_quote
  Although... if you're going to force him to decide according to an algorithm, why don't you just run the algorithm for him and tell him the answer?
#+end_quote

For what I envision here the difference would be minimal in the end. In one case you uplift the individual and he decides what to do, in the other, the very same optimal decision, that he himself would make, is calculated for him. I think the first one is a bit more polite, but it's not really a deal-breaker in my mind. The question of guaranteeing the trustworthiness of all the systems involved is a separate matter here, but it case of The Culture it's not terribly pertinent from what I can see.

#+begin_quote
  Regarding the earthworm thing: it's not a technical challenge, it's the fact that in order to make a human brain you need to fill it up with something. There just isn't enough stuff in an earthworm's brain to fill a human brain, so the majority of this human's personality will have to be created on the spot or derived from some other source.
#+end_quote

An earthworm makes for a bit of a tricky analogy here. If we are uplifting an earthworm (and we might as well go for the truly representative example of this kind and uplift a simple replicator), then, sure, we'll have a difficult time choosing a set of values for it, since it does not really have any in it's base form. I've implicitly used the assumption that there is no such paradigm shift when uplifting to superintelligence level from human one. There are no values 2.0, for which humans don't have analogues. I think it's human-understandable values all the way up.

#+begin_quote
  And would you really recognize it when you see it?
#+end_quote

You're not implying the authors have been secretly writing superintelligent agents for years and nobody recognised them, are you? That would be pretty hilarious. On a more serious note, yeah it would take one hell of an author. Maybe Eliezer will try his hand at it, when he's finished with HPMOR. He, apparently, likes a challenge.
:PROPERTIES:
:Author: AugSphere
:Score: 2
:DateUnix: 1415106560.0
:DateShort: 2014-Nov-04
:END:

*********** u/starfries:
#+begin_quote
  I think it should be answered by the most capable mind that has an optimal representation of the subject's values as opposed to the default intelligence the subject has at the moment.
#+end_quote

A reasonable viewpoint, and the only thing I would say is that I don't think it's necessary for a Culture citizen to uplift themselves in order to do this; they can just plug themselves into a Mind and have the Mind do all the relevant simulations and whatnot while showing you the results. I suppose it requires you to trust the Mind in question, but I think a massive uplift presents its own dangers, like the newly created Mind developing a sudden case of self-preservation and refusing to go back to human form even if objectively the human (and everyone else) was better off that way, or a human with dangerous personality traits that are magnified by the transition.

#+begin_quote
  I think the first one is a bit more polite, but it's not really a deal-breaker in my mind.
#+end_quote

/shrug/

Incompatible philosophies, I suppose. The Culture regards the freedom to choose very highly and so everyone has the right to make stupid decisions (as long as no one is hurt). I will point out, though, that there's no rush to make a decision when it comes to an uplift because you can quite happily exist indefinitely as a Culture human until you decide it's time for a change. Spending a few extra decades as a human when your true calling was Mindhood is no big deal when you're functionally immortal.

#+begin_quote
  I think it's human-understandable values all the way up.
#+end_quote

Well, I can't refute this since we have no evidence but my gut feeling is that there value systems that are simply incomprehensible to humans. And I'm not sure direct extrapolation would make for a very good result because if you create, say, an adult with the values and morals of a baby you end up with a pretty poor excuse for an adult...

Anyways, if you do come across a well-written character of that sort I'd love to know.
:PROPERTIES:
:Author: starfries
:Score: 2
:DateUnix: 1415109336.0
:DateShort: 2014-Nov-04
:END:

************ So far my reading is that people in The Culture have fallen into a sort of a cultural trap, in that they came to greatly prefer exploitation to exploration as far as living satisfying lives goes. This has created a stagnant pleasure cruise like environment where social expectation prevents a vast majority of individuals from ever trying to reach beyond their familiar type of existence and even the few who somehow achieve a significantly enhanced level of intelligence spend their time babysitting the rest of the citizens and perpetuating the same stagnant society.

I think that the thing, that bugs me the most, is that The Culture has apparently reached the apex of its development. There is a sense that no more is to be gained and everything is as perfect as it will ever be already. An impression that, should you skip a hundred years and take a look at what has changed in The Culture, you'd find nothing of note. It would certainly be a nice place to live for a lazy gentleman like myself, but as far as civilisational dead ends go, it's pretty scary.

I shall see if my understanding of this culture changes after I have read the source. It's certainly intriguing enough.

Thank you for this discussion. It was very satisfying and I have enjoyed it.

#+begin_quote
  Anyways, if you do come across a well-written character of that sort I'd love to know.
#+end_quote

Will do.
:PROPERTIES:
:Author: AugSphere
:Score: 3
:DateUnix: 1415112825.0
:DateShort: 2014-Nov-04
:END:

************* I'd say that's pretty accurate. I remember reading that very little happens in the Culture itself, so most of the stories tend to focus on other civilizations and the Culture's dealings with them. The Contact section is where most of the interesting, explorer types of people end up anyways.

I guess it's a matter of opinion whether it's a trap or a voluntary stopping point. To me it's like a town built at the foot of a mountain; the Last Homely House for organic creatures. People are free to climb and risk the danger and glory of Sublimation but for most it's a pleasant enough place to live out their lives. There is still some development in the Culture (technology improves a bit over the course of the series) but I get the impression that it's about as advanced as a civilization can become while still supporting human-level citizens with meaningful agency (and in that sense about as advanced a civilization as an author can write without diving into the murky depths of transhuman main characters). Still, their society isn't above criticism, even in the books themselves.

I'm glad we had this discussion too. You raised some interesting points that I hadn't thought about before, so thank you for that.
:PROPERTIES:
:Author: starfries
:Score: 2
:DateUnix: 1415115653.0
:DateShort: 2014-Nov-04
:END:


************* I have to add though - Cultural specifics aside, I still don't understand what you find objectionable about a stable society. Would you prefer one that never reached the Culture's level but was constantly improving? What if the Culture was fully posthuman but just as stable?
:PROPERTIES:
:Author: starfries
:Score: 2
:DateUnix: 1415118342.0
:DateShort: 2014-Nov-04
:END:

************** u/AugSphere:
#+begin_quote
  Would you prefer one that never reached the Culture's level but was constantly improving?
#+end_quote

That's the thing though. If it's consistently improving, then it will surpass The Culture sooner or later and I would indeed prefer the non-stagnant one.

There is always something more to do in the universe to make it a better place (whatever "better" means in accordance with your values). If your civilisation is stagnating on pleasure cruise ships instead of plunging into a singularity with all their might, then, somewhere, the universe you could otherwise improve remains suboptimal. Be it children starving to death or stars burning valuable fuel to heat up dead rocks or some civilisation being insufferably happy and peaceful (if you happen to have values of a cartoon villain for some reason) there is something you could have changed if only your civilisation was still improving.

Stability is a surrender. It's a decision to leave the unknown children to starve, when you could have done more.
:PROPERTIES:
:Author: AugSphere
:Score: 3
:DateUnix: 1415119935.0
:DateShort: 2014-Nov-04
:END:

*************** I meant consistently improving in an asymptotic way :p Suppose that the calculations for successive improvement take longer for each iteration so that your technology follows an atan curve or something. Is it worth continuing to run on this treadmill if your advancement is fundamentally limited?

Some more hypotheticals - what if someone else whom you trust as sharing your values has hit the singularity before you? What do you do when you've reached a level where no further improvement is possible? What if further self-improvement directly conflicts with your values (if for example you need to destroy another civilization to continue). Is there a need for self-improvement if you /can't/ make the universe better, or when you already have all the capabilities you need?
:PROPERTIES:
:Author: starfries
:Score: 2
:DateUnix: 1415121392.0
:DateShort: 2014-Nov-04
:END:

**************** Well, if you are dealing with some kind of sigmoidal improvement, then, of course, sooner or later the diminishing returns will make the decision to invest in further improvement irrational as far as world optimisation goes, if you have any other values at all.

What you appear to be asking, is how much I value self-improvement and the answer is: I don't really know. I've struggled to formulate a coherent value system for myself for some time and so far I have not really achieved much success. It certainly has some inherent value for me, apart from serving as means to some other end, but would I destroy another civilization to continue to self-improve? Who knows. Would probably decide on a case by case basis by comparing utilities of the alternatives, I guess.
:PROPERTIES:
:Author: AugSphere
:Score: 2
:DateUnix: 1415122812.0
:DateShort: 2014-Nov-04
:END:

***************** Fair enough. I'm glad your answer is more nuanced than "self-improvement above all". And now I really have to get some work done. Thanks for the discussion.
:PROPERTIES:
:Author: starfries
:Score: 2
:DateUnix: 1415123057.0
:DateShort: 2014-Nov-04
:END:


*** u/artifex0:
#+begin_quote
  From what I read so far the Culture seems to be a bit of a dystopia in its own right. An eternity of wondering the stars with ships full of stagnant human level intellects enforcing moral preference of majority on anything in sight with magic tech. Quite chilling.
#+end_quote

Life in the Culture isn't really touched on in this fanfic, but what makes it a utopia in the novels is the degree of personal freedom and the complete lack of scarcity. Some Culture ships are worlds in and of themselves, with billions of inhabitants and enough room for every single one to live in a mansion surrounded by gardens if they chose to. Since Minds are willing to do all the work, humans and drones can choose to either help out, spend all their time creating artwork, pursuing hobbies and games, and having passionate love affairs, or do useful things outside of the Culture, usually working with Contact or Special Circumstances.

They're hedonistic and not extremely ambitious, but not really stagnant either- the Culture produces a lot of lower-case 'c' culture, and while it's implied that their level of technology is nearly as advanced as it can get without them Subliming to another plane of existence, they do make technological advancements from time to time.

In general, the Culture actually doesn't force its morals on other civilizations unless it's threatened. It certainly doesn't have a Prime Directive, but it engages in ordinary diplomacy with less advanced civilizations, along with the occasional back-room deal or clever bit of political intrigue to stop a genocide or stall an atrocity. Of course, when directly threatened, it responds very aggressively.

In the novels, the intelligence of the Minds is shown by them doing things like having individual conversations with millions of people at once, pulling off complicated Xantos gambits and conspiracies, or being able to imagine things far outside the scope of human capabilities. Not seeing a whole lot of the clever plans in this fanfic, but then they are trying to puzzle out this universe without the benefit of a sourcebook.
:PROPERTIES:
:Author: artifex0
:Score: 3
:DateUnix: 1415053385.0
:DateShort: 2014-Nov-04
:END:

**** [[http://lesswrong.com/lw/qk/that_alien_message][Some say]], that a true super-intelligence would create general relativity solely from observing three frames of a video of an apple falling to the ground. I'm not entirely certain of that level of cognitive ability, but you've gotta admit, that handing their tech out to practically everyone they meet is a bit of a dumb move. Especially when they are somehow surprised someone tries to steal it later. I don't care how much hedonistic eudaimonia you cram in your space arcology, if the citizens are approximately as dumb as me, then something has gone terribly wrong in your utopia.
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1415094433.0
:DateShort: 2014-Nov-04
:END:


** I enjoyed the 'negotiations' with the Necrons.

In story, the Necrons (Oldcrons for those familiar with 40k lore) have an internal fault state in their AI which prevents them from trading with the Culture, despite it being in their best interests, or even discussing or negotiating for such. They get around this by effectively starting a short war with both sides 'claiming' the trade goods as spoils. the Necrons wanted [[#s][spoiler]]

Which cannot possibly end badly.
:PROPERTIES:
:Author: JackStargazer
:Score: 3
:DateUnix: 1415048328.0
:DateShort: 2014-Nov-04
:END:


** So I hit Chapter 40, and a Tzeentchian Chaos Sorcerer has just started acausally trading (ala Timeless Decision Theory/Basilisk) and blackmailing with the Eldar and a Necron who I assume is Orikan the Diviner. They are using pre-commitment and future sight in order to fight whole proxy wars on the future timelines. It's interesting, if alarming.
:PROPERTIES:
:Author: JackStargazer
:Score: 2
:DateUnix: 1415052042.0
:DateShort: 2014-Nov-04
:END:

*** That is my favorite part. Though watching fluffy culture ethics meet grimdark is entertaining too. I won't say this is my favorite fic, but it is a good time filler and seemed worth submitting. The time manipulation fun was one of the parts that decided it for me.
:PROPERTIES:
:Author: andor3333
:Score: 1
:DateUnix: 1415053602.0
:DateShort: 2014-Nov-04
:END:


*** That was pretty much my least favourite part, as I found each Tzeentchian chapter absolutely atrociously written, to the point of being essentially unreadable....
:PROPERTIES:
:Author: thakil
:Score: 1
:DateUnix: 1415092937.0
:DateShort: 2014-Nov-04
:END:


** There seem to be Culture specific buzzwords and phrases. Are they explained in the fanfic too or is that something I will be missing if I have no clue about the culture?
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1415060660.0
:DateShort: 2014-Nov-04
:END:

*** A little both ways really. I think you can figure a lot out from context but there will be a few things you may not catch. Mostly I think it can be figured out as you go. That said, if you do get interested in the culture novels, I recommend Player of Games.
:PROPERTIES:
:Author: andor3333
:Score: 1
:DateUnix: 1415075774.0
:DateShort: 2014-Nov-04
:END:

**** Hmm as a Culture fan I'll have to dissent. Player of games is VERY heavy handed. Probably the best first culture novel for a this sub-reddit's members would be surface detail, because it deals with technology created afterlives and true post-singularity production capabilities.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 2
:DateUnix: 1415130593.0
:DateShort: 2014-Nov-04
:END:
