#+TITLE: [RST][C][HSF][EDU][TH] That Alien Message by Eliezer Yudkowsky: "Imagine a world much like this one, in which, thanks to gene-selection technologies, the average IQ is 140 (on our scale). One day, the stars in the night sky begin to change..."

* [[http://web.archive.org/web/20180115145446/http://lesswrong.com/lw/qk/that_alien_message/][[RST][C][HSF][EDU][TH] That Alien Message by Eliezer Yudkowsky: "Imagine a world much like this one, in which, thanks to gene-selection technologies, the average IQ is 140 (on our scale). One day, the stars in the night sky begin to change..."]]
:PROPERTIES:
:Author: erwgv3g34
:Score: 30
:DateUnix: 1569505837.0
:DateShort: 2019-Sep-26
:END:

** Someone recently posted Starwink by Alicorn, which is a retelling of this story. I think it is wholly more entertaining and engaging, and it stays true to the original despite being less instructive. [[http://alicorn.elcenia.com/stories/starwink.shtml]]
:PROPERTIES:
:Author: over_who
:Score: 43
:DateUnix: 1569506554.0
:DateShort: 2019-Sep-26
:END:

*** u/Roxolan:
#+begin_quote
  "You will use the concierges," he told her firmly. "You will tell them as soon as you are inconvenienced. I don't care if you are inconvenienced by the air conditioning, your commute, our hardware, the time zone, your podiatrist, the government, or your bagel slicer. We hired you for your brain, we don't want to drive you insane, and that means that instead of taking extra hours out of your leisure time - let alone your sleep - we are taking them out of anything else that bothers you. The concierge department's job is to fix your problems. Complain to them."
#+end_quote

This is my fetish.

 

I see what you mean about more engaging and less instructive.

The original is an intuition pump: "okay, I know you can't imagine a superhuman AI; but a well-organised society of geniuses working on a problem under extreme time dilation is /kind/ of like a superhuman AI and maybe you can picture /that/". Eliezer only does the bare minimum of humanising his "characters", and doesn't bother explaining the aliens much, because nitpicking the metaphor would defeat the point.

Whereas Alicorn is /all about/ delightfully nitpicking the metaphor.
:PROPERTIES:
:Author: Roxolan
:Score: 22
:DateUnix: 1569528157.0
:DateShort: 2019-Sep-26
:END:


*** Agreed that it's more entertaining and engaging, but there's a few things (e.g. what is being created in the bathtub) that only make sense if you've read the original.

"That Alien Message" stands alone better than "Starwink" does, but, assuming you've read both, "Starwink" is certainly the superior telling of the story. Which is not surprising, as the latter is told as a narrative and the former as a thought experiment.
:PROPERTIES:
:Author: Nimelennar
:Score: 11
:DateUnix: 1569527318.0
:DateShort: 2019-Sep-26
:END:

**** u/Argenteus_CG:
#+begin_quote
  but there's a few things (e.g. what is being created in the bathtub) that only make sense if you've read the original.
#+end_quote

I mean, you lose a few specifics, but it's pretty easy to guess from context that it's nanotech or an equivalent for their physics, because that's just obviously what you'd do in that situation.
:PROPERTIES:
:Author: Argenteus_CG
:Score: 9
:DateUnix: 1569532172.0
:DateShort: 2019-Sep-27
:END:

***** Made from mixing six "bespoke chemicals" (manufactured by a society that doesn't seem too much more advanced than ours) and a household cleaner together? With nothing more than "looked awfully funny" and "their house wasn't there any more" for additional hints?

I hadn't the foggiest what that was supposed to signify until I went back and read EY's piece. I've baked /cakes/ that have had more complex recipes than this nanotech.
:PROPERTIES:
:Author: Nimelennar
:Score: 10
:DateUnix: 1569537308.0
:DateShort: 2019-Sep-27
:END:

****** I've been wondering if those chemicals were also created by other people following similar instructions, though it's not clear why that would be better than giving one guy the full set of instructions. It's not like many people would be less likely to mess up.
:PROPERTIES:
:Author: archpawn
:Score: 4
:DateUnix: 1569541397.0
:DateShort: 2019-Sep-27
:END:


****** I mean, even if they hadn't had the bathtub section at all, it's still just the obvious thing you'd DO to take over the universe of the ones simulating your own, assuming they were stupid enough for that sort of thing to work.
:PROPERTIES:
:Author: Argenteus_CG
:Score: 2
:DateUnix: 1569539500.0
:DateShort: 2019-Sep-27
:END:

******* Again, I don't see that as particularly obvious. With access to a universe that has computers which can trivially stimulate a human brain, I'm probably going to think robotics before I think nanotech.
:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1569554153.0
:DateShort: 2019-Sep-27
:END:

******** Nanotech is just robotics on a smaller scale, and there's no particular reason you WOULDN'T want to be able to manipulate things on a very small level, given that a properly coordinated nanotech swarm is also a great way to manufacture your macrorobotics as well. You COULD start with macrorobotics first, I suppose, but if you're technologically advanced enough to do so, why not skip the extra step and push the "I win" button right away?

Unless by "robotics" you mean AI research? In that case, you have more of a point, but waiting a bit to try that is perhaps understandable given that THEY were (in a sense) rogue AIs themselves.
:PROPERTIES:
:Author: Argenteus_CG
:Score: 6
:DateUnix: 1569558374.0
:DateShort: 2019-Sep-27
:END:

********* u/Nimelennar:
#+begin_quote
  if you're technologically advanced enough to do so
#+end_quote

There's the sticking point.

I don't doubt that after all those years, humanity is that advanced. However:

- It doesn't look like the AI creators are that advanced
- The laws of physics outside of the simulation are different than those inside (e.g. five dimensions instead of four),
- You can perhaps infer some of those laws from the creators' knowledge beyond the ones they know themselves, but not /all/ of them, and
- Any experiments humanity does to either confirm or deny those laws of physics have to be done in creator-time, rather than simulation-time.

It's not my contention that it's a bad ending: just one that should have been explained better, as it wasn't as obvious what was going on as the author intended.
:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1569589120.0
:DateShort: 2019-Sep-27
:END:


** This is an inversion of [[http://ttapress.com/CrystalNights.pdf][Greg Egan's /Crystal Nights/]] or [[https://www.1pezeshk.com/wp-content/pics/2013/01/microcosmicgodtheodoresturgeon-111104040008-phpapp02_2.pdf][Theodore Sturgeon's /Microcosmic God/]].

Also worth examining David Brin's /Stones of Significance/.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 14
:DateUnix: 1569507076.0
:DateShort: 2019-Sep-26
:END:


** I realize that this is a thought experiment or whatever, but it is a remarkably unrealistic view of humanity. We all agree to surrender our freedom and go into cryo? We don't try to find bugs in the universe? We plan and execute a secret attack without trying to communicate? I don't see us ever having the unity this story displays, especially since it's not like smart people are more prone to unity.
:PROPERTIES:
:Author: somerando11
:Score: 6
:DateUnix: 1569539115.0
:DateShort: 2019-Sep-27
:END:

*** To be fair they are genetically manipulated and even still, with thousands of years of living in modern society you'd expect natural selection or some other optimization process to select / make humans better at cooperating in large numbers.

​

We are bad at it currently because this is still very new, our software is still made for tribal environments, give us a couple millennium of society living and the software is likely to adapt to it in some way or another.
:PROPERTIES:
:Author: fassina2
:Score: 2
:DateUnix: 1569669815.0
:DateShort: 2019-Sep-28
:END:


** I could have sworn I was on [[/r/SneerClub]] with that title.
:PROPERTIES:
:Author: major_fox_pass
:Score: 11
:DateUnix: 1569521584.0
:DateShort: 2019-Sep-26
:END:

*** Rent free baby
:PROPERTIES:
:Score: 6
:DateUnix: 1569548028.0
:DateShort: 2019-Sep-27
:END:


*** What do you mean?
:PROPERTIES:
:Author: Roxolan
:Score: 3
:DateUnix: 1569525089.0
:DateShort: 2019-Sep-26
:END:

**** References to IQ and eugenics at the forefront of a story which doesn't need to focus so much about either of those things to be successful. It's the sort of thing I'd expect to show up on [[/r/SneerClub]], along with comments like "of course he leads with those lines".
:PROPERTIES:
:Author: major_fox_pass
:Score: 20
:DateUnix: 1569527970.0
:DateShort: 2019-Sep-26
:END:

***** i too was a little confused about why that part was relevant to the rest of the story
:PROPERTIES:
:Author: tjhance
:Score: 10
:DateUnix: 1569530125.0
:DateShort: 2019-Sep-27
:END:


** This reminds me a lot of [[https://en.wikipedia.org/wiki/Dragon%27s_Egg][Dragon's Egg]] by Robert L Forward, but with way less uplift required.
:PROPERTIES:
:Author: edwardkmett
:Score: 3
:DateUnix: 1569541870.0
:DateShort: 2019-Sep-27
:END:

*** Yeah it reminded me of that story too.

There's a fair few sci-fi stories on that premise. /Spin/ tries it with humans, /Permutation City/ goes off the rails because of it, pretty sure one of the petty robot tyrants of Stanisław Lem's /Cyberiad/ orders one...

(The point Eliezer is making with it is original though, AFAIK.)
:PROPERTIES:
:Author: Roxolan
:Score: 3
:DateUnix: 1569542662.0
:DateShort: 2019-Sep-27
:END:

**** Very much so. At the very least Yudkowsky draws a much finer point.

The scene in Dragon's Egg where they give up waiting for the transfer of a human encyclopedia part way through because they'd already extrapolated the rest aimed pretty close to this mark, though.

When I read it around 1985 it blew 10-year-old-me's mind.
:PROPERTIES:
:Author: edwardkmett
:Score: 4
:DateUnix: 1569546085.0
:DateShort: 2019-Sep-27
:END:


** In this story, Eliezer is of the opinion Einsteins theory was an unlikely a hypothesis. In his sequence post [[https://www.lesswrong.com/posts/MwQRucYo6BZZwjKE7/einstein-s-arrogance][Einstein's arrogance]]

#+begin_quote
  A Bayesian superintelligence, hooked up to a webcam, would invent General Relativity as a hypothesis---perhaps not the dominant hypothesis, compared to Newtonian mechanics, but still a hypothesis under direct consideration---by the time it had seen the third frame of a falling apple. It might guess it from the first frame if it saw the statics of a bent blade of grass.
#+end_quote

vs

#+begin_quote
  To assign more than 50% probability to the correct candidate from a pool of 100,000,000 possible hypotheses, you need at least 27 bits of evidence (or thereabouts).
#+end_quote

Seems a bit contradictory to me?
:PROPERTIES:
:Author: You_cant_buy_spleen
:Score: 4
:DateUnix: 1569652759.0
:DateShort: 2019-Sep-28
:END:

*** u/fassina2:
#+begin_quote
  Seems a bit contradictory to me?
#+end_quote

How so?

Both imply an AI with enough intelligence, or his analogous example, need little evidence to eliminate a large percentage of hypothesis to explain some phenomena..
:PROPERTIES:
:Author: fassina2
:Score: 3
:DateUnix: 1569670382.0
:DateShort: 2019-Sep-28
:END:

**** Hmm I guess you're right, thanks
:PROPERTIES:
:Author: You_cant_buy_spleen
:Score: 1
:DateUnix: 1569718257.0
:DateShort: 2019-Sep-29
:END:
