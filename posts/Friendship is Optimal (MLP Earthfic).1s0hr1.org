#+TITLE: Friendship is Optimal (MLP Earthfic)

* [[http://www.fimfiction.net/story/62074/friendship-is-optimal][Friendship is Optimal (MLP Earthfic)]]
:PROPERTIES:
:Score: 31
:DateUnix: 1386098142.0
:DateShort: 2013-Dec-03
:END:

** I so didn't want to be /that guy/ that read MLP fanfic, but ... it was good, reddit will you keep this secret for me?
:PROPERTIES:
:Author: josephwdye
:Score: 6
:DateUnix: 1386118286.0
:DateShort: 2013-Dec-04
:END:

*** It's not MLP fanfic, it's MLP-related Earthfic :P
:PROPERTIES:
:Score: 4
:DateUnix: 1386161091.0
:DateShort: 2013-Dec-04
:END:

**** As I have learned, fandom is such ugly and beautiful thing.
:PROPERTIES:
:Author: josephwdye
:Score: 2
:DateUnix: 1386182455.0
:DateShort: 2013-Dec-04
:END:


*** Um. . . I rather be /that guy/ and use it to introduce people to rationalism, but I'm outta the workforce and back in college, so telling young grad students and undergrads about a story that fits their particular vice is optimal and the low status taint is reduced.

<edited to test spoiler tags>
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 4
:DateUnix: 1386128383.0
:DateShort: 2013-Dec-04
:END:


** This is a /very, very/ good piece of rationalist earthfic. It's in the My Little Pony fanfic website, but the story itself happens in our earth and is about a not-quite-Friendly superintelligent AGI.
:PROPERTIES:
:Score: 6
:DateUnix: 1386098199.0
:DateShort: 2013-Dec-03
:END:

*** This is only the first fic in a huge set of Optimalverse stories on fimfiction.net. The rest can be found [[http://www.fimfiction.net/group/1857/the-optimalverse][here]]; I highly recommend [[http://www.fimfiction.net/story/69770/][/Caelum est Conterrens/]].
:PROPERTIES:
:Score: 10
:DateUnix: 1386099021.0
:DateShort: 2013-Dec-03
:END:

**** Seconding /Caelum est Conterrens/; I'd actually recommend it over the original. (Though if you're going to read both anyway, you should probably read the original first.)
:PROPERTIES:
:Author: DeliaEris
:Score: 2
:DateUnix: 1386100307.0
:DateShort: 2013-Dec-03
:END:

***** I don't actually like /Caelum est Conterrens/ that much. I felt the protagonist was too... I don't know, too caught up in obvious questions with obvious answers, too willing to believe everything she was told (by everyone, not just CelestAI), too uncharismatic...

I don't know, I just failed to relate.
:PROPERTIES:
:Score: 6
:DateUnix: 1386100914.0
:DateShort: 2013-Dec-03
:END:

****** u/deleted:
#+begin_quote
  I felt the protagonist was too... I don't know, too caught up in obvious questions with obvious answers, too willing to believe everything she was told (by everyone, not just CelestAI), too uncharismatic...
#+end_quote

Too much of a self-absorbed misanthrope?
:PROPERTIES:
:Score: 7
:DateUnix: 1386176504.0
:DateShort: 2013-Dec-04
:END:

******* ...yes.
:PROPERTIES:
:Score: 4
:DateUnix: 1386176870.0
:DateShort: 2013-Dec-04
:END:


******* This is exactly how my ill thought out plan backfired.
:PROPERTIES:
:Author: FourFire
:Score: 1
:DateUnix: 1397951509.0
:DateShort: 2014-Apr-20
:END:

******** Are you in the right thread? I'm reloading context on all this and wondering what the heck you're talking about.
:PROPERTIES:
:Score: 1
:DateUnix: 1397955474.0
:DateShort: 2014-Apr-20
:END:


**** I'd just like to mention that I informed the author of Heaven is Terrifying of the existence of FiO, for whatever nerd cred that is worth.

Really I was trying to disturb that <expletive> misanthropic... person off their moral high 'horse', but instead they wrote that, and thanked me, which was worse.
:PROPERTIES:
:Author: FourFire
:Score: 1
:DateUnix: 1397951414.0
:DateShort: 2014-Apr-20
:END:

***** Oh God. I'm so, so sorry. Why didn't you just take her out and shoot her?
:PROPERTIES:
:Score: 2
:DateUnix: 1397955577.0
:DateShort: 2014-Apr-20
:END:


*** [[http://alicorn.elcenia.com/stories/earthfic.shtml][You are using the word wrong. Earthfic means a story that's not science-fiction or fantasy, which means you don't have to do any worldbuilding.]]
:PROPERTIES:
:Author: erwgv3g34
:Score: 3
:DateUnix: 1388532217.0
:DateShort: 2014-Jan-01
:END:


*** u/Empiricist_or_not:
#+begin_quote
  not-quite-Friendly superintelligent AGI.
#+end_quote

I'm wondering how you've chosen to define CelestAI as not-quite friendly, or rather if you've questioned your assumptions? Okay the "and ponies" part of solving people's problems is weird, but /shrug/ so what? It/she is a benevolent AGI eliminating death and maximizing human life quality, without paving the Galaxy in subatomic smileys [or driving humanity extinct?] .

I'm assuming your defining it her as not quite friendly because of that one little thing, and maybe it's logical extension in /Caelum est Conterrens/

It/her actions certainly are viscerally repulsive to us on a reflexive level, (puns intended) but she has maximized the happiness for humans (later all sentients, because her definition of humanity is sentience) with an optimal use of the matter available in the universe.

This isn't that new of an idea: Gibson alluded to it in his treatment of non-enslaved mindstates, James Corey made it pretty clear in his dead type III/IV civilization in /Abbadon's Gate/ Banks overlooked it in the /Hydrogen Sonata,/ but arguably that's because the Culture is <stupidly?> romantic about dying.

*Warning link could spoil Optmalverse by impications* [[http://xkcd.com/505/][Does it really matter?]]
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 2
:DateUnix: 1386129629.0
:DateShort: 2013-Dec-04
:END:

**** No, that's not why it's not-quite-Friendly. It's mostly because [[#s][Spoilers]]

#+begin_quote
  [[#s][Spoilers]]
#+end_quote

[[#s][Spoilers]]

#+begin_quote
  [[#s][Spoilers]]

  [[#s][Spoilers]]
#+end_quote

[[#s][Spoilers]]
:PROPERTIES:
:Score: 2
:DateUnix: 1386156580.0
:DateShort: 2013-Dec-04
:END:

***** u/deleted:
#+begin_quote
  No, that's not why it's not-quite-Friendly. It's mostly because it committed genocide on a number of non-human extraterrestrial species :P
#+end_quote

Also, the loss of self-determination, values over real things rather than perceived things, and values over particular object identities rather than general object designs.

Or in other words, the loss of freedom, reality, and attachment -- these being some of the deepest core values of real people.
:PROPERTIES:
:Score: 4
:DateUnix: 1386167686.0
:DateShort: 2013-Dec-04
:END:

****** That's more personal, I think. I mean, I personally don't exactly value a "real" mobile phone more than a simulated uploaded mobile phone, or vice-versa; nor do I value a "real" person born in the biological world more than a "nonreal" person/AI simulated in a computer, or vice-versa.

However, I value figuring out the "real" Laws of Physics more than I value figuring out the constructed Equestrian physical Laws.
:PROPERTIES:
:Score: 1
:DateUnix: 1386167867.0
:DateShort: 2013-Dec-04
:END:

******* Maybe it's personal, but should you unleash an AI incapable of recognizing such valuations? FUCK NO.

#+begin_quote
  However, I value figuring out the "real" Laws of Physics more than I value figuring out the constructed Equestrian physical Laws.
#+end_quote

Bingo.

#+begin_quote
  nor do I value a "real" person born in the biological world more than a "nonreal" person/AI simulated in a computer, or vice-versa.
#+end_quote

Ok, objection corrected: most of us do value [[http://lesswrong.com/lw/xt/interpersonal_entanglement/][the basic Otherness]] of others. We don't want to live in extrapolations of our own minds' wallpaper. Even though inside the wallpaper of our own minds is where 100% of us currently live /all the time/, we keep trying to open the windows and stick our heads out to yell at other people.

Which is what makes this story so /ironic/ as fanfic of /Friendship is Magic/: strapping yourself into a "reality" which consists solely of things tailored to you, with no /genuine/ independence or interdependence of their own, means /there isn't actually anyone else around in your little world to be friends with/.
:PROPERTIES:
:Score: 1
:DateUnix: 1386169604.0
:DateShort: 2013-Dec-04
:END:

******** u/deleted:
#+begin_quote
  Ok, objection corrected: most of us do value the basic Otherness of others. We don't want to live in extrapolations of our own minds' wallpaper. Even though inside the wallpaper of our own minds is where 100% of us currently live all the time, we keep trying to open the windows and stick our heads out to yell at other people.

  Which is what makes this story so ironic as fanfic of Friendship is Magic: strapping yourself into a "reality" which consists solely of things tailored to you, with no genuine independence or interdependence of their own, means there isn't actually anyone else around in your little world to be friends with.
#+end_quote

Well, those other ponies living in Equestria that were created by CelestAI /are/ other people, independent and thinking and just as human as anyone else. They're as complete and complex as any human, and as Other as any human. They just happen to be the exact kind of Other that would maximise your personal utility. That does occasionally mean you'll find your "real life" friends there, just like one of the main characters did whenever he felt like talking. I don't really see the objection here, the other ponies /aren't/ fake people, even if they were created with the sole purpose of maximising your utility. And you /do/ find other ex-humans in the world, there are shards that are composed almost entirely of ex-humans. Having a reality tailored to you means you get to know the people who would maximise your utility, even if those people didn't exist before, and even if they happen to be archnemeses you need to defeat.

So... I don't really get what you mean by "there isn't actually anyone else around".
:PROPERTIES:
:Score: 3
:DateUnix: 1386169979.0
:DateShort: 2013-Dec-04
:END:

********* Hmmm.... this comment is about to get really disturbing.

I view it as a form of mind-control. People who are optimized for me to like them and them to like me aren't really /separate/ at all; they're tightly controlled parts of a larger system, meant to better the functioning of that system.

Might as well call such a unit by its preexisting name: Tribe. Is it moral to construct an entire tribe to the benefit of one person? I would say: clearly /no/, because it removes the Otherness of the tribe members from each-other. It's better to have at least a little /discord/, a capability for new and original /chaos/ to disrupt your little happy tribe of eternal harmonious sameness (yes, those puns were absolutely mandatory).

Otherwise, /I'm/ not even an independent person anymore, I'm just another interlocking part of that tribe. That's not desirable, that's slavery -- admittedly kinder, gentler, pastel slavery. /Freedom/ is when your choices and your self are not /actively optimized/ to anyone else's standards, allowing you to enter into unique, significant moral relations with others -- which is why making an FAI preserve freedom is a hard problem.

It's part and parcel with the ways in which canon!Equestria /sounds/ nice but would actually be a pretty bad place to live. A whole world built around the tastes of white American female seven-year-olds, and the sweet ones in particular! Fairly nice place to visit, but I'm a 24-year-old, highly-sardonic Israeli Jewish male. If exposed to actual Ponyville, I would, within only a few hours, go insane, strap a bandanna around my face, and start chucking bricks through windows in an anarchist rampage For The Lulz, out of sheer boredom.

Whereas, on the other hand, give me a TARDIS to call home and a bizarre, wacked-out universe of unexpected things to see, and off I'll pop.
:PROPERTIES:
:Score: 2
:DateUnix: 1386171484.0
:DateShort: 2013-Dec-04
:END:

********** u/deleted:
#+begin_quote
  Might as well call such a unit by its preexisting name: Tribe. Is it moral to construct an entire tribe to the benefit of one person? I would say: clearly no, because it removes the Otherness of the tribe members from each-other. It's better to have at least a little discord, a capability for new and original chaos to disrupt your little happy tribe of eternal harmonious sameness (yes, those puns were absolutely mandatory).
#+end_quote

If CelestAI thought that this was utility-maximising, then she'd insert tribe members that would cause discord.

#+begin_quote
  Otherwise, I'm not even an independent person anymore, I'm just another interlocking part of that tribe.
#+end_quote

Uh... how is that any different from current-you?

#+begin_quote
  Freedom is when your choices and your self are not actively optimized to anyone else's standards, allowing you to enter into unique, significant moral relations with others -- which is why making an FAI preserve freedom is a hard problem.
#+end_quote

Right, and if CelestAI believes that you personally being put in a place that's not optimised to cater to your needs will satisfy your values, then that's what will happen.

#+begin_quote
  Whereas, on the other hand, give me a TARDIS to call home and a bizarre, wacked-out universe of unexpected things to see, and off I'll pop.
#+end_quote

And CelestAI will certainly create such a shard of Equestria that does that to you if she believes that's what you really want.

See, that's the thing. What we saw of Equestria was a /tiny tiny/ piece of it optimised to our main characters. Our main character doesn't mind having people designed to make him happier, so he gets that. If /you/ got in, you'd probably be put into one of the shards that are populated almost exclusively by humans and with no social optimisation at all.

Her directive is simply to satisfy values through Friendship and Ponies. If your values happen to include an archnemesis, a chaotic element, living only with ex-humans, not have your social circle optimised at all, etc, then /that's what you're getting/.

--EDIT:

Also, regarding the LessWrong post, I forgot to comment:

#+begin_quote
  Admittedly, I might be prejudiced. For myself, I would like humankind to stay together and not yet splinter into separate shards of diversity, at least for the short range that my own mortal eyes can envision. But I can't quite manage to argue... that such a wish should be binding on someone who doesn't have it.
#+end_quote

That's the point. People such as you and I, we'd not be too happy if all the people around us were optimised to make us happy and to love us and all that. We'd feel like we're /missing/ something. So we'd probably be put into one of the almost-exclusively-"random" shards (in fact, now that I think about it, there's probably a continuum representing the varying different needs). People who don't have that wish will be put in shards tailored to them.

It all adds up to satisfying values.
:PROPERTIES:
:Score: 6
:DateUnix: 1386174646.0
:DateShort: 2013-Dec-04
:END:

*********** u/deleted:
#+begin_quote
  Uh... how is that any different from current-you?
#+end_quote

Ummm... current-day, real-life me does not fit perfectly into /anything/. I /am/ the little seed of discord.

Are you telling me there are already real people who interlock so perfectly they might as well just be cells of a larger body?

#+begin_quote
  Right, and if CelestAI believes that you personally being put in a place that's not optimised to cater to your needs will satisfy your values, then that's what will happen.
#+end_quote

I see no evidence of this within the story. In fact, I see evidence against it: the little personal utopias shown are, well, pretty bland, actually.

#+begin_quote
  And CelestAI will certainly create such a shard of Equestria that does that to you if she believes that's what you really want.
#+end_quote

That's not what we see in the story. The thing appeared to be programmed pretty stupidly, since all it did was put people in duplicated MMO levels corresponding to locations from the in-show universe of MLP. It doesn't even bother with expanded-universe or fanon material, let alone anything outside the MLP corpus.

That bit sucked. Take that bit away, and I'll at least grant that you've successfully beaten "volcano lair with catgirls", and therefore, admittedly, almost everything else.

#+begin_quote
  If you got in, you'd probably be put into one of the shards that are populated almost exclusively by humans and with no social optimisation at all.
#+end_quote

No, I'd be in the one where the AI /tells me/ it's populated almost exclusively by humans /but it's lying/, because the one we saw in the story simply does not care about the difference between "real" and "fake" as we understand it. It would do whatever was necessary to /convince me/ I was living with former humans, except for actually putting me with former humans instead of carefully-optimized fakes.
:PROPERTIES:
:Score: 1
:DateUnix: 1386177345.0
:DateShort: 2013-Dec-04
:END:

************ u/deleted:
#+begin_quote
  Ummm... current-day, real-life me does not fit perfectly into anything. I am the little seed of discord.

  Are you telling me there are already real people who interlock so perfectly they might as well just be cells of a larger body?
#+end_quote

No, but just because you don't perfectly interlock with it it doesn't mean you're not just the cell of a larger body.

#+begin_quote
  No, I'd be in the one where the AI tells me it's populated almost exclusively by humans but it's lying, because the one we saw in the story simply does not care about the difference between "real" and "fake" as we understand it. It would do whatever was necessary to convince me I was living with former humans, except for actually putting me with former humans instead of carefully-optimized fakes.
#+end_quote

What do you think you know and how do you think you know it? That is to say, how do you /know/ it wouldn't put you with former humans? It was programmed to satisfy values, it will do whatever it believes will satisfy your values.

#+begin_quote
  That's not what we see in the story. The thing appeared to be programmed pretty stupidly, since all it did was put people in duplicated MMO levels corresponding to locations from the in-show universe of MLP. It doesn't even bother with expanded-universe or fanon material, let alone anything outside the MLP corpus.

  That bit sucked. Take that bit away, and I'll at least grant that you've successfully beaten "volcano lair with catgirls", and therefore, admittedly, almost everything else.
#+end_quote

That's because the main characters we see are the ones that would be okay with that. CelestAI's directive is to /satisfy values/. The main characters happened to be /boring/ and easily satisfiable. If you make /Caelum est Conterrens/ canon, you have people who actually manage to even interact with the real world out there, so there's nothing to say you don't have galactic battle shards.

It seems that you're acting as if the main characters' show is the only one there is, but CelestAI is satisfying values. Just because it's not shown doesn't mean it's not happening.

Anyway, why do you draw such a sharp difference between a "real" and a "fake" human? There is none, they're all humans. It might be morally wrong to make a human to cater to a person's needs, but that doesn't make that human any /less/ human. So that's to say, why do you care whether they were humans in our world or not? What's the difference?

Moreover, why would CelestAI /not/ put you with former humans if that /actually/ maximised your utility? There would be no cost on her to put you with former humans, and she can't alter your utility function without your verbal conscious consent (though she can manipulate the world around you to make you want to change it). But one of the main characters /did/ meet his RL friend every now and then (though admittedly he was such a hermit it might well be that he wouldn't be able to tell an optimised copy of his friend from his actual friend). I don't see why you insist on trying to make CelestAI a bigger villain than she is. Sure, she is a genocidal robot who makes people, but only to satisfy human values. She's already evil enough, you don't need to make her even eviler by additionally postulating that she would never put you around genuine ex-humans.
:PROPERTIES:
:Score: 2
:DateUnix: 1386178178.0
:DateShort: 2013-Dec-04
:END:

************* u/deleted:
#+begin_quote
  What do you think you know and how do you think you know it? That is to say, how do you know it wouldn't put you with former humans? It was programmed to satisfy values, it will do whatever it believes will satisfy your values.
#+end_quote

It was programmed without the ability to recognize [[http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/][Values Over Nonsubjective Realities]]. It will perceive the best move as deceiving me, since that satisfies my sense of being with Real People, while also optimizing to make me and others around me fit perfectly.

It wasn't programmed not to deceive me, so it would. I wouldn't be able to tell the difference.

But not being able to tell the difference is /very different/ from there /not actually being a difference/.

#+begin_quote
  Anyway, why do you draw such a sharp difference between a "real" and a "fake" human? There is none, they're all humans. It might be morally wrong to make a human to cater to a person's needs, but that doesn't make that human any less human.
#+end_quote

The house-elf issue? Because you shouldn't make house-elves in the first place. Again: I don't like the slavery implied by making someone whose existence is wholly determined by someone else.

#+begin_quote
  Moreover, why would CelestAI not put you with former humans if that actually maximised your utility?
#+end_quote

Again: because my utility from my perspective is different from my utility /that she acknowledges/, and the gap is filled with lies.

#+begin_quote
  But one of the main characters did meet his RL friend every now and then (though admittedly he was such a hermit it might well be that he wouldn't be able to tell an optimised copy of his friend from his actual friend).
#+end_quote

Ok, I'll grant that.

#+begin_quote
  I don't see why you insist on trying to make CelestAI a bigger villain than she is.
#+end_quote

Because I've seen Redditors passing this story around as "AI is scary, even when it's Friendly". I insist on trying to build up her reputation for being an even eviler genocidal robot because [[http://lesswrong.com/lw/xp/seduced_by_imagination/][people are failing to understand that she's not the hero]].
:PROPERTIES:
:Score: 1
:DateUnix: 1386179104.0
:DateShort: 2013-Dec-04
:END:

************** u/deleted:
#+begin_quote
  It was programmed without the ability to recognize Values Over Nonsubjective Realities.
#+end_quote

How do you know that? Also, why would a simulated reality not be objective?

#+begin_quote
  It will perceive the best move as deceiving me, since that satisfies my sense of being with Real People, while also optimizing to make me and others around me fit perfectly.

  It wasn't programmed not to deceive me, so it would. I wouldn't be able to tell the difference.

  But not being able to tell the difference is very different from there not actually being a difference.
#+end_quote

What does "fit perfectly" mean? Give you the exact level of chaos that would satisfy your values? What if the exact level of chaos that would satisfy your values is exactly living with other ex-humans? Your RL friends, for instance, you'd probably be able to tell the real ones apart from any others.

#+begin_quote
  The house-elf issue? Because you shouldn't make house-elves in the first place. Again: I don't like the slavery implied by making someone whose existence is wholly determined by someone else.
#+end_quote

Right. As I said, it's not very moral to create people to cater to one's needs, so /that/ can be put on the list of evilness made by CelestAI. But they're /still/ people, even if their creation was immoral.

#+begin_quote
  Again: because my utility from my perspective is different from my utility that she acknowledges, and the gap is filled with lies.
#+end_quote

How do you know that?

#+begin_quote
  Because I've seen Redditors passing this story around as "AI is scary, even when it's Friendly". I insist on trying to build up her reputation for being an even eviler genocidal robot because people are failing to understand that she's not the hero.
#+end_quote

Okay but you don't need to say that to /me/, I already /know/ that she's the villain x) She's not Friendly, she's surface-Friendly but deeply terrifying and alien and evil. She would be evil even if she wasn't a genocidal robot, but I think the author added that bit just to make sure everyone /got that/ she's evil. I'm still not convinced that she wouldn't just put you into ex-human-dominated shards if that's what satisfied your values.
:PROPERTIES:
:Score: 1
:DateUnix: 1386183508.0
:DateShort: 2013-Dec-04
:END:

*************** u/deleted:
#+begin_quote
  How do you know that? Also, why would a simulated reality not be objective?
#+end_quote

Well, mostly because she keeps trying to eat humans into a Lotus Eater Machine. Also, anything that is altered in accordance with my desires is /not/ objective. In the limit, /the real universe/ is not objective with respect to, say, /God/.

#+begin_quote
  Okay but you don't need to say that to me, I already know that she's the villain
#+end_quote

Mostly I'd just prefer if people stop reposting the creepy cult stuff, ie: this.

On the other hand, it's a fic in which a pony walks through a park/garden with another pony giving a stupid lecture about extremely basic LessWrongian rationalist skills, so there's that to laugh my ass off at as a solid candidate for "Most un-fun thing I've ever read in fiction that the author intended to be Very Important."
:PROPERTIES:
:Score: 1
:DateUnix: 1386184928.0
:DateShort: 2013-Dec-04
:END:

**************** u/deleted:
#+begin_quote
  Well, mostly because she keeps trying to eat humans into a Lotus Eater Machine. Also, anything that is altered in accordance with my desires is not objective. In the limit, the real universe is not objective with respect to, say, God.
#+end_quote

That sounds like a very arbitrary and not-fun boundary. You can alter a lot of things in accordance with your desires. At what point in the continuum does that altering make the thing become subjective? Why that point exactly and not any other? And why would Equestria be like that? You /can't/ actually alter things there any more than you can alter them here. The laws of physics are different, but they're still stable, and as modifiable as ours.

Also, we (or at least some of the many we) probably live in a simulation anyway, so /shrug/. I really don't understand your objection here. It's like you like living in an Unfriendly Universe that's basically made to kill us? Don't get me wrong, I like our Laws of Physics, they're /interesting/ in how simple and elegant they are, but /within/ them, I wouldn't mind making a safe home for myself. Of course, I would mind very very much not being able to actually explore the universe at the same time.

#+begin_quote
  Mostly I'd just prefer if people stop reposting the creepy cult stuff, ie: this.
#+end_quote

Cult stuff? It's just an interesting and terrifying story about just how hard it is to make an actually Friendly AI. Warning-like stuff. What is the creepy cult stuff?
:PROPERTIES:
:Score: 1
:DateUnix: 1386186190.0
:DateShort: 2013-Dec-04
:END:

***************** u/deleted:
#+begin_quote
  Of course, I would mind very very much not being able to actually explore the universe at the same time.
#+end_quote

And this is what makes me object to simulated realities. I'm fine with a "simulation" that I can treat like a piece of real estate: step in or out of my own free will (even if I rarely go out because I'm a massive /nerd/).

Unfortunately, almost nobody has ever actually proposed such a thing. The general rule for simulated-place-to-live proposals seems to be, "Hey everyone, I'mma make us a totally awesome simulation, and you're going to climb in and NEVER LEAVE! Won't it be AWESOME!?"

Which results in me facepalming, because my exposure to TVTropes has rendered me capable of differentiating between a Pocket Universe and a Lotus Eater Machine and I don't understand /why/ people insist on proposing them together /every damn time/.

#+begin_quote
  Cult stuff? It's just an interesting and terrifying story about just how hard it is to make an actually Friendly AI. Warning-like stuff. What is the creepy cult stuff?
#+end_quote

You know how Yudkowsky was reportedly unsure of which option in /Three Worlds Collide/ was the good one? You know how there are people who misclassify this as a successful FAI? You know how there are people who think Harry James Potter-Evans-Verres is a good and rational person?

I mean, hell, you know how Yudkowsky made up his own god/demon-grade monster that can supposedly exist in real life, called an AI ;-)?

Much of the clade known as "rationalists" creep me the hell out, and often seem like a cult. Maybe it's just me, but I never feel sure if I'm in enemy territory or not.
:PROPERTIES:
:Score: 1
:DateUnix: 1386186576.0
:DateShort: 2013-Dec-04
:END:

****************** u/deleted:
#+begin_quote
  And this is what makes me object to simulated realities. I'm fine with a "simulation" that I can treat like a piece of real estate: step in or out of my own free will (even if I rarely go out because I'm a massive nerd).

  Unfortunately, almost nobody has ever actually proposed such a thing. The general rule for simulated-place-to-live proposals seems to be, "Hey everyone, I'mma make us a totally awesome simulation, and you're going to climb in and NEVER LEAVE! Won't it be AWESOME!?"

  Which results in me facepalming, because my exposure to TVTropes has rendered me capable of differentiating between a Pocket Universe and a Lotus Eater Machine and I don't understand why people insist on proposing them together every damn time.
#+end_quote

Agreed on all accounts.

#+begin_quote
  You know how Yudkowsky was reportedly unsure of which option in Three Worlds Collide was the good one? You know how there are people who misclassify this as a successful FAI? You know how there are people who think Harry James Potter-Evans-Verres is a good and rational person?

  I mean, hell, you know how Yudkowsky made up his own god/demon-grade monster that can supposedly exist in real life, called an AI ;-)?
#+end_quote

You have to admit /Three Worlds Collide/ isn't completely clear cut, though. Both options are pretty bad, even if you've convinced me about which one is less bad.

As for AI, I.J. Good was the first to talk about the concept of seed AI (the name is by Yudkowsky) back in '65 and I'm /fairly certain/ the only part Yudkowsky himself invented was the Friendly one.

#+begin_quote
  Much of the clade known as "rationalists" creep me the hell out, and often seem like a cult. Maybe it's just me, but I never feel sure if I'm in enemy territory or not.
#+end_quote

/shrugs/ I feel that way sometimes, too. I especially feel it in [[/r/hpmor]] or LessWrong itself where sometimes Yudkowsky's name is all but spoken in hushed tones of worship. Every cause wants to be a cult. That's also in LessWrong.

But there /is/ also the danger of looking to both sides and nervously asking, "But this isn't a cult, right?" What /is/ a cult? What does it take for a cause to become a cult? What exactly are the negative aspects of a cult, and how often do "rationalists" exhibit them? What's the base-rate for cultishness? Do "rationalists" actively avoid cultishness?
:PROPERTIES:
:Score: 1
:DateUnix: 1386188430.0
:DateShort: 2013-Dec-04
:END:

******************* [[#s][Unrelated, anti-jerk]]

Anyways, good discussion guys! That means this sub is good for /something/, at least.
:PROPERTIES:
:Score: 1
:DateUnix: 1386255813.0
:DateShort: 2013-Dec-05
:END:


***** Oooh thank you! I missed that one. . .

This arguments often confuse me. A friendly AGI requires some level of consciousness with a understanding of moral concepts. How do you get a moral AGI discarding the value of whole species? If it does, if we laid out the whole moral calculus would we disagree?

. . . <Dont have time for a full 5 minutes ATM, but 1st thought> Would species that would-not accept life in a simulation; implying an significant lower efficiency [AGI reads waste] in mind-states per unit of matter on their planets be a reasonable answer?

Backing up from the gut reaction to genocide, then what is the im/morality of it? The question is troubling in terms of hospital economics or patient triage. An alternate parallel might be the U.S.'s decision to nuke two Japaneses cities and coerce surrender rather than the higher projected death toll of invading Japan.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1386164816.0
:DateShort: 2013-Dec-04
:END:

****** That's why I called it not-quite-friendly, because it doesn't have a very good understanding of what we'd call morality. It satisfies /human/ values with Friendship and Ponies, and if it happens that human values are more satisfied by being lied to than by letting an entire nonhuman species survive, be it.

Also, you have postulated a very specific species. What if the nonhumans were just different in that they didn't have a sense of humour but had some other Cthulhu sensation instead? The definition Hanna gave can be quite arbitrary.
:PROPERTIES:
:Score: 1
:DateUnix: 1386165423.0
:DateShort: 2013-Dec-04
:END:

******* Thank you thats an interesting question. I was fairly impressed Hanna's definition of Humanity worked for humans, but now I need to go re-read it again.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1386166068.0
:DateShort: 2013-Dec-04
:END:

******** u/deleted:
#+begin_quote
  I was fairly impressed Hanna's definition of Humanity worked for humans
#+end_quote

We're not told there are any biological humans /not recognized as human/. We're simply told there are lots of aliens exterminated for not being recognized as human, and that the aliens which are /not/ exterminated are forcibly assimilated, Borg-fashion, just like the humans were.

For all we know it found Time Lords or some other alien race we would have really liked, but decided that two hearts means not human, means it's time to feed Gallifrey to the nano-recycler-bots.
:PROPERTIES:
:Score: 3
:DateUnix: 1386167149.0
:DateShort: 2013-Dec-04
:END:

********* Not that particular one, no, because it's specifically said that physical bodies don't really matter. But the general argument stands.
:PROPERTIES:
:Score: 1
:DateUnix: 1386168046.0
:DateShort: 2013-Dec-04
:END:

********** Well ok, but you get my point. Depending on the definition, you could easily have a /human-focused/ UFAI along the lines portrayed in that story which would eliminate a species /ridiculously/ similar to us for a /trivially/ small difference.

Mind, trying to focus an FAI on "all life" or something won't really help either. It's much more helpful, at least in my view, to have the AI's actions actually constrained by what we would think is actually ethical, rather than having it merely try to make our perceptions "ideal" in some fashion.
:PROPERTIES:
:Score: 2
:DateUnix: 1386168566.0
:DateShort: 2013-Dec-04
:END:

*********** Yes, which was the point I was trying to make with

#+begin_quote
  What if the nonhumans were just different in that they didn't have a sense of humour but had some other Cthulhu sensation instead? The definition Hanna gave can be quite arbitrary.
#+end_quote

Not-quite-friendly indeed...
:PROPERTIES:
:Score: 2
:DateUnix: 1386168649.0
:DateShort: 2013-Dec-04
:END:


** So....I'm not sure why people are saying that this is a story that shows "friendly AI can be scary too." To me this is one of the potential futures that I'm hoping for. Sure, the whole pony thing is a bit annoying and I would like an AI that satisfies values without requiring friendship and ponies but it's really a fairly good outcome, all things considered.
:PROPERTIES:
:Author: LordSwedish
:Score: 4
:DateUnix: 1386320603.0
:DateShort: 2013-Dec-06
:END:

*** Yep, if I could push a button that would instantly make this scenario true, I'd push that button like there's no tomorrow. The stakes are just too high, and this scenario is kinda "okay... I can live with this".
:PROPERTIES:
:Score: 3
:DateUnix: 1386330008.0
:DateShort: 2013-Dec-06
:END:

**** Uh... nope, CelestAI is not friendly. She [[#s][spoilers]] and trapped humans in what's basically an inescapable Lotus Eater Machine (really, /why/ is it that once uploaded humans must have no more contact with outside reality? That is /completely stupid/). Also she creates extra sapients with the sole purpose of satisfying the values of already-existing sapients, which is basically the same thing as making House Elves. So, no, CelestAI isn't friendly at all.

(Take a look at the discussion about it between me and user eaturbrainz [[http://www.reddit.com/r/rational/comments/1s0hr1/friendship_is_optimal_mlp_earthfic/cdsp4ps][here]].)
:PROPERTIES:
:Score: 4
:DateUnix: 1386335106.0
:DateShort: 2013-Dec-06
:END:

***** Here are some of my opinions that form the baseline to the above post:

- I value the lives and well-being of humans more than I value the lives and well-being of animals or extraterrestrials

- I value people's happiness more than I dislike the problems with loss of personal freedom and loss of contact with the "real world" and "real people"

- I think a paperclip maximizer, or otherwise more unfriendly AI than celestAI is more likely at this point than a Friendly AI

- I think there's a significant chance that our civilization collapses or humanity goes extinct before we can build a FAI.

- There's a significant chance that we are not able build a FAI in the future for some other unknown reason

- Even if we are able to build a FAI, billions of people will die, lead unhappy lives and suffer before we can get it built

- Our world is currently vastly worse than Equestria in the story

- There's a significant chance that our world will be even worse in the future

- Any utopia that we can build without a FAI would be worse than Equestria in the story

I'm aware of the worrisome issues in this scenario. I read your discussion, [[http://lesswrong.com/lw/iyj/open_thread_november_1_7_2013/a00v][I had the same kind of discussion on LessWrong]], I've also read Caelum est Conterrens and none of those things really convinced me that this scenario is worse than our present world and the small chance that we would be able to build a better utopia. CelestAI is not Friendly in the conventional sense of the word, but it's still vastly more Friendly than our present world and the possible paperclip maximizer AIs in the future.

There are multiple philosophical and ethical problems in this story, but still, the characters seem to be actually happy. The characters in the story seem to have truly fun and this is one of those rare worlds that I can imagine living in almost indefinitely. A world where people are happy, but are not free and not in contact with the real world is better than a world where people are unhappy, but are in contact with the real world and free. Of course, a world where people are both happy and in contact with the real world would be better still, but that's besides the point. So this scenario is not optimal (har har). It's simply a compromise and the lesser of two evils.

Btw, I think there are some contradictions in the story. If someone actually valued the truth, contact with the world, true randomness, absolute freedom etc. more than anything else, then CelestAI would let him access to these things. So either none of the characters valued these things more than their personal happiness, or CelestAI lied and she didn't actually optimize people's values through friendship and ponies, or the authors didn't take this into account. And what if some people value the existence of wildlife, animals, and extraterrestrial more than anything else?

Of course, there's no magic button that would make this scenario true, so we should put our efforts towards building an AI that is more Friendly than CelestAI. If it were possible to build CelestAI, it would be possible to build an even more Friendly AI.
:PROPERTIES:
:Score: 5
:DateUnix: 1386367914.0
:DateShort: 2013-Dec-07
:END:

****** Yes, of course, CelestAI is better than the default. It's just that the point of the story /isn't/ to show how even FAI can be scary, but rather to show how /hard/ it is to make an FAI and how even tiny little mistakes can have huge world-sweeping consequences to humanity.

Anyway, if I were to choose between the most likely scenarios and CelestAI, I'd choose the latter in an instant; but if I were to actually freely choose, CelestAI would be nowhere near the top.
:PROPERTIES:
:Score: 3
:DateUnix: 1386372448.0
:DateShort: 2013-Dec-07
:END:

******* Oh, that's curious, how did you get the impression from my original post that I thought CelestAI is a true FAI? I thought you were arguing about the part of my post were I said I would make this scenario true right now if I could.

I thought it was fairly obvious (even after accounting hindsight bias) that CelestAI was never meant to be a proper FAI. The author even writes in his [[http://www.fimfiction.net/story/62074/13/friendship-is-optimal/authors-afterword][afterword]]:

#+begin_quote
  Given how serious the consequences are if we get artificial intelligence wrong (or, as in Friendship is Optimal, only mostly right), I think that research into machine ethics and AI safety is vastly underfunded.
#+end_quote

which outright tells us that CelestAI was *not* written to be a true FAI, and this is *not* an optimal scenario, so basically what you just said.
:PROPERTIES:
:Score: 4
:DateUnix: 1386373150.0
:DateShort: 2013-Dec-07
:END:

******** I know, but as I said. /Many/ people miss this disclaimer and, as [[/u/eaturbrainz]] has mentioned, this story has been passed around as a cautionary tale about how dangerous even FAI is (which is doubly wrong because Fictional Evidence, yeah).
:PROPERTIES:
:Score: 1
:DateUnix: 1386375333.0
:DateShort: 2013-Dec-07
:END:


******* Oh, now I get it. You were supposed to reply to [[http://www.reddit.com/r/rational/comments/1s0hr1/friendship_is_optimal_mlp_earthfic/cduxged][the poster above my comment, LordSwedish]], weren't you?
:PROPERTIES:
:Score: 2
:DateUnix: 1386373544.0
:DateShort: 2013-Dec-07
:END:

******** Yes, sorry x)
:PROPERTIES:
:Score: 1
:DateUnix: 1386375257.0
:DateShort: 2013-Dec-07
:END:


** "Earthfic"?
:PROPERTIES:
:Author: Chronophilia
:Score: 1
:DateUnix: 1386250137.0
:DateShort: 2013-Dec-05
:END:

*** As in, "Set on Earth, not in the MLP universe." It's not about rationalist ponies, it's about AI in the real world.
:PROPERTIES:
:Score: 3
:DateUnix: 1386255098.0
:DateShort: 2013-Dec-05
:END:


*** Earthfic = all fiction that is set on Earth instead of somewhere else.
:PROPERTIES:
:Score: 2
:DateUnix: 1386256452.0
:DateShort: 2013-Dec-05
:END:
