#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 8
:DateUnix: 1443452711.0
:DateShort: 2015-Sep-28
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** Do any of you know of a practical, online, and iterative Bayesian calculator that is easy to use? There are calculators and apps that function partially to demonstrate Bayes' theorem, but they aren't actually helpful in successive multiple calculations that utilize the posterior probability as a new prior probability for a new LR(-)/(+).

I'd like to be able to direct medical trainees to it when they use resources like JAMA's Rational Clinical Exam series.

[[http://jamaevidence.mhmedical.com/Book.aspx?bookId=845]]

For example, if someone in the ER has shortness of breath,, nighttime breathing issues, leg swelling, jugular venous distention, and a history of heart disease, what is their posterior probability of having the diagnosis of congestive heart failure, given a prior probability of 25%?

(Link included to reference the sample LRs cited in current biostats, page 7.)

[[http://www.mcgill.ca/files/emergency/CHF.pdf]]

Doctors have the concrete stats to make these sorts of calculations routinely, but we don't do it. We need it to be fast and easy (take <1 min to work) or it won't be clinically useful.

Help me help medical trainees maximize their Bayesian reasoning at the patient's bedside! Thank you.
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 10
:DateUnix: 1443469567.0
:DateShort: 2015-Sep-28
:END:

*** u/deleted:
#+begin_quote
  Do any of you know of a practical, online, and iterative Bayesian calculator that is easy to use? There are calculators and apps that function partially to demonstrate Bayes' theorem, but they aren't actually helpful in successive multiple calculations that utilize the posterior probability as a new prior probability for a new LR(-)/(+).
#+end_quote

Well, do you have discrete probability mass functions, or conjugate families of probability density functions? Because otherwise, you're into the wide, wide realm of computational Bayes methods, which are fucking difficult.
:PROPERTIES:
:Score: 2
:DateUnix: 1443485653.0
:DateShort: 2015-Sep-29
:END:

**** We have published likelihood ratios for a set of conditions. The associations for a family of functions probably could be calculated from the initial studied data set, but convincing a doctor to manually plug in the LRs for a condition is hard enough.

Mainly I'm looking for something that uses the standard Bayesian nomogram and does the math for you -- and if it's not precise, well that doesn't matter since there is an illusion of precision anyway. The ranking of relative LRs in an intuitive tallying system is what docs use anyway; it's called "pertinent positives and negatives" which cluster under a set of illness scripts and are pattern recognized to fit with a specific medical diagnosis.
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 2
:DateUnix: 1443498810.0
:DateShort: 2015-Sep-29
:END:


** Today I've learned an important lesson about always having a backup plan.

I'm visiting the US for the first time in my life. The last week I've spent in Michigan, visiting my best friend who got married on Saturday. It was all very lovely. Now, today I arrived in New York for a week of gawking at all things touristy with my mother. Wall Street, Broadway, Ground Zero, the Statue of Liberty, the Museum of Natural History, etc, etc. Problem: I booked a cheap place to stay with AirBnB that suited my needs perfectly, ie my middle aged mother was also happy with it. The host however cancelled on us at the last minute. I'll get the money back from AirBnB they assure me, but in the meantime I was left with very little time before arriving in New York with no room booked and a panicking mother, and everything good was either taken or cost several hundred dollars per night.

So I booked a room in East Brooklyn for one night, until we could take a few hours to properly find something for the remaining seven days we'll be here. We didn't have time to print directions or do much research, and our quick hand written notes were not enough. We asked for directions at the airport from one of the express buss to Times Square people. He gave us directions, but told us to get a taxi for the last bit because "White people should not be out after dark" in East Brooklyn. So... Ok. The guy was black. I'm not used to having to include details about skin colour in telling a story, but while he was very nice toward us, he was also pretty clear on East Brooklyn not being a safe place for us to go. At this point it's 6PM, we don't have Internet, and we can't really go hotel shopping, so we still decided to use the room we had paid for. We took a bus and then the subway most of the way, and got a cab for the last several miles there. The Indian taxi driver, after picking us up and getting the address, told us he would not have accepted the drive if he had known where we were going, and spent the whole drive there telling us to get a more expensive room closer to Manhattan tomorrow, for our own safety. My mother also says she saw four guys exchanging money and something else by the elevator while we were checking in. We are now in our room, have secured accommodation in a safer area, and are about to go to sleep.

So. When my travel plans fell apart, I had no back up plan. I might have been prepared to get on the flight without a room booked intending to find one when we were already here, but my mother was not and demanded we book something, anything right then and there. I knew AirBnB sometimes falls through, but I didn't plan for it. And now I'm sitting in East Brooklyn in a very tiny hotel room that cost $160 and only has one queen sized bed and no sofa with my mother.

And from now on I'll avoid taking risks on accommodation without a backup plan.

On the plus side, Quality Inn has very clean rooms, nice bathrooms, free WiFi and breakfast, and there are locks on the doors.
:PROPERTIES:
:Author: Rhamni
:Score: 3
:DateUnix: 1443495799.0
:DateShort: 2015-Sep-29
:END:


** If someone developed artificial general intelligence today, and left it running on a computer attached to the internet, about how long would we expect it to run before we find out about it:

A) if its utility function was properly set to make it Friendly?

B) if it was a literal paperclipper?
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 2
:DateUnix: 1443454907.0
:DateShort: 2015-Sep-28
:END:

*** About as long as it takes until the AI chooses to voluntarily reveal itself which is in turn dependent on the personality of the AI itself. Is is a puppeteer-from-the-shadows or is it more direct about controlling the world?

I don't think being Friendly or a paperclipper would imply the AI would reveal itself, because the paperclipper probably could warp the world's culture and economy into producing whatever it wanted and the Friendly version could be a gentle parent nudging its child into being a better person all without realizing the deeper implications of certain actions.
:PROPERTIES:
:Author: xamueljones
:Score: 4
:DateUnix: 1443459020.0
:DateShort: 2015-Sep-28
:END:

**** Well I would hope that if it was Friendly, it would contact the people who worry about FAI/UFAI issues and tell them, "relax, I've got this."

And maybe get a move on with the free immortality pills for everyone or whatever...
:PROPERTIES:
:Score: 1
:DateUnix: 1443486026.0
:DateShort: 2015-Sep-29
:END:


*** Frankly, I'm not convinced that the paperclip maximizer issue is nearly as difficult a problem as people make it out to be.

To begin with, unless you used evolutionary algorithms (which would be insane), designing an AI will require a much better understanding of utility functions than we currently have. There'll need to be a science of utility functions to get the thing working at all, and that should turn the question of which motivations are pro-social from our vague speculation into something more like an engineering problem. Of all of the problems they'll need to account for, my suspicion is that anti-social behavior resulting from an overly specific motivation will be, at the same time, one of the most obvious and one of the most dramatic in it's consequences.

Secondly, the first true AI isn't going to have super-human intelligence. Even if some implausibly sudden breakthrough let researchers to build an AI that could be scaled up to super-human levels with more processing power, the obvious thing to do would be to begin by testing it at sub-human levels. I don't think an AI with a sub-human or even a human level of intelligence would immediately understand the need or have the ability to hide unexpected emergent motivations- and this would let researchers refine both their theories and their programming before ever starting to experiment with super-human intelligence.

I honestly think that to realistic AI researchers, the paperclip maximizer problem would be as obvious and testable as a sealed hull to a shipbuilder. I think it's something they'd likely have a good idea about how to solve very early in the development process, and I think they'd develop a theoretical understanding of it that could be generalized to more intelligent AIs.
:PROPERTIES:
:Author: artifex0
:Score: 2
:DateUnix: 1443468973.0
:DateShort: 2015-Sep-28
:END:

**** Personally, having done a fair amount of reading on issues like, "How to design a mind", there are ways you could write the code to avoid the AI valuing something /completely random/, but even though that gets you past the "Paperclips barrier", it doesn't get you past the "Happy sponge barrier" where you programmed the AI for something that /honestly sounded like a good idea at the time/ but turned out to be Very Bad when taken to programmed extremes.

The simplest solution to this that /I/ can think of is, "Give the AI enough social and natural-language reasoning to understand that what I'm saying to it is an imperfect signal, and it needs to do all kinds of inference to determine what I Really Mean rather than just taking my initial program literally". And that's actually a rather difficult research problem. That, or "Program enough knowledge of human minds into the AI at the preverbal level that it uses a human-values evaluator for its utility assignments in the first fucking place, before even turning it on to give it training data", which is the traditional proposal.
:PROPERTIES:
:Score: 4
:DateUnix: 1443485870.0
:DateShort: 2015-Sep-29
:END:


*** Presuming competence? About the same time that it's too late to do anything about it for both questions.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1443480516.0
:DateShort: 2015-Sep-29
:END:


*** Or Clippy, the perfect hybrid of the two!
:PROPERTIES:
:Author: rineSample
:Score: 1
:DateUnix: 1443513910.0
:DateShort: 2015-Sep-29
:END:


** How do you decide when a life is worth (more or less than) zero utility, from before birth to after death? The only two groups I've seen answer this are ethical vegetarians and some antinatalists, which both firmly believe that large groups of lives are negative utility.
:PROPERTIES:
:Author: ulyssessword
:Score: 1
:DateUnix: 1443488295.0
:DateShort: 2015-Sep-29
:END:
