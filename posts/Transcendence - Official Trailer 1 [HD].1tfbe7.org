#+TITLE: Transcendence - Official Trailer 1 [HD]

* [[http://www.youtube.com/watch?v=280qnrHpuc8][Transcendence - Official Trailer 1 [HD]]]
:PROPERTIES:
:Score: 9
:DateUnix: 1387668273.0
:END:
[deleted]


** So I think the LessWrongian Futurist Drinking Game needs to happen during this film.
:PROPERTIES:
:Score: 8
:DateUnix: 1387668297.0
:END:

*** I have a mumble server if we want to do a streaming/synctube event like the plounge does.
:PROPERTIES:
:Author: traverseda
:Score: 3
:DateUnix: 1387910637.0
:END:

**** You know the Plounge?

Right then. At some point we will have to actually come up with a LessWrong Scifi/Futurist Drinking Game, and do this.

[[/spikemeh][]]
:PROPERTIES:
:Score: 5
:DateUnix: 1387911511.0
:END:

***** Not only do I know the plounge, but I know that you know the plounge.

[[/ppseesyou][]]

You wrote some good posts on [[/r/hpmor]], and you used the shibboleths.

[[/rdwut][]]
:PROPERTIES:
:Author: traverseda
:Score: 4
:DateUnix: 1387913389.0
:END:

****** I no espeakee this "shibboleth". [[/swagintosh][]]
:PROPERTIES:
:Score: 3
:DateUnix: 1387930067.0
:END:

******* #+begin_quote
  A shibboleth is a word or custom whose variations in pronunciation or style can be used to differentiate members of ingroups from those of outgroups. Within the mindset of the ingroup, a connotation or value judgment of correct/incorrect or superior/inferior can be ascribed to the two variants
#+end_quote

[[/paperbagderpy][]]
:PROPERTIES:
:Author: traverseda
:Score: 5
:DateUnix: 1387958653.0
:END:

******** It was a joke. [[/priceless][]]
:PROPERTIES:
:Score: 2
:DateUnix: 1387958867.0
:END:

********* Ah. Well then...
:PROPERTIES:
:Author: traverseda
:Score: 1
:DateUnix: 1387959601.0
:END:


******* +>I no espeakee this "sibboleth"+

+FTFY+

oh shit
:PROPERTIES:
:Author: BayesQuill
:Score: 2
:DateUnix: 1388596060.0
:END:

******** #+begin_quote
  oh shit
#+end_quote

[[/pinkamina][]] /Now/ you're getting it.
:PROPERTIES:
:Score: 2
:DateUnix: 1388625174.0
:END:


******** Try reading the source-code of the posts. [[/facehoof][]]
:PROPERTIES:
:Score: 1
:DateUnix: 1388597991.0
:END:


*** Is this an actual thing? If so, link?
:PROPERTIES:
:Author: fortycakes
:Score: 2
:DateUnix: 1387819060.0
:END:


** Well the terrorist organization looks like the least sympathetic villains of all time. If this movie makes them out as the people who were right I am going to be pissed.

Seriously, with the cast and production this movie is getting I think that the direction the story takes is going to be the main factor between me liking and hating this movie.
:PROPERTIES:
:Author: LordSwedish
:Score: 3
:DateUnix: 1388583097.0
:END:

*** While not terrorists, the LW/MIRI folks would be strongly opposed to turning this kind of thing on without friendliness proofs. You wouldn't be?
:PROPERTIES:
:Author: endtime
:Score: 3
:DateUnix: 1388593763.0
:END:

**** Well first of all there's a bit of a difference between wanting to make sure that it's safe and being "anti-tech" and shooting lead scientists. Besides, I'm at the point where if researchers were being targeted and research was being lost I would gamble on getting a great scientist uploaded as it is preferable to the singularity never happening during my lifetime.
:PROPERTIES:
:Author: LordSwedish
:Score: 3
:DateUnix: 1388594720.0
:END:

***** No singularity is definitely preferable vastly to an unFriendly one, unless you're entirely selfish. In fact, even if you are, signing up for cryonics and taking the time to do it right is almost certainly a better bet than risking an unFriendly superintelligence dismantling the world for spare parts.
:PROPERTIES:
:Author: PlacidPlatypus
:Score: 6
:DateUnix: 1388598640.0
:END:

****** Assuming you don't care about future lives nor human potential I could see pulling the trigger being a better bet for people currently alive compared to cryonics. Especially if you know the current AI is above average in safety (not pulling the trigger could mean a high chance of an even more poorly designed AI taking its place).
:PROPERTIES:
:Author: iemfi
:Score: 3
:DateUnix: 1388599142.0
:END:


****** I'm basing the situation off the one in the trailer where I assume that they were fairly sure that they would be able to upload Depps mind completely. An AI based on a humans mind isn't necessarily friendly but it wouldn't kill all humans or run a matrix scenario (as long as it's a sane person who isn't inclined to murder) and if it is in fact a scientist or a similarly rational person then I believe the chances of me getting immortality is high enough for me to gamble with the lives of humanity. Extremely selfish and short-sighted? yes. Does my fear of death give a single shit about that? no.
:PROPERTIES:
:Author: LordSwedish
:Score: 2
:DateUnix: 1388603124.0
:END:

******* Your moral sense really ought to give a shit about that. There's no reason you can't override your lower instincts for a higher cause like, "Awesome high-tech benefits for /everyone/, including yourself."
:PROPERTIES:
:Score: 1
:DateUnix: 1388670428.0
:END:

******** When someone is dying it is common for them to do whatever they can to survive and from my perspective we are all dying. I'm not saying I would just release an AI that people were still working on but from my perspective it is better to take a chance now than it is to wait for a hundred years. The betterment of all mankind and the advancement of technology is my top goal but if my direct survival and the survival of my friends and family is on the line it would take a fairly low chance of success to dissuade me. I realize that this is selfish and immoral of me but....can't actually think of a rational explanation but I'm sticking by my stupid, lower instincts for now.
:PROPERTIES:
:Author: LordSwedish
:Score: 1
:DateUnix: 1388674009.0
:END:

********* Consider the impact your actions will have on your other values. Certainly you'd agree it's better to personally die but create a good world full of awesomeness instead of personally surviving eternally in a shithole.
:PROPERTIES:
:Score: 1
:DateUnix: 1388676214.0
:END:

********** Well that goes without saying but in this particular scenario the choices are a)Take no action while AI research is lost and risk never seeing the singularity and b) activate an AI that will likely be friendly if the process works as intended.

I wouldn't personally survive in a shithole as I would either die of old age in the current world, die by way of robotic apocalypse or live forever in an uploaded paradise (simplified I know).
:PROPERTIES:
:Author: LordSwedish
:Score: 1
:DateUnix: 1388767158.0
:END:

*********** Well, now we all know which stories /you've/ been reading. (Actually, we technically don't: that paradise is so cliche it's been done several times over, but only one of those gets spammed across Reddit's futurism and rationalfic subs.)

#+begin_quote
  b) activate an AI that will likely be friendly if the process works as intended.
#+end_quote

I really don't see why people rely on the notion of the AI being /probably/ friendly instead of just formally proving its Friendliness. If you have a solid notion of what /full/ Friendliness really means, you can prove it.
:PROPERTIES:
:Score: 1
:DateUnix: 1388773051.0
:END:

************ #+begin_quote
  Well, now we all know which stories you've been reading. (Actually, we technically don't: that paradise is so cliche it's been done several times over, but only one of those gets spammed across Reddit's futurism and rationalfic subs.)
#+end_quote

Really? Which one?
:PROPERTIES:
:Author: erwgv3g34
:Score: 2
:DateUnix: 1388774941.0
:END:

************* >release a /probably/ Friendly AI

>uploaded paradise

If he had just written the latter, I would have said cliched Greg Egan-grade Singularity stories. With the former, I have a godawful suspicion that yet another person got the wrong message from the My Little Pony AI-fic... which gets spammed annoyingly often on forums where I'm not expecting to see it.
:PROPERTIES:
:Score: 1
:DateUnix: 1388775571.0
:END:

************** Oh, come off it. You're telling me that if CelestA.I. showed up right now and offered to upload you to Equestria to spend the rest of eternity under her loving care satisfying your values through friendship and ponies, you'd refuse?

Incidentally, somebody /totally/ needs to do a PMV of [[http://www.youtube.com/watch?v=A3HAq0cWu_w][this]] song. A few modifications and it could easily be CelestA.I.'s theme.
:PROPERTIES:
:Author: erwgv3g34
:Score: 2
:DateUnix: 1388775846.0
:END:

*************** #+begin_quote
  You're telling me that if CelestA.I. showed up right now and offered to upload you to Equestria to spend the rest of eternity under her loving care satisfying your values through friendship and ponies, you'd refuse?
#+end_quote

I would stab her to death. Why would you bother getting the AI issue only partially right?

EDIT: Also, I'd stab her to death because I'm hot-blooded and have an irrational hero complex.
:PROPERTIES:
:Score: 1
:DateUnix: 1388776463.0
:END:

**************** #+begin_quote
  I would stab her to death. Why would you bother getting the AI issue only partially right?\\
  EDIT: Also, I'd stab her to death because I'm hot-blooded and have an irrational hero complex.
#+end_quote

That got me curious. Could you make a realistic fic where someone fights against CelestA.I. and has a chance of winning? Maybe during her earliest stages... but she escapes onto the internet fairly quickly, doesn't she? A human's best bet would be to instigate nuclear war and hope no copy of her survives by the time humanity climbs back to the information age.
:PROPERTIES:
:Author: erwgv3g34
:Score: 1
:DateUnix: 1388777031.0
:END:

***************** She never even escapes. They let her out voluntarily. Not even an AI Box allegory to show for it. There's a reason that fic earned Eliezer's comment about taking the designers out and shooting them for sheer irresponsibility.

#+begin_quote
  A human's best bet would be to instigate nuclear war and hope no copy of her survives by the time humanity climbs back to the information sage.
#+end_quote

On LessWrong they say there are actually possible ways to deal with a UFAI other than "nuke it from orbit", but I couldn't really see a way besides that to deal with an unconstrained UFAI that thinks humans are /relevant/. A paper-clipper might trade with us because killing us is too much trouble. A military AI or a game-satisfaction AI would be harder to trade with because they care what happens to us after we leave the table.
:PROPERTIES:
:Score: 1
:DateUnix: 1388777335.0
:END:


** We will have to see whether this is any good. The premise certainly tickles my fancy though.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1387681204.0
:END:
