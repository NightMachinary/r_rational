#+TITLE: [D] Wednesday Worldbuilding Thread

* [D] Wednesday Worldbuilding Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 8
:DateUnix: 1494428659.0
:DateShort: 2017-May-10
:END:
Welcome to the Wednesday thread for worldbuilding discussions!

[[/r/rational]] is focussed on rational and rationalist fiction, so we don't usually allow discussion of scenarios or worldbuilding unless there's finished chapters involved (see the sidebar). It /is/ pretty fun to cut loose with a likeminded community though, so this is our regular chance to:

- Plan out a new story
- Discuss how to escape a supervillian lair... or build a perfect prison
- Poke holes in a popular setting (without writing fanfic)
- Test your idea of how to rational-ify /Alice in Wonderland/

Or generally work through the problems of a fictional world.

^{Non-fiction should probably go in the Friday Off-topic thread, or Monday General Rationality}


** How would you go about designing a science fiction setting that wants to make at least some attempts in the direction of scientific plausibility? My problem here is that science has this awful tendency to keep advancing and spin off in weird directions so that any setting will probably look really weird after just five or ten years.

This isn't an issue for one-and-done stories, but if I like a setting enough to write it in then I'm probably going to like it enough to want to visit it on multiple occasions and I'd prefer to not create a setting that looks fine at first but ages badly and gradually transitions into soft science fiction over the course of its lifetime.

There are a few possibilities that occur to me:

- Get over it. This isn't as much of a problem as I think it is, and most readers don't actually care as much as I do.
- Set the story after some sort of technological collapse (either full or partial) and insert schizo tech on the justification that there are lots of neat gizmos that people know how to replicate (at least in some cases) but don't know to improve on. Probably easiest to do if, for example, there are machines that take care of production and nobody knows how the machines work.
- Use a science fiction setting with no humans at all, and therefore an entirely different history of scientific advancement. It's plausible that another civilization could miss some of the advances that we've made (and maybe make some that we've missed), or at least it's plausible enough to shut up the critic in my head.
- Since the setting will eventually become alternate history /anyway/, make it an alternate history from the beginning. In that case, I just have to figure out what the most interesting point of divergence would be. I'm tempted to go for something in the early 20th Century, but I'm not sure.
:PROPERTIES:
:Author: callmebrotherg
:Score: 7
:DateUnix: 1494460437.0
:DateShort: 2017-May-11
:END:

*** I think that alternate history of technology is a sorely underdeveloped topic in the alternate history field. For example, it occurred to me a few months ago that it would have been /completely plausible/ for the telegram to be invented around ~1800, and this would have completely changed technological development. In fact, someone actually /did/ invent the telegram at that point, it was just extremely inefficient because he was trying to use a different wire for each symbol - the thing that was missing was Morse Code, the idea that you could encode all desired symbols as different patterns of signal on a single wire.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 8
:DateUnix: 1494462599.0
:DateShort: 2017-May-11
:END:


*** (partially joking, partially serious answer)

/Go big or go home. Fourth option, divergence point: Big Bang./

The quantum fluctuation that led to the birth of our universe was subtly different, leading to laws of physics that are different from those of our home universe. These laws of physics are similar enough to our universe that it's still possible for life to evolve (and maybe even look similar to us), but higher-level technology functions based on completely different principles.

- Advantages: You maintain the hard sci-fi atmosphere, but no one can accuse you of scientific implausibility, and you can pick the laws of physics to be whatever you find interesting and/or convenient.
- Disadvantages: The amount of effort required for consistency and plausibility might make it more trouble than it's worth; it's also not going to suit those people who insist that their sci-fi stories be possible in /our/ universe. (Then again, those people are pretty much doomed to be disappointed with any sort of science fiction, so...)
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 7
:DateUnix: 1494461232.0
:DateShort: 2017-May-11
:END:

**** If only I were a physicist...
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1494462701.0
:DateShort: 2017-May-11
:END:


**** I'm assuming you've already read [[http://lesswrong.com/lw/hq/universal_fire/][Universal Fire]], but that's another potential sticking point. If at some point in the future it turns out that some straightforward-seeming aspect of the fictional universe conflicts with an unrelated change somewhere, there's no good way to either predict it or handle it post-change.
:PROPERTIES:
:Author: waylandertheslayer
:Score: 1
:DateUnix: 1494521186.0
:DateShort: 2017-May-11
:END:


*** Personally I find post-collapse and non-human-society stories tend to be... implausible, as there are too many arbitrary coincidences required to let readers identify with, well, anything in the story.

Plan "just get over it" is inevitable, even if you go for alternative history. "Alternative future" is a fun genre though - posit some technological-but-not-social divergence point within the last ~decade or so, and roll it forward to asteroid mining for 3D printers (if the hype was justified...). The trick is to own it up front, and explicitly note that it's "the Future^{tm} as of YYYY, if X".
:PROPERTIES:
:Author: PeridexisErrant
:Score: 3
:DateUnix: 1494464117.0
:DateShort: 2017-May-11
:END:

**** I can understand a non-human society being difficult for readers to identify with, but what do you feel makes post-collapse stories difficult for readers to identify with what's going on?
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1494464505.0
:DateShort: 2017-May-11
:END:

***** Simply that post-collapse society would be (IMO) highly unlikely to resemble the ~2010s more than e.g. the ~1900s; I think there's a common misconception about the resilience of modern technology and social norms.

Obviously modern society is equal to the task of keeping it all working, but that relies on global coordination and infrastructure. A generation after collapse there would be no batteries, few modern weapons, massive problems with famine and plagues (rurals areas may be ok, but avoid cities!), etc.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 3
:DateUnix: 1494468660.0
:DateShort: 2017-May-11
:END:

****** Oh, sure, but plenty of people read century-old novels, don't they? The task of creating a sufficiently-alien culture might be difficult, but I'm not sure that too many readers would find it impenetrable.

Or am I misunderstanding?
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1494468761.0
:DateShort: 2017-May-11
:END:

******* I think we just have different taste in novels and thresholds for plausible weirdness :D
:PROPERTIES:
:Author: PeridexisErrant
:Score: 3
:DateUnix: 1494483926.0
:DateShort: 2017-May-11
:END:


*** - Set it far enough in the future that the technology is unrecognisable anyway.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1494577694.0
:DateShort: 2017-May-12
:END:


** Let's look at a scenario that involves a sudden empowerment event that respects previously-artificial natural borders.

Suddenly, superpowers are introduced to the world, and exactly one person in each country gets those superpowers - the leader of that country. For example, in the United States, the superpowered individual would obviously be the President. This introduces some major questions, obviously. Here are some of the biggest that occur to me.

- In some countries, there would be legitimate question as to who gets superpowers. Does the President of Germany get powers, or the Chancellor? My assumption is that the Prime Minister gets the UK's powers, but if the monarch got them instead, that fact alone would cause major changes to British politics because it would change how people mentally frame things.
- What happens if the Thing-Granting-Powers' world map is distinct from the internationally-recognized one in some way? (A bit of this is inevitable, since the internationally-recognized powers themselves don't even agree on the nation list.) Say, it fucks absolutely everyone over by giving powers to Abu Bakr al-Baghdadi, recognizing ISIS as a sovereign nation.
- Transfers of power. Peaceful transfers of power are pretty clear - the old leader loses their power when they leave the office, and the new leader gains a new power when they're sworn in. But what if there's an actual dispute, or if an elected leader launches a self-coup to stay in power, and tries to stonewall impeachment attempts? What happens if we wind up with multiple people running around claiming to have the same position simultaneously? Will the Thing-Granting-Powers settle the dispute itself by choosing one of them to give powers to? At what point does it recognize one side of a civil war as having succeeded well enough to count as a distinct country with a distinct leader? At what point does it recognize a country as having been conquered thoroughly enough to strip its leader of powers?
- Of course, the actual strengths of the powers have a huge effect on the world. The stronger the powers are in general, the more they'll really matter beyond giving supernatural legitimacy to nations. If some world leaders can face down armies of normals, persuade anyone to do anything, or have some strong "Thinker" power, that's a game-changer. The less of a correlation there is between the preexisting strength of a nation and the strength of that nation's leader-power, the more the empowerment will serve to equalize tiny, irrelevant countries with the old dominant nations. Tuvalu has about 1 leader for every 10,000 people, while India only has 1 leader for every 1.3 billion people! It would hurt nations more the larger they are, because they have more to defend and only one superpowered person to do it. I'd say the most interesting thing for balance would be to have a correlation between preexisting nation strength and power strength, but not a 1:1 correlation - so sometimes the Thing-Granting-Powers will shake things up by giving a critical nation an unimpressive power, or an obscure nation a game-changing power.
- The way some powers seem innately aligned with "good" or "evil", and how that effects their wielders' image, is a common theme in superhero stories. In this specific scenario, how would that effect things? How bad is it for a country if its leader happens to have a power that's fueled by death, or controls people, or is obviously useless outside of a fight to the death? How good is it for a country if its leader happens to have a healing power, or one that can be used for humanitarian purposes, or one with a light motif? How would powers be spun differently by their nations' supporters and enemies?
- How does the "meta" for nations work going forward? Secession attempts will be taken a lot more seriously, but some smarter nations may split up and decentralize, figuring that they'll be stronger as a group of small allies with a lot of powers than as one oversized nation with a single power. The Thing-Granting-Powers may call bullshit on this, responding to overly close alliances formed in this way by only giving the dominant nation a power. If the United States wanted fifty powers, for example, they'd need to actually balkanize; they couldn't just issue a statement saying "oh BTW the federal government is just an international body now (but still has all the same authority it did before)". It /is/ possible to munchkin this scenario, I think - just not by "fooling" the Thing-Granting-Powers. You can't fool it. It knows better.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 7
:DateUnix: 1494459296.0
:DateShort: 2017-May-11
:END:

*** u/callmebrotherg:
#+begin_quote
  What happens if the Thing-Granting-Powers' world map is distinct from the internationally-recognized one in some way?
#+end_quote

Depending on how it views the matter of tribal sovereignty, aboriginal populations in the U.S. and possibly elsewhere might also get more influence by dint of their superpowered leaders.

Also, does a state's power stay the same from leader to leader, or can it change during the transfer of power? Personally, I like the idea that powers change but orbit the same "theme."
:PROPERTIES:
:Author: callmebrotherg
:Score: 7
:DateUnix: 1494459746.0
:DateShort: 2017-May-11
:END:

**** I like that idea too; I was already firm that they change, and I hadn't come up with the idea that they orbit the same "theme" (kind of like powers running in families in Worm?) but I like it. Probably gradually alters in nature over time - I'm already firm that they gradually alter in /strength/ over time to correlate with national strength.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 3
:DateUnix: 1494460106.0
:DateShort: 2017-May-11
:END:

***** Regarding secession, since you like themes:

Any individual power can be thought of as having multiple components. For example, a power as simple as "shooting laser beams at people" can be divided into "lasers/light" and "energy beams."

If a country successfully secedes (whatever "successfully" means in this case) then the theme for that counter's leadership will develop from unused components in the power of the other country's leader. For example, if somebody seceded from the United States and the President at the time had "shoot laser beams" as a power, with "laser/light" being the running theme (and other Presidents having had powers like "bend light to become invisible"), then the leader of the seceding country would have powers that draw on the theme of "energy beams" (with "energy" interpreted very liberally, for the most interesting results).
:PROPERTIES:
:Author: callmebrotherg
:Score: 3
:DateUnix: 1494460771.0
:DateShort: 2017-May-11
:END:

****** I was kind of thinking along these lines, yeah. Similarly, if a country annexes another country, then the next time there's a leadership change, their power will have elements of the annexed country's theme.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 2
:DateUnix: 1494462748.0
:DateShort: 2017-May-11
:END:


*** u/ulyssessword:
#+begin_quote
  In some countries, there would be legitimate question as to who gets superpowers...
#+end_quote

Now I want to read a crackfic where Queen Elizabeth II gets 16 superpowers (from the United Kingdom, Canada, Australia, New Zealand, Jamaica, Barbados, the Bahamas, Grenada, Papua New Guinea, Solomon Islands, Tuvalu, Saint Lucia, Saint Vincent and the Grenadines, Belize, Antigua and Barbuda, and Saint Kitts and Nevis).

#+begin_quote
  ...the new leader gains a new power when they're sworn in.
#+end_quote

Is this one power per person, one power per person/country pair, or one power per leadership term?

If it's one per person, then it would make sense to have as many people as possible cycle through being the leader of a country, in hopes on getting something useful (like precognition) instead of something useless (like flying). Due to practical concerns, it would likely only be open to the leaders of the official opposition parties for a limited amount of time before the election (1 month each?), because they need real power to reveal their superpowers.

If the same power is maintained across country leaderships, this could even be done in other countries, similar to how some politicians start at the city level, move to the state level, then go federal.

If it's per-term, then it might make sense to elect a /set/ of people, with the one(s) with useless power(s) retiring in favor of their pre-elected successors. This could be done as simply as swapping between a President and a Vice President until the current leader gets something good enough.

#+begin_quote
  ... tiny, irrelevant countries...
#+end_quote

The [[https://en.wikipedia.org/wiki/Principality_of_Sealand][Principality of Sealand]] would be about 4% superpowered people if it counted. This would be a /huge/ boost to the micronation movement, and perhaps enough to make a few hundred on its own.
:PROPERTIES:
:Author: ulyssessword
:Score: 6
:DateUnix: 1494466063.0
:DateShort: 2017-May-11
:END:


*** u/Jakkubus:
#+begin_quote
  Transfers of power. Peaceful transfers of power are pretty clear - the old leader loses their power when they leave the office, and the new leader gains a new power when they're sworn in. But what if there's an actual dispute, or if an elected leader launches a self-coup to stay in power, and tries to stonewall impeachment attempts? What happens if we wind up with multiple people running around claiming to have the same position simultaneously? Will the Thing-Granting-Powers settle the dispute itself by choosing one of them to give powers to? At what point does it recognize one side of a civil war as having succeeded well enough to count as a distinct country with a distinct leader? At what point does it recognize a country as having been conquered thoroughly enough to strip its leader of powers?
#+end_quote

What about the powers and their level being dependant on how much people in particular country actually considers empowered individual a leader?
:PROPERTIES:
:Author: Jakkubus
:Score: 3
:DateUnix: 1494527887.0
:DateShort: 2017-May-11
:END:


*** For a story like this, I would have the powers be determined by the nature of the folklore in the respective nations. So the president of America gets something relating to Native American folklore or relating to the holidays like Thanksgiving. Also the power is the same for all presidents.

This way, a country can be determined to exist or to have disappeared based on whether or not a sufficiently unique culture with an accompanying position of power currently exists.

Tie the existence of the power to the culture, not the governmental position. The position only determines who gets the power.
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1494524128.0
:DateShort: 2017-May-11
:END:

**** Is the Native American folklore really the dominant culture of America?

What about severely multicultural countries, or places where cultures correlate very poorly with borders (e.g. a good chink of Africa)?
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1494577618.0
:DateShort: 2017-May-12
:END:

***** u/xamueljones:
#+begin_quote
  Is the Native American folklore really the dominant culture of America?
#+end_quote

It's not, but it was the /first/.

#+begin_quote
  What about severely multicultural countries, or places where cultures correlate very poorly with borders (e.g. a good chink of Africa)?
#+end_quote

Then how about which ever culture appeared first and if there's a tie, then it comes down to which one is more prevalent at the time.
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1494579060.0
:DateShort: 2017-May-12
:END:

****** u/CCC_037:
#+begin_quote
  Then how about which ever culture appeared first and if there's a tie, then it comes down to which one is more prevalent at the time.
#+end_quote

Hmmmmm.

In South Africa, the /first/ would probably be the Khoisan culture. The Khoisan were more or less minding their own business down here, when one day the Bantu peoples wandered down from further north at around the same time as European explorers in ships started landing in the far South. (Then a lot of complicated stuff happened). Now the /biggest/ cultural group is the aggregate of the Bantu groups (with different but related cultures) - the most economically powerful cultures have strong European roots (but have since changed a bit, as cultures do) - and the Khoisan culture is of minimal effect to go with their minimal numbers. (There's still a few people around, but I understand the culture has been largely destroyed, mostly by being overcome by other cultures but there was a fair amount of generations-ago war in there as well).

By the whichever-appeared-first rule, President Zuma (who draws from Zulu (a subset of Bantu) culture himself) would get powers based on Khoisan culture and mythology. (How long does this last? If the last member of the Khoisan in South Africa dies, do Zuma's powers abruptly shift?)

What happens as cultures change?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1494582594.0
:DateShort: 2017-May-12
:END:


** Let's expand on what [[https://www.reddit.com/r/rational/comments/5ldb9z/how_would_a_society_deal_with_an_easily_teachable/][this]] idea leads to.

Essentially, every human has a Death Note-lite. Any human can instantly kill any other human with a thought, if the former has enough information about the latter. ('Enough information' in information-theoretic sense, but for the sake of simplicity, let's assume that means a descriptor unique for that human: a name, a face, a username, etc; or their unique combination.)

1. If every human was granted this ability in Stone Age, could a large society or a technological civilization arise, even in theory?

2. In classical era?

3. In modern era?

4. You are tasked with designing the perfect world for these humans (population: ~1 billion). A genie would implement it. Restrictions: you can only use the already-developed technology; the ensuring civilization cannot consume more resources than /Kardashev I/; the population must be situated on Earth.

5. *Edit:* Same as 4, except at any point in time, there's N 'normal' humans alive. Normal humans cannot kill with a thought and cannot /be/ killed with a thought. If a normal human dies, a random deathnoter becomes normal, which both he/she and the other normal humans become magically aware of.

   What is the lowest N that would make a functional society possible?
:PROPERTIES:
:Author: Noumero
:Score: 5
:DateUnix: 1494446495.0
:DateShort: 2017-May-11
:END:

*** 1. No.
2. No.
3. No.
4. Total anonymity is probably the only way to even /remotely/ guarantee the possibility of safety, but that's really really hard to do (especially since you limited us to already-existing technology). And even if you /do/ manage to implement this somehow, this makes a large-scale society nearly impossible to run, since you have no way of specifying who's who, making economic transactions impossible in principle. Truthfully, I doubt there's a feasible way to go about constructing a society with insta-kill powers unless the agents populating that society have a completely different psychology from humans. I realize this is an unsatisfying answer, but the fact is that the incentive structure present in the scenario you give makes things so unstable that I'm not sure an equilibrium state other than "everyone dies" even /exists/.

TL;DR: Giving a single person Death Note-esque powers is bad. Giving /everyone/ such powers is an extinction event.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 6
:DateUnix: 1494448670.0
:DateShort: 2017-May-11
:END:

**** Well, let's turn it around. If there is zero privacy, you /could/ kill anyone, but it would be known that you did it, and you'd probably die soon after. Do you hate someone so much that you'd be willing to die to kill them?

Terrorists and the insane could do a lot of damage though... so the perfect society would have to produce few or none of those.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 3
:DateUnix: 1494449623.0
:DateShort: 2017-May-11
:END:

***** u/696e6372656469626c65:
#+begin_quote
  but it would be known that you did it
#+end_quote

Um, how? Unless I missed something in [[/u/Noumero]]'s post, there's no way to tell who killed someone using their power (or even that the person was killed via supernatural means at all). And even if there were some way to track who killed who, as you said yourself,

#+begin_quote
  Terrorists and the insane could do a lot of damage though... so the perfect society would have to produce few or none of those.
#+end_quote

and there's no way to guarantee that /with our current technology levels/, which [[/u/Noumero]] limited us to.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 2
:DateUnix: 1494450779.0
:DateShort: 2017-May-11
:END:

****** Yep, there's no way to tell who did it.

#+begin_quote
  and there's no way to guarantee that /with our current technology levels/, which [[/u/Noumero]] limited us to.
#+end_quote

Because if I didn't do that you'd instantly solve everything by putting FAI in charge or something. I know how [[/r/rational][r/rational]] thinks.
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1494450956.0
:DateShort: 2017-May-11
:END:

******* I see.

I was thinking not of supertech but just a society that is safe and stable, and all children were raised in loving families and mental illness detected early and treated before the powers set in (I assume infants being hungry or toddlers throwing a tantrum does not results in mass murder, but that the power sets in later in life).

I do however have to concur with [[/u/696e6372656469626c65]] and say that no, a society of death noters would cease to be a society fairly quickly if there were no accountability.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 3
:DateUnix: 1494452053.0
:DateShort: 2017-May-11
:END:

******** Let's try something else, then. Suppose that instead of being forced to populate your society with /humans/, you also get to design a new type of mind with which to fill that society.

1. Easy mode: You get to design both the society itself and the type of mind that will populate it. Can you create a societal arrangement that is stable in the long term? (Again, with [[/u/Noumero]]'s caveat that the technology level of the society in question cannot exceed our own.)
2. Hard mode: You get to design the mind, but not the societal arrangement. The Death Noters start in the Stone Age with whatever psychology you specify. Can you specify a psychology such that a species of Death Noters with that psychology will eventually grow into a large-scale technological civilization?

--------------

*EDIT:* I will also impose the additional restriction that whatever mind design you come up with must have comparable intelligence to humans. This is for the same reason as [[/u/Noumero]]'s caveat: no FAI-style solutions.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 2
:DateUnix: 1494452739.0
:DateShort: 2017-May-11
:END:

********* Attempt at hard: Make a mind just like the human one, except that it doesn't want to use the death note.
:PROPERTIES:
:Author: Gurkenglas
:Score: 6
:DateUnix: 1494453781.0
:DateShort: 2017-May-11
:END:

********** ...I confess, this is a novel solution that I did not anticipate beforehand. But I think the most likely result is that you'll end up with people torturing other people in order to get /them/ to use their Death Note powers. At the very least, the situation isn't as inherently unstable as it would be with actual humans, but still, I imagine things would get ugly quite quickly, and it's questionable whether such an arrangement would ever manage to even invent the scientific method, much less elevate itself to a higher technological level.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1494454268.0
:DateShort: 2017-May-11
:END:

*********** Ok, hard mode attempt: Make a mind just like the human one, but using the deathnote power is not instinctual. Noone knows how to use it, and the process by which one can activate it would be a convoluted mess so long and arduous that finding it the odds of finding it by chance would be as close to 0 as I could get.
:PROPERTIES:
:Author: Nuero3187
:Score: 2
:DateUnix: 1494544069.0
:DateShort: 2017-May-12
:END:


********* That seems way easier.

A hivemind without sense of personal identity which considers other 'individuals' a part of itself, therefore incapable of using the power of killing at all.

A more interesting one: humanlike minds with less intense emotions and fewer cognitive biases, designed to naturally develop enlightened self-interest and long-term thinking in early age; add sociopathy to the mix to make it more interesting. The ensuring society would be ridiculously cutthroat but I think functional.
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1494455233.0
:DateShort: 2017-May-11
:END:

********** u/696e6372656469626c65:
#+begin_quote
  A hivemind without sense of personal identity which considers other 'individuals' a part of itself, therefore incapable of using the power of killing at all.
#+end_quote

An actual hivemind is impossible given current technology levels, so I assume you're talking about a mind whose sense of empathy is so strong that it views other individuals as equivalent to itself despite not actually sharing their experiences and thoughts. How would such a species respond to scarcity? For example: suppose a food shortage occurs, and there's not enough food to ensure everyone lives. How would a hypothetical race of such people allocate their food supply? Distributing the food equally will simply cause everyone to die of malnutrition. (A sort of reverse [[https://en.wikipedia.org/wiki/Tragedy_of_the_commons][tragedy of the commons]], if you will.)

--------------

*EDIT:* Never mind, randomization works (obviously). Don't know why I didn't think of it.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 2
:DateUnix: 1494456448.0
:DateShort: 2017-May-11
:END:


********* I mean this should be /really/ easy, just make people's minds such that everyone universally possesses certain qualities that you would like. For instance everybody invariably ending up with a moral system similar to your own, and nobody ever developing mental illnesses.\\
So nobody will ever want to use the death note except in scenarios you would consider acceptable, and nobody is ever deluded such that they believe it is acceptable to use the death note when it's not.\\
This is basically the equivalent of giving a GAI your ethical system instead of trying to place restrictions on its actions in hopes of preventing it from doing things you don't want.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1494459424.0
:DateShort: 2017-May-11
:END:

********** Well, yes, it is easy in principle. The hard part is (as always) in practice. So something like this

#+begin_quote
  just make people's minds such that everyone universally possesses certain qualities that you would like. For instance everybody invariably ending up with a moral system similar to your own, and nobody ever developing mental illnesses.

  So nobody will ever want to use the death note except in scenarios you would consider acceptable, and nobody is ever deluded such that they believe it is acceptable to use the death note when it's not.
#+end_quote

is not good enough. For one thing, you still haven't specified what your moral system /is/. And you can't skirt the issue by saying "everyone's a mind-clone of me", either, because that's not possible without engineering knowledge /considerably/ beyond our current capabilities. There's also the fact that you're specifying a /psychology/ here, not a set of hardcoded rules--and psychological tendencies can change over time due to a whole host of potential influences. You're allowed to postulate outlandish things like minds with hivemind-esque levels of empathy, but saying "everyone has my morals forever" just doesn't cut it, unfortunately.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1494460196.0
:DateShort: 2017-May-11
:END:

*********** There's ways of getting around a direct engineering of value system. Just specify that that genes are changed such that people invariably end up with nearly the same moral instincts. Then define that moral instinct in terms of being one which would if replacing your current one cause you to make the exact same moral decisions you would normally make. Point is you can easily use conditionals that basically rely on a simulation of oneself.

As for the objection about not having the sufficient engineering knowledge, well that objection could apply to pretty much /any/ mind engineering including the hivemind example, since we just don't understand enough about human brains. So it's not clear in what way mind-clones are more complicated than inventing some new hivemind psychology.\\
Also I never said we need hardcoded rules, the basic idea is simply to replace the genes that usually result in people developing moral systems with genes that are far more specific and less open to environment in developing their function, to cut down variation.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1494461290.0
:DateShort: 2017-May-11
:END:

************ u/696e6372656469626c65:
#+begin_quote
  There's ways of getting around a direct engineering of value system. Just specify that that genes are changed such that people invariably end up with nearly the same moral instincts. Then define that moral instinct in terms of being one which would if replacing your current one cause you to make the exact same moral decisions you would normally make. Point is you can easily use conditionals that basically rely on a simulation of oneself.
#+end_quote

This is the standard definition of reflective consistency, yes. Unfortunately, /it doesn't work as an answer to the question I posed/, which asks you to describe a /specific/ type of mind. Does this mean I want a source code for a computer program written out in C that, when compiled, produces the mind in question? No. What it /does/ mean, however, is that "a mind that shares my values, whatever those happen to be" is sufficiently vague that I consider it underspecified.

#+begin_quote
  As for the objection about not having the sufficient engineering knowledge, well that objection could apply to pretty much any mind engineering including the hivemind example, since we just don't understand enough about human brains. So it's not clear in what way mind-clones are more complicated than inventing some new hivemind psychology.
#+end_quote

The hivemind psychology is just that: a /psychology/. It's a general property that can exist across a variety of possible minds, and even if we don't know how to make one, it's at the very least plausible that an entire species might possess such a psychology. Mind-clones, on the other hand, are all copies of a /single/ mind by definition, which is biologically impossible without external engineering. It's in this sense that I say we don't have the engineering knowledge to do what you're saying.

Or, to put things another way: you can tell the genie to create a species with an extremely high level of empathy, and this will be a /species-wide property/--if two members of the species reproduce, their offspring will also possess a high level of empathy. If you try to tell the genie "every person in this society has an exact copy of my mind", on the other hand, your society falls apart the instant a baby is born because /that baby will not be a mind-clone/, and there's no way to /make/ it be a mind-clone without engineering knowledge that /we don't have/.

#+begin_quote
  Also I never said we need hardcoded rules, the basic idea is simply to replace the genes that usually result in people developing moral systems with genes that are far more specific and less open to environment in developing their function, to cut down variation.
#+end_quote

This is not how genes work. Like, I get what you're trying to do here, I really do, but this is simply not how genetics works. There /are/ no "genes that usually result in people developing moral systems", and there's no genetic arrangement specific enough to hardwire a particular /brain design/ into every member of a species. The best you can do is provide a tendency for people to be sociopaths, or to consume large amounts of glucose, or to want multiple sexual partners, etc. But trying to specify a full /moral system/ in the genes of a particular species is an impossible task.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1494462337.0
:DateShort: 2017-May-11
:END:

************* u/vakusdrake:
#+begin_quote
  This is not how genes work. Like, I get what you're trying to do here, I really do, but this is simply not how genetics works. There are no "genes that usually result in people developing moral systems", and there's no genetic arrangement specific enough to hardwire a particular brain design into every member of a species. The best you can do is provide a tendency for people to be sociopaths, or to consume large amounts of glucose, or to want multiple sexual partners, etc. But trying to specify a full moral system in the genes of a particular species is an impossible task.
#+end_quote

I disagree with the claim you couldn't arrange genetics such that the resulting neurology would consistently develop into a particular range of desired moral systems. Mainly I think you're forgetting just how much information is already encoded. From the perspective of a truly alien amoral entity it would likely appear that most humans are already hard coded with a relatively small range of moral systems. I think it's underappreciated just how similar most people's moral beliefs already are once you strip away differing models of reality and just how complex people's moral instincts are, and i'm sure you're aware that some of these things like a desire for fairness are present in other animals. If genetics can consistently produce complex intuitions about fairness than why exactly is it such a stretch that you could change things so that the moral systems that would arise would contain less variations? I'm not talking about something quite as complex as anything you're likely imagining either, after all most of my moral judgements are based on the most self consistent interpretation of my gut feelings anyway, I'm not proposing that you somehow biologically encode some bizarre kantian ethics system. It's only necessary for my purposes that people be much more skeptical (to avoid bizarre models of reality confounding things) and have gut feelings about ethics very similar to my own.\\
Also I don't think you should really be so confident that even something like genetic memory is totally beyond what DNA is capable of producing. It can already be used to encode arbitrary computer data, so building a system that builds complex memory systems based on that information doesn't seem impossible (even if it might not be the type of system that would evolve naturally). To say that such genetic code that built a specified memory structure wasn't /possible/ would seem to be to make a rather bold claim about the fundamental limits about how complex and detailed a structure can be made via biological processes.

#+begin_quote
  This is the standard definition of reflective consistency, yes. Unfortunately, it doesn't work as an answer to the question I posed, which asks you to describe a specific type of mind. Does this mean I want a source code for a computer program written out in C that, when compiled, produces the mind in question? No. What it does mean, however, is that "a mind that shares my values, whatever those happen to be" is sufficiently vague that I consider it underspecified.
#+end_quote

What I said is hardly vague, since all you would need to do is run simulations of a vast number of minds and compare them to simulations of my own mind in order to determine which conditions will lead to minds within a range that produce informed moral judgements the simulations of me deem acceptable. That's why I mentioned the bit about using conditionals based on simulations of yourself.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1494466619.0
:DateShort: 2017-May-11
:END:

************** u/696e6372656469626c65:
#+begin_quote
  What I said is hardly vague, since all you would need to do is run simulations of a vast number of minds and compare them to simulations of my own mind in order to determine which conditions will lead to minds within a range that produce informed moral judgements the simulations of me deem acceptable. That's why I mentioned the bit about using conditionals based on simulations of yourself.
#+end_quote

Okay, let me try a different tack. This part of what you said, right here?

#+begin_quote
  run simulations of a vast number of minds and compare them to simulations of my own mind
#+end_quote

/I can't do that./ You haven't given me a /mind/; you've given me a /process for getting a mind/, and it's not even a process I'm capable of carrying out. To put it in programming terms: my original query asked for an object of type Mind; instead, however, you provided me a call to a function with /return type/ Mind. The problem is that this function is nothing more than a prototype, and so when I try to call it, I get an error. It's in this sense that I say your suggestion doesn't answer my question.

#+begin_quote
  From the perspective of a truly alien amoral entity it would likely appear that most humans are already hard coded with a relatively small range of moral systems. I think it's underappreciated just how similar most people's moral beliefs already are once you strip away differing models of reality and just how complex people's moral instincts are, and i'm sure you're aware that some of these things like a desire for fairness are present in other animals.
#+end_quote

The thing you're missing here is that human behavior, like that of most animals, is largely driven by /instinct/, not moral systems. Now, we happen to have a high-enough level of abstract reasoning skill that we're able to come up with and /describe/ a moral system that our actions are roughly consistent with, but from a purely biological perspective, it's our subconscious tendencies and desires that drive us (what Freud would call the id).

In other words: if you're trying to describe a (biological) mind in terms of moral imperatives, you're working on a higher level of abstraction that, from a reductionist point of view, simply does not exist. It's fine to talk about morality, but when your reference class is the /space of biologically plausible minds/, you're much better off talking about psychological tendencies (such as, again, the empathy-driven hivemind example). Which is to say:

#+begin_quote
  It's only necessary for my purposes that people be much more skeptical (to avoid bizarre models of reality confounding things) and have gut feelings about ethics very similar to my own.
#+end_quote

/This/ is a much better way of putting things than "everyone has the same morals I do". But even so, we run into the same problem as before: by describing these hypothetical people in terms of your own mind, you're offloading the vast majority of the complexity into a single word, "I". You're not giving any detail here--a black box labeled "I" would be about as informative. Here, try this question:

Would a society full of [[/u/vakusdrake]]'s mind-clones with insta-kill powers /really/ be stable? How sure are you that, given Death Note powers, you /wouldn't/ give into the temptation after a while? Maybe you're quite sure, I don't know--but that's the point: /I don't know./ I don't know because I don't have a good description of what your mind is like because /you didn't give me one/. Sure, you gave me a hypothetical process for finding out, but all that does is make a call to a function that doesn't exist. As far as worldbuilding goes, it's a non-answer, a dodge.

Hope that makes my viewpoint a bit more clear.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1494471582.0
:DateShort: 2017-May-11
:END:

*************** u/vakusdrake:
#+begin_quote
  The thing you're missing here is that human behavior, like that of most animals, is largely driven by instinct, not moral systems. Now, we happen to have a high-enough level of abstract reasoning skill that we're able to come up with and describe a moral system that our actions are roughly consistent with, but from a purely biological perspective, it's our subconscious tendencies and desires that drive us (what Freud would call the id).
#+end_quote

As I said before the point here is not to imbue some complex arbitrary moral system, but merely to insure they all instinctively have the same moral intuitions as myself. After all as I said before my ethical systems are generally the most self consistent interpretation of my moral intuitions. So the idea here is that when it comes to authoritarianism, murder or anything else they will have gut feelings which align with my own. Like I mentioned previously a massive amount of the core of people's ethics seems to be greatly determined by genetics, so I don't think the idea that you could control the gut feelings that one's ethical systems are built on is implausible.

However when it comes to specificity I think I was looking at this rather differently than you since my strategy was essentially to come up with the best strategy in universe as it were, based on what I know about the specifications of the setting (and thus I would assume that if I give clear instructions to the genie for how to get the information it needs to fulfill my instructions things ought to work out).

But if we're not allowing those sorts of tricks then I would just describe a laundry list of moral intuitions that I would be instilling, as a sort of poor approximation for just directly instilling my ethical system which while a terrible idea for inhuman or superintelligent entities should work quite well for humans. Ultimately I find that solution in many ways less satisfying since it forces a clear divergence in my behavior in and out of setting, since in setting I would have massive incentive to spend as much time and effort as possible coming up with an approximation for my moral system.

Whereas out of setting (so what I'm giving you as an example of something that may sort of resemble what I real version ought to resemble) I'm not going to do that (not that coming up with an explicit list of moral intuitions to stick in ideal humans isn't my idea of fun, just that I don't currently have too much time on my hands and i'm probably already spending more time writing this than I can justify) and would instead try something simpler like Rawlsian veil-of ignorance style contractualism as a base (so sort of like some of the slatestarcodex articles on the topic), then tack on much more intense versions of some of my moral intuitions so as to try to compensate for the lack of certain important intuitions I would likely not think to stick in. So yeah stick in a /much/ stronger aversion to violating people's preferences and a far great desire for altruism (with primary focus on making people able to satisfy their preferences, to guard against paternalism).\\
Then as I mentioned before just go through say the sequences and eliminate every flaw in human thinking you can, make people care far more about the truth and less personally attached to their beliefs as well as generally more curious. Plus since you said no superhuman intelligence I'll increase mental abilities as much as I can within those bounds, so everybody's a genius with an amazing ability to model others thinking.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1494530859.0
:DateShort: 2017-May-11
:END:


**** u/Noumero:
#+begin_quote

  1. No. 2. No. 3. No.
#+end_quote

Heh.

#+begin_quote
  Really, I doubt there's a feasible way to go about constructing a society with insta-kill powers unless the agents populating that society have a completely different psychology from humans; the incentive structure inherent in the scenario you specify makes the whole thing unstable
#+end_quote

Hmm. Wouldn't the following work?

All people are hiding in private residences scattered across the globe. Travel out of them is not permitted. Communication is conducted through anonymous boards. Every residence is connected to a railway through which regularly-arriving automated trains provide it with live necessities. Some residences have mostly-automated factories or farms, which produce shipments for the trains, and power plants which provide the whole system with energy.

If something breaks, you post a request to fix it online, and a script tells specialists about it without specifying which residence posted it; they arrive on the next few trains and fix it. Failure to fulfill one's duty is punished by decreasing the flow of goods into the residence, or by revealing that one's personal information online.

Reproduction... by perhaps artificial insemination --- we're trusting mothers to not kill their children already --- with strict birth control, so that there's never more adult humans than functional residences.

--------------

^{It's going to break down due to technological failures in a decade even if I didn't miss anything crucial, isn't it?}
:PROPERTIES:
:Author: Noumero
:Score: 1
:DateUnix: 1494450270.0
:DateShort: 2017-May-11
:END:

***** u/696e6372656469626c65:
#+begin_quote
  they arrive on the next few trains and fix it
#+end_quote

Nitpick: this part should probably be changed so that the item gets sent to the specialists instead in order to minimize the chance that someone meets someone else.

Anyway, nitpicks aside, the main problem you still have (other than technological failure) is that any open communications channel can be exploited to transmit arbitrary types of information. Sufficiently determined people could, for instance, post a series of fix-requests for items whose first letters, when read in sequence, form a word or phrase. Randomization might suffice to address this particular issue, but people could also simply intentionally hide messages inside broken items that can be seen only when opening them to fix them up. All it takes is one or two people who are willing to cooperate to get themselves out of the hellhole they're in, and we suddenly have ourselves a conspiracy on our hands--one that we have no way of stopping almost by definition, since there's no centralized government to stop it /with/.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 3
:DateUnix: 1494451505.0
:DateShort: 2017-May-11
:END:

****** Hmm. Propaganda tailor-made to exuberate the paranoia and fear of other people to reduce the probability of cooperation beyond the already established system? Granted, it only makes the conspiracy less likely, i.e. pushes it farther in time, and has no way of acting against it once it arises.

--------------

*Edit:* Holy hell, I forgot about the children. Mortaility rate among mothers would be around 100% if parenting is not anonymized as well.

Giving people Death Note-esque powers is such a beautifully bad idea.
:PROPERTIES:
:Author: Noumero
:Score: 4
:DateUnix: 1494451988.0
:DateShort: 2017-May-11
:END:

******* If you didn't see it already, I did post a [[https://www.reddit.com/r/rational/comments/6adbg5/d_wednesday_worldbuilding_thread/dhe78k8/][comment]] above that slightly modifies your question in order to make things easier (where by "easier" I mean "actually possible").

(Side note: This entire chain of comments is [[/r/nocontext]] gold.)
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 2
:DateUnix: 1494453436.0
:DateShort: 2017-May-11
:END:


*** I'm assuming that a message does not automatically identify its sender. If the set of all messages from some sender does not identify them, you can use that as a username.

Attempt at 4: Let people form groups of up to about a hundred people each. Between groups, no identification-revealing contact is permitted. Within a group, there is zero privacy. If anyone dies by Death Note, everyone in their group is killed.

You might relax the last bit by letting a judge review whether the killing was reasonable, such as a group killing a member that tried to take the group hostage. The judge would become able to kill them all, but he's already supposed to decide their fate.
:PROPERTIES:
:Author: Gurkenglas
:Score: 4
:DateUnix: 1494455009.0
:DateShort: 2017-May-11
:END:

**** I'm pretty sure it ends with virtually every group dying within a few years.

In general, humans are emotional, short-sighted and selfish. Sooner or later someone in any group will have a bad day, find someone to blame for this, and get the entire group killed; the larger the group, the sooner it happens. And making groups smaller would run into the same conspiracy problem as discussed above.
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1494456132.0
:DateShort: 2017-May-11
:END:


*** New scenario: same as 4, except at any point in time, there's N 'normal' humans alive. Normal humans cannot kill with a thought and cannot /be/ killed with a thought. If a normal human dies, a random deathnoter becomes normal, which both he/she and the other normal humans become magically aware of.

What is the lowest N that would make a functional society possible?

[[/u/696e6372656469626c65][u/696e6372656469626c65]], [[/u/KilotonDefenestrator][u/KilotonDefenestrator]], [[/u/Gurkenglas][u/Gurkenglas]]?
:PROPERTIES:
:Author: Noumero
:Score: 3
:DateUnix: 1494457296.0
:DateShort: 2017-May-11
:END:

**** Okay, so this is where things start getting complicated. First of all, note that for higher values of N (say, exceeding 50% of the population), the Death Noters are likely to become normal themselves fairly quickly. So in order to not have our scenario quickly reduce to a society of normal people, we have to postulate a fairly low N.

The question then becomes one of how Death Noters interact with "normals". A key thing to consider here is the fact that normals /cannot be killed via Death Note powers/. This means that a functional government is now possible, and is likely to be composed predominantly of normals. (The upper echelons will probably consists exclusively of normals, since having a Death Noter in a position of visible power is simply far too risky.) This then leads us to ask the obvious next question: /how large must a government (including enforcers) be in this situation to be effective?/ An answer to this question will give us a minimum feasible value for N.

I'll be needing to head out shortly, so I'll leave the analysis here for now. Anyone else is welcome to add to what I said.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 4
:DateUnix: 1494459331.0
:DateShort: 2017-May-11
:END:


*** Just add the option of knowing who is the one that used his power (A hihgly enough civilization can do it) and is no much different form current word. I'm pretty sure I can kill anyone except high governement offcials but I know I will be thrown in prison.
:PROPERTIES:
:Author: hoja_nasredin
:Score: 1
:DateUnix: 1494536307.0
:DateShort: 2017-May-12
:END:

**** No can do. Everyone having an /untraceable/ instant surefire killing method is /the point/.
:PROPERTIES:
:Author: Noumero
:Score: 1
:DateUnix: 1494536859.0
:DateShort: 2017-May-12
:END:


*** In theory? Yes, a large society /could/ arise in the face of this power, but it would be a good deal less likely. (The odds of a large society arising become substantially better if it's possible to tell the identity of the person who instantly killed someone). I'm not really sure how this is particularly different to giving everyone in a given society a sniper rifle, except that this ability is easier and quicker to use and has no range limit.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1494577976.0
:DateShort: 2017-May-12
:END:
