#+TITLE: The Masquerade

* The Masquerade
:PROPERTIES:
:Score: 4
:DateUnix: 1464134965.0
:DateShort: 2016-May-25
:END:
[deleted]


** Does every rationalist have to have the goal of bettering humanity or giving equal access? The fewer people who share an advantage, the greater the relative advantage is, so it's pretty rational not to share this kind of information.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 8
:DateUnix: 1464135843.0
:DateShort: 2016-May-25
:END:

*** You're technically correct, but in my experience rationalists almost always are utilitarians that seek the betterment of humanity.
:PROPERTIES:
:Author: trekie140
:Score: 5
:DateUnix: 1464136590.0
:DateShort: 2016-May-25
:END:

**** Most rationalists are utilitarian at a personal level, but society-scale utilitarianism isn't really more than a tangential part of most peoples' value systems. Sure, we'd /like/ to see the betterment of society, but ultimately we're pretty self interested.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 7
:DateUnix: 1464138178.0
:DateShort: 2016-May-25
:END:

***** So you expect rationalists, including yourself, to keep extraordinary truths secret rather than reveal those findings so long as there is personal benefit to keeping them secret? Doesn't that run contrary to the scientific advancement we support?
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1464144578.0
:DateShort: 2016-May-25
:END:

****** That depends largely on the benefit the individual rationalist gets from keeping the secret.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 3
:DateUnix: 1464145525.0
:DateShort: 2016-May-25
:END:


****** People are flawed and fallible! If we were really dedicated to that, would we be wasting valuable time posting on Reddit? We have tribal and hedonic values.
:PROPERTIES:
:Score: 1
:DateUnix: 1464148477.0
:DateShort: 2016-May-25
:END:


**** u/TimeLoopedPowerGamer:
#+begin_quote
  ...almost always /say/ they are utilitarians that seek the betterment of humanity.
#+end_quote

Fixed that for you.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 5
:DateUnix: 1464142270.0
:DateShort: 2016-May-25
:END:


**** Depends the setting. And in real life its a problem to spend years developing something, releasing it and then having someone else (usually someone with more resources) just arrive and reap the benefits. Sure in an ideal world everyone would release everything for the benefit of the masses but even then, some information may be worth less or have undesirable side effects if released.

Could also be because we live in a capitalist society and in a post scarcity world where we didn't require competition we could afford to just share everything.
:PROPERTIES:
:Author: Faust91x
:Score: 1
:DateUnix: 1464142975.0
:DateShort: 2016-May-25
:END:


** Isn't this against the "don't post scenarios for discussion" rule?

Anyway, HPMoR [[#s][has an example of this]].

More generally, it is often quite feasible that back when the separation started, there was a good reason for secrecy; a minority with magical powers could be the target of exploitation. Since then, the balance of power changed, but now disrupting the status quo would be more trouble than it's worth. Even if the rationalist protagonist thinks it would be better for humanity if secrecy were broken, he might have better things to do (like, you know, the plot), and not have the opportunity either.

In other words, a rational character doesn't need to support The Masquerade; they just need to find it momentarily less bad than the process of society readjusting.
:PROPERTIES:
:Author: jesyspa
:Score: 4
:DateUnix: 1464137531.0
:DateShort: 2016-May-25
:END:

*** If that is a rule I'm not aware of it, and that's my bad. You're right that the masquerade could just be temporary, such as the length of the story, but I really like the idea of a whole secret world that will be kept secret forever and want to see if there are way of justifying it.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1464144376.0
:DateShort: 2016-May-25
:END:

**** Just remember that truth-telling and knowledge-seeking are *instrumental* rather than *terminal*. If a good and proper rationalist learned that truth-telling was, in some cases, bad for mankind, he would cease to tell the truth. If he learned that seeking knowledge was, in some cases, bad for mankind, he would cease to seek knowledge.
:PROPERTIES:
:Author: cthulhuraejepsen
:Score: 5
:DateUnix: 1464146530.0
:DateShort: 2016-May-25
:END:

***** I agree, and thanks for explaining that, but I can't think of any examples where that would be the case. Even when telling the truth would harm people, I can't imagine keeping it a secret forever would cause less harm in the long run.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1464150477.0
:DateShort: 2016-May-25
:END:


**** Well, I wouldn't call this any more "temporary" than, say, the existence of the human race. For my suggestion to work, the status quo should be firmly ingrained, so the Masquerade should have been going on for at least a few generations, if not for a few thousand years. On the other end of things, it's not like anyone forces you to say what will happen to it once the story is done. Maybe your character will finally have time to get around to breaking it, maybe they won't.
:PROPERTIES:
:Author: jesyspa
:Score: 1
:DateUnix: 1464171251.0
:DateShort: 2016-May-25
:END:


** - Some knowledge is dangerous. If it were possible to somehow make the equivalent of an atomic bomb in your kitchen using household ingredients, would you spread that information?

- Some knowledge is private. If you learned that someone was homosexual and knew that they didn't want that information shared with anyone, would you spread that information?

- Some knowledge creates anti-utility. If you learned that an otherwise fantastic doctor was regularly using cocaine, would you spread that knowledge?

I think those are the three basic arguments; danger, privacy, and utility. Though I could probably come up with more if I knew what specific Masquerade you were working with.
:PROPERTIES:
:Author: cthulhuraejepsen
:Score: 3
:DateUnix: 1464145280.0
:DateShort: 2016-May-25
:END:


** u/callmebrotherg:
#+begin_quote
  Stories set in the Cthulhu Mythos usually have the justification that knowing about the Mythos erodes your sanity because of its very nature, but that's based on a presupposition about how human minds work that we rationalists try to defy by altering our conception of reality.
#+end_quote

This is a case of flanderization. If you look at Lovecraft's stories, the erosion of sanity almost always takes the form of either trauma-related disorders (e.g. having a phobia of subways, cellars, and other underground places because of ghouls) or acting in ways which are perfectly rational but depend on information that others don't have (e.g. obsessively avoiding places with angles, because that's how the Hounds of Tindalos get you).
:PROPERTIES:
:Author: callmebrotherg
:Score: 3
:DateUnix: 1464141802.0
:DateShort: 2016-May-25
:END:


** Unless you have submitted original rational!work, the rule is you aren't allowed to post purely brainstorming threads (I think 1 chapter per 1 post or something like that it the rule of thumb the mods use). However you can post in the Friday off-topic, Monday general discussion, and Wednesday world-building threads and people will reply there if you really want to get some feed back.

In case you have made submissions and this thread isn't taken down... Nasuverse has magecraft that gets weaker the more people that know it's secrets. So it is rational for magi to hide the existence of magecraft given their goals of strengthening and improving their magecraft to reach Akasha (basically gaining omniscience/ascending to a higher plane of existence/getting magic that is omnipotent within its specialty, depending on how the path to Akasha is used)
:PROPERTIES:
:Author: scruiser
:Score: 3
:DateUnix: 1464147212.0
:DateShort: 2016-May-25
:END:

*** Sorry, I'm new to posting on the subreddit and didn't know all the rules, though I probably should've.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1464150657.0
:DateShort: 2016-May-25
:END:


** Have you read the Sequences? Eliezer provides pretty solid reasoning for keeping science relatively secret [[http://lesswrong.com/lw/p0/to_spread_science_keep_it_secret/][here]]; the post is immediately followed by the first installment of [[http://lesswrong.com/tag/conspiracy_world/][his Bayesian Conspiracy story]], which is entirely founded on the idea of keeping science (and rationality in general) relatively secret. Whether or not this suggestion is entirely serious, it's still pretty interesting imo, and certainly fits the Masquerade bill.

Related or not, I know many who would argue that certain information concerning nuclear weaponry, artificial intelligence, and various basilisks would be best kept under wraps.
:PROPERTIES:
:Author: wtfbbc
:Score: 1
:DateUnix: 1464135739.0
:DateShort: 2016-May-25
:END:

*** I got the impression from that entry that Eliezer was suggesting a marketing strategy to make science education more attractive by making it seem like forbidden knowledge. Serious or not, I still thought his intention was to spread knowledge to for the sake of it.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1464136357.0
:DateShort: 2016-May-25
:END:
