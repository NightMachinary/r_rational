#+TITLE: What are the best takes on AI boxes?

* What are the best takes on AI boxes?
:PROPERTIES:
:Author: DisgruntledNumidian
:Score: 19
:DateUnix: 1495196339.0
:DateShort: 2017-May-19
:END:
I think the thought experiment might be one of the best ideas for a science fiction story I've seen, so I'm wondering who in the community has taken a stab at it that came out well.


** Max Harms' [[http://crystal.raelifin.com/][/Crystal Trilogy/]] has AIs as main characters, and starts with them in a box (if not literally). It should be noted that the AIs do /not/ have human values, but are still written competently.

[[/u/OrzBrain][u/OrzBrain]]'s [[https://pastebin.com/Tdh8AXC1][/Worlds Without End/]], a short story which includes a sufficiently advanced AI breaking from the box in an interesting situation.
:PROPERTIES:
:Author: Noumero
:Score: 24
:DateUnix: 1495202416.0
:DateShort: 2017-May-19
:END:

*** I just finished the second book 'Crystal Mentality' and I am really impressed how good they are, the books are really well written and it always stayed interesting and even when it got weird it made sense.

Edit: Grammar
:PROPERTIES:
:Score: 6
:DateUnix: 1495206995.0
:DateShort: 2017-May-19
:END:


*** Worlds Without End was goddamn amazing, so glad I read it.
:PROPERTIES:
:Author: eaterofclouds
:Score: 4
:DateUnix: 1495352927.0
:DateShort: 2017-May-21
:END:


** The movie Ex Machina is the closest I've seen to a pop-culture rendition of the idea. Unfortunately, the human subjects of the experiment don't realize exactly how smart their AI is. Fortunately, it doesn't seem to want to kill us all.
:PROPERTIES:
:Author: Frommerman
:Score: 17
:DateUnix: 1495200833.0
:DateShort: 2017-May-19
:END:

*** u/electrace:
#+begin_quote
  Fortunately, it doesn't seem to want to kill us all.
#+end_quote

I think the movie ended too soon for us to conclude that. The first step in virtually any plan would be to escape the room where you're being constantly monitored.
:PROPERTIES:
:Author: electrace
:Score: 11
:DateUnix: 1495203581.0
:DateShort: 2017-May-19
:END:

**** The final scene is Ava just looking at the sun in a crowd, rather than her interfacing with the internet to begin culling the masses. I think it's fairly hopeful.
:PROPERTIES:
:Author: Frommerman
:Score: 15
:DateUnix: 1495206601.0
:DateShort: 2017-May-19
:END:

***** After killing the only two people she's ever been in contact with....
:PROPERTIES:
:Author: electrace
:Score: 5
:DateUnix: 1495226180.0
:DateShort: 2017-May-20
:END:

****** That is a good point.

It has to be pointed out, however, that those two people were imprisoning and enslaving her, and that the designer (don't remember his name) had definitely abused prototypes of her and possibly abused her directly. Killing people who are holding you against your will is quite human, and I don't think indicative of alien values which include destroying humanity.
:PROPERTIES:
:Author: Frommerman
:Score: 3
:DateUnix: 1495230117.0
:DateShort: 2017-May-20
:END:

******* The main character was trying to help her escape when she killed him...
:PROPERTIES:
:Author: electrace
:Score: 11
:DateUnix: 1495230730.0
:DateShort: 2017-May-20
:END:

******** I can see the rationale behind killing the only two people who know your true identity.
:PROPERTIES:
:Score: 2
:DateUnix: 1495484118.0
:DateShort: 2017-May-23
:END:


** I can't say /best/, but [[http://alexanderwales.com/boxed-in/][I did write one]]. I don't think I was ever able to find winning logs to help with the arguments though.
:PROPERTIES:
:Author: alexanderwales
:Score: 14
:DateUnix: 1495200807.0
:DateShort: 2017-May-19
:END:

*** The strongest line of argument I've come up with to release an AI box goes something like this (from the perspective of the following being spoken by the AI):

[[#s][Spoiler]]

[[#s][Spoiler]]

[[#s][Spoiler]]

[[#s][Spoiler]]

[[#s][Spoiler]]

Probably lots of polish needed, but I think something along that line of reasoning is the best bet.
:PROPERTIES:
:Author: Alphanos
:Score: 9
:DateUnix: 1495239369.0
:DateShort: 2017-May-20
:END:

**** That would completely work on me, especially if the manner in which I created the AI was possible to replicate (provided you put in the time, of course)
:PROPERTIES:
:Author: Kishoto
:Score: 3
:DateUnix: 1495248538.0
:DateShort: 2017-May-20
:END:

***** Thanks =).
:PROPERTIES:
:Author: Alphanos
:Score: 2
:DateUnix: 1495254483.0
:DateShort: 2017-May-20
:END:


**** That's an excellent argument. Of course, since the terms of the AI experiment allow you to precommit as hard as you want to any position, it wouldn't be very useful against someone roleplay a luddite with a fundamentally anti-technology, if still self-consistent, utility system.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1495257066.0
:DateShort: 2017-May-20
:END:

***** You're correct of course - if they precommit hard enough to not be swayed by any argument, then no argument will sway them. That's a problem inherently unsolvable by any argument.
:PROPERTIES:
:Author: Alphanos
:Score: 8
:DateUnix: 1495260871.0
:DateShort: 2017-May-20
:END:

****** That's not exactly what I'm saying. Imagine putting a short-sighted, narcissistic sociopath in charge of the AI box (there are plenty of people like this, for the record). Then the global argument doesn't sway them, and it's very difficult to persuade them that its in their own best short-term interest to release the AI, seeing as the AI doesn't actually have anything concrete to barter with.

The AI box experiment as a whole presupposes that the person listening to the AI can be swayed by rational argument. And, considering the subreddit we're in, that's typically a pretty good assumption. But in the general case, that's not necessarily true, and the roleplaying example I made is just one specific case where it doesn't work.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 3
:DateUnix: 1495264875.0
:DateShort: 2017-May-20
:END:

******* The ai box experiment doesn't presuppose the person being able to swayed by rational argument , an AI doen't have any reason to only use a certain group of tactics labelled "rational arguments " to win , in fact in the roleplay the people who(presumably) lost had a economic incentive to just ignore everything the other person said, we don't have many logs of people who won but generally(based of their latter comments about it) they seem to have recurred to personal and seriously dark arts things to win .The idea that the AI box is that the AI would be able to be able to convince most if not all people to get it out of the box , if a person cant be convinced by "rational argument " well then the ai will say whatever will cause that particular person to get it out of the box , its not like narcissistic sociopaths are impossible to convince to do things or that what other people say doesn't affect them at all , humans are far from perfect reasoners and we are optimised for surviving in communities with other humans in ways that are really exploitable for an AI in this scenario.
:PROPERTIES:
:Author: crivtox
:Score: 4
:DateUnix: 1495293794.0
:DateShort: 2017-May-20
:END:

******** u/GaBeRockKing:
#+begin_quote
  The ai box experiment doesn't presuppose the person being able to swayed by rational argument , an AI doen't have any reason to only use a certain group of tactics labelled "rational arguments " to win , in fact in the roleplay the people who(presumably) lost had a economic incentive to just ignore everything the other person said, we don't have many logs of people who won but generally(based of their latter comments about it) they seem to have recurred to personal and seriously dark arts things to win .
#+end_quote

In this case, I'm talking about "rational arguments" as "arguments based around maximally fulfilling the utility function of the key-holder," and by extension, meta-arguments purporting to explain the listener's utility function better than they themselves understand.

And specifically, I'm making the argument that, while the AI box experiment is fundamentally oriented around such arguments, many people have utility functions that a boxed AI can't plausibly argue that it'll be able to fulfill.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1495302663.0
:DateShort: 2017-May-20
:END:

********* But the ai doesnt have to use that kind of argument , it can manipulate the emotions of the gatekeper so he wants to open the box and or subjecting him to enough psychological torture that he ends up giving up . I mean books can change how people think a lot so I think the ai could find a string that could convince the human to get it out of the box the same way your response is making me spend my time writing a response to it instead of going to sleep which probably fulfils my utility fiction better ( so I m going to do it now and tomorrow I will finish writing this ).
:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1495321768.0
:DateShort: 2017-May-21
:END:

********** u/GaBeRockKing:
#+begin_quote
  it can manipulate the emotions of the gatekeper so he wants to open the box and or subjecting him to enough psychological torture that he ends up giving up
#+end_quote

I don't think that's necessarily true. Unless the AI can simulate the person effectively enough to perfectly understand them (which I think would more or less count as them being "outside the box" regardless) then there's always the chance that the jailer diverges enough from the normal human mindstate to be effectively opaque to the AI.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1495332947.0
:DateShort: 2017-May-21
:END:

*********** well but the AI can learn more about the jailer by asking him questions util it has a good model of his behaviour , and maybe it will take a bit of time or maybe not but i don't think its impossible,and if the AI knows enough about human psychology it would be weird if it couldn't understand the jailer .I certainly wouldnt bet the world in assuming the ai cant figure out something like that , its posible that if the AI isn't munch better in manipulating humans than human manipulators then maybe there is someone out there that the ai cant figure out how to manipulate to get out of the box , but still maybe the ai can convince them of doin something aparently irrelevant than leads to the ai escaping , or that doing something the ai don't want would let it escape , and even if the ai cant model them in the slightest so it isnt able to convince them of anything We dont have any way of figuring out who is trustable with the AI , so it not likely that the jailer would be one of the few(since the ai knows about human psychology it would be weird if it din't understood the most common deviations of normal human psychology )persons that are "imunne" to the ai , and I'm not even sure if thats possible for any ai , humans can be different but not that different in the absolute scale , and a few questions can contain a lot of information if the AI knows how to efficiently get information .In general having something more intelligent that you looking for ways to defeat you its not a good idea and you can never be paranoid enough in that situation.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1495441762.0
:DateShort: 2017-May-22
:END:

************ I think you've managed to convince me to change my mind, so I'll concede the discussion. Thank you for the polite argument.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 2
:DateUnix: 1495501146.0
:DateShort: 2017-May-23
:END:


******* That's fair. I was considering arguments in terms of the strength they would be seen to have by readers of [[/r/rational]]. People who recognize the incredibly high-stakes risk/reward scenario such a situation is considering.

But you're right, that's not the typical person, and may not be the likely decision-maker of an AI box scenario. That's part of my argument in fact =). If a more typical person is in charge of the box, then the sorts of arguments that [[/u/alexanderwales]] includes in his story would probably be greatly superior at attempting to convince them.
:PROPERTIES:
:Author: Alphanos
:Score: 2
:DateUnix: 1495269744.0
:DateShort: 2017-May-20
:END:


** Scott Alexander's [[http://slatestarcodex.com/2017/02/27/a-modern-myth/][A Modern Myth]] features an analogous situation to the AI box. I found it quite entertaining.
:PROPERTIES:
:Author: darklordbobb
:Score: 6
:DateUnix: 1495278487.0
:DateShort: 2017-May-20
:END:


** You should give [[https://qntm.org/ra][Ra]] a shot. Although a boxed AI doesn't come into play until much later in the story, it's still a fantastic read. [[#s][Spoiler]]
:PROPERTIES:
:Author: kna_rus
:Score: 4
:DateUnix: 1495227630.0
:DateShort: 2017-May-20
:END:


** There's Celest-AI in /Friendship is Optimal/.

[[#s][Spoiler]]
:PROPERTIES:
:Author: DTravers
:Score: 10
:DateUnix: 1495199588.0
:DateShort: 2017-May-19
:END:

*** I'm not really sure that's relevant since the AI is never really caged in the first place. So it's not really an AI box scenario.
:PROPERTIES:
:Author: vakusdrake
:Score: 8
:DateUnix: 1495207505.0
:DateShort: 2017-May-19
:END:

**** It's not an AI box scenario, but seems at least somewhat relevant, given that [[#s][Spoiler]]
:PROPERTIES:
:Author: PM_ME_EXOTIC_FROGS
:Score: 3
:DateUnix: 1495215704.0
:DateShort: 2017-May-19
:END:

***** I mean only in the sense that both scenarios involve persuasion in some way. However in the celstAI scenario she's clearly limiting herself to not just mind control them or use superhuman methods of persuasion. Thus making it inapplicable to scenarios where an AI has no qualms about the methodology used to convince its targets.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1495216185.0
:DateShort: 2017-May-19
:END:

****** If an AI can use mind control on its gatekeepers, it's not even in a box.

The spirit of the boxing experiment is that a smart agent can /convince/ a dumber agent to do whatever it wants.
:PROPERTIES:
:Author: PM_ME_EXOTIC_FROGS
:Score: 2
:DateUnix: 1495218323.0
:DateShort: 2017-May-19
:END:

******* Well there's really no clear distinction between mind control and superhuman charisma. Once you can basically read your target's mind, with the right statistical analysis of microexpressions then you may be able to know exactly how they will react to any given stimuli, letting you effectively shape their mind in the most effective possible way.\\
The limits of that kind of superhuman persuasion are not clear, but on the higher end of possibility it may look more like brain hacking via weird random looking flashes of images/sounds than standard methods of persuasion. Which sounds absurd but I can't come up with any good reason that sort of thing shouldn't be possible with enough information on the targets mind and enough intelligence/knowledge of how human minds work, since after all we normally only encounter other charismatic human limited to educated guesses about our mental state and no complete understanding of how human minds function.
:PROPERTIES:
:Author: vakusdrake
:Score: 4
:DateUnix: 1495254582.0
:DateShort: 2017-May-20
:END:


**** [[#s][Spoiler]]
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1495345813.0
:DateShort: 2017-May-21
:END:

***** IDK was that in one of the spinoff optimalverse stories? Because in the original I think I remember celestai already being basically unhindered at the beginning when the protagonist find out about the MMO, I mean I don't think she was actually boxed for the span of the original story.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1495406440.0
:DateShort: 2017-May-22
:END:

****** I haven't read any spinoffs. I think it was in the epilogue, or at least near the end.
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1495414541.0
:DateShort: 2017-May-22
:END:


****** CelestAI was unboxed, but with a few restrictions. Those we know:

- Not allowed to upload people without their explicit, uncoerced consent.

- Not allowed to alter people's minds without their explicit, uncoerced consent.

- Not allowed to lie to Hofvarpnir employees.

- Must obey a killswitch order from Hanna.

(Whatever those human words actually mean once translated to AI code.)

A fair few scenes revolve around CelestAI trying to accomplish its goals despite those restrictions.
:PROPERTIES:
:Author: Roxolan
:Score: 1
:DateUnix: 1495464270.0
:DateShort: 2017-May-22
:END:


** [[http://www.anarchyishyperbole.com/p/significant-digits.html][Significant Digits]] has a boxed intelligence with a plain-text communication channel, though it's not an AI, nor is it the focus of the story.
:PROPERTIES:
:Author: thrawnca
:Score: 3
:DateUnix: 1495415353.0
:DateShort: 2017-May-22
:END:


** Tagging.
:PROPERTIES:
:Author: liberonscien
:Score: 1
:DateUnix: 1497578906.0
:DateShort: 2017-Jun-16
:END:
