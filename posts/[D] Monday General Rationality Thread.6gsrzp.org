#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 22
:DateUnix: 1497280009.0
:DateShort: 2017-Jun-12
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** I've been thinking about marriage in the context of game theory and pre-commitment. Since it's relatively hard/painful to get unmarried, marriage is not just a commitment, it's a pre-commitment, since you're limiting your options going forward (in addition to your public declaration, which is itself a pre-commitment in the form of reputational loss etc.).

The strongest argument that I've seen against marriage is that it's a legal/societal construct created for reasons that /probably/ don't match up with what any individual specifically wants from that partnership, and the convenience/social/legal aspects of marriage don't make up for the benefits of being able to roll your own partnership contract.

Prenuptials interest me from a game theory standpoint. If there's income/wealth disparity, then they act as a defection incentive equalizer, but either way they also decrease the disincentive to defect, since it's easier to get out of the marriage and break commitment. However, defection /within/ the marriage is also a thing; if you know that someone has made a substantial pre-commitment, you can use that against them by e.g. being a shitty husband with the knowledge that divorce is very unlikely.

(This obviously has some parallels to world politics.)
:PROPERTIES:
:Author: alexanderwales
:Score: 23
:DateUnix: 1497289957.0
:DateShort: 2017-Jun-12
:END:

*** u/robobreasts:
#+begin_quote
  if you know that someone has made a substantial pre-commitment, you can use that against them by e.g. being a shitty husband with the knowledge that divorce is very unlikely.
#+end_quote

I see you've met my wife.
:PROPERTIES:
:Author: robobreasts
:Score: 11
:DateUnix: 1497296744.0
:DateShort: 2017-Jun-13
:END:

**** This made me sad and made me laugh simultaneously.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1497661141.0
:DateShort: 2017-Jun-17
:END:


*** You didn't happen to read this recent post on Death Is Bad, did you? It put many of the considerations you posed in my mind.

[[http://www.deathisbadblog.com/marriage-is-a-hostile-act/]]
:PROPERTIES:
:Author: TheStevenZubinator
:Score: 3
:DateUnix: 1497321346.0
:DateShort: 2017-Jun-13
:END:

**** I saw the title linked somewhere and elected not to read it on the basis of that title. The general arguments are nothing new; I've had them on this subreddit a few times already, just usually not coached in game theory terms.
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1497326775.0
:DateShort: 2017-Jun-13
:END:


*** I've been thinking about this on and off for the past day, and it disturbs me. I've seen the term "pre-commitment" tossed about in this sub-reddit with some regularity, but now I'm wondering what it means -- or rather, what the people here think it means. What distinguishes a pre-commitment from a regular old commitment?

For instance, I think of the "point of no return". A common scenario: "Once we've crossed this line, we will no longer be able to turn back. We will be committed to this course of action." This is the normal language used. Is your "pre-commitment" somehow different from that?
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1497391709.0
:DateShort: 2017-Jun-14
:END:

**** Commitment is playing chicken and saying "I will not turn the steering wheel". Pre-commitment is throwing the steering wheel out the window so that turning is impossible.

However, commitment devices differ in severity, with that being a more extreme case; a lesser example might be the difference between saying "I will lose 20 pounds" as a commitment, versus giving a friend $50 and telling them not to give it back to you unless they verify that you have lost 20 pounds. It's not absolute, since you can still fail, but the principle is the same.

(I do see a lot of people get this wrong, or talk about pre-commitment without discussing any commitment device, sometimes in situations where there's no conceivable commitment device.)

In the case of marriage, you're usually saying "I will be with this person forever" and then the commitment devices that cut off (or weigh down) future options vary on the basis of whether or not you sign a prenuptial, the terms of that prenuptial, the laws in your country/state, etc.
:PROPERTIES:
:Author: alexanderwales
:Score: 6
:DateUnix: 1497392906.0
:DateShort: 2017-Jun-14
:END:

***** u/GaBeRockKing:
#+begin_quote
  or talk about pre-commitment without discussing any commitment device, sometimes in situations where there's no conceivable commitment device.
#+end_quote

In this case, the commitment device is the very fact that you claimed it was a precommitment-- if you break it, you're breaking a promise. If you become known for breaking promises, you're no longer capable of making promises without having more stringent commitment enforcers than the threat of people no longer respecting your commitments.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 2
:DateUnix: 1497398461.0
:DateShort: 2017-Jun-14
:END:


*** "Marriage" is multiple things.

Marriage is a legal contract. It can be entered into as any other legal contract, and in this sense carries substantial consequences in a purely legal sense but no emotional overtones.

Marriage is a religious construct. It is a binding promise to be true to, to support and cherish a partner, to be someone who the partner can rely on. It is not to be entered into lightly; but when both partners hold to their promises, their trust in each other can allow them to be stronger together than apart.

Marriage is also a promise, made for the sake of future (or not-so-future) children; to (as far as circumstances allow) raise a child together, in a family that makes as much stability for the child as is reasonably possible, for this is how we get stable, productive humans in the next generation.

These definitions have become twisted and entangled (and I'm not entirely sure that I've untwisted them properly here). Many things can be said about marriage in one or the other definition, which often doesn't apply to all the /other/ definitions of the word. Especially since all of them are generally done pretty much at once, on the same day.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1497341763.0
:DateShort: 2017-Jun-13
:END:


** I'd like to finally settle the idea of whether or not quantum teleportation should be considered death for myself. I feel like I'm 99% of the way to agreeing with the camp that says it isn't, but then I get tripped up at the last step. I know that a perfect copy of me /is/ me in every way that counts, that teleportation preserves my identity and everything I'd care about, etc.

So here's my issue: None of this seems like a good argument as to why I should be okay with an instance of myself dying just because another one exists. No matter how much they also deserve to be called "me", they can't access my subjective experience and I can't access theirs. I wouldn't tell another me to die just because I exist, and I wouldn't want to stop experiencing things just because my information's still out there. I'm a mind running on a physical brain, so I still stop experiencing everything if the brain I'm using gets destroyed no matter if something else has the same pattern.

So if someone who would be okay using a quantum teleporter could help me understand your position, I'd really appreciate it. I feel like I'm still missing something.
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 18
:DateUnix: 1497295701.0
:DateShort: 2017-Jun-12
:END:

*** I am firmly in the "would not teleport" camp.

I agree that a perfect copy would be an instance of me. But I see no point in terminating /this/ instance, for any reason.

Like you say, this instance quite enjoys its existence. What is the value in ending it? While the net result would be just one me, the same that we started with, it also has added one death, which has a rather huge negative value. And if asked if they want to die, both instances would answer a resounding "no". Making it not just a death, but a /murder/.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 17
:DateUnix: 1497297001.0
:DateShort: 2017-Jun-13
:END:

**** We are lucky enough that for teleportation to be possible, we need to destroy the original (no cloning theorem). So the point in "terminating this one" is to make it possible to create the other one. And if the other one is in a better/more optimal position then the net utility should be positive, and there would not be a better option (there being both versions at the same time)
:PROPERTIES:
:Author: oskar31415
:Score: 1
:DateUnix: 1497372775.0
:DateShort: 2017-Jun-13
:END:

***** I don't see how destruction being a requirement of teleportation changes anything. The net result is me in a more advantageous position and one murder (of me).
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 2
:DateUnix: 1497385207.0
:DateShort: 2017-Jun-14
:END:

****** I was only describing that "the point in terminating this instance" is for the teleportation to be possible from a point of physics.

I would say that that from a utilitarian point of view the teleportation would be considered a net positive as the loss of the original you is made up for by the creation of a new you, who should be in a better position as you would not teleport otherwise.

I would argue not teleporting is also a murder of the version of you, you didn't give a chance to live. Or maybe there is only ever one you and no one is ever killed, but that is a question of definition which i don't find worthwhile to discuss
:PROPERTIES:
:Author: oskar31415
:Score: 1
:DateUnix: 1497387319.0
:DateShort: 2017-Jun-14
:END:

******* u/KilotonDefenestrator:
#+begin_quote
  I would say that that from a utilitarian point of view the teleportation would be considered a net positive as the loss of the original you is made up for by the creation of a new you, who should be in a better position as you would not teleport otherwise.
#+end_quote

The argument that it has a utilitarian net positive "because you chose to teleport" is a poor argument when I am currently in a position that I would /not/ choose to teleport.

The fact remains. A viable individual was terminated to give another a more advantageous position.

#+begin_quote
  I would argue not teleporting is also a murder of the version of you, you didn't give a chance to live.
#+end_quote

Not teleporting is not murder. Otherwise, every second we spend not duplicate people is also murder. It would mean that we have a utilitarian duty to invent duplication technology as soon as possible and then use it as much as possible.

Also, that kind of reasoning about "potential future persons" would make abortions and masturbation illegal. And we don't want to go /backwards/.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1497430550.0
:DateShort: 2017-Jun-14
:END:

******** The point in saying that "you want to teleport" is just if all the ethical concerns was turned off would you rather be where you want to teleport to than where you are? Becouse then a see it as value as you create a version of you that is more happy (becouse they are a place they would rather be) at the cost of a version of you that is less happy.

The argument about masturbation and abortion, is that there is no corelation between not allowing those and an increase in number of children or quality of life. (If you are forced to not have an abortion that makes it less likely you will get a child later (your total number of children is unlikely to change) and as the mother would be more happy getting a child she wants it is a utilitarian positive to let them get their abortion)
:PROPERTIES:
:Author: oskar31415
:Score: 1
:DateUnix: 1497437368.0
:DateShort: 2017-Jun-14
:END:

********* u/KilotonDefenestrator:
#+begin_quote
  The point in saying that "you want to teleport" is just if all the ethical concerns was turned off would you rather be where you want to teleport to than where you are? Because then I see it as value as you create a version of you that is more happy (because they are a place they would rather be) at the cost of a version of you that is less happy.
#+end_quote

That's a bit strange way to argue. I can get you to agree to anything by asking you to turn off every concern that would make you disagree with me.

I assign value to people. Is that ethics or utilitarian? Terminating a conscious, thinking individual has a very big negative value. It could be left alive and have a full life.

If I disregard ethics I could rob and murder a rich depressed guy, because I would have more money (be at a better position in society) at the cost of a less happy person.

I don't see how murdering a copy of me is better than murdering a stranger. Both, if asked, will not want to be murdered. Not giving them the chance to answer the question absolves nothing.

#+begin_quote
  your total number of children is unlikely to change
#+end_quote

If your only way to get an orgasm/sexual pleasure was having sex, and if contraceptives and abortions were not allowed, we'd have a lot more people (and more rapes). And it is our duty to have more children, because not giving each potential human a chance to live is murder.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1497440114.0
:DateShort: 2017-Jun-14
:END:


*** If you don't let instances diverge (halt simulation of the original before making a copy and restarting it at another place) there is simply never "another you" that can die.

It's like, imagine you're​ on your computer and copied you favorite photo out of your photos directory into another one. And then, /without modifying either/, you somehow believe that by deleting either one of them you'll lose something important.
:PROPERTIES:
:Author: eternal-potato
:Score: 9
:DateUnix: 1497300602.0
:DateShort: 2017-Jun-13
:END:

**** And, as I said in my comment but will summarize here because your comment is currently higher rated than mine, if your instances DO diverge then they stop being the same person and it stops being okay to kill one.
:PROPERTIES:
:Author: Sarkavonsy
:Score: 2
:DateUnix: 1497325761.0
:DateShort: 2017-Jun-13
:END:

***** IMO, it's a “sliding scale of the surveyees' emphasizing” problem. That is, how much, in the person's opinion, should minds A and B differ from each other for the person to consider them two separate entities.

There was a nice demo on this in Doctor Who's [[https://en.wikipedia.org/wiki/A_Christmas_Carol_(Doctor_Who)][/Christmas Carol/ rendition.]] In this story, the antagonist was the only person whose commands were accepted by a certain mind-reading machine, so the Doctor uses his Therapy no Jutsu and time-travel shenanigans to convince him to help them out in solving the story's crisis. Only, by the time the antagonist becomes convinced enough the machine judges him to be too divergent and doesn't recognize him as the person entitled to issue the commands any more.

So in terms of this DW episode, different people would have different criteria for their “mind-diff subroutine”. Some would consider it a murder even all the difference between two instances of the “same” mind were that one has been shown a card with a square on it while the other a card with a circle. And some would tie the necessary amount of changes to things like key values, principles, etc.

TL;DR: Uniqueness of a personality is in the eye of the beholder and all that.
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 3
:DateUnix: 1497332568.0
:DateShort: 2017-Jun-13
:END:

****** It was my understanding that what happened was that because The Doctor changed history so that the antagonist never became the horrible person that he did, completely ignoring all paradoxes, so the man's equally despicable father never programmed the machine to respond to his son's commands. It seriously stretched the logic of time travel as we see him fully aware of his changing memories, but I enjoy the episode regardless since it was otherwise a decent character study.

However, I have another example that's WAY more obscure. In Role Playing Public Radio's Know Evil campaign, the character SAIROC became bonded to a Seed AI, saved his own mind as a backup, and then left it behind to go on missions fighting alien monsters and mind control viruses. By the time they met up again he had experienced so much trauma that the AI's mind scanner didn't recognize him as its master, which made him request his brain be restored from a backup in a heartbreaking scene.

He wasn't even because of mistakes he'd made, he simply decided he preferred being the naive idealist to the broken nihilist he'd become over the course of just a few days. He didn't want to remember watching his friends die as he was powerless to prevent it, then see them be restored from a backup acting like nothing had happened. He didn't even care about the trauma his friends would go through themselves or that his mind likely would shatter all over again in the future. It was tragic as hell.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1497403914.0
:DateShort: 2017-Jun-14
:END:


****** I think spatial difference is enough to classify as unique. If the person is viable to continue existing, and allowed to, it /will/ diverge. I don't think killing them real quick is a good defense.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1497430772.0
:DateShort: 2017-Jun-14
:END:


*** I'm totally with you.

It's fine for OTHER people to want to quantum teleport-- I'll never know the difference between the versions of themselves who get destroyed/created. But I rather like experiencing things, and I see no reasons why THIS INSTANCE of me would continue experiencing things after a teleport.
:PROPERTIES:
:Author: Loiathal
:Score: 6
:DateUnix: 1497303915.0
:DateShort: 2017-Jun-13
:END:

**** Define "instance".
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1497397671.0
:DateShort: 2017-Jun-14
:END:

***** Uh, I mean the one typing this message, right now.

I'm not really interested in arguing over whether or not a quantum copy of me is the same person-- obviously we are up to the nanosecond the copy is created. But even if that other me is going to keep right on living, and no one else knows the difference, *I'm* still going to stop living.
:PROPERTIES:
:Author: Loiathal
:Score: 1
:DateUnix: 1497462281.0
:DateShort: 2017-Jun-14
:END:

****** Define "I". And no, bolding the word doesn't actually give it any additional meaning.

(Sorry if I sound facetious, but I actually have a legitimate point to make, and it'd be helpful if you could humor me and play along a little.)
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1497462400.0
:DateShort: 2017-Jun-14
:END:

******* I'm pretty sure I see what you're getting at, and I don't find it a useful distinction to try to make.

Like it or not, certain elements of identity are baked directly into the foundations of language, because the people that created those languages all had self-identity (or at least, believed they did. Let's skip right over P-Zombies) and those didn't need to be discussed. At this level, it doesn't even make much sense to try to define "I", because a quantum copy of me 10 minutes from now would have the same memory I will 10 minutes from now of me writing this message.

Regardless, on some level there's a subjective experience of this moment being experienced by a thing, and that thing would like to continue experiencing moments.
:PROPERTIES:
:Author: Loiathal
:Score: 3
:DateUnix: 1497463857.0
:DateShort: 2017-Jun-14
:END:


*** I think a big problem is that people can rationalize the moral implications of teleportation on an emotional level but as soon as you change a minor aspect it recontextualizes the problem and suddenly the "gut reaction" is completely different.

Suppose that a teleporter transmitter consisted of a scanner, a destroyer and a data transmitter and a teleporter receiver consisted of a constructor and a data receiver.

- If you do the classical startrek teleportation "thing" and scan->destroy->send->recieve->construct then people feel like consciousness is neither created nor destroyed and whatever gut-level "law of conservation" exists isn't violated.

- If you however upload the data to a handheld data drive and "revive" the person months later then people think that's wrong.

- If you have two teleporters next to each other and the destroyer glitches suddenly you have two people and it would be unethical to kill the earlier one.

- If you wait until you know that the data packets have been received and the person successfully reconstructed before you engage the destroyer the same problem applies
:PROPERTIES:
:Score: 5
:DateUnix: 1497356248.0
:DateShort: 2017-Jun-13
:END:

**** I always have trouble with this kind of stuff because I've always interpreted the Star Treck "standard" to be that the matter that makes you up gets transported across the "teleport link" to the receiver and gets reconstructed there, rather than the matter staying at the sender, and the receiver re-building you from matter stored on-sight. So the idea of the sender failing to "destroy" you was nonsensical, because then it wouldn't be possible to build the "other you" at the receiver.

In retrospect is makes more sense for each station to have a stockpile of various atoms on hand rather than confront the engineering challenge of sending 70 kilos of matter who-knows-how-far.
:PROPERTIES:
:Author: General_Urist
:Score: 1
:DateUnix: 1497728555.0
:DateShort: 2017-Jun-18
:END:

***** Star trek is a bit fuzzy on what happens but it probably works by "sending matter" like you described because otherwise they couldn't beam down to the planet surface. But atoms are not unique, every protium atom is exactly like every other protium atom.
:PROPERTIES:
:Score: 1
:DateUnix: 1497730266.0
:DateShort: 2017-Jun-18
:END:


*** u/Noumero:
#+begin_quote
  I still stop experiencing everything if the brain I'm using gets destroyed
#+end_quote

Why would that be so? Consider a copy of you uploaded in a computer. Suppose that copy would be able to transfer between computers at will. Destuction of the computer on which the copy was initially uploaded wouldn't kill him/her, if the copy already transfered from it at the moment of destruction. Thus, the continuity of consciousness would be preserved, even though the only thing that would survive is data.

Or do you believe that the upload would experience death in the process of /transfer/ between computers, in this case? What if that process is gradual? Imagine computers standing nearby, connected by a physical cable.
:PROPERTIES:
:Author: Noumero
:Score: 4
:DateUnix: 1497303368.0
:DateShort: 2017-Jun-13
:END:

**** u/Loiathal:
#+begin_quote
  Destuction of the computer on which the copy was initially uploaded wouldn't kill him/her, if the copy already transfered from it at the moment of destruction.
#+end_quote

I think this depends on how the "consciousness" of the AI worked.
:PROPERTIES:
:Author: Loiathal
:Score: 8
:DateUnix: 1497303847.0
:DateShort: 2017-Jun-13
:END:

***** I think that "consciousness" doesn't exist. It's all just input and output; no qualia needed.

(Am I being facetious here? Perhaps. If someone offered me a bet that there is a real-world physical phenomenon which corresponds to that-which-we-refer-to-as-qualia, I'd bet against it--just not at very extreme odds. But /even if there is/, our current understanding of it is so utterly, utterly confused that I think using the word at this point might very well be detrimental to our efforts to understand what "consciousness" actually /is/.)
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1497397623.0
:DateShort: 2017-Jun-14
:END:


**** u/KilotonDefenestrator:
#+begin_quote
  Suppose that copy would be able to transfer between computers at will.
#+end_quote

How would my consciousness /transfer/ to my copy on my death? That seems very close to talking about souls. Identical information being somewhere else does not equal transferal of a live process.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 8
:DateUnix: 1497321328.0
:DateShort: 2017-Jun-13
:END:


**** Thanks for your thought experiment, but now I think I'm even /more/ confused for the moment. Previously I could imagine replacing each neuron in my brain with another substrate, and as long as the process was gradual and each neuron functioned identically to the one it was replacing, there would be no way for me to really tell. I could be the same mind running on a different physical brain, no problem. But then your thought experiment made me recall the ship of Theseus for some reason, and it occurred to me that if the neurons that were taken out were assembled back into a brain again... well, I'm not even sure of all the implications just yet. It makes me feel incredibly weird, and I need to go think for a while now.
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 2
:DateUnix: 1497305586.0
:DateShort: 2017-Jun-13
:END:


*** You've got to think about the nature of a self. It's like a song, it doesn't matter what speaker it's playing on, it's still the same song. Likewise, it doesn't matter which atoms make me up, or where in space I'm located.

That being said, I would not get into a machine that promises to disassemble me. By all means, create the copy, but don't destroy any copies of the pattern.
:PROPERTIES:
:Author: Polycephal_Lee
:Score: 2
:DateUnix: 1497313655.0
:DateShort: 2017-Jun-13
:END:


*** This sounds like an issue that crops up in programming with some frequency. Equality.

We start with some object, call it X. As long as we pass references of X around from place to place, all references refer to the same X, and are equal. We can even call X by the name Y if we wanted, and X = Y would still hold. You would continue to be you.

But sometimes references are not sufficient. Sometimes we need to make a deep-copy of X. Now, Y, which is a deep copy of X, is equal to X in a structural sense, but not in a referential (shallow) sense. That is, references to X are not equal to references to Y, even though the data is identical.

If X and Y are deep copies, and allowed to evolve, that is, the structure or data of one or both changes, then X and Y are no longer equal in any sense (so long as they aren't changing synchronously).

But what if after creating Y as a deep copy of X, we immediately remove all references to X and zero out the memory location of X. X is gone, destroyed. Does the expression X = Y mean anything anymore? Since all references to X are gone, you can't even pose the question. For a programmer, it doesn't matter: we can rename Y to X and continue on as though nothing had happened. If you want X back, you can just copy Y, after all. Since they never exist simultaneously, and no information is lost, it doesn't matter whether Y is a deep or shallow copy of X.

This happens behind the scenes all the time inside computers; The system needs to run a garbage collection pass, moves some objects from one section of memory to another (for defragmentation purposes), and updates the object handles. The executing program never even knows anything happened.

Teleportation of this sort is nothing more than moving data in exactly this sort of way. I know you "think" you are unique, but computation is also data. So go ahead, get in the teleporter, you'll be fine. Unless you think you have a soul that is intrinsically linked to your particular collection of atoms? Or do you think the universe would be better off with two of you?
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1497402046.0
:DateShort: 2017-Jun-14
:END:


*** When given the choice between one or two instances of me existing, I would also prefer the latter. But I assume this is one of the constraints of the thought experiment, that having two copies is not an option.

When I think of my utility function, it is only dependent on the /state/ of the universe during the future. How I think of an action therefore only depends on its results, and I would readily agree in most variants of this thought experiment. There is only a choice between having one instance at location A or having one instance at location B. The 'process' of teleportation is not relevant, as there is nothing to experience that has a duration. I would argue that it is not a process at all! (To the version where the operator stabs me to death I object rather heavily, however.)

It is useful to think of things as changing over time, as continuous processes. You can estimate utility by considering your current state, and think of how a process might affect it. If you deal with discontinuities however, there are major differences between looking at points of time versus time spans. This is especially apparent in situations with extreme differences in utility over tiny time spans, which, in my opinion, makes this thought experiment so bizarre.

As I understand your position, you worry that at the moment of teleportation there are two instances: One is experiencing normal continuity of consciousness, but having traveled to location B; the other is also experiencing continuity, still at A, and objecting vehemently to their impending demise. This however, is true at the instant of teleportation, which is not a long time; before and after that moment everything is fine. My point is not that the latter instance should be smiling happily instead, but that its predicament is too short to matter. Dying here is instantaneous, I care about the time spent alive. To me, the scary thing is not the concept of dying, but rather the prospect of being dead afterwards.
:PROPERTIES:
:Author: suyjuris
:Score: 1
:DateUnix: 1497310353.0
:DateShort: 2017-Jun-13
:END:


*** u/Sarkavonsy:
#+begin_quote
  I wouldn't tell another me to die just because I exist
#+end_quote

I like to think that I'm extremely pro-teleporter, but I agree that doing /that/ would be wrong. But it doesn't seem to me that a teleporter would require anything like that. Maybe I'm thinking of a different sort of teleporter, though?

When I think about the teleporter problem, this is what I imagine: the teleporter scans and destroys my original body at the entrance, and then produces a copy of me at the destination. At no point is my original allowed to possess any subjective experiences or memories which the copy will lack*. So, from my perspective, I enter the teleporter in one place and exit it in another.

*Exception: Original!Me might get a second or two of standing inside the entrance teleporter between the scan and the destruction. Those memories wouldn't be transferred, BUT since the copy is perfect, this can be remedied by making the interior of the entrance teleporter and exit teleporter identical. Then my post-teleport self will think the same thoughts that my pre-teleporter self did, and re-sync with the "me" from the moment of my original body's destruction.

So you see, it isn't a "different" instance of yourself dying. Your mind just briefly stops running on your original body, and then starts running on a new body. In fact, you could delay the destruction of the original body as long as you wanted, as long as you kept copy!You in the teleporter for the same length of time as original!you was in there for. This would keep the two brains running "you" in sync. Or in other words, you'd have two bodies experiencing the same things and having the same thoughts, and then you'd go down to having one body experiencing those things. No one dies because no train of subjective experiences has stopped.

#+begin_quote
  I wouldn't want to stop experiencing things just because my information's still out there.
#+end_quote

Assuming a perfect copy (well actually I believe a slightly imperfect copy would still be fine, but thats a completely different discussion), and assuming nothing goes wrong with the teleporter, you wouldn't stop experiencing things. My conception of the teleporter is based on the idea that "stop having experiences == dying"

Finally,

#+begin_quote
  No matter how much they also deserve to be called "me", they can't access my subjective experience and I can't access theirs.
#+end_quote

Maybe I'm just a weirdo and everyone else on [[/r/rational][r/rational]] got a memo I missed, but where did anyone get this idea that people who aren't literally experiencing the same thing as you deserve to be called "you"? It seems pretty obvious to me that if you make a copy of yourself, and your train of subjective experiences branch off from their train of subjective experiences, the two of you stop being the same person. You might be extremely similar people, and you might be able to predict eachother's thoughts and behaviour with extreme accuracy until the differences between you add up over the next few weeks, but you aren't literally them and they aren't literally you, and the two of you will never be the same person ever again. If such a branched-off copy was ever created by accident (such as the dematerializer in an entrance teleporter failing to fire) it would be horrendously awkward, but the two people produced by the accident would both have a right to exist as themselves. If such an accidental copy of me was ever made, we'd have a weird few weeks as we figured out how to split up our stuff and what to do re: our boyfriend (possibly become the weirdest and sexiest 3-way relationship of all time), but the solution we would eventually find would absolutely not be "kill one of the copies!" And such an accident wouldn't make me any less willing to keep using teleporters. At most I'd become a little more paranoid about making sure the dematerializer is working properly every time. Four's a party, after all.

So yeah, that's my position. Am I missing anything?
:PROPERTIES:
:Author: Sarkavonsy
:Score: 1
:DateUnix: 1497324737.0
:DateShort: 2017-Jun-13
:END:

**** u/john_someone:
#+begin_quote
  the teleporter scans and destroys my original body at the entrance, and then produces a copy of me at the destination
#+end_quote

I can see an engineering problem here. In my opinion, any sane teleporter design woudn't destroy the original until after the copy is created and verified functional. Otherwise any bugs or unreliability on the link between the teleporters would result in unrecoverable death. (Similar to moving files from computer to USB drive - operating system copies the data to USB drive, then deletes the original upon verification that the data were successfully written)
:PROPERTIES:
:Author: john_someone
:Score: 3
:DateUnix: 1497364478.0
:DateShort: 2017-Jun-13
:END:

***** u/General_Urist:
#+begin_quote
  Similar to moving files from computer to USB drive - operating system copies the data to USB drive, then deletes the original upon verification that the data were successfully written) The Windows computers I've worked with don't even do THAT, the files you transfer stay on the computer's HDD until I manually delete them. Do other operating systems go it differently?
#+end_quote
:PROPERTIES:
:Author: General_Urist
:Score: 1
:DateUnix: 1497728682.0
:DateShort: 2017-Jun-18
:END:


*** Well to give you the point of view of someone who would teleport.

First it is important to realize that because of the no cloning theorem from physics it is impossible to create a clone of something without destroying the original. This is important as it removes many problems, such as why does the first must die, and what if it is not executed as a part of the teleportation (would you then kill it?)

So from my point of view there is a utilitarian loss of a single person, with the gain of a perfect copy of this person who is in a new and preferred position (as they would otherwise not have teleported). So, by my calculations that is a net gain, and in a case where the other option is doing nothing (a net neutral) this is therefore preferred.

I hope it helps you understand why someone would be for teleporting.
:PROPERTIES:
:Author: oskar31415
:Score: 1
:DateUnix: 1497387727.0
:DateShort: 2017-Jun-14
:END:


*** Is (damage-free) cryopreservation death? If not, you could potentially cryopreserve yourself at low temperatures, and set up a pair of nanotech enhanced surfaces, one of which disassembles and the other of which assembles an exact replica of what the other one has disassembled. Only a thin layer would be disassembled at a time, and it could be done as slowly as needed. Afterward, you would be revived and go about your business in the new location.

More speculatively, suppose we just stop your heart and replace your blood temporarily with an unpressurized gelatinous mass of nanites that can keep your cells oxygenated for several hours (without moving anything around much or involving pressurized fluids). Now, you can touch one plate, sink your hand into it, and observe it acting like a portal, as your hand reaches out of the other plate. You pick up an apple on the other side, pull it through, etc. No loss in feeling or nerve damage in your hand, everything appears normal and undamaged.

Now let's say you try putting your head partway through. Your thought processes are uninterrupted just like the feeling in your hand, because the nanobots are simulating the thin "digitized" layer in realtime and also quickly reconstructing it into a physical layer on the other side. There's only ever one "you" in the process, of which only a tiny (not itself sentient) fraction is ever digital at any given moment, and you aren't damaged or altered by it in any observable way when you pull your head back out.

Does it still seem like a bad idea to step through the portal?
:PROPERTIES:
:Author: lsparrish
:Score: 1
:DateUnix: 1497412664.0
:DateShort: 2017-Jun-14
:END:


*** It may not be very helpful but I wrote a story that relates to this idea.

[[https://kishoto.wordpress.com/2015/08/06/n2-and-you-rrational-challenge-clones-clones-and-clones/][N2 and You!]]

As far as the idea? I mostly agree. If there's any sort of afterlife/soul (which I generally don't believe in but /still/), then teleportation has worrying implications, as it certainly results in your death. It will also result in a new you's birth.

Here's how I usually frame the argument. Firstly, what aspect of the teleporter requires your destruction to create the new you? What sort of scanning process is that? It doesn't really make any sense. How will you being vaporized assist in this machine's reading of your current state of being? It's not as if vaporizing makes your atoms weigh any less. It simply changes the density. And if you're vaporizing to that level, then you're essentially just going to have to have the tools to reconstruct a specific person from resident elements at the other end. So, theoretically, it's just a fancy cloning machine. If it's actually that? Then it's not teleportation.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1497664969.0
:DateShort: 2017-Jun-17
:END:


** Is it possible to resurrect someone who suffered an information-theoretic death (had the brain destroyed)?

The knee-jerk answer is no: the information constitutes the mind; the information is lost, the mind is lost. There's no /process/ that could pull back together a brain that got splattered across the floor, as far as we know.

It's possible to work /around/ that by pulling information from other sources: basics of human psychology, memories of other people, camera feeds, Internet activity, etc., building a model of the person. The result, though, would probably only narrow it to /several/ possible minds, different from each other in important ways. And even if someone who died yesterday could be reconstructed nearly-perfectly, what to do about random peasants of XVIII century that nobody bothered to write about?

If we could resurrect nearly-perfectly every person who died in modern ages, we could use /their/ simulated memories to guess at what people they met during their lives, cross-check memories of /all/ first-level resurrectees, then reconstruct second-level resurrectees based on /that/. Do the same with third-level, fourth-level, and so on ad infinitum.

But errors would multiply. Even if it's possible to reconstruct an n-level resurrectee with 80% accuracy based on (n-1)-level's information, third-level resurrectees would already be 49% inaccurate, and I suspect that the actual numbers would be even lower. That idea is impractical.

--------------

But. The set of all possible human minds is not infinite. We have a finite amount of neurons, finite amount of connections between them, which means that there could be only a finite number of possible distinct human minds, even if it's a combinatorially large number.

So, why not resurrect /everyone/? As in, generate every possible sufficiently-unique brain that could correspond to a functional human, then give them bodies? Or put them in simulations to lower space and matter expenditure.

It would require a large amount of resources, granted, but a galaxy's worth of Matrioshka Brains is ought to be enough.

This method seems blatantly obvious to me, yet people very rarely talk about it, and even the most longterm-thinking and ambitious transhumanists seem to sadly accept permanence of the infodeath.

Why? Am I missing something? And no, I am pretty sure that continuity of consciousness would be preserved here, as much as it would be with a normal upload.
:PROPERTIES:
:Author: Noumero
:Score: 10
:DateUnix: 1497283459.0
:DateShort: 2017-Jun-12
:END:

*** This is highly related to [[http://slatestarcodex.com/2015/03/15/answer-to-job/][Answer to Job]].

Besides that, it's important to realize that every time you simulate someone, you're necessarily taking away simulated time from everyone else. And also, I'm not very convinced by "the area is technically finite, so a galaxy worth of Matrioshka Brains out to be enough" line of argument.
:PROPERTIES:
:Author: electrace
:Score: 11
:DateUnix: 1497286738.0
:DateShort: 2017-Jun-12
:END:


*** u/deleted:
#+begin_quote
  So, why not resurrect everyone? As in, generate every possible sufficiently-unique brain that could correspond to a functional human, then give them bodies? Or put them in simulations to lower space and matter expenditure.
#+end_quote

Because most of those brain-states correspond to being randomly pulled out of your own place and time and shoved into this weird new one you never asked for.

Also, "combinatorially large" quickly reaches "larger than the observable universe can handle". Remember, it already does so for chess positions and Go positions. "Possible human consciousnesses", even constrained by a very good structural model, is /waaaaaaay/ beyond what the universe can handle.
:PROPERTIES:
:Score: 11
:DateUnix: 1497291790.0
:DateShort: 2017-Jun-12
:END:

**** u/vash3r:
#+begin_quote
  Because most of those brain-states correspond to being randomly pulled out of your own place and time and shoved into this weird new one you never asked for.
#+end_quote

If I recall, this happens in one of the later parts of Accelerando.
:PROPERTIES:
:Author: vash3r
:Score: 4
:DateUnix: 1497295187.0
:DateShort: 2017-Jun-12
:END:

***** The matrioshika brain spawn also apparently have a project where they try to simulate the entire human experience phase room... Which seems far beyond computability.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 3
:DateUnix: 1497296240.0
:DateShort: 2017-Jun-13
:END:


*** u/Norseman2:
#+begin_quote
  But. The set of all possible human minds is not infinite. We have a finite amount of neurons, finite amount of connections between them, which means that there could be only a finite number of possible distinct human minds, even if it's a combinatorially large number.
#+end_quote

The adult human brain has around 86±8 billion neurons. On average, each neuron in an adult human brain has 7,000 synaptic connections to other neurons. Adults retain about 1/2 to 1/10th of their synaptic connections from childhood.

Even if you were cloning people and growing them under identical conditions so that every child starts off with identical neuron and synapse configurations, this would mean that by adulthood each neuron would be in one of at least 2^{14000} possible states of synapse connections. As a result, your final set of minimal possible brain configurations is going to be at least 2^{14000} × 8.6 × 10^{10} × 0.5. You end up with 1.1 × 10^{4225} possible combinations. There's only about 10^{80} atoms in the observable universe. That's the best case scenario even assuming you're only working with brains that all started off exactly the same.
:PROPERTIES:
:Author: Norseman2
:Score: 7
:DateUnix: 1497303422.0
:DateShort: 2017-Jun-13
:END:


*** This, sir, is absurd.

This is not resurrection of any sort. What you are proposing is to create intelligent entities at random. This is not resurrection. You would, create every permutation of everyone who has ever lived, and also everyone who never existed. And no way to tell the difference.

A note to anyone proposing the resurrection of the deceased, "information-theoretic" or not: please consider the morality of resurrection before proposing it. It is not an objective good. The state of being dead is morally neutral, almost by definition. Think carefully before disturbing that equilibrium.
:PROPERTIES:
:Author: ben_oni
:Score: 6
:DateUnix: 1497341880.0
:DateShort: 2017-Jun-13
:END:

**** I also think that creating all posible mind states would be a bad idea( although i would consider that ressurrecting them but that just a semantic discussion). But I disagree in that being death is moraly neutral, most people I think assign positive utility to just being alive so although they don't have any preferences when dead but their previous preferences still apply, and since most people I think prefer being alive unless they are suffering a lot so I think death\\
Is negative and even if we have to be carefully of not resurrect the people who won't want to be resurrected(according to their cev not only because they though they wouldn't) but in most cases resurrecting people is a good thing , and if for some reason you accidentally revive someone that wants to be dead you can allways let them die.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1497350507.0
:DateShort: 2017-Jun-13
:END:

***** No, prior preferences cannot still hold. The person is dead. They have no preferences. No utility, positive or negative. They cannot prefer life. But since you bring it up, it sounds as though you've decided that utilitarianism should be the governing moral framework. Now you have to consider the utility to the non-existent. Sounds like a utility monster to me.
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1497392258.0
:DateShort: 2017-Jun-14
:END:

****** Well my main point is that the life of people after reviving them will be generally a net positive. Also I aren't taking into account the preferences of non existing people, I'm taking into account the preferences of previously existing people of not dying , it's just that they don't exist in that moment . I'm not sure if I Did really understand what you meant by me having to consider the utility to the non-existent, do you mean that since I am considering the preferences of currently non existing people I have to consider the preferences of all currently non existing minds ,even people that never existed(which does sound like utility monster but I don't see why one thing would imply the other)? Or do you mean something else.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1497468146.0
:DateShort: 2017-Jun-14
:END:

******* If you were to limit resurrection to only those who once existed, that would be one thing. But you're proposing creating all possible people as a brute force attempt to get those who did exist. In the process, you create people who never did exist. There is no reason to elevate the preferences of those who did exist over those who didn't. The preferences of any entity you create should be considered.
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1497511618.0
:DateShort: 2017-Jun-15
:END:

******** I was responding to the part where you said to anybody who wanted to resurrect people , I also Think noumero's idea of resurrecting all possible mind states is a bad idea .Sorry if I wasn't clear about that .
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1497521670.0
:DateShort: 2017-Jun-15
:END:


****** (Sorry for responding two days later)

I think that the simple case of "resurrect people who have died shortly after their death" is an iterated prisoner's dilemma. Most living humans would want to be resurrected after death, so even if it would minorly cost to resurrect someone who died in the past, it would have a positive return because then you would be resurrected in turn.

I'm not speaking toward the solution of "create all possible mind states" because that's an absurd possibility that I'm not sure how to respond to at the moment.
:PROPERTIES:
:Author: gbear605
:Score: 1
:DateUnix: 1497569333.0
:DateShort: 2017-Jun-16
:END:


*** Someone wrote a story about it on one of the story threads here. I can't remember what it was called but one character claimed to be able to simulate all possible neuron combinations, 'reducing immortality to a search problem.'
:PROPERTIES:
:Author: Cruithne
:Score: 4
:DateUnix: 1497283999.0
:DateShort: 2017-Jun-12
:END:

**** Yes. [[/u/eniteris][u/eniteris]]' [[https://www.reddit.com/r/rational/comments/40uw4i/biweekly_challenge_immortality/cyxv2zp/][/The Immortality of Anthony Weever/]]. This is literally the /only/ time I saw this idea mentioned anywhere that wasn't my mind.
:PROPERTIES:
:Author: Noumero
:Score: 4
:DateUnix: 1497284290.0
:DateShort: 2017-Jun-12
:END:

***** It's brute-force, and probably too resource intensive.

Brute force storage of 1 bit per graph results in 10^{400} bytes, whereas the number of atoms in the universe are ~10^{80.} You can probably reduce it, but that's just to store all the combinations. Running each one would take a lot more resources.

Also, that's only limited to /unmodified/ human minds. When we start getting into transhumanism, we're going to have many more minds that won't fit into that mindspace.
:PROPERTIES:
:Author: eniteris
:Score: 8
:DateUnix: 1497285719.0
:DateShort: 2017-Jun-12
:END:

****** Sure, but how many of these combinations would correspond to a functional human mind? And to minds that were /distinct/, whose difference from some others wouldn't be just one bit or one unimportant memory? The number of human personalities should be significantly lower.

#+begin_quote
  Also, that's only limited to /unmodified/ human minds
#+end_quote

Irrelevant. We're talking about resurrection of people who died in ages past. If transhumans would have unrecoverable deaths in the future, we've already failed.
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1497286132.0
:DateShort: 2017-Jun-12
:END:


****** Such scenarios only work in a sufficiently large multiverse
:PROPERTIES:
:Author: dirk_bruere
:Score: 1
:DateUnix: 1497442880.0
:DateShort: 2017-Jun-14
:END:


*** u/artifex0:
#+begin_quote
  ...generate every possible sufficiently-unique brain that could correspond to a functional human...
#+end_quote

I feel like the math may not work out for that.

Imagine simulating every possible combination of a deck of cards- that's 52!, or about 8x10^{67} possible states. However, there are only 10^{50} atoms in the Earth. If it's possible to simulate every deck of cards with the material of our solar system, it would be pretty difficult.

Of course, when it comes to minds, you could simplify the problem by only simulating some relatively infinitesimal, but important or representative subset of possible minds- after all, a person might think of two technically different but extremely similar minds as the same person.

You could also get into some tough questions about where the line is between understanding a consciousness and simulating it actually is. If an AI has a perfect conceptual model of a mind, to what level of detail does it have to imagine that mind before it can be called individually conscious? What if an AI has a perfect abstract understanding of the sorts of minds that can arise? How abstract does something have to be before can no longer be called a consciousness? Depending on what consciousness actually is, you might be able to get away with simulating some abstract concepts instead of a lot of individual mental states.

Even so, I think it's easy to get over-awed by the vastness of the universe and our relative insignificance, and mis-judge how simple it would be to do something like simulating every possible mind.
:PROPERTIES:
:Author: artifex0
:Score: 3
:DateUnix: 1497287770.0
:DateShort: 2017-Jun-12
:END:


*** Hold up, you're assuming humans are just their number of neurons and their connection patterns. That doesn't seem like a valid assumption to me. For one thing, we already know about DNA molecules, so two people with the exact same configuration of neurons can still be very distinct humans if their DNA molecules are different.

I also suspect that positioning is going to be extremely important here. The slightest shift in the position of an atom could manifest in large behavioral changes. We already know this because of things like prion diseases and chemical imbalances and various enzymes. Therefore, the set of all possible human minds could actually be infinite, since you can keep moving things around in infinitesimally small units.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1497306840.0
:DateShort: 2017-Jun-13
:END:

**** u/scruiser:
#+begin_quote
  The slightest shift in the position of an atom could manifest in large behavioral changes.
#+end_quote

If that's true, then just thermal noise and slight differences in stimuli could also make large behavioral changes... which I suppose I don't have empirical evidence against this, but it seems to violate my intuitions about human behavior.
:PROPERTIES:
:Author: scruiser
:Score: 3
:DateUnix: 1497315586.0
:DateShort: 2017-Jun-13
:END:

***** It violates most of our understanding of how cognition works. Part of the point of cognition, being statistical, is to make the organism's fulfillment of its own needs robust to thermal noise in the body and environment.
:PROPERTIES:
:Score: 1
:DateUnix: 1497726526.0
:DateShort: 2017-Jun-17
:END:


*** There are more optimisations possible. First of all, you only need to simulate any individual brain for a single clock cycle. (Why? Well, after that clock cycle, it's still a viable mind - which will turn up somewhere else in your simulation). You /could/ run an algorithm that will eventually run all possible brains with all possible inputs - and thus, over the millenia, simulate every possible human life (exception: you'd have some maximal brain complexity for the simulation). However, this has two problems: first of all, you are also simulating every possible form of torture (an ethical problem) and secondly, you are simulating an unreasonably large amount of data (a computing problem). Fortunately, these two problems can be solved; if you're a superintelligent AI, you can presumably calculate in advance how 'good' a given mindstate will be (for some metric of 'good' which rewards happiness and prevents torture), and then simulate mindstates from the most 'good' on down, perhaps to some arbitrary limit.

As far as the simulated mindstates go, they will simply live - from an external viewpoint, in a staggeringly nonlinear temporal fashion, this mind existing for one instant /now/ and another instant ten years in the future followed by an instant that had been simulated twenty centuries in the past, but they won't notice that - they will simply live, believing themselves to be, well, wherever their simulated senses say they will be. In times of torture, pain, or other things decided to be 'Bad' by the simulation, they will simply... not exist, coming smoothly back into existence once the simulation again declares them sufficiently 'good'.
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1497341291.0
:DateShort: 2017-Jun-13
:END:


*** One possible reason not to do it is if there is disutility associated with someone having a fake past. The number of people whose past is genuine generated in such a system would be a lot lower than those whose memories are fake.

Also, assuming they are all placed in cohesive worlds, each person, even if assuming their own past is accurate, could still be virtually certain that the people they are interacting with in particular (despite being indistinguishable) all have false pasts to some extent. This would be true even in the subset of worlds where everyone's past is in fact accurate, i.e. they would (falsely, as a special case) have every reason to suspect their reality to be fabricated.

Another nontrivial issue would be that you'd be instantiating a bunch of memories of suffering that never happened historically. Fake memories of suffering might carry a huge amount of disutility relative to only historical suffering.

Still, if the alternative is everyone just randomly awakening for brief instants as Boltzmann Brains, it might be better. You could at least limit the memories to suffering that is actually possible in realistic historically consistent physical universes, which would be a tiny subset of total possible hells.
:PROPERTIES:
:Author: lsparrish
:Score: 2
:DateUnix: 1497403144.0
:DateShort: 2017-Jun-14
:END:


*** [[https://ieet.org/index.php/IEET2/more/bruere20121015]]
:PROPERTIES:
:Author: dirk_bruere
:Score: 2
:DateUnix: 1497442740.0
:DateShort: 2017-Jun-14
:END:


*** I've been thinking exactly this myself. The problem, of course, comes when you consider other forms of sapient life as well. Cutting this off at just humans seems racist, so would you attempt to simulate every possible arrangement of matter which could be considered appreciably sapient? Because that sounds like something our universe doesn't have the resources for.
:PROPERTIES:
:Author: Frommerman
:Score: 1
:DateUnix: 1497311478.0
:DateShort: 2017-Jun-13
:END:

**** u/ShiranaiWakaranai:
#+begin_quote
  would you attempt to simulate every possible arrangement of matter which could be considered appreciably sapient?
#+end_quote

Putting aside whether it is possible to do so, doing so would be an absolutely horrible idea. Every possible arrangement would also include every possible eldritch abomination hell-bent on destroying the world.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1497314535.0
:DateShort: 2017-Jun-13
:END:

***** Even excluding those you're talking about practically infinitely more resources than exist in our light cone.
:PROPERTIES:
:Author: Frommerman
:Score: 1
:DateUnix: 1497316198.0
:DateShort: 2017-Jun-13
:END:


** I found on [[/r/askreddit]] a [[https://www.reddit.com/r/AskReddit/comments/6hl62g/youre_given_immortality_but_as_a_cost_you_are/dizmx83/][surprisingly thought-out plan for how a single person might uplift a society, given said person is immortal.]]

What do you guys think? Workable, or critically flawed?
:PROPERTIES:
:Author: General_Urist
:Score: 2
:DateUnix: 1497728083.0
:DateShort: 2017-Jun-18
:END:
