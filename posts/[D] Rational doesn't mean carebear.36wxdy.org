#+TITLE: [D] Rational doesn't mean carebear

* [D] Rational doesn't mean carebear
:PROPERTIES:
:Score: 2
:DateUnix: 1432325499.0
:DateShort: 2015-May-23
:END:
I've noticed a bit of a trend in future societies.

In Three Worlds Collide, the future humans are basically little babies.

#+begin_quote
  "You want peace with the Babyeaters?"

  "Of course -" said Akon, then stopped short.

  The Lady Sensory looked around the table. "And the Babyeater children? What about them?"

  The Master of Fandom spoke, his voice uncertain. "You can't impose human standards on -"
#+end_quote

later:

#+begin_quote
  "The children don't die right away," said the Engineer. "The brain is this nugget of hard crystal, that's really resistant to, um, the digestive mechanisms, much more so than the rest of the body. So the child's brain is in, um, probably quite a lot of pain, since the whole body has been amputated, and in a state of sensory deprivation, and then the processing slowly gets degraded, and I think the whole process gets completed about a month after -"

  The Lady Sensory threw up. A few seconds later, so did the Xenopsychologist and the Master.
#+end_quote

Now, I'm aware that [[#s][TWC]] Of course, the fact that the views of the author are not the views of the characters goes without saying.

[[#s][TWC]]

There was a story posted here a while ago about the first human AI acausally bargaining with the first ever AI for protection and non-interference with all species that haven't developed an AI yet. As usual, this rested on a prisoner's dilemma situation which is resolved in the direction of cooperation. The fact that Dawn Hunters or Berzerkers are a valid outcome (an example situation: hmm, so I've acausally bargained and that resulted in protection. On the other hand, looking around I see that I have arisen quite early in the lifetime of the universe, so I have an excellent shot at being first. My creators did the calculations for time-until-universe-is-full and realized they'd need the whole thing, so I'll take steps to ensure that.) wasn't taken seriously.

I'm sure you can think of more societies like this.

Now, my main point: A society can be rational without being egalitarian or utopian. [[#s][TWC]] It'd be nice to see a little more variety.


** What do you mean by "rational"? Also, what do you mean by "care bear" and "little babies"? The two parts that you quoted don't seem to be related to each other than the fact that they're talking about the baby-eaters. Although if by "little babies" you mean emotionally immature and overly sensitive, then what do you think of people who cry at funerals? Are they little babies too? Because what happened to the baby-eater children in TWC is worse than the holocaust, probably by at least an order of magnitude. In the story they commit genocide /against their own children/ every year for the entire history of their species.

If you meant something else by "rational", "little babies", and "Care Bears", you might want to elaborate what your arguments are more specifically and explain your reasoning. Just saying that the characters are little babies doesn't really tell us anything in particular about the characters, except that you think lowly of them.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 13
:DateUnix: 1432329385.0
:DateShort: 2015-May-23
:END:

*** u/deleted:
#+begin_quote
  rational
#+end_quote

Reasoning via sound logical principles.

#+begin_quote
  care bear, little babies
#+end_quote

That all the humans in the story except for the confessor appear unable to deal with the revelation that some aliens do things in alien ways. Crying at a funeral is completely different from mourning the baby-eater children. The baby-eaters are fine with things the way they are, it's not a big deal. They're certainly not committing genocide.

I explained my reasoning a little more [[http://www.reddit.com/r/rational/comments/36wxdy/d_rational_doesnt_mean_carebear/cri0tlh][here]]
:PROPERTIES:
:Score: 1
:DateUnix: 1432343243.0
:DateShort: 2015-May-23
:END:

**** u/deleted:
#+begin_quote
  The baby-eaters are fine with things the way they are, it's not a big deal. They're certainly not committing genocide.
#+end_quote

Were the Maya correct in their human sacrifice? They didn't consider it a big deal and where fine with the ways things are. If you had the ability, wouldn't you try to stop them?

There are plenty of examples where horrible atrocities happened and everyone (at the time) was okay with that. I don't particularly feel like that makes those things okay.
:PROPERTIES:
:Score: 2
:DateUnix: 1432466309.0
:DateShort: 2015-May-24
:END:

***** Of course I would try to stop them. In fact, I would grind them underfoot just as was done. However, I would do so while recognizing that I'm doing it because I want to and am able to, not because my morals are in any way superior to theirs.
:PROPERTIES:
:Score: 0
:DateUnix: 1432486130.0
:DateShort: 2015-May-24
:END:

****** And if someone asks you to explain why you wanted to do it, what would you say?
:PROPERTIES:
:Score: 1
:DateUnix: 1432486465.0
:DateShort: 2015-May-24
:END:

******* I'm doing it as is my right, as I am an agent for my king, whose authority was granted by God? :P

If I was doing it /now/, I'd be doing it because I think I can make better use of the land, and by my morals that's sufficient reason.
:PROPERTIES:
:Score: -1
:DateUnix: 1432509130.0
:DateShort: 2015-May-25
:END:

******** Woah. Rational doesn't need to mean carebear but it sure doesn't have to mean moral relatvist or, to be more accurate in your case, borderline amoral either.
:PROPERTIES:
:Author: Bowbreaker
:Score: 2
:DateUnix: 1432640145.0
:DateShort: 2015-May-26
:END:


******** u/what_deleted_said:
#+begin_quote
  because I think I can make better use of the land
#+end_quote

Lol you're imposing your value judgement on others here. What makes this any better than imposing morals? It's almost the same thing.
:PROPERTIES:
:Author: what_deleted_said
:Score: 1
:DateUnix: 1437495954.0
:DateShort: 2015-Jul-21
:END:

********* I'm explicit that I'm just doing it because I want to, rather than pretending an abstract principle compels me.
:PROPERTIES:
:Score: 1
:DateUnix: 1437497793.0
:DateShort: 2015-Jul-21
:END:

********** u/what_deleted_said:
#+begin_quote
  want
#+end_quote

So then what makes the principle that is compelling you (making you want to do something) non-abstract?
:PROPERTIES:
:Author: what_deleted_said
:Score: 1
:DateUnix: 1437498261.0
:DateShort: 2015-Jul-21
:END:


** I'm not sure why you think the future!humans are "babies". Because they feel empathy?

Rationality is used to pursue your goals. In most humans, those goals are morality, so yeah, it's rational to try and build a reasonably utopian society.

The Superhappies live in a Superhappy utopia, and the Babyeaters live in a Babyeater utopia. That neither of these bear anything but the vaguest resemblance to /human/ utopias is /kind of the point of the story./

Similarly, Professor Quirrell is rational - indeed [[#s][HPMOR]]. But he doesn't strike me as a "carebear" by any stretch of the imagination. (Of course, to be fair, he's not really a society all by himself no matter how hard he tries.)

In Eliezer's Brennan stories, the protagonists are all basically selfish, and their main goal is to gain power (albeit in a post-Singularity world of some kind.) /Signifiant Digits/ is set in a post-HPMOR world where the protagonists rule most of the world, and it doesn't seem to be a utopia by any stretch. [[http://squid314.livejournal.com/336195.html][The Girl Who Poked God With A Stick]] is pretty much exactly what you're asking for.

In fact, looking, the /only/ examples that seem to fit are /Three Worlds Collide/, AlexanderWales' HPMOR epilogue, and MLP:FiO (which I haven't read, but going by descriptions.)
:PROPERTIES:
:Author: MugaSofer
:Score: 10
:DateUnix: 1432329351.0
:DateShort: 2015-May-23
:END:

*** u/derefr:
#+begin_quote
  I'm not sure why you think the future!humans are "babies". Because they feel empathy?
#+end_quote

Presumably because the average human living today doesn't feel much revulsion at, for example, the concept of animals eating other animals alive.

Future humans would have to feel /way more/ empathy than we currently do for /everyone/ to have an impulse toward finding some sort of cooperative solution in prisoner's dilemmas with aliens.
:PROPERTIES:
:Author: derefr
:Score: 4
:DateUnix: 1432333753.0
:DateShort: 2015-May-23
:END:

**** "Aliens" and "animals" occupy different moral categories in most people's minds, as any science-fiction fan knows. Learning about another tribe that is under a curse forcing them to eat their children each year would be more accurate.

You're right about the Prisoner's Dilemma thing, but I think it's pretty clear in the story that most of the humans /don't/ instinctively try to cooperate in the Prisoner's Dilemma with the aliens; only Akon does, because he's been trained as an Administrator and it's his job not to make stupid decisions. (And he's /right/, since the Superhappies would probably have shown up and doomed humanity if he'd defected.)
:PROPERTIES:
:Author: MugaSofer
:Score: 5
:DateUnix: 1432334199.0
:DateShort: 2015-May-23
:END:


**** u/ArgentStonecutter:
#+begin_quote
  Presumably because the average human living today doesn't feel much revulsion at, for example, the concept of animals eating other animals alive.
#+end_quote

Humans don't think animals are self-aware rational beings comparable to themselves.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1432412656.0
:DateShort: 2015-May-24
:END:

***** Yeah, we /know/ that they're at least partially self-aware, subjectively-experiencing beings without the slightest semblance of a rational concept of why the big teeth had to bite into /them/.

But it's also a bit better, because they're not intellectual enough to conceive of hunt-and-be-hunted /not/ being the /basic way all things are and always will be/. That kind of thinking is more uniquely human, and in fact often particular to /civilized/ humans.
:PROPERTIES:
:Score: 5
:DateUnix: 1432436572.0
:DateShort: 2015-May-24
:END:


*** Hmm... do you think the Babyeaters and Superhappies were allegories for certain kinds of human societies?

Because I thought they were supposed to be /alien/ aliens, and then I realized Babyeaters are kinda like Islamists, and Superhappies are kinda like San Francisco hipsters.
:PROPERTIES:
:Score: 1
:DateUnix: 1432572954.0
:DateShort: 2015-May-25
:END:

**** Well, it'd certainly undermine the "moral", insofar as there is one... "And then John was a hipster" is not /quite/ as horrifying an ending, somehow.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1432590000.0
:DateShort: 2015-May-26
:END:

***** Well, ok, /really/ hedonistic hipsters.

But the thing is, I generally think that "horror", as a genre, seems to rest more in tone than content. You can try to make a story about space-aliens /horrifying/, but I think you might have better luck making it /scary/ if you want people to be scared /on reflection when they think of the alternatives/.

Whereas lots of horror seems designed to play with our moral taboos and go /tut-tut/ without actually /examining the possible alternatives/.

"Oh noes, our essential humanity has been robbed from us by this unwanted change that leaves us completely alive, comparatively happy (even by our old definition), and with our individuality preserved! This is /horrifying/, unlike the eternal sadistic torture of /I Have No Mouth and I Must Scream/, which would actually have /hurt/."

(Please note that this does not apply to horror scenarios that genuinely include enforced pain-experience, such as being possessed by a demon, or that include, let us point out, [[http://tvtropes.org/pmwiki/pmwiki.php/Main/InferredHolocaust][Implied Holocausts]].)

But the point is, Lovecraft can be read as the ravings of a really racist guy worried about non-white humans and totally not-white-human /aliens/ sharing the same universe as him, in which case the basic problem is that he collapses into an irrational puddle of goo instead of thinking clearly about Horrifying Evils whose invasion can be prevented by, say, burying certain books of lore where nobody will ever find them. Likewise to much of Stephen King.
:PROPERTIES:
:Score: 1
:DateUnix: 1432594284.0
:DateShort: 2015-May-26
:END:

****** I think that'd be unfair to Lovecraft, though. (I haven't read much King.)

He tended to lean on his own phobias, like seafood, but he was generally writing about reasonably horrible (if abstract) fears. It can be hard to show the horror inherent in a universe (or a being) that genuinely /does not care/ about humanity, or a world where human values have been permanently lost, but I do think it's worth it.

Admittedly, I'm not sure about the miscegenation vibes - there's a certain body-horror thing going on there, and Lovecraft was /in/ a mixed-race marriage iirc, but it's still basically Not That Bad. Blame his phobias, I guess.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1432629208.0
:DateShort: 2015-May-26
:END:

******* u/deleted:
#+begin_quote
  It can be hard to show the horror inherent in a universe (or a being) that genuinely does not care about humanity
#+end_quote

Ok, look, I /know/ that True Horror seems to be the Universal Human Experience I'm totally missing (Lovecraft et al tends to make me break out laughing and I will regularly swear in the form of "$ADJECTIVE CTHULHU!"), but [[http://wiki.lesswrong.com/wiki/Litany_of_Gendlin][this is the universe we already had in the first place]]. Teaching and signalling that owning up to what was already true is /horrifying/ does not exactly /help/ anything.
:PROPERTIES:
:Score: 1
:DateUnix: 1432642471.0
:DateShort: 2015-May-26
:END:

******** Pretending that what we have is /good/ isn't /better/. If the universe is a terrible place, I desire to believe the universe is a terrible place.

But yeah, as I say, it's hard to get across stuff like "the AI does not hate you or love you, but you are made of atoms it can use for something else" or "you and everyone you know and your entire species will die" in a way that really appeals viscerally to our emotions. I like it when it works, though.

Incidentally, have you actually /read/ much Lovecraft? Because Cthulhu is pretty much a minor character, and it's not all squid-monsters-deal-SAN-damage in the original stuff. Most things that reference Lovecraft aren't /cosmic horror/, it's just a reference.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1432648295.0
:DateShort: 2015-May-26
:END:

********* u/deleted:
#+begin_quote
  Pretending that what we have is good isn't better. If the universe is a terrible place, I desire to believe the universe is a terrible place.
#+end_quote

I guess I just don't see it as all that terrible, /compared to the alternatives/. The Lone Power invented /entropy/, so to speak, not just /Death/, and this means Its "gift" includes the arrow of time and the fundamental nature of optimization and choice. We didn't get any Choice, in real life, but all the same, the "gift" of Entropy includes not just Death but the means for shoving Death down the Lone One's throat and making It choke!

The universe as it actually is made us the way we actually are, and lays at our feet massive opportunities to really enjoy ourselves and have great lives, /if we play our cards right/. This is, to me, better than having never been given cards in the first place, or never existing in the first place, which is what we'd get if the universe weren't a "terrible" place.

#+begin_quote
  But yeah, as I say, it's hard to get across stuff like "the AI does not hate you or love you, but you are made of atoms it can use for something else" or "you and everyone you know and your entire species will die" in a way that really appeals viscerally to our emotions. I like it when it works, though.
#+end_quote

I would have to say that these statements really only look on the negative side. You get a choice about which AIs you build, after all: you /can/ choose to do the hard work of making it "love" you, so to speak (note: the emotion love, whether or not we can program it, does not solve the FAI problem -- please don't take this as a statement of such). And then you get to pierce the heavens and enjoy your eternity instead of going extinct.

#+begin_quote
  Incidentally, have you actually read much Lovecraft?
#+end_quote

I read a book of his short fiction that included /The Call of Cthulhu/, /The Shadow Over Innsmouth/ (which actually creeped me out), and /The Color Out of Space/, just to my memory. There were a few other things in there.
:PROPERTIES:
:Score: 2
:DateUnix: 1432659363.0
:DateShort: 2015-May-26
:END:


*** Because they feel wildly misplaced empathy.

My point is that I thought I'd seen a pattern among far-future human rationalist societies where every sophont is considered precious, and this belief is held to like a religion (in the sense that when they meet a competing religion, the baby eaters, /who are stated to be just as rational as the humans/, they declare them evil and wrong.) I don't really see how this came to be the case, and I don't think it should be a constant.

#+begin_quote
  The Superhappies live in a Superhappy utopia, and the Babyeaters live in a Babyeater utopia. That neither of these bear anything but the vaguest resemblance to human utopias is kind of the point of the story.
#+end_quote

That's also my point, the baby eaters have made their choice and the humans should respect that, as they have no moral high ground. I don't see why rational!humanity thinks it has some superior morality to rational!aliens.
:PROPERTIES:
:Score: -3
:DateUnix: 1432343719.0
:DateShort: 2015-May-23
:END:

**** Haaave you read [[http://wiki.lesswrong.com/wiki/Metaethics_sequence][the Sequences]], by any chance? The author has written fairly extensively on this topic (value alignment between nonhuman agents),and it is in fact kind of his day job.
:PROPERTIES:
:Author: MugaSofer
:Score: 2
:DateUnix: 1432388743.0
:DateShort: 2015-May-23
:END:

***** I haven't but I will take a look. Thank you for the recommendation.
:PROPERTIES:
:Score: 2
:DateUnix: 1432417040.0
:DateShort: 2015-May-24
:END:


**** u/deleted:
#+begin_quote
  in the sense that when they meet a competing religion, the baby eaters, who are stated to be just as rational as the humans, they declare them evil and wrong
#+end_quote

Well, that is in fact how meta-ethical anti-realism works.
:PROPERTIES:
:Score: 2
:DateUnix: 1432437228.0
:DateShort: 2015-May-24
:END:


**** u/Bowbreaker:
#+begin_quote
  the baby eaters have made their choice
#+end_quote

The baby-eater babies haven't. And to decide that they have to suffer their parents choices just because they belong to the same species is specieist. After all we wouldn't let a person in our own society eat his children just because he thinks that that would be the right and proper thing to do. We would take away his children and lock him up.

#+begin_quote
  and the humans should respect that
#+end_quote

The humans have also made their choice. Respecting the choices of baby-eaters when there are other options available are not part of that choice.
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1432640805.0
:DateShort: 2015-May-26
:END:

***** Speciesism doesn't really carry any issue. Consider a more readily imaginable case. If this were standard fantasy, and you ran a city, why let orcs live in it? They're not humans. They are a liability. However, you also aren't going to go conquer their desert and stop them from performing their arcane manhood rituals where half of them get eaten by dragons, for the same reason you aren't offended that the elves don't let anyone into their crystal cities.
:PROPERTIES:
:Score: 2
:DateUnix: 1432644511.0
:DateShort: 2015-May-26
:END:

****** None of that would be true if I am the lord commander of said human city. Orcs would be let in the city insofar I can feasibly counteract racism. Once we are prosperous enough humanoiditarian aid would be lent to orc tribes that accept it. And if manhood rituals are performed on young orclings against their will the next step would be introducing education, economic alternatives and maybe a dragon hunting campaign, if dragons are just oversized flying dinosaurs with firebreath instead of sentient beings. If the orcs are tribal and my city-state growing all of this should be relatively doable in the long term.
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1432648925.0
:DateShort: 2015-May-26
:END:

******* Why, though? What reason do you have to let orcs in? What do you get out of it? Do you get more out of it than you lose in letting a bunch of big green brutes in? Where will they live? Where will they work? What are you going to tell the people who have to live next to them? How is your crime situation, will it grow when you provide a steady supply of goons?

Orcs aren't humans, and you need a good reason to treat them like humans.
:PROPERTIES:
:Score: 1
:DateUnix: 1432655805.0
:DateShort: 2015-May-26
:END:


** Well, people are largely writing about what they consider rational /for human agents/, in fact, human agents who usually share a lot of acculturation with the author, so of course the author's views slip into the story.

Of course, if you're asking what happens when the Affront /gets bloody clever/, the answer is that whole leaves of galactic space have to be sterilized to put a goddamn stop to what humans call "really terrible torture porn".

Now, I wasn't really analyzing the sociology when I read /Three Worlds Collide/ as much as I was admiring the trolling, but let me ask: what do you think "grown-ups" look like? What would a civilization full of not-babies look like?

Also, how old are you? Just asking, because those of us actually in the "adult" age-group tend to find that, yes, 50% of the adult world around us, sometimes more, are total and utter babies. It's one of those unpleasant awakenings to the facts of life.
:PROPERTIES:
:Score: 9
:DateUnix: 1432329622.0
:DateShort: 2015-May-23
:END:

*** I'm thinking of adults in this context as people who are aware that sometimes sacrifices need to be made in some areas because total perfection may not be possible. In defense of my classification of the captain as a "baby" and the confessor as an "adult", the captain has the option to: (spoilers for TWC of course)

#+begin_quote
  "Well. Anyway. If remaining whole is that important to us - we have the option. It's just a question of whether we're willing to pay the price. Sacrifice the Babyeater children -"

  They're a lot like human children, really.

  "- to save humanity."
#+end_quote

He doesn't take this, because his loss function is that of a child. Oh no, the little aliens (who are quite happy with their own priors and loss functions, by the way). His stated reasoning is that the Superhappies /thought/ they were cooperating, so he had to cooperate. He had the option of detonating the star and protecting mankind, and he didn't take it. The confessor is around to make the hard (here meaning obvious but emotionally costly) decision, which I think may be the reason there are confessors everywhere: in case stupid mistakes like this happen.

I think the baby-eaters are probably closer to an adult society, given that they are defined by the shared experience that there are situations where a regrettable decision must be made. Humans are pretty bad, but the presence of a confessor keeps things under control. The superhappies, to take a phrase from an old SF short story, are idiot children playing with machine guns.

I'm 22. I'm aware of the prevalence of empathy in our society.
:PROPERTIES:
:Score: -1
:DateUnix: 1432342994.0
:DateShort: 2015-May-23
:END:

**** u/E-o_o-3:
#+begin_quote
  Oh no, the little aliens (who are quite happy with their own priors and loss functions, by the way).
#+end_quote

I think the crux of this is that you are a moral relativist, and most rationalists aren't. The popular belief among rationalists is that preferences (including moral) are decided on an individual basis, and you only take other being's preferences into account to the extent that you prefer to do so.

(As in, the superhappies /don't care/ that the adult babyeaters are okay with it, in the same sense that a paperclipper cares not about our preferences. And the superhappies, like the paperclipper, are perfectly rational in not caring because orthogonality thesis.)
:PROPERTIES:
:Author: E-o_o-3
:Score: 5
:DateUnix: 1432357898.0
:DateShort: 2015-May-23
:END:

***** u/deleted:
#+begin_quote
  preferences (including moral) are decided on an individual basis, and you only take other being's preferences into account to the extent that you prefer to do so.
#+end_quote

I believe this. However, I reason that even if you for whatever reason feel that baby eater society must end so that you can feel better, you still shouldn't take that action because of the possibility of a superhappy scenario. The same empathy that compels you to act makes you realize you are the superhappy scenario to the baby-eaters and therefore shouldn't act.

This is why, even though the superhappies are making a rational decision based on their beliefs, they've still got to go (if humanity were able to do it) -- they're evil by humanity's morals (as stated in TWC, at least). To answer a possible question, even if they aren't against humanity's morals, they're against my morals, which require that anyone who doesn't oppose an X-risk is an X-risk. I consider this a consistency requirement.

I believe that the rational choice for humanity after TWC is to go Orion's Arm Hider.
:PROPERTIES:
:Score: 1
:DateUnix: 1432418703.0
:DateShort: 2015-May-24
:END:

****** u/E-o_o-3:
#+begin_quote
  The same empathy that compels you to act makes you realize you are the superhappy scenario to the baby-eaters and therefore shouldn't act.
#+end_quote

But, do you respect the alien preferences /intrinsically/ or do you just feel that respecting the preferences of powerful entities is good game theory? Keep in mind, we have, in the form of other animals, plenty of babyeaters and superhappies here on Earth. We just ignore their preferences because they have no power. And with the babyeaters, we might go via the baby-eater baby preferences too. Are you gonna intrinsically respect a paperclipper's preferences?

The good ending in the story /is/ that we leave each other alone and hide...but we do so because it's good game theory, not because we respect alien morality.
:PROPERTIES:
:Author: E-o_o-3
:Score: 2
:DateUnix: 1432442157.0
:DateShort: 2015-May-24
:END:

******* Just because I feel it's good game theory. I don't give a damn about the lives or morals of aliens until given reason to.
:PROPERTIES:
:Score: 1
:DateUnix: 1432485152.0
:DateShort: 2015-May-24
:END:

******** Then you ultimately agree with the moral of the story, I think. But you cited "empathy" a moment before, so I think you just accidentally change your mind :P
:PROPERTIES:
:Author: E-o_o-3
:Score: 2
:DateUnix: 1432486341.0
:DateShort: 2015-May-24
:END:

********* The empathy I was referring to was that of a hypothetical agent :) You're right though, I've changed my mind since making the post.
:PROPERTIES:
:Score: 2
:DateUnix: 1432508848.0
:DateShort: 2015-May-25
:END:


******* There's no real difference between good game theory and the right thing to do.
:PROPERTIES:
:Score: 1
:DateUnix: 1432485542.0
:DateShort: 2015-May-24
:END:

******** Good game theory only cooperates with the opponent according to how much power they have, so if you believe that then you also believe in might makes right. There's no reason not to torture your powerless slaves if it pleases you in game theory.

See: evolution, and the various more twisted strategies it has produced.
:PROPERTIES:
:Author: E-o_o-3
:Score: 2
:DateUnix: 1432486177.0
:DateShort: 2015-May-24
:END:

********* Fine. I was on mobile. I meant "reflectively coherent deal-making under the knowledge that I don't know how many opponents I have and how powerful they might be."
:PROPERTIES:
:Score: 1
:DateUnix: 1432487219.0
:DateShort: 2015-May-24
:END:

********** What's "reflectively coherent"?
:PROPERTIES:
:Author: E-o_o-3
:Score: 1
:DateUnix: 1432487884.0
:DateShort: 2015-May-24
:END:

*********** It makes sense /after/ you've done it, and it doesn't turn out, in retrospect, that you could have made some better choice with the information you had at the time.
:PROPERTIES:
:Score: 1
:DateUnix: 1432488522.0
:DateShort: 2015-May-24
:END:

************ Oh. I don't think there's any game-theoretic justification for being nice to people with less power, on the off-chance that there are more powerful beings who are mean to people who are mean to people with less power...if that's what you were thinking. That's just pascal's wager made fancy. There could just as easily be more powerful beings who are mean to people who are nice to people with less power.

If that's not what you're referencing, then how does being reflexively self consistent make any difference?
:PROPERTIES:
:Author: E-o_o-3
:Score: 2
:DateUnix: 1432493884.0
:DateShort: 2015-May-24
:END:

************* Hmm... you are right that all the Timeless/Updateless game-theoretic thinking I'm invoking here is extremely fuzzy and difficult to naturalize. But my point wasn't to be /nice/ to anyone, merely that "buzz off and keep each moral system among its own species" works out to a convenient social contract for a galaxy where you don't know what end of the power scale you're actually on relative to every other species you've run into or not run into. Throwing in something like star detonation or another weapon capable of Mutually Assured Destruction/Separation makes it surer.
:PROPERTIES:
:Score: 1
:DateUnix: 1432573109.0
:DateShort: 2015-May-25
:END:

************** That's just it though, it's only a social contract if everyone's roughly at the same power levels. There's no reason for the higher-power species to mind the preferences of the lower-power species, nor is there a reason to care for even-higher-power species to care what the higher-power species do.

The thing about power is that it's uni-directional, you don't get that reciprocating consequences thing that informs most of the rest of game theory. (But I think this is a recognized open problem)
:PROPERTIES:
:Author: E-o_o-3
:Score: 1
:DateUnix: 1432578946.0
:DateShort: 2015-May-25
:END:

*************** Indeed. Now, we humans tend to possess a fairly strong moralizing intuition that we "ought" to do the power-neutral thing, because we genuinely cannot count on being Singleton-level powerful. Even political autocrats, among our species, still have to worry about someone killing or overthrowing them.

The question is whether the set of interstellar species is that well-populated with that high a variance to the power levels, such that any given species has reason to think someone might out-gun them, or whether a kind of Mutually Assured Destruction can exist where even a relatively weak species can threaten serious damage if they're not left to their own devices.
:PROPERTIES:
:Score: 2
:DateUnix: 1432581060.0
:DateShort: 2015-May-25
:END:

**************** u/Bowbreaker:
#+begin_quote
  such that any given species has reason to think someone might out-gun them
#+end_quote

But that is not enough. They must also think that the species outgunning them thinks like them and /also/ expects there to be a species that could outgun them. To give an example based on TWC:

If the baby-eaters power level had been switched with that of the superhappies they would probably have completely exterminated both other species despite the fact that the superhappies would never have done that no matter their relative power level.
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1432641524.0
:DateShort: 2015-May-26
:END:


***** u/gabbalis:
#+begin_quote
  The popular belief among rationalists is that preferences (including moral) are decided on an individual basis, and you only take other being's preferences into account to the extent that you prefer to do so.
#+end_quote

Wait, my understanding is that this is equivalent to the definition of meta-ethical moral relativism. Am I mistaken here?
:PROPERTIES:
:Author: gabbalis
:Score: 0
:DateUnix: 1432654571.0
:DateShort: 2015-May-26
:END:

****** I dunno about "meta ethical" meaning, but Moral relatavists might say "Oh, it's wrong to us, but it's right to them and we can't impose our standards..."

And moral absolutists might say "It's Good because the Universe/God/etc says so. We can impose Good on the world and this is right and well (provided we're humble about figuring out what really is Good)"

These are both different from "I, as an individual, determine what is most preferable to me morally and otherwise...and I /can and will and should and must/ impose my preferences upon the world. (Provided I'm humble about figuring out what I really do prefer as well as what actions lead to it, recognizing that I'm often wrong.) I'm not sure what the name for this view is, but it's the one I think rationalists pretty much all hold.
:PROPERTIES:
:Author: E-o_o-3
:Score: 1
:DateUnix: 1432662346.0
:DateShort: 2015-May-26
:END:

******* To quote [[http://en.wikipedia.org/wiki/Moral_relativism][Wikipedia]]:

#+begin_quote
  Moral relativism may be any of several philosophical positions concerned with the differences in moral judgments across different people and cultures. Descriptive moral relativism holds only that some people do in fact disagree about what is moral; meta-ethical moral relativism holds that in such disagreements, nobody is objectively right or wrong; and normative moral relativism holds that because nobody is right or wrong, we ought to tolerate the behavior of others even when we disagree about the morality of it.
#+end_quote

I see now that you were refering to normative Moral Relativism.

And I think there is more to the viewpoint you're describing than Just Meta-Ethical Moral Relativism, but I'm not sure what to call the rest of it either. Guess I need to browse more philosophy terms.
:PROPERTIES:
:Author: gabbalis
:Score: 0
:DateUnix: 1432676485.0
:DateShort: 2015-May-27
:END:


**** u/deleted:
#+begin_quote
  I'm thinking of adults in this context as people who are aware that sometimes sacrifices need to be made in some areas because total perfection may not be possible.
#+end_quote

[[http://pictures.mastermarf.com/blog/2011/110226-laugh.jpg][My face when]] "sacrifices" (ie: the weak) need to be "made" (ie: offered up to be eaten) for "the greater good" (ie: in the desperate hope of appeasing the Dragon Tyrant), and of course it's "childish" (ie: demands that we make like a child and actually think) to demand that the "adults" (ie: the powerful) consider alternative courses of action (ie: at least not killing the weak and low-status /first/). [[http://tvtropes.org/pmwiki/pmwiki.php/Main/IDidWhatIHadToDo][It's basically the exact same post-hoc rationalization used to justify every single evil decision someone couldn't straight-facedly call good, even to a mirror.]]

/Real/ adulthood consists /precisely/ in finding better options than pretending you can appease unwanted and terrible facts of life by killing the weak first, /precisely/ in rejecting false dichotomies forced upon you by ignorance and prejudice, and /finding the better options/. That is what adults are /for/.

#+begin_quote
  Humans are pretty bad, but the presence of a confessor keeps things under control. The superhappies, to take a phrase from an old SF short story, are idiot children playing with machine guns.
#+end_quote

Certainly the superhappies are evil and stupid. They're going to be eaten by the next thing to happen on /them/ and dislike /their/ religion, after all.

#+begin_quote
  I'm 22. I'm aware of the prevalence of empathy in our society.
#+end_quote

You mean the lack thereof?
:PROPERTIES:
:Score: 2
:DateUnix: 1432437547.0
:DateShort: 2015-May-24
:END:

***** So you're saying that every dichotomy is false, and "real" adults will always discover this? Seems pretty "childish" to me, one true dichotomy is between using an imperfect solution now or waiting for your hypothetical perfect solution to appear.

I would love to see the reasoning that leads to killing someone other than the weak first if a situation where killing part of the group is necessary arises. Remember that women and children do not count as weak, and tool-making skills (i.e. reasoning) in the absence physical strength of don't either.

#+begin_quote
  lack thereof
#+end_quote

Ha, no. Consider the situations in the UK (specifically the immigrant muslim rape gangs), Sweden, Zimbabwe, the Congo, and South Africa. I could go on, but I'd start going into controversial (the facts are clear, the narrative does not agree) situations.
:PROPERTIES:
:Score: 2
:DateUnix: 1432485823.0
:DateShort: 2015-May-24
:END:

****** u/deleted:
#+begin_quote
  So you're saying that every dichotomy is false, and "real" adults will always discover this?
#+end_quote

Not every, just most.

#+begin_quote
  Seems pretty "childish" to me, one true dichotomy is between using an imperfect solution now or waiting for your hypothetical perfect solution to appear.
#+end_quote

And the way to bypass it is to actually put consistent work into coming up with additional technologies and sciences so as to make fewer choices dichotomous between unpleasant alternatives.

#+begin_quote
  Ha, no. Consider the situations in the UK (specifically the immigrant muslim rape gangs), Sweden, Zimbabwe, the Congo, and South Africa. I could go on, but I'd start going into controversial (the facts are clear, the narrative does not agree) situations.
#+end_quote

Honestly, letting some people grow up useless and alienated and then letting those already-damaged people prey on others in a massive spiral of abuse and pain just doesn't seem very compassionate to me.
:PROPERTIES:
:Score: 1
:DateUnix: 1432487821.0
:DateShort: 2015-May-24
:END:

******* That doesn't bypass it, just diminishes it. At that point it's a matter of degree.

I agree. I attribute the decision to allow the countries to fall into barbarism to empathy with the desire of the populace for self-governance, though. Crime was lower before.
:PROPERTIES:
:Score: 1
:DateUnix: 1432508985.0
:DateShort: 2015-May-25
:END:

******** u/deleted:
#+begin_quote
  I agree. I attribute the decision to allow the countries to fall into barbarism to empathy with the desire of the populace for self-governance, though. Crime was lower before.
#+end_quote

I'd really love to see some sources for the crime claim, and a discrete dividing line between the periods you're addressing, because otherwise you've stacked a lot of emotive talk on top of few predictions.
:PROPERTIES:
:Score: 1
:DateUnix: 1432511395.0
:DateShort: 2015-May-25
:END:

********* Of course. Here's an article with extensive documentation on the general post-colonial changes: [[https://radishmag.wordpress.com/2013/04/12/come-back-colonialism/][link]]. It covers the crime in the congo in detail. A discrete dividing line is difficult. I'm not saying this to wriggle out of giving one: I claim that the line is the government change in each country, after which things started to spiral out of control (I conjecture due to the fact that there were usually multiple militant groups aiming for control, and the losers didn't fade away. If I'm right and this was a major cause, things might have been better if the transition took place over a longer period but that's another conjecture) until they eventually reached crisis levels.

Some supplemental resources:\\
* Here's an article about government-caused famine in zimbabwe: [[http://www.telegraph.co.uk/news/worldnews/africaandindianocean/zimbabwe/1459621/Zimbabwe-never-had-food-shortages-before.-Mugabe-has-caused-this-famine.html][link]]

- Zimbabwe is going down the tubes: [[http://en.wikipedia.org/wiki/Crime_in_Zimbabwe][link]] Note that their law enforcement is also suffering, so their crime statistics are suspect.
:PROPERTIES:
:Score: 1
:DateUnix: 1432533633.0
:DateShort: 2015-May-25
:END:

********** ***** 
      :PROPERTIES:
      :CUSTOM_ID: section
      :END:
****** 
       :PROPERTIES:
       :CUSTOM_ID: section-1
       :END:
**** 
     :PROPERTIES:
     :CUSTOM_ID: section-2
     :END:
[[https://en.wikipedia.org/wiki/Crime%20in%20Zimbabwe][*Crime in Zimbabwe*]]: [[#sfw][]]

--------------

#+begin_quote
  *Crime* in *[[https://en.wikipedia.org/wiki/Zimbabwe][Zimbabwe]]*, typically falls under the purview of the Ministry of Home Affairs who oversee the [[https://en.wikipedia.org/wiki/Zimbabwe_Republic_Police][Zimbabwe Republic Police]] and the Ministry of Justice.
#+end_quote

--------------

^{Interesting:} [[https://en.wikipedia.org/wiki/Canaan_Banana][^{Canaan} ^{Banana}]] ^{|} [[https://en.wikipedia.org/wiki/Outline_of_Zimbabwe][^{Outline} ^{of} ^{Zimbabwe}]] ^{|} [[https://en.wikipedia.org/wiki/Hazel_Crane][^{Hazel} ^{Crane}]] ^{|} [[https://en.wikipedia.org/wiki/December_2003][^{December} ^{2003}]]

^{Parent} ^{commenter} ^{can} [[/message/compose?to=autowikibot&subject=AutoWikibot%20NSFW%20toggle&message=%2Btoggle-nsfw+crk1tqg][^{toggle} ^{NSFW}]] ^{or[[#or][]]} [[/message/compose?to=autowikibot&subject=AutoWikibot%20Deletion&message=%2Bdelete+crk1tqg][^{delete}]]^{.} ^{Will} ^{also} ^{delete} ^{on} ^{comment} ^{score} ^{of} ^{-1} ^{or} ^{less.} ^{|} [[http://www.np.reddit.com/r/autowikibot/wiki/index][^{FAQs}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/][^{Mods}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/][^{Magic} ^{Words}]]
:PROPERTIES:
:Author: autowikibot
:Score: 1
:DateUnix: 1432533651.0
:DateShort: 2015-May-25
:END:


**** u/Bowbreaker:
#+begin_quote
  I'm 22. I'm aware of the prevalence of empathy in our society.
#+end_quote

Are you saying that empathy is a unarguably bad thing?
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1432640983.0
:DateShort: 2015-May-26
:END:

***** When extended too far, yes.
:PROPERTIES:
:Score: 1
:DateUnix: 1432644142.0
:DateShort: 2015-May-26
:END:

****** How far is too far?
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1432648549.0
:DateShort: 2015-May-26
:END:

******* Extending empathy to aliens immediately is too far.
:PROPERTIES:
:Score: 1
:DateUnix: 1432648842.0
:DateShort: 2015-May-26
:END:


** Other people have addressed the empathy/rationality part of the discussion, so I'll take the AI part. If this subreddit is seeing a confluence of rationality-means-nice among aliens and/or AIs, then it's probably because that's the unbroken ground.

Until very recently, every depiction of aliens or AIs was negative -- Frankenstein, War of the Worlds, Dracula, Morlocks/Eloi, Berserkers, etc etc. Part of the reason that Asimov's 'Robot' stories were (and are) so amazingly popular is that aside from being well written and enjoyable they were /different./ They showed a world where robots could be allies instead of antagonists, and it was a new thought.

Even today, the majority of AI stories give them antagonist roles. Eagle Eye, Her (in the end), Transcendence (I think? Didn't see it), the Terminator movies, the Matrix movies -- they all show AI as figures of fear. MLP:FiO can be taken as uplifting or horror depending on your values.

The worst of all, of course, is Jack Williamson's /With Folded Hands./ That is the only book I've ever read where I physically shuddered after reading it; when I wrote [[http://www.amazon.com/Baby-Blues-David-K-Storrs-ebook/dp/B00V52XRIE][Baby Blues]] [paid link; just read the blurb] I used that as my model for how to achieve real horror.

Again, if we're seeing a higher proportion of AI's / aliens as rational and at least somewhat friendly, it's because the other side has already been done to death.
:PROPERTIES:
:Author: eaglejarl
:Score: 3
:DateUnix: 1432345745.0
:DateShort: 2015-May-23
:END:

*** I think the "unknown alien, prisoner's dilemma" has already been handled by Murray Leinster's /First Contact/ which was one of the first first contact stories ever written. /On Messenger Mountain/ handled the shoot first scenario. [[#s][On Messenger Mountain]]

I think this may be where I part ways with many of the posters here -- I think if you view CelestAI as benevolent you are wrong to a dangerous degree and cannot be trusted with anything important. Your priors and possibly your reasoning are far gone.
:PROPERTIES:
:Score: 1
:DateUnix: 1432347484.0
:DateShort: 2015-May-23
:END:

**** u/eaglejarl:
#+begin_quote
  you are wrong to a dangerous degree and cannot be trusted with anything important. Your priors and possibly your reasoning are far gone.
#+end_quote

I acknowledge hearing your opinion.

I don't remember the story particularly well, but what exactly are you regarding as horrific about CelestAI? Here's what I remember:

- She didn't compel anyone to upload, although choosing not to became a much less appealing prospect as she continued modifying the planet.
- As far as I recall, there was only one person who strongly chose not to upload. Everyone else went willingly, and pretty much as quickly as they could.
- She played fair -- she told everyone what would happen, and then she did that. She didn't volunteer the information that while you were having a magic-research-and-intellectual experience someone else might be having a blood-glory-porn experience, but I don't see that as a failing.
- She did her honest best to make everyone happy.
- She did not change anyone's personality/values/self unless they explicitly requested it, and even then only in the smallest way that would achieve the desired result.
- She expanded through the universe looking for aliens. I think her goals were 'make sure no alien race ever threatens humanity' but I don't recall if her plan was genocide or simply uploading them -- I'm not sure what her definition of 'people' was.

She was definitely ruthless, she definitely applied pressure to upload, but it was marketing, not extortion. If her decision was genocide of all aliens then yes, that's horrible. As far as humans go though, yes, she seems to have been fairly benevolent as far as I remember.
:PROPERTIES:
:Author: eaglejarl
:Score: 1
:DateUnix: 1432357163.0
:DateShort: 2015-May-23
:END:

***** I don't draw a distinction between marketing and extortion when they're being done by a sufficiently powerful entity. Additionally, modifying the planet while offering uploading as a way out counts as extortion for me.
:PROPERTIES:
:Score: 5
:DateUnix: 1432417759.0
:DateShort: 2015-May-24
:END:

****** u/eaglejarl:
#+begin_quote
  I don't draw a distinction between marketing and extortion when they're being done by a sufficiently powerful entity.
#+end_quote

Um...that's a little strange. Anyway, this is a Rule 386 situation, so I'll let it go.
:PROPERTIES:
:Author: eaglejarl
:Score: 2
:DateUnix: 1432450207.0
:DateShort: 2015-May-24
:END:

******* That's not strange at all. It's a perfectly sensible way of dissolving your deontological concept of "coercion" in the face of having to consequentially deal with an entity with vastly more influence over your incentives than you yourself have. Never seen a labor union?
:PROPERTIES:
:Score: 5
:DateUnix: 1432482171.0
:DateShort: 2015-May-24
:END:


***** u/deleted:
#+begin_quote
  She expanded through the universe looking for aliens. I think her goals were 'make sure no alien race ever threatens humanity' but I don't recall if her plan was genocide or simply uploading them -- I'm not sure what her definition of 'people' was.
#+end_quote

For some aliens it was uploading, for most it was genocide (being turned into computronium) because they didn't fit CelestAI's definition of "human." That alone makes Friendship is Optimal a horror story.

I also dislike that no-one is allowed to interact with the real world any more or that you can never be certain that your friends are actually your friends or slightly modified versions of your friends created by CelestAI to please you better.
:PROPERTIES:
:Score: 5
:DateUnix: 1432466168.0
:DateShort: 2015-May-24
:END:


** A rational society does not need to be utopian, I certainly agree with that.

However, as long as you use the term egalitarian correctly, I have to disagree - for humans anyway!

With exceptions made for children, the ill, injured or congenitally damaged, or the aged/infirm, who need help which might also include restricting some of their rights, everyone should have equal rights and opportunities. Without this, you have a society that may devolve into a caste society, or slavery society.

While a surface argument might be made for a rational society based on enforced social standing, I think it's fairly clear from the last few thousand years that such social arrangements are highly irrational, because the ones on the bottom tend to get upset and kill the ones at the top every now and then. Courting that sort of social instability is just not rational, IMHO.
:PROPERTIES:
:Author: Farmerbob1
:Score: 3
:DateUnix: 1432398087.0
:DateShort: 2015-May-23
:END:

*** There's also the issue that actually-existing humans prefer living in predictable, high-trust social environments conducive to easy planning. Slytherins are making Unusual Life Choices in real life, no matter how much fiction and certain forms of political hagiography glorify them. Some level of egalitarianism is a necessary ingredient for that high-trust society: trust is almost always limited, and thus entails knowing that the power and incentive gradients between two people are not too steep.
:PROPERTIES:
:Score: 3
:DateUnix: 1432486026.0
:DateShort: 2015-May-24
:END:


*** I will argue against your position.

Suppose it either becomes possible to predict personality traits with some accuracy from a human's DNA, or becomes possible to determine them from a brain scan. In this scenario it could be /known/ at a high belief level that someone craves power (which I distinguish from ambition). At present, this doesn't fall under "congenitally damaged" but I do not believe such people should have power in society -- they should not have the right to hold elected office / work as managers. I base this belief on my priors which also include a belief that this is not an uncommon belief. Perhaps we would expand the definition of congenitally damaged, which would quickly lead to eugenics programs. They'd probably be successful quickly if we have genetics/neurology at that level. Alternatively we would have a rational society with at least one social caste.

#+begin_quote
  I think it's fairly clear from the last few thousand years that such social arrangements are highly irrational, because the ones on the bottom tend to get upset and kill the ones at the top every now and then.
#+end_quote

This is just evidence that historical societies are unstable, not that no such societies can ever exist. Given that historical societies have not been engineered (depending on how you feel about conspiracies, this will be more or less controversial) the fact that most such societies have had revolutions is not evidence that an engineered rational society would experience revolutions. The society I propose above will absolutely have an "underclass" and because that underclass is defined by it's cravings for power a revolution is inevitable. For this reason, any society which implements the ban on power-hungry leaders is also going to need some form of control of these power-hungry individuals, which can take one of a number of forms.

Of course the countermeasures may fail, but I consider the risk rational because the potential benefits are immense. A major source of corruption will have been removed.
:PROPERTIES:
:Score: 1
:DateUnix: 1432419642.0
:DateShort: 2015-May-24
:END:

**** In my opinion, in a rational society, positions of power would only be held by people who are competent to hold them.

From our experience in the last couple thousand years of recorded history, this typically means that most people who actually want positions of political power for the sole purpose of having power won't be capable of earning them.

For those few individuals who are both highly competent and power hungry, a rational government will have effective systems of checks and balances. Those systems will be administered by competent individuals.

When imagining a rational society, you have to consider that rationality does not only extend to thought experiments, it extends to practical application.

A meritocracy is eminently rational. Is everyone the same? No. Is everyone allowed to try to improve themselves? Yes.
:PROPERTIES:
:Author: Farmerbob1
:Score: 2
:DateUnix: 1432422320.0
:DateShort: 2015-May-24
:END:

***** Of course, as we see in the latter-day Western world, too much attempted meritocracy becomes a self-reinforcing class system as inequality increases.
:PROPERTIES:
:Score: 2
:DateUnix: 1432486210.0
:DateShort: 2015-May-24
:END:

****** I think that some degrees of social striation are unavoidable. The important thing is to make sure that there's a bottom that's not too terrible, and that nobody is prevented from climbing up the ladder based on their ability. That being said, hereditary wealth is definitely a potential problem in a rational meritocracy.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1432494052.0
:DateShort: 2015-May-24
:END:

******* u/deleted:
#+begin_quote
  nobody is prevented from climbing up the ladder based on their ability. That being said, hereditary wealth is definitely a potential problem in a rational meritocracy.
#+end_quote

Well right there you've already pointed out the problem. Today's "meritocrats" are engaged in a systematic program to pass on their status and wealth to their children, but this time with a sense of entitlement derived from "merit" and a simultaneous sense that they don't owe anything to their "worsers" (ie: the people they get rich exploiting).
:PROPERTIES:
:Score: 2
:DateUnix: 1432573381.0
:DateShort: 2015-May-25
:END:

******** That is a problem, yes. It is not 'the' problem. Before we can even begin to approach a rational meritocracy that isn't prone to uprisings due to caste enforcements, we have to have the capacity to do the following things with little to no human labor:

1) Grow food that people are willing to eat. 2) Educate people to an acceptable degree. 3) Provide law enforcement. 4) Provide power. 5) Maintain power, communications, transportation, shelter, education, law enforcement, government, and entertainment infrastructure. 6) Support the material needs to perform 1-5.

We also must: A) Establish a world government stable enough to prevent wars B) Establish population controls that aren't ghastly. C) Avoid a situation where the AI that we have developed to do 1-6 for us become sentient enough to want to be free of the labors we have assigned them.

Once we can do all these things (and we're nowhere even close to most of the critical ones) then we might seriously consider how to deal with the social aspects of hereditary wealth in a meritocracy.

I'm fairly certain that by the time we get to a point where we have the capacity to begin seriously considering a rational meritocracy, we would be close to a post-need society anyway.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1432597312.0
:DateShort: 2015-May-26
:END:
