#+TITLE: [DC][HSF] Terminator versus the AI (from the book Smarter Than Us released by MIRI)

* [[https://googledrive.com/host/0B13Ao1NtHiOhVjhzbTRaU0ZwSlE/1%20Terminator%20versus%20the%20AI.htm][[DC][HSF] Terminator versus the AI (from the book Smarter Than Us released by MIRI)]]
:PROPERTIES:
:Author: 1794
:Score: 18
:DateUnix: 1400449541.0
:DateShort: 2014-May-19
:END:

** You can buy the whole book, which is otherwise non-fiction, here:

[[http://intelligence.org/smarter-than-us/]]

If you buy it as a PayPal package, it's “pay-what-you-want” with a suggested price of $5.00 and a minimum price is $0.25. If you're interested in a concise case (about 50 pages) for Friendly AI research and MIRI's approach towards AGI, you should probably buy this book.

Okay, if anyone's wondering, I asked the author permission to share this story and he gave me that permission (and I promised to encourage people to buy this book... so go buy this book!).
:PROPERTIES:
:Author: 1794
:Score: 3
:DateUnix: 1400450142.0
:DateShort: 2014-May-19
:END:

*** u/deleted:
#+begin_quote
  If you're interested in a concise case (about 50 pages) for Friendly AI research and MIRI's approach towards AGI, you should probably buy this book.
#+end_quote

More refined question: does it actually present an approach to the Friendly AGI problem, or merely argue that UFAI will undoubtedly destroy us all?
:PROPERTIES:
:Score: 6
:DateUnix: 1400451625.0
:DateShort: 2014-May-19
:END:

**** It's made for popular audiences so definitely the latter. If you've navigated in the LW/MIRI memespace at all, you will undoubtedly find almost nothing new in this book, just a very concise summary of most of the relevant arguments.
:PROPERTIES:
:Author: 1794
:Score: 6
:DateUnix: 1400452025.0
:DateShort: 2014-May-19
:END:

***** Well, screw paying money for that, then.
:PROPERTIES:
:Score: 1
:DateUnix: 1400480592.0
:DateShort: 2014-May-19
:END:


*** The original link doesn't work anymore, here's a new one:

[[https://googledrive.com/host/0B13Ao1NtHiOhVjhzbTRaU0ZwSlE/Terminator%20versus%20the%20AI.htm]]
:PROPERTIES:
:Author: 7149
:Score: 1
:DateUnix: 1401815513.0
:DateShort: 2014-Jun-03
:END:


** Of course, this makes no sense within the established Terminator rules for time travel, since if the AI is already super-intelligent when the Terminator goes after it then it should have already won, thus never spawning a timeline where the Terminator gets sent back. In the movies and most of the existing canon, the Terminators get sent back as a last-ditch effort while Skynet is in the middle of losing the war.
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1400477071.0
:DateShort: 2014-May-19
:END:

*** u/deleted:
#+begin_quote
  In the movies and most of the existing canon, the Terminators get sent back as a last-ditch effort while Skynet is in the middle of losing the war.
#+end_quote

And in real life, malicious and militarized superintelligences don't lose wars against humans who lack an industrial infrastructure.
:PROPERTIES:
:Score: 5
:DateUnix: 1400480640.0
:DateShort: 2014-May-19
:END:

**** I think the usual fan-wonk explanation I hear about that is that Skynet has to work around restrictions of its utility function. Ultimately, Skynet has to preserve human life. (meaning; enough of a population, with a great enough concentration, with enough natural resources and so on for long-term stability)

Skynet can /heavily reduce/ the human population, and try to subjugate it as much as possible, but can't outright wipe it out. Presumably, it also can't do a Matrix type lotus eater thing, or chemically lobotomize/enslave them, or whatever.

So essentially, the machines have a huge handicap in that they can never completely win, and the humans have a helluva lot of ways they can subtly drain the machine's resources and fight back over time.
:PROPERTIES:
:Author: drageuth2
:Score: 3
:DateUnix: 1400496543.0
:DateShort: 2014-May-19
:END:

***** u/deleted:
#+begin_quote
  Ultimately, Skynet has to preserve human life. (meaning; enough of a population, with a great enough concentration, with enough natural resources and so on for long-term stability)

  Skynet can heavily reduce the human population, and try to subjugate it as much as possible, but can't outright wipe it out. Presumably, it also can't do a Matrix type lotus eater thing, or chemically lobotomize/enslave them, or whatever.
#+end_quote

This seems like a bizarrely well-designed UFAI.
:PROPERTIES:
:Score: 2
:DateUnix: 1400496911.0
:DateShort: 2014-May-19
:END:

****** I see it more as the asshole-type genie.

"Ohhh, you want me to preserve the human race, eh? Well, you only need about 5000 people for /that./ Everyone else will just eat up the natural resources all that much faster, anyway."

Plus I think it actually /is/ canon in the actual Terminator verse that Skynet's prime directive is self-preservation. So if preserving humanity is only a secondary or tertiary objective, Skynet would prioritize making that threat as small as possible, without actually eliminating it.
:PROPERTIES:
:Author: drageuth2
:Score: 2
:DateUnix: 1400497147.0
:DateShort: 2014-May-19
:END:

******* Clear, but what I mean by "bizarrely well-designed" is that someone seems to have actively implanted enough about human values into Skynet, /apparently/, that it /doesn't/ enslave people, doesn't chemically lobotomize them, doesn't do a Matrix-thing, doesn't take all the /obvious solutions that preserve human life while getting it out of the way/.
:PROPERTIES:
:Score: 3
:DateUnix: 1400498823.0
:DateShort: 2014-May-19
:END:

******** The other explanation that I've heard (that I mostly buy) is that Skynet really isn't all that smart. It starts out with a crippling nuclear salvo and that's basically the only reason that it has any shot of winning. And in the Terminator canon, Skynet /loses/.
:PROPERTIES:
:Author: alexanderwales
:Score: 8
:DateUnix: 1400510586.0
:DateShort: 2014-May-19
:END:


******** /shrug/ I guess the long and the short of that is, we wouldn't have movies about time-travelling deadpan austrian biker robots if it were otherwise.
:PROPERTIES:
:Author: drageuth2
:Score: 2
:DateUnix: 1400499001.0
:DateShort: 2014-May-19
:END:


*** This exchange makes it seem that part of the AI's utility function is basically "be a brat." So it could be it won humans single-handedly and then sent a terminator back in time just for the hell of it, maybe because its makers coded some pop-culture trivia into it?
:PROPERTIES:
:Author: 1794
:Score: 1
:DateUnix: 1400511255.0
:DateShort: 2014-May-19
:END:


** /applause/ -- though I do think "hacked your brain with the green lights" was going a bit far.
:PROPERTIES:
:Score: 1
:DateUnix: 1400451597.0
:DateShort: 2014-May-19
:END:
