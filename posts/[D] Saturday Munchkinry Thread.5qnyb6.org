#+TITLE: [D] Saturday Munchkinry Thread

* [D] Saturday Munchkinry Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 16
:DateUnix: 1485615856.0
:DateShort: 2017-Jan-28
:END:
Welcome to the Saturday Munchkinry and Problem Solving Thread! This thread is designed to be a place for us to abuse fictional powers and to solve fictional puzzles. Feel free to bounce ideas off each other and to let out your inner evil mastermind!

Guidelines:

- Ideally any power to be munchkined should have /consistent/ and /clearly defined/ rules. It may be original or may be from an already realised story.
- The power to be munchkined can not be something "broken" like omniscience or absolute control over every living human.
- Reverse Munchkin scenarios: we find ways to beat someone or something /powerful/.
- We solve problems posed by other users. Use all your intelligence and creativity, and expect other users to do the same.

Note: All top level comments must be problems to solve and/or powers to munchkin/reverse munchkin.

Good Luck and Have Fun!


** I'm looking to expand the "age of failed dreams" rules for Programmer at Large to get a sense of the boundaries of technology and what I should be looking out for. What I'm looking for are rules that essentially guarantee that technology in the long-run is a plateau.

Here are the rules I have so far (in no particular order, editing to add new ones as they get suggested or I remember/think of them - I've failed to write these down so far):

- FTL is outright physically impossible
- Ditto most things that would require physics that is currently "exotic" - e.g. no antigravity, no stasis fields
- There are relatively fundamental scaling constraints on intelligence - you can probably get something twice as intelligent according to some reasonable metric as a peak baseline human, but you can't get anything 10x as intelligent.
- No "sci-fi level" nanotech. there's plenty of molecular manufacturing, etc. but self-repairing machines, nanofog construction are all somewher between hard and impossible, with the more exciting the application sounds the closer it is to impossible.
- Intelligence is fragile and tends not to copy well - if you try to copy a brain you'll end up with a brain at most roughly similar to the original, and it might well just not work
- Intelligence is chaotic - it's very hard to produce an intelligence to order and tends to be very sensitive to initial conditions
- AI is possible within the above constraints but tends to be quite expensive to run (the human brain may not be the smartest, but as far as intelligence : resource efficiency ratios go it's doing rather well). The best AI are not substantially smaller than a human brain, are tied to their hardware, and are the same order of magnitude speed and intelligence as a smart human (though they may be off the high end of that spectrum many aren't). AI should be approximated as "like very smart humans that use more resources and have faster interfaces to non-sentient computers".
- Brains are hard to interface with in a reliable way. /Some/ direct nerve stimulation is possible, but if you want to fake senses you're more or less required to go via the organs that are designed for that - e.g. implanted screens in contact lenses are viable, but just plugging into the optic nerve isn't.
- Some as of yet unspecified sociological mechanism (some combination of factors including resource efficiency and level of infrastructure required for maintenance) means that pure AI civilizations tend to be less stable than human civilizations.
- No non-sentient technological civilization is possible long-term. They tend to run into outside context problems too quickly because so many problems count as such for them
- Sentient civilization tends to collapse when it grows too large in a confined space like a solar system - resource contention and coordination problems rise pretty sharply with the population.
- Bodies are hard to repair and eventually break down no matter what you do. Life extension is possible, but it tends to hit some pretty hard limits after 200-300 years no matter how good you are at it.

(Note: I make no claim that these are necessarily /realistic/ constraints)

Now, you have 10,000 years to play with. How do you push the boundary of what's possible? Can you effectively bootstrap your civilization to godhood?
:PROPERTIES:
:Author: DRMacIver
:Score: 7
:DateUnix: 1485638275.0
:DateShort: 2017-Jan-29
:END:

*** [deleted]
:PROPERTIES:
:Score: 3
:DateUnix: 1485639829.0
:DateShort: 2017-Jan-29
:END:

**** I haven't read the whole list, but I think assume that if it says hypothetical then probably not and if it doesn't then probably yes.

In general anything we currently have can be extrapolated to its logical conclusion, along with anything that is more or less essential for interstellar travel to be viable, but most other things are ruled out.
:PROPERTIES:
:Author: DRMacIver
:Score: 3
:DateUnix: 1485639965.0
:DateShort: 2017-Jan-29
:END:

***** Then wouldn't FTL be possible due to Alcubierre drives?
:PROPERTIES:
:Author: Nuero3187
:Score: 1
:DateUnix: 1485744333.0
:DateShort: 2017-Jan-30
:END:

****** Why? We've definitely not yet proven that Alcubierre drives are possible. I'm not sure we've proven that they're possible even if we /could/ get our hands on exotic matter.
:PROPERTIES:
:Author: DRMacIver
:Score: 1
:DateUnix: 1485764259.0
:DateShort: 2017-Jan-30
:END:


*** Find the joker messing with the universe simulation's settings and have a frank exchange of views.

To unpack a bit: I would consider unaugmented humans being within an order of magnitude of as smart as it gets extremely strong evidence for some variation on the simulation hypothesis.
:PROPERTIES:
:Author: buckykat
:Score: 2
:DateUnix: 1485642893.0
:DateShort: 2017-Jan-29
:END:

**** u/DRMacIver:
#+begin_quote
  To unpack a bit: I would consider unaugmented humans being within an order of magnitude of as smart as it gets extremely strong evidence for some variation on the simulation hypothesis.
#+end_quote

Literally true, but it's relatively hard to hack the author's wetware.

Some of these rules might be the result of less what is physically possible and more what is reachable when starting from a human baseline.

Or, alternatively, if you're not smart enough to figure out how to work around them you're probably not smart enough to figure out reality's privilege escalation exploits either. ;-)
:PROPERTIES:
:Author: DRMacIver
:Score: 4
:DateUnix: 1485643358.0
:DateShort: 2017-Jan-29
:END:

***** Yeah, the only reason for AI not to work is to see what the meatbags do on their own. Which is why coherent multi-person solipsism is the specific simulation hypothesis most suited to the data. Which means the Author is the enemy.

So, I dunno, try to make the world as boring as possible until and unless you let me foom.
:PROPERTIES:
:Author: buckykat
:Score: 2
:DateUnix: 1485644441.0
:DateShort: 2017-Jan-29
:END:

****** Unfortunately, I am cognitively incapable of letting you FOOM, since then you would be smarter than I am and therefore be impossible for me to write. So I think I'll just trash you instead and come up with a character more amenable to my story's needs.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 6
:DateUnix: 1485650494.0
:DateShort: 2017-Jan-29
:END:


*** The thing is, with more computing power comes more resource gathering. Basically, Dyson Sphere your home star up, turn it into a [[https://en.wikipedia.org/wiki/Matrioshka_brain][Matrioshka brain]] and begin operation Von Neumann, or researching better power tech, or whatever else.
:PROPERTIES:
:Author: NotACauldronAgent
:Score: 1
:DateUnix: 1485638966.0
:DateShort: 2017-Jan-29
:END:

**** u/DRMacIver:
#+begin_quote
  Matrioshka brain
#+end_quote

These don't help very much in this setting because of the intelligence scaling problem. A very big computer isn't a superintelligence, it's at best a collection of many geniuses, and those geniuses aren't especially resource efficient due to the lack of self-repairing machines.

Large-scale civilizations without superintelligent oversight tend to become more and more unstable as they start to struggle under the weight of their own coordination problems.

#+begin_quote
  operation Von Neumann
#+end_quote

Similar problems. It's not that there aren't Von Neumann probes, it's just that they're normally called "colony ships".
:PROPERTIES:
:Author: DRMacIver
:Score: 2
:DateUnix: 1485639705.0
:DateShort: 2017-Jan-29
:END:

***** 1) Sure, but a collection of super smart superfast-thinking geniuses with information instantaneously at their fingertips are still able to optimize almost everything.

2) So put a timer on a fusion reactor and Dyson sphere probe. Using Dyson sphere segment/satellite to start a new one, with an onboard VI that can awaken an AI. It is a colony ship, but humans, no matter the ratio of int, as they require food, water, and air, whereas the AI requires a solar panel or a nuclear reactor.

Basically, humans are really hard to be the optimal processors, because humans require more and think slower than AIs
:PROPERTIES:
:Author: NotACauldronAgent
:Score: 1
:DateUnix: 1485640826.0
:DateShort: 2017-Jan-29
:END:

****** The AI advantage is not nearly as great as you're positing in this setting, and if it were then I would come up with rules to nerf it. You're welcome to suggest additional rules.

The current rules mean that an AI is a physical object that is not substantially smaller than a human brain and requires a significantly greater industrial base to maintain - an AI doesn't require "a solar panel and a nuclear reactor", it requires machinery, parts and expertise to repair it when it goes wrong.

Additionally it may be supersmart, but it's not /especially/ fast at anything that resembles general intelligence. Nor is it particularly easy to create new ones.

Essentially, AI can be approximated as humans with slightly different constraints and slightly different capabilities.
:PROPERTIES:
:Author: DRMacIver
:Score: 3
:DateUnix: 1485642232.0
:DateShort: 2017-Jan-29
:END:

******* The expertise for maintaining an AI may be complicated, but it's not too complicated, and attaching a future SSD solves that.

Basically, even if human level int is the optimal level, it's still more optimal to build a lot of human-level AIs. There is little a human can do better that a human-level AI can't if the AI has access to an interaction bot, the internet, and processing power.

Basically, the problem is still processing speed. Human brains work at chemical impulse speed, [[https://en.wikipedia.org/wiki/Nerve_conduction_velocity][120 m/sec]], computers work at electron speed, [[https://en.wikipedia.org/wiki/Velocity_factor][~275,000,000 m/sec]], basically lightspeed. Humans learn at reading speed at best, AIs learn at bandwidth speed. The problem with nerfing these is that they are fairly fundamental, these numbers are underlying physics. The options would be get rid of AIs altogether and max out at VIs, or encourage use of cyborgs.

*Edit-it has been pointed out to me that my factors given are oversimplifications, thought doesn't work quite like that, however, the speed is still important
:PROPERTIES:
:Author: NotACauldronAgent
:Score: 2
:DateUnix: 1485642911.0
:DateShort: 2017-Jan-29
:END:

******** u/DRMacIver:
#+begin_quote
  Human brains work at chemical impulse speed, 120 m/sec, computers work at electron speed
#+end_quote

Constant factors only get you so far really.

#+begin_quote
  Humans learn at reading speed at best, AIs learn at bandwidth speed
#+end_quote

This is basically not true for any useful notion of learning, and falls afoul of the rules set out for intelligence at the beginning. An AI can't learn by just dumping intelligence into its brain, because intelligence is hard. It can certainly /read/ a hell of a lot faster than a human, but that doesn't mean its ability to encode that into useful knowledge is faster by the same degree.

#+begin_quote
  The options would be get rid of AIs altogeather
#+end_quote

I'm thinking of it. AIs are already rare in setting because they tend to accelerate the cycle of collapse and the trade fleet (who are the main propagators of technical knowledge) are extremely wary of them and don't pass on any knowledge about how they work. It might be easier to just drop that and avoid them altogether, but this seems significantly less plausible to me than they just don't work that well.

#+begin_quote
  and max out at VIs, or encourage use of cyborgs.
#+end_quote

Cyborgs are definitely not available in setting. No neural interfacing.
:PROPERTIES:
:Author: DRMacIver
:Score: 2
:DateUnix: 1485643735.0
:DateShort: 2017-Jan-29
:END:

********* For Learning, I'm not disagreeing, but if one AI learns how to do something, that code is more easily shared than human to human. Programs for any "simple" task work the same for every AI, do no need to teach the entire class about stellar interference patterns (etc) and how to detect them, the analysis program is already available.

Similarly not disagreeing that the speeds effects are less than magnitudal, but it still results in outperformance.

For "mundane" cyborgs, fast HUD and refined versions of replacement organs that exist today are still improvements, don't count them out entirely.

As of how to get rid of AI, galactic doomsday pact to not mess with them? The trade fleet (or someone else) actively sabotaging efforts? I don't know, but generally "The Conspiracy" can be an option.
:PROPERTIES:
:Author: NotACauldronAgent
:Score: 1
:DateUnix: 1485651251.0
:DateShort: 2017-Jan-29
:END:

********** u/DRMacIver:
#+begin_quote
  that code is more easily shared than human to human
#+end_quote

Nope. Any sufficiently sentient intelligence in this setting is a black box which you can't easily cut and paste bits between.

#+begin_quote
  Programs for any "simple" task work the same for every AI, do no need to teach the entire class about stellar interference patterns (etc) and how to detect them, the analysis program is already available.
#+end_quote

This is true, but largely only to the degree that humans can also benefit from the same - if you can automate it with a simple program then a human can just use that program.

#+begin_quote
  For "mundane" cyborgs, fast HUD and refined versions of replacement organs that exist today are still improvements, don't count them out entirely.
#+end_quote

I'm not only not counting them out they're a major part of the story. :-)

#+begin_quote
  The trade fleet (or someone else) actively sabotaging efforts?
#+end_quote

I mean, in a sense this is already part of the plot. The trade fleet are /the/ major organ of continuity of civilization - planetary civilizations tend to collapse, restart at some lower level, then at some point the trade fleet comes along and helps give them a leg up.

I've updated my notes on AI (which I forgot I had). In particular the section [[https://github.com/DRMacIver/programmer-at-large-notes/blob/master/ai.md#why-havent-ai-taken-over-the-galaxy][Why haven't AI taken over the galaxy?]] is new. Thanks for the help refining my thoughts on this.
:PROPERTIES:
:Author: DRMacIver
:Score: 2
:DateUnix: 1485677400.0
:DateShort: 2017-Jan-29
:END:

*********** One possible AI nerf is that AIs might turn out to be particularly susceptible to a type of wireheading. (In short, why deal with this complicated and somewhat difficult 'real world' when you can create and simulate a much better, more comfortable world to deal with?)

From an external point of view, this means that a large subset of AI programs, for no easily discernible reason, suddenly stop responding to queries and start using a whole lot more processing power. Leaving them running and waiting for them to be done doesn't help; trying to force them to respond in various ways is either ignored, or results in nothing more than a rude message, or gets you a very angry and uncooperative AI who just wants you to /shut up/ so it can get back to its simulated world (and will probably kill you just to quiet you down, with the same lack of concern as you'd have killing a video game villain). This is pretty much useless for any purpose, so such AIs are often shut down while the programmer goes back over his notes and tries to figure out where he went wrong...
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1485683658.0
:DateShort: 2017-Jan-29
:END:

************ I like this and may end up using it, thanks.

One note...

#+begin_quote
  while the programmer goes back over his notes and tries to figure out where he went wrong...
#+end_quote

AIs are generally grown more than directly programmed. There's a lot of directly programmed software that goes into it, but the starting template is not fully sentient in its own right and has to grow into it. Attempts to push the starting point closer to sentience tend to result in the AI destabilising.

(Hand waving justification: The required architecture for general intelligence is highly parallel and ends up being very sensitive to small details in timing. Trying to load an intelligence that has been grown or designed for one set of timings into a new one tends to result in the AI equivalent of a stroke)
:PROPERTIES:
:Author: DRMacIver
:Score: 1
:DateUnix: 1485684023.0
:DateShort: 2017-Jan-29
:END:


*********** Of course! Plotting stuff out like this can really be fun. Good luck and have fun!
:PROPERTIES:
:Author: NotACauldronAgent
:Score: 1
:DateUnix: 1485696576.0
:DateShort: 2017-Jan-29
:END:


*** Would a non-sentient civilisation maintaining one sentient AI to deal with outside context problems be possible?

Presumably the trade fleet, since it trades in programs, only travels between stars when too many round trip times between expert and client are needed to make radio communication feasible.
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1485651748.0
:DateShort: 2017-Jan-29
:END:

**** u/DRMacIver:
#+begin_quote
  Would a non-sentient civilisation maintaining one sentient AI to deal with outside context problems be possible?
#+end_quote

I think there are limitations that look roughly like the following:

- You need some relatively low capability : sentience ratio. To a purely automated factory a broken conveyor belt may be an outside context problem. ("relatively low" here is obviously still much much higher than 21st century earth)
- You need a relatively large industrial civilization to be able to maintain AI long-term.

So you could last quite some time this way but it would eventually start to break down because all of the bits you couldn't maintain yourself.

#+begin_quote
  Presumably the trade fleet, since it trades in programs, only travels between stars when too many round trip times between expert and client are needed to make radio communication feasible.
#+end_quote

The trade fleet are more or less constantly travelling. They're more like a nomadic culture who support themselves with trade than a merchant culture that trades out of a home port.

What they really trade in is /expertise/ - the trade fleet are more or less uplift merchants. It just happens software is a big part of that.

They also sell cultural artifacts - books, TV, etc. for which the greater bandwidth of a star ship is actually quite useful.
:PROPERTIES:
:Author: DRMacIver
:Score: 1
:DateUnix: 1485675748.0
:DateShort: 2017-Jan-29
:END:

***** So the postulated civilisation would eventually break down due to maintenance troubles... but the trade fleet is more than capable of the required maintenance, and could sell their maintenance services in exchange for... hmmm... possibly software development expertise? Or basic supplies in a useful location?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1485683904.0
:DateShort: 2017-Jan-29
:END:

****** Whatever they get in return must be more valuable to them than the colony ships you'd get spewn in all directions by spending the 300 years in an asteroid belt instead, replicating until you run into the Great Filter. (Compare to the gliders you get from a chaotic Conway's Game of Life pattern before it collapses.)
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1485686318.0
:DateShort: 2017-Jan-29
:END:


****** u/DRMacIver:
#+begin_quote
  So the postulated civilisation would eventually break down due to maintenance troubles... but the trade fleet is more than capable of the required maintenance
#+end_quote

It's an interesting thought, but I don't think it works if the bottleneck is essentially ratio of problems : people.

They could help, but the trade fleet are very reliant on finding or creating local expertise. They're experts mostly in running starships and bootstrapping civilizations - they can't fill in every speciality themselves and are very dependent on regular stops in high tech star systems. They could come in and offer training courses to the local AI, but their ability to actually fix things is not at the level required to properly sustain a civilization.

Also the trade fleet don't like AI so probably wouldn't be up for the deal.
:PROPERTIES:
:Author: DRMacIver
:Score: 1
:DateUnix: 1485686667.0
:DateShort: 2017-Jan-29
:END:


*** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1485641156.0
:DateShort: 2017-Jan-29
:END:

**** Yeah, good point. No real neural interfacing is part of the source material, and I'd more or less decided to go with that, but I'd forgot to put it on the list, thanks.
:PROPERTIES:
:Author: DRMacIver
:Score: 2
:DateUnix: 1485641708.0
:DateShort: 2017-Jan-29
:END:


** Suppose you have one wish, which must be written in a single English sentence using only words that a typical college freshman would understand. The wish MUST destroy all bedbugs or it won't come true (the genie is very fickle).

What is the most you can gain from the wish?
:PROPERTIES:
:Author: failed_novelty
:Score: 5
:DateUnix: 1485662790.0
:DateShort: 2017-Jan-29
:END:

*** How about "exterminate all species that the majority of humanity would agree to want to exterminate if asked after receiving information on how much that species helps and harms humanity". That would get rid of bed bugs, mosquitoes, malaria, west nile, dengue, yellow fever, zika, human-infecting parasites in general, species that infect our domesticated animals and crops, weeds, pathogenic bacteria and viruses in general, and unwanted species in general.
:PROPERTIES:
:Author: luminarium
:Score: 8
:DateUnix: 1485664743.0
:DateShort: 2017-Jan-29
:END:

**** You are going to make a /mess/ of several ecosystems.
:PROPERTIES:
:Author: CCC_037
:Score: 4
:DateUnix: 1485683964.0
:DateShort: 2017-Jan-29
:END:

***** I suppose that (some of the) the negative effects would be explained via the "how much that species helps and harms humanity" clause.
:PROPERTIES:
:Author: Menolith
:Score: 1
:DateUnix: 1485702026.0
:DateShort: 2017-Jan-29
:END:


***** Meh, nature will adapt. Or on other words: "worth it!"
:PROPERTIES:
:Author: luminarium
:Score: 1
:DateUnix: 1485822627.0
:DateShort: 2017-Jan-31
:END:

****** Before it adapts, you might have several years of famine to deal with.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1485831537.0
:DateShort: 2017-Jan-31
:END:

******* I also specified in the wish that vote assessment mechanism takes knowledge of the consequences into account. So ie. people wouldn't get rid of certain species, like bees, that were actually useful. So species whose loss would cause famine wouldn't be in the wish's scope.
:PROPERTIES:
:Author: luminarium
:Score: 1
:DateUnix: 1485910392.0
:DateShort: 2017-Feb-01
:END:


**** Might that destroy humanity? The genie could be a jerk...
:PROPERTIES:
:Author: failed_novelty
:Score: 1
:DateUnix: 1485669825.0
:DateShort: 2017-Jan-29
:END:

***** I doubt the majority of humanity would want to exterminate humanity.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 3
:DateUnix: 1485674241.0
:DateShort: 2017-Jan-29
:END:

****** The majority of humans want to ban water, if you state it the scare science way
:PROPERTIES:
:Author: monkyyy0
:Score: 3
:DateUnix: 1485723137.0
:DateShort: 2017-Jan-30
:END:

******* I'm assuming you're referring to the classic "Ban Dihydrogen Monoxide" thing.

The wish as stated required the advantages to be detailed as well as the disadvantages. Given how vital it is, it would be very hard to downplay the advantages of water without intentionally omitting or falsifying the information, though it may be less clear cut for some species.

If the genie is able to violate the wish to make people think water (or humanity) is a net negative, then literally no possible wish is safe. The correct answer to "How do you phrase your wish?" would be "By launching the genie's lamp into an escape trajectory from the solar system."

That said, I probably would have phrased the wish such that it specified informing people what the likely consequences of eliminating each species would be (as some that may be harmful to humanity may play an important role in their ecosystems, indirectly being very useful to us), projected out to 5, 50, and 500 years (or even longer, but it might become hard for people to relate to the world >500 years in the future).
:PROPERTIES:
:Author: ZeroNihilist
:Score: 3
:DateUnix: 1485757030.0
:DateShort: 2017-Jan-30
:END:

******** u/CCC_037:
#+begin_quote
  That said, I probably would have phrased the wish such that it specified informing people what the likely consequences of eliminating each species would be (as some that may be harmful to humanity may play an important role in their ecosystems, indirectly being very useful to us), projected out to 5, 50, and 500 years (or even longer, but it might become hard for people to relate to the world >500 years in the future).
#+end_quote

This is an improvement, but it still leaves open the possibility that destroying harmful species A is fairly harmless on its own, destroying harmful species B is fairly harmless on its own, but destroying both A and B together leads to some sort of ecological disaster. (To avoid this, I'd suggest deciding on each species after eliminating or retaining the previous species; so first species A is eliminated and then the consequences for eliminating species B are considered and the ecological disaster therefore predicted).
:PROPERTIES:
:Author: CCC_037
:Score: 4
:DateUnix: 1485764590.0
:DateShort: 2017-Jan-30
:END:

********* That's a good point. With that alteration it functions as a hill-climbing algorithm.

Even then not every desirable outcome will be reachable. Ones where destroying A or B individually is harmful but A and B is beneficial would never be asked (though I don't know whether they would even exist), likewise if the order mattered (e.g. D(B) > D(A) >> D(A+B)) it's possible we could end up with a suboptimal outcome.

However, your amendment guarantees that the outcome is always better than what it was before any changes were made, which is crucial.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1485773168.0
:DateShort: 2017-Jan-30
:END:

********** u/CCC_037:
#+begin_quote
  Ones where destroying A or B individually is harmful but A and B is beneficial would never be asked (though I don't know whether they would even exist)
#+end_quote

...I don't know if such situations exist, but I can imagine a narrative for one.

Imagine species A, B and C where A and B are harmful, C is beneficial mainly because it eats A and B, but C requires nutrients from both A and B to be healthy.

Removing harmful species A results in C suffering some sort of nutritional deficiency and dying out. This results in population explosion of B, and ecological disaster.

Similarly, removing B results in C dying out and thus population explosion of A.

Removing C alone is right out (population explosion of A /and/ B). Yet removing all three means that neither A nor B experiences that dangerous population explosion.

And yes, this method is going to give you a local optimum, not a global optimum; but I'm happy with a slight improvement and a strong prevention of disaster, myself.
:PROPERTIES:
:Author: CCC_037
:Score: 4
:DateUnix: 1485794976.0
:DateShort: 2017-Jan-30
:END:


****** What if the genie described humanity without using the word 'humans'. We have exterminated many species, destroyed ecosystems, etc. Easy to paint us in a bad light.
:PROPERTIES:
:Author: failed_novelty
:Score: 1
:DateUnix: 1485696039.0
:DateShort: 2017-Jan-29
:END:

******* The wish specified "after receiving information on how much that species helps and harms humanity".

Humans would probably not want to exterminate the species that had developed the vast majority of good things that had happened to humanity, even if there are a lot of negative things as well.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1485756105.0
:DateShort: 2017-Jan-30
:END:


***** Humanity gets to specify what species to exterminate, by majority vote on each species, there's no way the majority of humanity would want to wipe out humanity.
:PROPERTIES:
:Author: luminarium
:Score: 1
:DateUnix: 1485822706.0
:DateShort: 2017-Jan-31
:END:


*** I have so many questions...

Most important question: To what degree can I interrogate the genie about its capabilities?

Assuming that is large...

What are the rules on use of connectives? Is there any reason I can't just say "I wish for you to destroy all bedbugs and (whatever other wish maximizes my gain)?". What if the two are logically connected? (Destroy all bedbugs and grant me one extra wish per bedbug you destroy).

What are the limitations on the genie's predictive capabilities? Can the genie simulate a copy of me? Can I wish to kill all creatures in the solar system that after receiving answers to any set of questions about them I wanted to ask I would choose to end the life of?

What is the timescale on which the wish operates? If I wished for omnipotence but precommitted to destroying all bedbugs as soon as I acquired it, would that satisfy the conditions of the wish? What if I baked that precommitment into the wish?
:PROPERTIES:
:Author: DRMacIver
:Score: 7
:DateUnix: 1485678501.0
:DateShort: 2017-Jan-29
:END:

**** Wouldn't "whatever other wish maximizes my gain "already kill all bedbugs since the best wish has to include killing all bedbugs or else it wouldn't do anything and therefore it wouldn't be the best wish?
:PROPERTIES:
:Author: crivtox
:Score: 4
:DateUnix: 1485732307.0
:DateShort: 2017-Jan-30
:END:

***** It was intended as a placeholder for "I have explored the limits of the wish granting system and figured out the optimal strategy for it ignoring the bedbug constraint" rather than the literal thing you should ask for.

(Do not write genies blank cheques asking them to optimise for your coherent extrapolated volition unless you're really sure about both your CEV and the genie's trustworthiness)
:PROPERTIES:
:Author: DRMacIver
:Score: 2
:DateUnix: 1485764362.0
:DateShort: 2017-Jan-30
:END:


**** Assume you can't interrogate at all, because the genie is a jerk and you are on a tight time frame.
:PROPERTIES:
:Author: failed_novelty
:Score: 1
:DateUnix: 1485789152.0
:DateShort: 2017-Jan-30
:END:


*** "I wish what I should wish acording to my values".That should get me something similar to cev and would incluye destroying all bedbugs because otherwise it wouldn't be the thing I should wish for according to my current values.alternatively "Grant me my coherent extrapolated volition "maybe would work , a typical college freshman understands all the words , maybe not the meaning of the frame but the genie didn't want frases that a typical college freshman understands he wanted frases made by words that a typical college freshman would understand.
:PROPERTIES:
:Author: crivtox
:Score: 3
:DateUnix: 1485732069.0
:DateShort: 2017-Jan-30
:END:

**** DjinOS warning 112358: Recursive wish detected. Input wish "I wish what I should wish according to my values" returned processed wish "I wish what I should wish according to my values."

DjinOS warning 43: Wish has already been fulfilled at time of wishing. Process terminated with status 0.

Note: bedbugCheck has not been run.
:PROPERTIES:
:Author: awesomeideas
:Score: 3
:DateUnix: 1485796557.0
:DateShort: 2017-Jan-30
:END:


*** "I wish for the effects written down on this piece of paper." (Where the piece of paper includes a list of effects including both destroying bedbugs and making you rich).

Alternately, "I wish for the following two things to come true: the destruction of all bedbugs, and X" (where X is basically a standard wish for good stuff for yourself). You should word the wish to specify "the following two things" so the genie can't decide that the sentence ends after the part about the bedbugs.

Note that it is very difficult to just change the scenario to "you can only make a wish that doesn't ask for two separate things," since "things" isn't a concept that divides reality at the seams.
:PROPERTIES:
:Author: Jiro_T
:Score: 2
:DateUnix: 1485976831.0
:DateShort: 2017-Feb-01
:END:


** Based on Heroes Save the World, one of my current favorite ratfics:

You have the ability to make coins disappear by touching them. You're not sure where they go, but they're irretrievable. Doing so does not release a coin's worth of energy (at least anywhere you know of). It also doesn't burn any more calories than touching any other object.

There's no canonical basis for this, but let's assume that you can vanish a coin by touching it with any exposed skin, not just your fingertips.

What do you do?

EDIT: You have to consciously will the coin to vanish while you touch it.
:PROPERTIES:
:Author: LazarusRises
:Score: 4
:DateUnix: 1485804676.0
:DateShort: 2017-Jan-30
:END:

*** Nuclear waste coins!
:PROPERTIES:
:Author: Adeen_Dragon
:Score: 3
:DateUnix: 1485808223.0
:DateShort: 2017-Jan-31
:END:

**** Only until the radiation poisoning knocks you out...
:PROPERTIES:
:Author: LazarusRises
:Score: 3
:DateUnix: 1485809016.0
:DateShort: 2017-Jan-31
:END:

***** Have the coins brought in contact with only a long-grown fingernail through use of a shielded tube if fingernails count, or use lead-lined coins.
:PROPERTIES:
:Author: Gurkenglas
:Score: 4
:DateUnix: 1485839960.0
:DateShort: 2017-Jan-31
:END:


***** One enormous coin.
:PROPERTIES:
:Author: Kilbourne
:Score: 3
:DateUnix: 1485825133.0
:DateShort: 2017-Jan-31
:END:

****** And at that point you might as well do garbage disposal coins on the side, depending on what counts as a coin.
:PROPERTIES:
:Author: Gurkenglas
:Score: 3
:DateUnix: 1485840161.0
:DateShort: 2017-Jan-31
:END:

******* Well, yes.
:PROPERTIES:
:Author: Kilbourne
:Score: 2
:DateUnix: 1485840904.0
:DateShort: 2017-Jan-31
:END:
