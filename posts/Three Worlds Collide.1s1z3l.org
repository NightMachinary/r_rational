#+TITLE: Three Worlds Collide

* [[http://lesswrong.com/lw/y5/the_babyeating_aliens_18/][Three Worlds Collide]]
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 40
:DateUnix: 1386131823.0
:END:

** Okay, most of us have come here from [[/r/HPMOR]] as far as I know, but I was surprised not to see this [[/u/EliezerYudkowsky]] short up here.

*Assumes devil's advocate pose*

Postulate: The true ending was the irrational one, the rational ending was the first one where the Maximum Fun-Fun Ultra Super Happy People win.

Argument: Human quality of life would be higher, death would be reduced/eliminated, and it's effects mitigated by better communication of partial mind-states across generations, by <untranslatable#>.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 5
:DateUnix: 1386132447.0
:END:

*** Counterargument: value is complex. Human beings value many of the adaptations we've built up /against/ pain and sadness even if we don't particularly value the pain and sadness themselves. The true ending is rational, because we don't throw away our souls to turn bio-orgasmium.
:PROPERTIES:
:Score: 4
:DateUnix: 1386149351.0
:END:

**** Devil's Advocate: Yudkowsky himself said that he doesn't know which one would be the best scenario - he actually said he has fun watching people argue about which one he was really defending when he was defending neither.

My position: Sure value is complex, but are you sure you're not rationalising pain and sadness just like you used to rationalise death? And do please think for five minutes before answering. Have you actually examined your internal beliefs, weighed your reasons and found your true rejection? I used to think the true ending was the better one but I'm not so sure anymore. Mainly after reading [[http://alicorn.elcenia.com/stories/damagereport.shtml][this short story by Alicorn]].

In fact, I believe people romanticise pain and sadness as much as they romanticise death, and while sadness couldn't be eliminated (whatever lowest level of possible happiness there was would become the new highest level of possible sadness) (not sure the logic follows) pain should be.
:PROPERTIES:
:Score: 6
:DateUnix: 1386160968.0
:END:

***** #+begin_quote
  Sure value is complex, but are you sure you're not rationalising pain and sadness just like you used to rationalise death?
#+end_quote

I never rationalized death as being /a good thing overall/ (growing on the /Young Wizards/ series does that), but I was not a transhumanist. I'm still not, really: I don't believe in getting my hopes up when no attack on "the death problem" has been even /partially/ successful thus far. Fund research? Hell yes. Start preaching from the mountaintops about how with strange aeons even death has died? No.

#+begin_quote
  And do please think for five minutes before answering. Have you actually examined your internal beliefs, weighed your reasons and found your true rejection?
#+end_quote

Strangely enough, I've already considered this, and have a cached answer: my rejection is, in fact, that I think we acquire a lot of positive traits through the actions we take and the skills we learn to avoid painful outcomes. We all know, for instance, that children who grow up with riches and golden parachutes all their lives... never really grow up. Taking away pain /entirely/ doesn't enable someone to become the best version of themselves.

That doesn't mean we need to maximize pain, or simply accept levels of pain decided by Nature. Melioration is a real and proper attack on pain, and an ideal world should contain /just enough/ pain to enable learning to overcome it, /and no more than that/.

As I put it to someone on another subreddit: in a good future, people might well cry over spilled milk, having nothing more genuinely tragic or evil happening to them. However, crying over spilled milk or a stubbed toe, and then learning resiliency and discipline, is better than simply removing pain entirely and having a race of spoiled brats.

EDIT: Oh, and that doesn't even begin to take account of masochists.

#+begin_quote
  In fact, I believe people romanticise pain and sadness as much as they romanticise death, and while sadness couldn't be eliminated (whatever lowest level of possible happiness there was would become the new highest level of possible sadness) (not sure the logic follows) pain should be.
#+end_quote

And "romantic jealousy"? Remember, the Superhappies don't experience /attached, devoted love/ as we do, because they decided /romantic entanglements were too painful/.

Now, being in a long-distance and sometimes rocky long-term relationship myself, I feel qualified to rule on this one. Being attached and being in love is /way better/ than not, be that being single or screwing around (or /trying/ to screw around, in the case of us nerds :-p). I don't think anyone should give that up because it's /difficult/ sometimes.

TL;DR: The Superhappies /suck/, and I don't want my species joining theirs. We can and should do much, much better than them. I refuse to vote to scoop entire distinct color channels out of the sensory spectrum. If, on the other hand, you'd like to put new colors /in/, bizarre new sensations and emotions previously known only to Cthulhu Below, then go ahead ;-)!
:PROPERTIES:
:Score: 8
:DateUnix: 1386164157.0
:END:

****** #+begin_quote
  Strangely enough, I've already considered this, and have a cached answer: my rejection is, in fact, that I think we acquire a lot of positive traits through the actions we take and the skills we learn to avoid painful outcomes. We all know, for instance, that children who grow up with riches and golden parachutes all their lives... never really grow up. Taking away pain /entirely/ doesn't enable someone to become the best version of themselves.
#+end_quote

I was talking mainly about physical pain. I don't see why we should need it. Emotional pain /does/ sound like it's useful, however. You only become a grownup by being put in grownup situations.

#+begin_quote
  That doesn't mean we need to maximize pain, or simply accept levels of pain decided by Nature. Melioration is a real and proper attack on pain, and an ideal world should contain /just enough/ pain to enable learning to overcome it, /and no more than that/.

  As I put it to someone on another subreddit: in a good future, people might well cry over spilled milk, having nothing more genuinely tragic or evil happening to them. However, crying over spilled milk or a stubbed toe, and then learning resiliency and discipline, is better than simply removing pain entirely and having a race of spoiled brats.
#+end_quote

I like that.

#+begin_quote
  And "romantic jealousy"? Remember, the Superhappies don't experience /attached, devoted love/ as we do, because they decided /romantic entanglements were too painful/.
#+end_quote

I had forgotten about that, actually. I mean, I don't think romantic jealousy should be a thing because I personally feel it's quite silly to become possessive of a person you love, but that's part of my personal polyamorous philosophy. As for the removal of love, /bad/ thing.

Okay, you convinced me, I changed my mind. The Superhappy outcome is a bad one. Mainly because I think there should be some third alternative that is better. As you said, "We can and should do much, much better than them."

(But I wouldn't say no to keeping the old sensations /and/ adding the bizarre new ones previously known only to Cthulhu Below, either.)

TL;DR: I agree with you that the Superhappies suck and we should find a better third alternative. I don't think /physical/ pain should exist anymore because it's stupid, we can design better ways of making ourselves avoid physical damage without having to add the whole negative sensation thing, but other kinds of emotional pain can be useful in a human brain. This might not be valid for a posthuman, though.
:PROPERTIES:
:Score: 6
:DateUnix: 1386165092.0
:END:


***** Actually, let me counterpoint with an alternative imagining: imagine you were fully, completely immortal. As in, Captain Jack Harkness-level immortal: you can't even be blasted into atoms, it will take a full-blown Reality Warper to get rid of you, and even /they/ might not be able to.

/In that case/, why would you even /want/ to eliminate pain? You don't want to run short of possible sensations or values in your everlasting life, do you? Surely you'd prefer your own mind have more complexity and for life to be more interesting versus simply trying to simplify yourself into a drooling blob of orgasmium in fear of the Reaper?
:PROPERTIES:
:Score: 2
:DateUnix: 1386164691.0
:END:

****** Pain is by definition the sensation we want to avoid. If we grow to like pain, it won't be pain anymore, it'll be something else. So in the Captain Harkness level of immortality, avoiding pain is useless, and it stops being what we'd call pain on a philosophical level.

That is to say I agree to some extent.

However, complexity isn't a value in and of itself. If I had to pick two minds, Complexity would /not/ be anywhere /near/ the top of the choice criteria. And that's also a false dichotomy, we don't have to choose between having a highly complex pain-including life and a drooling blob of orgasmium. We can have a complex mind that doesn't experience highly negative physical sensations but is otherwise capable of qualia we can't even imagine as humans.
:PROPERTIES:
:Score: 2
:DateUnix: 1386165265.0
:END:

******* #+begin_quote
  However, complexity isn't a value in and of itself. If I had to pick two minds, Complexity would not be anywhere near the top of the choice criteria. And that's also a false dichotomy, we don't have to choose between having a highly complex pain-including life and a drooling blob of orgasmium.
#+end_quote

I agree that it's a false dichotomy, but not a /completely/ false dichotomy. To paraphrase Juergen Schmidhuber in one of his Goedel Machine papers, a self-modifying agent that determines its optimal program is /less complex/ than a conscious, living, thinking being will most efficiently obtain its goals by permanently sacrificing its own complexity and consciousness.

Some self-modifications towards less complexity can also be self-reinforcing on a much longer, slower timescale than that. Consider, for instance, if people decided, in slow succession, that they preferred to be more like children than what we today consider adults.

Actually, yes, consider /that one/, because I can't actually think what the right answer is there, so it's /interesting/.
:PROPERTIES:
:Score: 1
:DateUnix: 1386166500.0
:END:

******** I... find that very strange? I mean, children have very little cognitive ability, do not deal well with surprises, failures and not getting what they want, and would probably not do very well in our Big Unfriendly Universe by themselves.

I don't think adults would fare much better either, though. I'd think humanity would move... well, somewhere /else/ entirely. And this whole argument sounds like you're just [[http://lesswrong.com/lw/ix/say_not_complexity/][saying "complexity"]] and acting like you're done. It's not immediately obvious that just because a thing is less complex than what we currently understand as a conscious, living, thinking being it is necessarily /not/ conscious, living and thinking. Complexity is a /consequence/ of what we are and how we're set up, it's not the cause nor the goal.

Actually, it is likely that a good deal of what makes our brain complex is just the result of millions of years of Natural Selection's patchwork and at the very least the /first step/ out of human hardware will involve tremendous simplifications in our own selves. And physical pain looks a /lot/ like that. It's a feeling that was evolved purely and exclusively because those who had it exposed themselves to less dangers; it's a completely universally negative feeling that's supposed to be a reaction to injury and damage.

And then you have masochists. So the issue isn't as /clear cut/.

I think the solution will probably include not feeling /accidental/ physical pain, which is to say that we won't feel horrible levels of pain because we accidentally lost an arm in a woodchipper, just a beeping warning in our heads that our arm is gone and we should go get a new one (like Alicorn's story); while at the same time we should keep /voluntary/ physical pain.
:PROPERTIES:
:Score: 1
:DateUnix: 1386167676.0
:END:

********* Sorry, ok, let me clarify.

Complexity... two meanings here: computational complexity and emotional complexity. I'm talking about the emotional one. Now, it could be that simplifying our emotional spectrum leads to Bad Places, but you could also be right that some emotions are outmoded evolutionary adaptations. I can't think what use a completely mature species has for squick/disgust, for instance.

As for consciousness, Schmidhuber was using the "aware of my own awareness"/strange-loop definition. He pointed out that if you gave a Goedel Machine the problem of solving a maze, it would simply self-rewrite into a maze-solving algorithm and thus eliminate its own consciousness.

#+begin_quote
  I mean, children have very little cognitive ability, do not deal well with surprises, failures and not getting what they want, and would probably not do very well in our Big Unfriendly Universe by themselves.
#+end_quote

But a lot of people /like/ a child's cognition, because it's /simple/. It's a world of big, bold colors where you never experience the frustrations of nuance, of gray areas. It's a reality that beats you over the head, emotionally, and lots of people, for a semi-justifiable reason, like that a lot better than an adult world that's often so gray and complicated you don't feel like you're feeling or experiencing anything significant at all.

I'm not saying "We should revert to the cognitive level of children" is actually a /good idea/. I'm saying that it's an easy place to /make a mistake/ in your self-redesign.

#+begin_quote
  I don't think adults would fare much better either, though. I'd think humanity would move... well, somewhere else entirely.
#+end_quote

What do you mean here?
:PROPERTIES:
:Score: 1
:DateUnix: 1386168474.0
:END:

********** #+begin_quote
  I'm talking about the emotional one. Now, it could be that simplifying our emotional spectrum leads to Bad Places, but you could also be right that some emotions are outmoded evolutionary adaptations. I can't think what use a completely mature species has for squick/disgust, for instance.
#+end_quote

Which is why I say that just shouting complexity isn't enough. Coherent Extrapolated Volition is a /hard problem/.

#+begin_quote
  As for consciousness, Schmidhuber was using the "aware of my own awareness"/strange-loop definition. He pointed out that if you gave a Goedel Machine the problem of solving a maze, it would simply self-rewrite into a maze-solving algorithm and thus eliminate its own consciousness.
#+end_quote

I see. Interesting. We don't know that the strange-loop definition is sufficient for what we'd intuitively call personhood, though, just that it's necessary (I think). However, I disagree that if a conscious machine has as its maxim and only goal solving mazes it will probably self-modify to become a maze-solving algorithm. It would probably create an army of maze-solving machines to solve as many mazes as it could at the same time, and when it ran out of mazes, it might make new mazes. If its objective was solving a one specific maze, however, it'd probably self-modify into an unconscious maze-solving algorithm indeed. And it'd be "happy" all along, satisfying its own Utility Function.

#+begin_quote
  But a lot of people like a child's cognition, because it's simple. It's a world of big, bold colors where you never experience the frustrations of nuance, of gray areas. It's a reality that beats you over the head, emotionally, and lots of people, for a semi-justifiable reason, like that a lot better than an adult world that's often so gray and complicated you don't feel like you're feeling or experiencing anything significant at all.

  I'm not saying "We should revert to the cognitive level of children" is actually a good idea. I'm saying that it's an easy place to make a mistake in your self-redesign.
#+end_quote

I think those people don't actually /remember/ what being a child was like xP My bet is that the vast majority of them, if made into a child for one day and then on the next be allowed to retain the completely fresh memories, would change their minds on the spot. But yeah, I agree it's not an easy problem either.

#+begin_quote
  What do you mean here?
#+end_quote

I mean that post humans will probably not look psychologically like children or adults or anything we can actually currently imagine. Or at least, unlike anything we /have/ imagined.
:PROPERTIES:
:Score: 1
:DateUnix: 1386169027.0
:END:

*********** #+begin_quote
  Coherent Extrapolated Volition is a hard problem.
#+end_quote

Nastier than that (I've been meaning to type out a LessWrong post on this issue). There's a number of big issues with CEV itself:

- When you get down to it, CEV is a very /simple and intuitive/ meta-ethical specification on its face. "Your CEV is all things we can do to you and your environment such that you'll approve of the plan prospectively /and/ retrospectively, with this approval property holding under enactment of many successive plans."

- But what about in the middle? I mean, is it ethical to torture you for 300 years if it will, for instance, prevent the heat-death of the universe? You might approve before and after, but every "before" and every "after" are, at some point, the present. If we approve of our lives only when we're not actually living them, we've done something wrong there, too.

- CEV is only a /declarative/ description of what we meta-want. There's nothing written there about how to actually compute/deduce even /one/ pre/post-approved plan, even though we humans can think of some very easy /conservative improvements/ to our lives (things like: "nobody ever goes hungry, goes thirsty, gets horrible diseases, or dies, ever again" are generally considered pretty reasonable).

- The issue of what it means for the CEV's beneficiary to "approve" packs a metric fuckton of hidden complexity, and we haven't even considered the issue of whether the beneficiary is one human, a group of humans with possible speciation into diverse groups, a group of humans under enforced unity, or something else entirely. Hell, for AI purposes, would you even consider a single human being as a unitary agent, or is it more appropriate to model real humans' values as collections of disparate and interacting agents?

- That second bit about approval holding under successive plans packs a metric fuckton of hidden complexity. Successive CEV plans for 1 year of life should not add up, after 10 years, to something your original self would disapprove of, if the original had the knowledge available at the end of 10 years -- how do we enforce something like that given the limited foresight of the real world?
:PROPERTIES:
:Score: 2
:DateUnix: 1386172568.0
:END:

************ #+begin_quote
  But what about in the middle? I mean, is it ethical to torture you for 300 years if it will, for instance, prevent the heat-death of the universe? You might approve before and after, but every "before" and every "after" are, at some point, the present. If we approve of our lives only when we're not actually living them, we've done something wrong there, too.
#+end_quote

I cannot picture myself during any second of these 300 years thinking that 300 years of torture could possibly compare to an eternity of life for every other lifeform in the universe. But then again, too much pain might just turn off the smart part of my brain. I do know people can be trained to withstand any kind of pain.

But the rest of the objections are exactly /why/ CEV is a hard problem x)
:PROPERTIES:
:Score: 1
:DateUnix: 1386174364.0
:END:

************* #+begin_quote
  I cannot picture myself during any second of these 300 years thinking that 300 years of torture could possibly compare to an eternity of life for every other lifeform in the universe.
#+end_quote

Today you think it's worth being tortured for a day to prevent the heat-death of the universe. Tomorrow, you will still think it's worth being tortured for another day to prevent the heat-death of the universe.

Therefore, [[http://www.reddit.com/r/Futurology/comments/1rflkj/storming_the_ivory_tower_the_girls_who_walk_away/][Omelas and Kyubee decree that you are tortured for eternity]], /proof by induction on the positive integers/. Yet that's obviously a repugnant conclusion, against which our very souls rebel (except, of course, for those ingenious "rationalists" who have carefully trained themselves /not to be affected by their moral intuitions whatsoever/).

So the question is: if we decree it ok to make a partial human sacrifice (which can be as large as 300 years of torture or as small as having to hold a job instead of getting everything for free), but not to make a /whole/ human sacrifice, then how do we build our ethical decision process to ensure sacrifices will always be partial?

In more general terms, if some values are "louder" (they remain morally significant over a longer time horizon) and some values are "quieter" (they attenuate more quickly), how do we avoid an inversion of control where our short-term plans accidentally overprioritize short-term values at the cost of long-term ones -- not because of a moral decision but because "short term" might come to mean "over the next year" and that's all we can plan for? This is already a problem in many human institutions.

#+begin_quote
  But the rest of the objections are exactly why CEV is a hard problem x)
#+end_quote

Has anyone ever actually documented this? Like, I've read some of the FAI literature, but I don't recall people trying to actually explicate and unpack what the fuck this CEV thing actually is.
:PROPERTIES:
:Score: 1
:DateUnix: 1386176277.0
:END:

************** Okay so um... this is a very very complicated problem because if you take any finite number of people, their sacrifice will still be worth making reality last forever for the infinite number of other people. It only starts really being a problem if you need an infinite number of people to suffer forever because then you can actually compare the infinities (both countable, of course, because sapients are a discrete quantity) (or are they?)

That is to say, my soul does /not/ in fact rebel against the conclusion that torturing one single person forever is not a valid price to pay for the eternal life of an infinite number of other people. Even if the tortured person is /me/. My moral intuition says that the result - an infinite amount of life-worth-living to an infinite number of people - is worth the sacrifice. It also says that whatever third alternative that doesn't involve dooming anyone to eternal torture is to be preferred.

#+begin_quote
  Has anyone ever actually documented this? Like, I've read some of the FAI literature, but I don't recall people trying to actually explicate and unpack what the fuck this CEV thing actually is.
#+end_quote

I don't think so, but the impression I had was because CEV was already properly labelled as "magical." We have no idea how to do it, and we don't pretend we do either.
:PROPERTIES:
:Score: 1
:DateUnix: 1386176852.0
:END:

*************** #+begin_quote
  Okay so um... this is a very very complicated problem because if you take any finite number of people, their sacrifice will still be worth making reality last forever for the infinite number of other people.
#+end_quote

Who said the number of other people is infinite? We live in, as far as we know, a strictly finite universe. There is a finite amount of torture versus a finite amount of other people.

Now, we're preventing the heat-death of the universe, so we're at least hypothetically talking about being able to transform that finity into infinity, but come on. Surely we should be looking for ways to take the infinite torture in shifts, or make do without it entirely.

#+begin_quote
  That is to say, my soul does not in fact rebel against the conclusion that torturing one single person forever is not a valid price to pay for the eternal life of an infinite number of other people. Even if the tortured person is me. My moral intuition says that the result - an infinite amount of life-worth-living to an infinite number of people - is worth the sacrifice.
#+end_quote

But doesn't that mean that, again, through proof by induction, you'll sacrifice everyone to save everyone? How many people, at minimum, must be actually enjoying their lives to make the sacrifices worthwhile?

If I have a Kyubee torture everyone else in the universe all the time so that /I and I alone/ can a perfect life forever, is /that/ worth it?

Where's your sense of individual rights? Where is the cross-over point in your utilitarian spectrum at which this becomes unacceptable? And by the way, doesn't sum-total utilitarianism result in the Mere Addition Paradox, also known as Robin Hanson's Malthusian dystopia?

#+begin_quote
  I don't think so, but the impression I had was because CEV was already properly labelled as "magical." We have no idea how to do it, and we don't pretend we do either.
#+end_quote

Isn't the whole point of "rationalism" that instead of just looking away from things and saying, "That's magical!" we actually clarify our thinking, unpack our questions, do mathematical and empirical examinations, and /come to some freaking answers?/
:PROPERTIES:
:Score: 1
:DateUnix: 1386177952.0
:END:

**************** #+begin_quote
  Who said the number of other people is infinite? We live in, as far as we know, a strictly finite universe. There is a finite amount of torture versus a finite amount of other people.
#+end_quote

Nope. The current cosmological model (lambda-CDM) has the Universe being spatially flat and infinite in all directions.

#+begin_quote
  Now, we're preventing the heat-death of the universe, so we're at least hypothetically talking about being able to transform that finity into infinity, but come on. Surely we should be looking for ways to take the infinite torture in shifts, or make do without it entirely.
#+end_quote

Naturally, we'd find a third alternative, that's what I said.

#+begin_quote
  But doesn't that mean that, again, through proof by induction, you'll sacrifice everyone to save everyone? How many people, at minimum, must be actually enjoying their lives to make the sacrifices worthwhile?

  If I have a Kyubee torture everyone else in the universe all the time so that I and I alone can a perfect life forever, is that worth it?

  Where's your sense of individual rights? Where is the cross-over point in your utilitarian spectrum at which this becomes unacceptable? And by the way, doesn't sum-total utilitarianism result in the Mere Addition Paradox, also known as Robin Hanson's Malthusian dystopia?
#+end_quote

The cross-over point is when the number of tortured people actually becomes infinite. Since there are different sizes of infinity, when you have comparable infinities you can start measuring it.

As I said, our current best model for the Universe has it as spatially infinite. That also implies that there is an infinite number of copies of me elsewhere in the universe, and so there will likely be an infinite number of copies of me /not/ being tortured.

As for sum-total utilitarianism, I'm not sure it's the best way to go. Average utilitarianism sounds more useful in a spatially infinite inflationary quantum universe.

#+begin_quote
  Isn't the whole point of "rationalism" that instead of just looking away from things and saying, "That's magical!" we actually clarify our thinking, unpack our questions, do mathematical and empirical examinations, and come to some freaking answers?
#+end_quote

Yes, we /are/ doing it. It's part of the unsolved problems of FAI, with a lower priority than "making sure it won't go Unfriendly somewhere during its self-updating."
:PROPERTIES:
:Score: 1
:DateUnix: 1386178641.0
:END:

***************** #+begin_quote
  Nope. The current cosmological model (lambda-CDM) has the Universe being spatially flat and infinite in all directions.
#+end_quote

In which our Hubble Volume is strictly finite. We can start traveling at the speed of light, and there's only so far we'll get before our constituent protons decay.

#+begin_quote
  The cross-over point is when the number of tortured people actually becomes infinite.
#+end_quote

So if there are 100 people in the world and I torture 99 of them (I'm the 100th) so that I can personally have a perfect life forever while they all suffer, you consider that all right? Or let's tighten the screws: I torture them for 100 years, and then they die, and then I keep living a perfect life forever.
:PROPERTIES:
:Score: 1
:DateUnix: 1386179451.0
:END:

****************** #+begin_quote
  In which our Hubble Volume is strictly finite. We can start traveling at the speed of light, and there's only so far we'll get before our constituent protons decay.
#+end_quote

Right, if you're postulating that we eliminate the heat death of the universe but it will still keep an accelerated rate of expansion then it's not much use at all to torture anyone since our Hubble volume is strictly finite.

#+begin_quote
  So if there are 100 people in the world and I torture 99 of them (I'm the 100th) so that I can personally have a perfect life forever while they all suffer, you consider that all right? Or let's tighten the screws: I torture them for 100 years, and then they die, and then I keep living a perfect life forever.
#+end_quote

No, I said that we can start talking about the problem once we go infinite. If we have an infinite population, any finite subset of it has measure 0. In a finite population, finite subsets have nonzero measure and so we can start talking about them. If there are 100 people in the world then the number of people you can torture will be finite, and it will be defined as a fraction of the total. The same argument goes for infinite populations which need an infinite number of people being tortured so that we can actually measure that as a fraction of the other people.

I don't actually /know/ the answer to the problem, mind you. There are too many variables, my brain isn't that good a computer. Will these 100 people reproduce? Will they live forever? Will their lives be worth living? If they live forever, will they generate an infinite spawn? I don't /know/ what fraction of a population could be justifiably tortured to guarantee ininite life-worth-living to the remaining, I don't even know if the fraction is superior to 1%. All I know is that it's superior to 0%, and torturing any finite number of persons to guarantee infiniteness to an infinite number of persons equals torturing 0% of people.

And none of those are actually answers and I'm not sure I /could/ live a life-worth-living in a Universe that required me to ransom itself with the lives of my fellow humans. I don't think I can occupy the epistemic state you're asking me to occupy. And I think that a sufficiently intelligent agent /should/ be able to find a third alternative. But /I/ don't know the answer to this problem.
:PROPERTIES:
:Score: 1
:DateUnix: 1386182849.0
:END:


***************** #+begin_quote
  Yes, we are doing it. It's part of the unsolved problems of FAI, with a lower priority than "making sure it won't go Unfriendly somewhere during its self-updating."
#+end_quote

+Why? I would have figured that a /rational and self-analyzing/ AI won't allow itself to shift values during its self-update. It would simply perceive a deviant self-update as negative-utility and avoid that.+

EDIT: In order to do so it would need a Timeless Decision Theory whose mathematics is equivalent to the stable self-update problem. Though I still don't understand why the insistence on using model theory rather than some other branch of program verification.
:PROPERTIES:
:Score: 1
:DateUnix: 1386179516.0
:END:

****************** I'm pretty sure all other branches are provably equivalent to Model Theory.
:PROPERTIES:
:Score: 1
:DateUnix: 1386182240.0
:END:

******************* In what mathematical sense? What makes model theory this secret master art of program verification, so powerful that they don't even teach it in schools?
:PROPERTIES:
:Score: 1
:DateUnix: 1386182429.0
:END:

******************** No idea. I don't actually know enough Model theory to prove that, I'm just repeating something I heard at some point.
:PROPERTIES:
:Score: 1
:DateUnix: 1386183552.0
:END:

********************* Thanks anyway. I've been trying to find out, since I'm kind of a typetheoryfag.
:PROPERTIES:
:Score: 2
:DateUnix: 1386184101.0
:END:


**** Defecting as a party in a 3 way hostage dilemma is rational? <I'm not sure if this qualifies as a straw or steel man reduction> The Lord administrator's realization of humanity getting to the stars by lying to itself seems indicative.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1386164125.0
:END:

***** #+begin_quote
  Defecting as a party in a 3 way hostage dilemma is rational?
#+end_quote

How are the Superhappies "cooperating" by forcibly annexing and mutating the human race, again?
:PROPERTIES:
:Score: 3
:DateUnix: 1386164345.0
:END:

****** They were offering a 3 way set of mutations: to offer a change for each species to satisfy the others utility functions, as friends.

The really scary one is when you apply Harry's argument on getting hit over the head from "pretending to be wise" to pain.

ROT 13 till spoiler code works:

ONOL RNGRE PHYGHER NPPRCGF NA RIVY, FB RIVY UHZNAVGL JBHYQ VZCBFR N FBYHGVBA BA VG.

GURA PBZRF NYBAT GUR FHCREUNCCVRF. FHCREUNCCVRF NTERR JVGU GUR UHZNAF.

UHZNA PHYGHER NPPRCGF NA RIVY, FB RIVY FHCREUNCCVRF, JBHYQ VZCBFR N FBYHGVBA BA.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1386165864.0
:END:

******* #+begin_quote
  UHZNA PHYGHER NPPRCGF NA RIVY, FB RIVY FHCREUNCCVRF, JBHYQ VZCBFR N FBYHGVBA BA
#+end_quote

Yes, that was the point of the story.

#+begin_quote
  They were offering a 3 way set of mutations: to offer a change for each species to satisfy the others utility functions, as friends.
#+end_quote

They were not offering a /choice/ however, which strikes me as /not cooperating/.
:PROPERTIES:
:Score: 2
:DateUnix: 1386166056.0
:END:


******* Spoiler code works now; see sidebar.
:PROPERTIES:
:Author: AmeteurOpinions
:Score: 1
:DateUnix: 1386168216.0
:END:
