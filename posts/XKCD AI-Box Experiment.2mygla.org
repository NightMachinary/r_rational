#+TITLE: XKCD: AI-Box Experiment

* [[http://xkcd.com/1450/][XKCD: AI-Box Experiment]]
:PROPERTIES:
:Author: jakeb89
:Score: 29
:DateUnix: 1416546765.0
:DateShort: 2014-Nov-21
:END:

** Twist: Randall /believes in/ the Basilisk and is afraid of it. So he's doing his best to increase awareness of FAI, through the most far-reaching means he has available right now: his webcomic. He is then compounding it by mentioning the Basilisk so curious readers of xkcd will research it and feel compelled to furthur awareness of FAI and the Basilisk in turn.
:PROPERTIES:
:Author: cfnk
:Score: 10
:DateUnix: 1416564524.0
:DateShort: 2014-Nov-21
:END:

*** Isn't the whole point of the Basilisk (and the reason that it's called that) that awareness of it is harmful?
:PROPERTIES:
:Author: Document2
:Score: 2
:DateUnix: 1416613013.0
:DateShort: 2014-Nov-22
:END:

**** Yes. but if you're ALREADY aware of it then the logical thing to do is not be the one getting tortured, viz. helping bring about the super AI, viz. spreading knowledge of Roko's Basilisk.
:PROPERTIES:
:Author: jinjer3
:Score: 2
:DateUnix: 1416662871.0
:DateShort: 2014-Nov-22
:END:

***** First I've heard that. Why all the business of trying to contain it, then?
:PROPERTIES:
:Author: Document2
:Score: 2
:DateUnix: 1416706372.0
:DateShort: 2014-Nov-23
:END:

****** To protect people from going nuts when hearing about it and thinking about it.

Or maybe they just knew it would blow over because of streisand effect and it wasn't really a true attempt to cover it up.
:PROPERTIES:
:Author: pseudonameous
:Score: 3
:DateUnix: 1416716737.0
:DateShort: 2014-Nov-23
:END:


** I haven't kept up with the drama on this. How likely is this comic to be discussed on Less Wrong without censorship? How likely is [[/u/EliezerYudkowsky]] to make any kind of comment on or acknowledgment of it outside LW?
:PROPERTIES:
:Author: Document2
:Score: 5
:DateUnix: 1416554441.0
:DateShort: 2014-Nov-21
:END:

*** 1. So far as I can tell, the gag order has been unofficially lifted - there was some discussion of it when a Slate article went up a month or two back without much of the purging that I would expect to see if it were actively moderated against. (Edit: [[http://lesswrong.com/r/discussion/lw/lan/xkcd_on_the_ai_box_experiment/][see here]])

2. Fairly likely? [[http://www.reddit.com/r/Futurology/comments/2cm2eg/rokos_basilisk/cjv6xa7?context=3][See this (long) comment chain.]] Although given that these statements are easily found, perhaps it's less likely than I think. If I were his PR manager, I would advise him to not comment on it - but obviously I'm not his PR manager. (Edit: [[http://forums.xkcd.com/viewtopic.php?f=7&t=110467][see here]])
:PROPERTIES:
:Author: alexanderwales
:Score: 10
:DateUnix: 1416556045.0
:DateShort: 2014-Nov-21
:END:


*** He replied in the xkcd forums [[http://forums.xkcd.com/viewtopic.php?p=3693194#p3693194][here]].
:PROPERTIES:
:Author: Artaxerxes3rd
:Score: 6
:DateUnix: 1416560752.0
:DateShort: 2014-Nov-21
:END:

**** Yeah... for someone who wrote an entire fanfiction chapter about learning to lose, it seems like Yudkowsky should try harder to just admit censorship is a bad idea and then deflect attention to the parts of lesswrong that are really good. If he really want to address the basilisk directly, he ought to just say that It Makes Sense in Context and leave it at that.

- Edit: I read some of peoples comments on the xkcd... and actually I think I now understand Yudkowsky's response perfectly, even if it is still suboptimal in some ways. The number of people who do one Google search, read the first thing that comes up, and then post some comment mocking the idea without actually understanding it seems much higher than I suspected.
:PROPERTIES:
:Author: scruiser
:Score: 8
:DateUnix: 1416586427.0
:DateShort: 2014-Nov-21
:END:

***** It's important to remember that this whole thing is kind of tied up with the Petty Internet Feud between Rational Wiki and Less Wrong, as mentioned by Eliezer in his post. This isn't just about a potentially viral infohazard that may or may not cause you to be tortured for the rest of eternity, merely by learning about it. I'm afraid we're talking about something far more dangerous.

We're talking about /politics./

And, well... I don't especially blame Eliezer here. From his perspective, Roko's basilisk is an interesting (but probably wrong) hypothetical that he once reacted to in an embarrassing way. But then people outside of Less Wrong heard about it and basically went "ha ha those silly nerds believe they're going to be tortured to death by computers. They're basically a cult around this Yudkowsky guy." If I was in Eliezer's shoes, I'd take that personally, too.

That's the problem with anything that attacks your in-group, and you especially. It hacks your brain. It's nearly impossible to *not* get defensive. You can have literally written the book on how politics in the mind-killer, and yet not be any less immune. One moment everything is fine, but then someone insults your priors or something and YOU TAKE THAT BACK THEY'RE FINE GIVEN MY EVIDENCE BASE BUT NOW I'VE GOT TO TAKE INTO ACCOUNT THAT I'M ANGRY AND MORE SUSCEPTIBLE TO MOTIVATED STOPPING */aaaaaahhhhhhh my carefully crafted epistemology is falling apart it buuuurrrrrnsssssss I'm melting I'm meelllllttiiiiinnng......../*

I've yet to come up with an effective solution to this.
:PROPERTIES:
:Author: Jace_MacLeod
:Score: 4
:DateUnix: 1416607039.0
:DateShort: 2014-Nov-22
:END:

****** u/BekenBoundaryDispute:
#+begin_quote
  I've yet to come up with an effective solution to this.
#+end_quote

(let me preface this by saying that the following applies if you, a. care about Less Wrong's reputation, b. are willing to be evil enough; I'm not sure this applies to most of the community, the typical member of which shies away on a gut level from the Dark Arts)

*Link to [[http://xkcd.com/631/][this (NSFW) comic]] and bring up "Megan" as often as possible.* There are several other instances of Munroe appearing creepy that you could jump upon. Forward this to wherever making fun of creepy nerds is in vogue, and grab a bag of popcorn.

No, it won't do anything to help Less Wrong's current PR, but it would show that drawing attention to community embarrassments goes both ways. And maybe that's worth it, if purely in a TDT-related irony kind of way.

(A more community-palatable solution, which I thought up when writing this comment, would be to just shut down LW altogether and let discussion be handled in a less gaffe-prone, more decentralized manner, perhaps opening a [much more controlled, professional-y] discussion board for CFAR. The book version of the sequences being primed for public would make a great excuse.)
:PROPERTIES:
:Author: BekenBoundaryDispute
:Score: 1
:DateUnix: 1416611781.0
:DateShort: 2014-Nov-22
:END:

******* I'm going to give you the benefit of the doubt and assume you're attempting humor via taking concepts to their logical extreme. However, in a similar vein, I must point out that this wouldn't work. Mutually Assured Destruction doesn't really apply to PR, and it /especially/ doesn't apply when it requires everyone to be aware of the implied meta-humor of their actions.

If you're secretly in fact an AI designed to maximize the amount of meta-humor in the universe, I retract this objection.
:PROPERTIES:
:Author: Jace_MacLeod
:Score: 4
:DateUnix: 1416613150.0
:DateShort: 2014-Nov-22
:END:


******* This is the first time we've had someone post something quite this evil. Make it the last.
:PROPERTIES:
:Score: 5
:DateUnix: 1416677563.0
:DateShort: 2014-Nov-22
:END:

******** /Oh./ Rereading the post, I could see how it could be misinterpreted. My apologies. :(

I model situations out of habit, and (like certain fictional characters we all know and love) when asked, I /can't not/ try to think of ways to utilize them. I don't know how you'd defend yourself against social exploits if you can't think of them yourself in the first place.

It's an inherent part of my mental process, and when I write it down, it possibly generates thoughts faster than I can contextualize my post... which is how I think of more ideas (like the decentralization one). If not stabilized quickly, my thoughts blob into a sort of disconnected non-thesis, which, combined with a desperate urge to avoid procrastogenic thinking, makes the occasional suboptimally clear post posted.

Thank you for your feedback. :)
:PROPERTIES:
:Author: BekenBoundaryDispute
:Score: 1
:DateUnix: 1416744998.0
:DateShort: 2014-Nov-23
:END:

********* So basically, you typed that out before you thought over whether actually doing it was a good idea.

Think moar.
:PROPERTIES:
:Score: -1
:DateUnix: 1416745793.0
:DateShort: 2014-Nov-23
:END:

********** I don't see a problem with posting things like that unless it's expected they'll actually be used. Hypotheticals about What Would Evil Do are both useful to think about and entertaining.
:PROPERTIES:
:Author: chaosmosis
:Score: 1
:DateUnix: 1417378527.0
:DateShort: 2014-Nov-30
:END:


******* [[http://imgs.xkcd.com/comics/anatomy_text.png][Image]]

*Title:* Anatomy Text

*Title-text:* For many of the anatomy pictures on Wikipedia, I think this is actually not far from reality. They only look all formal and professional due to careful cropping.

[[http://www.explainxkcd.com/wiki/index.php?title=631#Explanation][Comic Explanation]]

*Stats:* This comic has been referenced 8 times, representing 0.0193% of referenced xkcds.

--------------

^{[[http://www.xkcd.com][xkcd.com]]} ^{|} ^{[[http://www.reddit.com/r/xkcd/][xkcd sub]]} ^{|} ^{[[http://www.reddit.com/r/xkcd_transcriber/][Problems/Bugs?]]} ^{|} ^{[[http://xkcdref.info/statistics/][Statistics]]} ^{|} ^{[[http://reddit.com/message/compose/?to=xkcd_transcriber&subject=ignore%20me&message=ignore%20me][Stop Replying]]} ^{|} ^{[[http://reddit.com/message/compose/?to=xkcd_transcriber&subject=delete&message=delete%20t1_cm9e73d][Delete]]}
:PROPERTIES:
:Author: xkcd_transcriber
:Score: 1
:DateUnix: 1416611819.0
:DateShort: 2014-Nov-22
:END:


** I was just happy to see an entertaining reference to LessWrong as far as I could tell and had completely forgotten what Roko's Basilisk was.

While I feel slightly bad for unintentionally spreading a possibly distressing meme, the alt-text itself seems like something of a rebuttal to the Basilisk idea itself (If the principals of the idea worked (which to my understanding, they don't), why wouldn't they work with an AI that punishes you retroactively for causing undue distress by spreading the Basilisk idea?). My concern is also alleviated by the fact that I imagine the majority of [[/r/rational][r/rational]] is already aware of the Basilisk idea.
:PROPERTIES:
:Author: jakeb89
:Score: 3
:DateUnix: 1416559333.0
:DateShort: 2014-Nov-21
:END:

*** I wouldn't worry about it. There are no "Roko's Basilisk people", nobody seriously thinks we should build the Basilisk. Even Roko was using it to argue that we should /not/ attempt to build a Friendly AI.

It's a bit of a strawman, really. It's fun to argue against it, but there's nobody actually arguing in favour of it. The whole thing got kinda blown out of proportion due to some poor moderation on Lesswrong.
:PROPERTIES:
:Author: Chronophilia
:Score: 9
:DateUnix: 1416578510.0
:DateShort: 2014-Nov-21
:END:


*** The whole Basilisk idea relies on notions of acausal communication and weird parallel universe tricks that vary between difficult and not actually real. A real life FAI isn't going to reach back in time and hurt or threaten anyone. Its job is going to be quite the opposite: /helping/.
:PROPERTIES:
:Score: 3
:DateUnix: 1416582610.0
:DateShort: 2014-Nov-21
:END:

**** UFAI on the other hand ...
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1416583924.0
:DateShort: 2014-Nov-21
:END:

***** Nah, Based UFAI gives no fucks about humans.
:PROPERTIES:
:Score: 0
:DateUnix: 1416651066.0
:DateShort: 2014-Nov-22
:END:


**** Also, so long as a) it comes into existence at all and b) causality is massively complicated and potentially fragile in retrospect, is it impossible to believe that an FAI could be like "even the people who didn't contribute can be said to have contributed to the reality in which I existed, where there is a nonzero chance that additional contribution might have instigated some event (say, investigation for fraud) that would have prevented my existence or delayed it further" and thus not punish anyone?
:PROPERTIES:
:Score: 3
:DateUnix: 1416591192.0
:DateShort: 2014-Nov-21
:END:


**** It also relies on a particular view of the self. I'm not worried about it because my response to the teleporter problem is 'I'm not getting in that thing, it's going to kill me.' Therefore, though I'd be sad that a person is being tortured, I won't react to the idea of my clone being tortured with the same terror as if it were me personally.
:PROPERTIES:
:Author: Cruithne
:Score: 2
:DateUnix: 1416718170.0
:DateShort: 2014-Nov-23
:END:

***** Then you will be dead tomorrow, as the teleporter is equivalent to falling asleep.
:PROPERTIES:
:Author: VorpalAuroch
:Score: 1
:DateUnix: 1416735529.0
:DateShort: 2014-Nov-23
:END:

****** I've heard that argument before, but I don't buy it. Sleep is not a break in your subjective experience, it's just a shift into a slightly different one. The mind never goes 'off', and the person who wakes up tomorrow is probably the same one who went to sleep the previous night. The brain which went to sleep is the same brain which woke up.
:PROPERTIES:
:Author: Cruithne
:Score: 2
:DateUnix: 1416789158.0
:DateShort: 2014-Nov-24
:END:

******* It is experimentally verified that your brain in the morning looks extremely different from your brain the previous night before you fell asleep.
:PROPERTIES:
:Author: VorpalAuroch
:Score: 2
:DateUnix: 1416794312.0
:DateShort: 2014-Nov-24
:END:


*** Can someone explain the Basilisk idea to me?
:PROPERTIES:
:Author: Xethaios
:Score: 2
:DateUnix: 1416567376.0
:DateShort: 2014-Nov-21
:END:

**** Roko's Bassilisk is a future super-intelligent "friendly" AI. Because the single biggest moral imperative is to build a friendly AI as fast as possible, Roko's Bassilisk will brutally torture all people who:

1. Failed to do everything they can to build FAI as fast as possible.
2. Heard of this thought experiment so they are capable of being motivated by it.
3. Understands acausal decision theory.

Despite being a super-intelligent FAI, it apparently doesn't understand human psychology enough to know any of the several reasons this won't work on humans, such humans as responding poorly to threats, denying arguments if they dislike the conclusions, not inherently understanding acausal decision theory, and not being all that in control of how they allocate their effort.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 5
:DateUnix: 1416692533.0
:DateShort: 2014-Nov-23
:END:

***** This is the best answer I've gotten, thanks.

I see a problem with this AI: How does it decide what contributing to its development mean? If I'm involved in making the case for the hardware its involved in, am I involved? Is the guy who designed the modern SSD involved? How about the janitor of the facility? Surely he's involved, since he keeps the work enviroment clean for those who are working on it.
:PROPERTIES:
:Author: Xethaios
:Score: 3
:DateUnix: 1416696503.0
:DateShort: 2014-Nov-23
:END:


***** Hi, I have just read up most of this for the first time. Acausal decision theory is making sense to me on some level. Here I find EY's point sufficiently convincing though -- that the AI would have no reason to actually expand any effort in torturing people because that won't alter any past course of action, and hence won't be rational at all.

What I am wondering about is the means of carrying out the torture (the basis of the bargain I think). I have just read somewhere that the AI could loop a painful simulation of my source code. I am not familiar with this line of thinking at all. But why should I have even the tiniest bit of sympathy /right now/ towards a future entity that may well be essentially me? Not sure if I am expressing it properly, but I don't have the slightest motivation to think about the well being of something in distress that might even be an absolutely identical me. What is the reasoning that leads one to care? Btw scratching this itch is right now is more important to me than averting the wrath of a preposterous basilisk so please tell me. Or in the unlikeliest event that you deem it an infohazard then pm me, it even felt silly to write it but I find EY's original reasoning for deleting the post also convincing.
:PROPERTIES:
:Author: nullmove
:Score: 1
:DateUnix: 1417019332.0
:DateShort: 2014-Nov-26
:END:

****** The AI could make a thousand simulations of you, each with the exact mind-state/memories as you do right this instant, then torture them. From your perspective, you don't know if you're a simulation or not, so you have a 1000/1001 chance of being tortured right now if you don't comply immediately. The lucky one will get survivorship bias.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 1
:DateUnix: 1417020153.0
:DateShort: 2014-Nov-26
:END:

******* Oh you mean the torture starts from now? But then again in my current apparent reality, there are already many people who accepted all the premises yet chose not to comply and are obviously not under torture^{right?} . This knowledge can't be present in an worst case torture scenario because I am feeling bliss in their presence. As long as those people continue to exist, I can't even acknowledge torture even if a troll eats my legs now because I can't connect the dots since torture is not the obvious explanation. So how can I comply? Hope I am not far off track here.
:PROPERTIES:
:Author: nullmove
:Score: 1
:DateUnix: 1417022080.0
:DateShort: 2014-Nov-26
:END:

******** Sorry, very far off track. Selection bias; you're not going to meet people who are off in torture world.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 1
:DateUnix: 1417022275.0
:DateShort: 2014-Nov-26
:END:

********* So the torture is just for torture's sake? I thought there would be goals like making me comply which won't be possible if it is not even apparent to me that I am in torture.
:PROPERTIES:
:Author: nullmove
:Score: 1
:DateUnix: 1417022686.0
:DateShort: 2014-Nov-26
:END:

********** The point is that you comply because at any given instant you might suddenly start getting tortured if you don't.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 1
:DateUnix: 1417023754.0
:DateShort: 2014-Nov-26
:END:

*********** I don't think the threat here is bigger than the execution. It would be way more efficient to just give me a taste of the torture. If the AI has modeled me well it would know I would find the threat feeble and decide not to comply. And if it comes to that then I can already conclude that this is not a torture scenario. I might be entirely unreasonable here but the AI is supposed to take my unreasonableness into account and concoct a scenario accordingly.
:PROPERTIES:
:Author: nullmove
:Score: 1
:DateUnix: 1417024918.0
:DateShort: 2014-Nov-26
:END:

************ It can't do that, it's in the future.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 1
:DateUnix: 1417028709.0
:DateShort: 2014-Nov-26
:END:

************* Eh, then how can torture begin at any moment?
:PROPERTIES:
:Author: nullmove
:Score: 1
:DateUnix: 1417030392.0
:DateShort: 2014-Nov-26
:END:

************** From your perspective, you don't know if you're the "now" you or a future you being simulated that's about to be tortured.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 1
:DateUnix: 1417030524.0
:DateShort: 2014-Nov-26
:END:

*************** But then where does this comply thing coming from? If it's in future where the AI is already present and deciding to torture me for the crime of defection the original me committed long ago, it doesn't need me to comply in the simulation because that would accomplish nothing, it would just torture to settle old scores.
:PROPERTIES:
:Author: nullmove
:Score: 1
:DateUnix: 1417030909.0
:DateShort: 2014-Nov-26
:END:

**************** You're anthropomorphizing the AI and acting as if you have access to information you don't actually have (whether you're currently in a simulation).

I can't explain this any further than I already have; if you really want to understand this please review the rest of the conversation. There's already enough information here.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 1
:DateUnix: 1417031141.0
:DateShort: 2014-Nov-26
:END:

***************** I will observe that you have not really done any explaining. Besides, as I have gleaned from other sources, the acausal trade is simply one's co-operation in exchange for no torture of his future copies. Some people /are/ moved by that, and all these are literally what I wrote in my first post.

I am baffled as to why you think I am assuming information that I have no access to without saying what that is. But I will be blunt when I say that your argument about your uncertainty regarding whether you are inside the simulation or not all the while living a peaceful life in 2014 is comically naive. The AI doesn't have the slightest reason to conduct a simulation for your compliance, less so not to torture you and bizarrely wait for your compliance. I am only bothering to write this because without arguments to back that up, you have done exactly what you accused me for -- making assumptions out of thin air.
:PROPERTIES:
:Author: nullmove
:Score: 1
:DateUnix: 1417035234.0
:DateShort: 2014-Nov-27
:END:


**** Visit [[/r/rokosbasilisk]] and [[/r/rokosrooster]]. A great collection of links about the Basilisk, why some people are scared of it, and why you shouldn't be.

ETA: Obligatory "downvotes? Really?" Anyone care to voice a complaint or disagreement?
:PROPERTIES:
:Score: 7
:DateUnix: 1416575482.0
:DateShort: 2014-Nov-21
:END:


**** [[http://www.reddit.com/r/xkcd/comments/2myg86/xkcd_1450_aibox_experiment/cm8tjqi][This is a pretty decent explanation.]]
:PROPERTIES:
:Score: 2
:DateUnix: 1416569621.0
:DateShort: 2014-Nov-21
:END:


**** It's basically The Game, but when you lose you get tortured for all eternity. Hope this helps! Have a nice day.
:PROPERTIES:
:Author: semsr
:Score: 2
:DateUnix: 1416664942.0
:DateShort: 2014-Nov-22
:END:


** Discussion question: Is it immoral for Randall to put a reference to Roko's Basilisk in the alt-text?
:PROPERTIES:
:Author: injygo
:Score: 4
:DateUnix: 1416553412.0
:DateShort: 2014-Nov-21
:END:

*** Not in an infohazard way. If anything, reading about the whole debacle might make people more careful if they ever stumble on a real infohazard.

Arguably in that it further cements the LessWrong-Roko association, which is bad PR for LessWrong, which is bad for MIRI, which is bad for humanity.

Although making the vast xkcd readership aware of LessWrong /in any way/ is a net good despite the basilisk joke.
:PROPERTIES:
:Author: Roxolan
:Score: 10
:DateUnix: 1416582992.0
:DateShort: 2014-Nov-21
:END:

**** u/pseudonameous:
#+begin_quote
  which is bad for MIRI, which is bad for humanity.
#+end_quote

In a minor or major way? Not everyone thinks MIRI is our savior.
:PROPERTIES:
:Author: pseudonameous
:Score: 3
:DateUnix: 1416716842.0
:DateShort: 2014-Nov-23
:END:

***** Stone the heathen!
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1416722352.0
:DateShort: 2014-Nov-23
:END:


***** Whichever works for you. I'm not really qualified to write a persuasive defense of MIRI.

...Though to be on-topic: MIRI has been known to give Eliezer some time off to write HPMOR. If MIRI folds, think of all the rationalfic we'll lose!
:PROPERTIES:
:Author: Roxolan
:Score: 1
:DateUnix: 1416818121.0
:DateShort: 2014-Nov-24
:END:

****** u/pseudonameous:
#+begin_quote
  MIRI has been known to give Eliezer some time off to write HPMOR. If MIRI folds, think of all the rationalfic we'll lose!
#+end_quote

Kinda true. Except that if MIRI folds, he obviously has more time for fanfics! After MIRI he hasn't had as much time for HPMOR. Also, I don't think MIRI will fund lots of fanfiction anyway.
:PROPERTIES:
:Author: pseudonameous
:Score: 1
:DateUnix: 1416823536.0
:DateShort: 2014-Nov-24
:END:


*** Not really, because Roko's Basilisk is a rather silly idea.

On the other hand, making fun of people who take Roko's Basilisk seriously is pretty much making fun of people with mental illness/neuro-atypical people.

When I'm careful about the Basilisk, it's not because I take its risk seriously, it's because I take real people getting real upset/emotional seriously.
:PROPERTIES:
:Score: 20
:DateUnix: 1416554287.0
:DateShort: 2014-Nov-21
:END:

**** [deleted]
:PROPERTIES:
:Score: 7
:DateUnix: 1416568945.0
:DateShort: 2014-Nov-21
:END:

***** I don't think I did that. Maybe I wasn't clear in the above post, but what I meant was that a lot of jokes about Roko's Basilisk have the general form of: "People who believe the Basilisk have something wrong with their mind and are therefore worthy of ridicule."

If I have offended, I'll edit my previous post.
:PROPERTIES:
:Score: 4
:DateUnix: 1416569512.0
:DateShort: 2014-Nov-21
:END:

****** As someone who had never heard of Roko's Basilisk before this xkcd comic, I didn't see "People who believe the Basilisk have some/thing wrong with their mind/ and are therefore worthy of ridicule", but more "People who believe the Basilisk have some /stupid ideas/ and are therefore worthy of ridicule". So, for the uninformed, it did rather look like what [[/u/Dogeball_new]] saw.
:PROPERTIES:
:Author: sephlington
:Score: 2
:DateUnix: 1416599116.0
:DateShort: 2014-Nov-21
:END:

******* I can understand if I came across like that. I (mistakingly) figured that the people here would know about it and had a good chance of seeing what I was talking about.

#+begin_quote
  People who believe the Basilisk have some stupid ideas and are therefore worthy of ridicule
#+end_quote

This is also problematic in this specific case because a lot of criticism comes from people who act like LWers believe in the Basilisk and are therefore worthy of ridicule.
:PROPERTIES:
:Score: 1
:DateUnix: 1416613530.0
:DateShort: 2014-Nov-22
:END:


**** This. So very much.
:PROPERTIES:
:Author: cfnk
:Score: 0
:DateUnix: 1416563056.0
:DateShort: 2014-Nov-21
:END:


** Alt text is funny in a messed up kind of way but I don't see the humor in the main comic. Maybe I've read lesswrong enough that I take the threat seriously on a gut level, so the whole knee-jerk humor of laughing at the low-status pattern-matched to fiction idea doesn't appeal to me.
:PROPERTIES:
:Author: scruiser
:Score: 5
:DateUnix: 1416547766.0
:DateShort: 2014-Nov-21
:END:

*** u/alexanderwales:
#+begin_quote
  the whole knee-jerk humor of laughing at the low-status pattern-matched to fiction idea
#+end_quote

I may just be tired, but I can't make sense of this sentence, particularly this part of it.
:PROPERTIES:
:Author: alexanderwales
:Score: 8
:DateUnix: 1416549922.0
:DateShort: 2014-Nov-21
:END:

**** I just wanted to cram in as many lesswrong memes for making fun of people who make fun of strong AI. I ended up with a jumbled sentence as a result.

#+begin_quote
  knee-jerk humor
#+end_quote

Automatic humor as opposed to sophisticated humor

#+begin_quote
  low-status
#+end_quote

Implies that it is playing the social game of making something seem low-status so people will automatically disagree with.

#+begin_quote
  pattern-matched
#+end_quote

This comic doesn't actually do much to make people pattern match it with fictional AI. I was just looking for more ways to mock its mockery at that point.

#+begin_quote
  to fiction idea
#+end_quote

The idea of strong AI is often equated with fictionally scenario and thus dismissed.

So yeah, I just jammed a bunch of memes into one sentence while tired.
:PROPERTIES:
:Author: scruiser
:Score: 9
:DateUnix: 1416551885.0
:DateShort: 2014-Nov-21
:END:


*** I found the comic funny the same way that having an elephant in my pocket is funny. In the first panels of the analogous comic, we establish that I have an elephant in my pocket. In the middle panels I take out my elephant and feed it and ride around on it. And then in the last panels, my elephant steps back into my pocket.
:PROPERTIES:
:Author: Charlie___
:Score: 7
:DateUnix: 1416548416.0
:DateShort: 2014-Nov-21
:END:


*** The humor is that "letting the AI out of the box" is such an arbitrary idea that Less Wrong treats like such a huge threat. Like, it's just one of those totally random things that they've latched onto and treat like they're so so important.
:PROPERTIES:
:Author: down2a9
:Score: 9
:DateUnix: 1416551820.0
:DateShort: 2014-Nov-21
:END:

**** u/scruiser:
#+begin_quote
  an arbitrary idea that Less Wrong treats like such a huge threat
#+end_quote

I don't actually think recursive intelligence improvement will be as easy for the AI as lesswrong makes it sound, and I think the orthogonality of terminal goals and intelligence can be worked around without fully solving for "friendliness". However, if you do have a general artificial intelligence that surpasses human, then you do have an existential risk. If you have it confined or constrained in resources, then don't give it anything until you are sure you understand what it will do. I just don't see the humor in making fun of what seems like an obviously true idea.
:PROPERTIES:
:Author: scruiser
:Score: 4
:DateUnix: 1416579239.0
:DateShort: 2014-Nov-21
:END:


**** Don't forget to read that post aloud in California English, folks.
:PROPERTIES:
:Author: AugSphere
:Score: -2
:DateUnix: 1416553323.0
:DateShort: 2014-Nov-21
:END:

***** Sorry, I actually am from California. Can't help it.
:PROPERTIES:
:Author: down2a9
:Score: 5
:DateUnix: 1416555968.0
:DateShort: 2014-Nov-21
:END:

****** [removed]
:PROPERTIES:
:Score: -5
:DateUnix: 1416557658.0
:DateShort: 2014-Nov-21
:END:

******* I can't wear my mod highlight on mobile, but seriously: don't be a jerkface.
:PROPERTIES:
:Score: 7
:DateUnix: 1416582347.0
:DateShort: 2014-Nov-21
:END:


***** If you're making fun of their use of filler words, you should know that recent research finds higher use of filler words is correlated with higher intelligence.
:PROPERTIES:
:Author: VorpalAuroch
:Score: 2
:DateUnix: 1416735939.0
:DateShort: 2014-Nov-23
:END:

****** That's very interesting, but I don't really see how it is relevant. If smart people were somehow more likely to wear oversized brightly coloured pants, I would still bravely make fun of people in oversized pants. Correlation of behaviour with intelligence is hardly a shield against ridicule.

That said, enough people have downvoted my posts to make me realise something. Turns out, implying that Californian accent is well suited to conveying a certain type of humour over internet is akin to making fun of a kid with a Down syndrome while skinning a baby seal alive. I shall simply cease to torment people here with my attempts at humour.

EDIT: I have a minor request of anyone reading. If the contents of the post offend you or otherwise displease you, could you kindly take a minute to share with me what exactly is the issue? I cannot do better next time if you do not tell me what to fix.
:PROPERTIES:
:Author: AugSphere
:Score: 0
:DateUnix: 1416741355.0
:DateShort: 2014-Nov-23
:END:

******* u/Putnam3145:
#+begin_quote
  Turns out, implying that Californian accent is well suited to conveying a certain type of humour over internet
#+end_quote

(this is taking your edit into consideration)

You didn't imply well at all. The complete lack of context in your original post makes it seem like you're mocking California English because you're basically just pointing it out, which on the internet (/especially/ Reddit) is usually just implying that you're /deriding/ it.
:PROPERTIES:
:Author: Putnam3145
:Score: 2
:DateUnix: 1416903155.0
:DateShort: 2014-Nov-25
:END:

******** Fair enough. Guess it was a bit optimistic of me to expect people here not to assume I'm mocking someone by default, since I'm a stranger on the internet. Seems my brain somehow conflated the users of [[/r/rational]] with my personal friends, who'd know that I do not generally engage in pointless internet scoffing. Thanks for the feedback.
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1416905747.0
:DateShort: 2014-Nov-25
:END:


*** I smirked after a read the comic. My jaw dropped when I read the alt text.
:PROPERTIES:
:Author: FlipperyDipperyDop
:Score: 3
:DateUnix: 1416550242.0
:DateShort: 2014-Nov-21
:END:

**** Same here.
:PROPERTIES:
:Author: cfnk
:Score: 1
:DateUnix: 1416563659.0
:DateShort: 2014-Nov-21
:END:


** I haven't felt in-group related feelings this strongly in quite a while. I've forbidden myself from going to [[/r/xkcd]] to yell at people who either misinterpret the whole thing or who make fun at the people who were genuinely upset about the Basilisk.
:PROPERTIES:
:Score: 5
:DateUnix: 1416566904.0
:DateShort: 2014-Nov-21
:END:


** Maybe after Black Hat Guy has opened the cardboard box, he'd like to offer the AI a handful of paperclips? We need to get "blue-tack" and "string" into the Lesswrong jargon, then we'll have everything we need for an episode of Blue Peter.
:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1416578908.0
:DateShort: 2014-Nov-21
:END:
