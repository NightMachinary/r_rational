#+TITLE: [BST] Trying to figure out what actual free will might look like. Alternately, being totally free of bias.

* [BST] Trying to figure out what actual free will might look like. Alternately, being totally free of bias.
:PROPERTIES:
:Author: callmebrotherg
:Score: 8
:DateUnix: 1449896220.0
:DateShort: 2015-Dec-12
:END:
[Determinism bits removed, as it's clear that "free will" is kind of meaningless. I was wondering if I was simply suffering from lack of imagination, but the consensus is that I had the right idea about that, so I'm only considering the other possible mechanism now]

I've been knocking around an idea for a weird magical drug that knocks your awareness out of this universe and into another one where you are not held back by the chemistry of the body and by buggy mental hardware. Does anyone have any suggestions for what I should read in order to figure out what /that/ might be like? I'm aware of some of the bigger rationality blogs, but would be interested in being referred to more (and also to relevant books, articles, and so on).

How the drug works is that you can analyze the information that you already have and decide where to go from here (and, unless you're bad off, /act/ on those decisions once you're returned to your body). You do not acquire information that you didn't have before. In the latter case it /might/ be best summed up as "you become a more rational actor, so that you are only limited by intelligence and lack of information."

EDIT: You may have to rely on heuristics in order to make decisions in a given frame of time, but you can /consciously/ choose those heuristics, rather than have to deal with countless troublesome biases that aren't being helpful (for example, you might not suffer from the halo effect, and this would be natural, rather than something which you'll have had to train).


** u/Roxolan:
#+begin_quote
  you become a perfectly rational actor, so that you are only limited by intelligence and lack of information.
#+end_quote

Hmm, this is a tricky one. The reason evolution gave us so many biases even though perfect rationality "wins", is that perfect rationality is /ridiculously/ computationally expensive. Like, "simulate all possible universes" expensive. A lot of our irrationalities are actually heuristics (i.e. ways to make half-decent decisions very fast). What the rationalist community does is try to intelligently design superior heuristics, but they're still very much heuristics.

So:

- If the drug makes people /literally/ perfectly rational, /and/ boosts their processing power to keep up with it, then they can deduce all that was and ever will be [[http://lesswrong.com/lw/qk/that_alien_message/][from the path of a falling leaf]], awaken their inner Contessa, and bulldoze through your plot in a very unsatisfying way.
- If the drug makes people literally perfectly rational, and /doesn't/ boost their processing power to keep up with it, then it makes them drooling vegetables, dutifully working out the first few calculations of the 3^^^3 they need to make any actual decision.
- If the drug just gives people some better heuristics, then we're out of the realm of formal definitions. So it's entirely up to you which specific less-wrong-than-usual thought process they get.
:PROPERTIES:
:Author: Roxolan
:Score: 14
:DateUnix: 1449910576.0
:DateShort: 2015-Dec-12
:END:

*** u/derefr:
#+begin_quote
  then they can deduce all that was and ever will be from the path of a falling leaf, awaken their inner Contessa, and bulldoze through your plot in a very unsatisfying way
#+end_quote

Ah, they /can/, but will they /want to/? Having that amount of "increased processing power" looks a lot like being stuffed into a Lotus Eater machine: you can prompt your brain to /imagine/ a possible world, and then just live in that world with your background cognition handling the physics. To be "truly rational", you probably have /enough/ processing speed to spend an "infinite" time in that world before any time passes at all in what you previously considered your "reality." The part of reality you're "embodied" within (if that's even still relevant---our own metric universe couldn't possibly hold your mind, it'd have to be somewhere else) is basically now just one of many mental worlds you have an avatar within, one that seems better left on pause.

For a while, I've been meaning to write a collection of stories in a setting similar to this: where not all AI fooms are singularities, because most AI preference functions actually lead to the AI either self-terminating or cutting off all contact to live a rich "internal life." (The story-setting presumes an exploit in the universe's computational substrate---which anything undergoing a foom would notice in due time---that allows for infinite-relative-to-the-parent-universe computational speed, which basically looks like the "perfectly rational agent" above.)
:PROPERTIES:
:Author: derefr
:Score: 3
:DateUnix: 1449962321.0
:DateShort: 2015-Dec-13
:END:


*** I was being insufficiently precise with "rational" then. Thanks for pointing that out.

(Although if one can spend a long time in this state and then return to the moment that the drug was used, then spending a long time figuring things out isn't too bad either. Still can't spend /too/ long, but it's a nice extra.)
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1449966325.0
:DateShort: 2015-Dec-13
:END:


** [deleted]
:PROPERTIES:
:Score: 12
:DateUnix: 1449896802.0
:DateShort: 2015-Dec-12
:END:

*** Probably.

#+begin_quote
  I would find it more appealing if it was a drug that gave you true self-awareness of your thought process, both the subconscious and the conscious. That would allow you to understand yourself better and make better decisions.
#+end_quote

That's an interesting way of putting it.
:PROPERTIES:
:Author: callmebrotherg
:Score: 4
:DateUnix: 1449898939.0
:DateShort: 2015-Dec-12
:END:


*** u/Muskwalker:
#+begin_quote
  Wouldn't you stop being yourself if you had free will? People's minds are deterministic because there's a pattern of cause and effect. Would a mind with true free will think in truly random patterns that don't follow any logic?
#+end_quote

Such randomness might solve the problem of whether the "free will" is truly "free", but I'm not sure that would still qualify as "will".
:PROPERTIES:
:Author: Muskwalker
:Score: 1
:DateUnix: 1449977874.0
:DateShort: 2015-Dec-13
:END:


** I think the most sensible way to think of "free will" is not "nondeterministic" but rather "a quality a general intelligence can possess relative to the general intelligence that created it". A mind has free will if it is capable of developing values more compatible or less compatible with the values of the mind that created it. It therefore doesn't really make sense to talk about humans' free will without some form of God; it's nonsensical, like trying to talk about parallel lines in a one-dimensional world.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 11
:DateUnix: 1449898815.0
:DateShort: 2015-Dec-12
:END:

*** u/Transfuturist:
#+begin_quote
  a quality a general intelligence can possess relative to the general intelligence that created it
#+end_quote

Not to its creator. Relative to any intelligence that is able to predict its actions in response to factors that they control.

CelestAI, for example, did not create biological humans, yet her control over them is plain to see.
:PROPERTIES:
:Author: Transfuturist
:Score: 12
:DateUnix: 1449905656.0
:DateShort: 2015-Dec-12
:END:

**** I like that definition; it makes the concept of "free will" interestingly similar to "consent", as a term for judging power dynamics. When you blackmail someone, you take away their ability to consent relative to you; when you give them a love potion (or rewire their preferences), you take away their free will relative to you.
:PROPERTIES:
:Author: derefr
:Score: 5
:DateUnix: 1449963052.0
:DateShort: 2015-Dec-13
:END:

***** With dependently-varying measures of 'take away,' yes.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1449964220.0
:DateShort: 2015-Dec-13
:END:


*** I like that definition of free will.
:PROPERTIES:
:Author: callmebrotherg
:Score: 2
:DateUnix: 1449898952.0
:DateShort: 2015-Dec-12
:END:


** [[http://lesswrong.com/lw/of/dissolving_the_question/]]

This article is useful. The general concepts people have of free will are often nonsensical. We can observe the situation that causes it- we in our heads can think about things and make decisions about how to behave. We try to come up with some free will thing that makes sense of that, disconnecting it from our brain, but that doesn't make sense. An explanation should explain how something happens, down to the level of bouncing atoms and electrons moving. An explanation that posits some mysterious substance that does stuff is no more useful than Phlogiston material.

If you want greater free will, I suggest doing it in a way that ties it into our experience of free will and how it can be curtailed.

Have a brain where there's much faster impulses and a much greater degree of connection. In representing a decision they'd have access to all of the emotional memories and decision making software that does stuff.

So, suppose you were deciding whether to rob a bank. You'd be able to remember what it felt like to be shot at, what it felt like to have lots of cash, see your brain calculate how well this would boost your social status among local criminals, be able to consider how you found violence easier due to your violent parents. Weigh up all the factors and make a decision.
:PROPERTIES:
:Author: Nepene
:Score: 5
:DateUnix: 1449967160.0
:DateShort: 2015-Dec-13
:END:

*** u/callmebrotherg:
#+begin_quote
  Have a brain where there's much faster impulses and a much greater degree of connection. In representing a decision they'd have access to all of the emotional memories and decision making software that does stuff.
#+end_quote

Ahhhh thank you. I'd considered a few other things, but "knowing the causes behind your motivations, etc. etc." hadn't occurred to me. Thanks.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1449967446.0
:DateShort: 2015-Dec-13
:END:

**** You're welcome.

[[http://www.nhs.uk/Conditions/Consent-to-treatment/Pages/Capacity.aspx]]

It's worth reading how people actually discuss free will when they're not in a philosophy debate and they have to deal with actual issues like "can we cut this person open."

You lack free will about a situation in these examples.

Someone with such an impairment is thought to be unable to make a decision if they cannot: If you can't- understand information about the decision remember that information use that information to make a decision communicate their decision by talking, using sign language or by any other means

So greater free will would mean a much greater memory, so you could remember key decision making stuff and things that could influence your decision, would mean the ability to coordinate that well into a decision, to sort through stuff well to understand it.
:PROPERTIES:
:Author: Nepene
:Score: 3
:DateUnix: 1449968273.0
:DateShort: 2015-Dec-13
:END:


** u/mercert:
#+begin_quote
  "you become a perfectly rational actor"
#+end_quote

This seems to me still like determinism.

Your mind or consciousness is still a box whereby certain inputs produce certain outputs.

The only way for it not be deterministic is if there is some randomness to thought processes...but this still isn't free will, quite the opposite. If your thoughts arbitrarily differ then you can in no way be said to have control of them.

The obvious solution to the seeming paradox is that there is no self, of course, and that your mind is subject to the same physical laws of whatever medium it is composed.
:PROPERTIES:
:Author: mercert
:Score: 3
:DateUnix: 1449917601.0
:DateShort: 2015-Dec-12
:END:

*** [[http://squid314.livejournal.com/332946.html][Relevant.]]
:PROPERTIES:
:Author: Roxolan
:Score: 1
:DateUnix: 1449922772.0
:DateShort: 2015-Dec-12
:END:

**** Certainly an interesting read and I appreciate you posting it, but would you mind expanding on its relevance?
:PROPERTIES:
:Author: mercert
:Score: 1
:DateUnix: 1449923471.0
:DateShort: 2015-Dec-12
:END:

***** It explores how, if you somehow gain knowledge of the perfectly rational things for you to do, then you cease to be a person, and just become a mechanical extension of the thing that gives you this knowledge.
:PROPERTIES:
:Author: Roxolan
:Score: 4
:DateUnix: 1449946806.0
:DateShort: 2015-Dec-12
:END:

****** That's a really cool concept, thanks for sharing it.
:PROPERTIES:
:Author: mercert
:Score: 1
:DateUnix: 1449989121.0
:DateShort: 2015-Dec-13
:END:


*** "No determinism" was only one of the two possibilities (and the one which seemed more impossible, but which I didn't want to discount without making sure that I wasn't overlooking something).

"Rational actor free of biases" was not meant to be synonymous with "free from determinism."
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1449966472.0
:DateShort: 2015-Dec-13
:END:

**** Ah I gotcha.
:PROPERTIES:
:Author: mercert
:Score: 2
:DateUnix: 1449988656.0
:DateShort: 2015-Dec-13
:END:


*** Yeah, when I saw the title I thought Bakker's /Prince of Nothing/ was a great example of a (somewhat) unbiased, hyper-rational protagonist . But even they have no answer for the issue of determinism.

Hell, even the issue of the divided self is a problem.
:PROPERTIES:
:Author: Tsegen
:Score: 1
:DateUnix: 1450199020.0
:DateShort: 2015-Dec-15
:END:


** Both of those things would kill me.

There is no perfect platonic "me" hidden in my brain that you can bring out, that is otherwise limited by some set of rules separate from me. I am the algorithm that restricts my actions. Removing my restrictions is removing me. The chemistry and the quirks of my hardware are the physical implementations of that algorithm. I'm not "held back" by those "imperfections". My software has been built on top of them for billions of years. That's my kernel you're throwing out! Removing that doesn't get you a more rational me, it gets you a vegetable, or a low functioning autistic, or a schizophrenic, or someone with late-stage dementia.

I am already me. If you want a better me, you'll have to actually change me, and you'll have to do with with a scalpel rather than a sledgehammer.
:PROPERTIES:
:Author: Anakiri
:Score: 3
:DateUnix: 1449942717.0
:DateShort: 2015-Dec-12
:END:


** Like you said you can't get out of determinism without just resorting to lazy writing. Everything has rules, or the results would be x = fish or something along those lines.
:PROPERTIES:
:Score: 2
:DateUnix: 1449896396.0
:DateShort: 2015-Dec-12
:END:

*** Nod. Didn't want to rule it out and find out that I was just lacking in imagination.
:PROPERTIES:
:Author: callmebrotherg
:Score: 2
:DateUnix: 1449898912.0
:DateShort: 2015-Dec-12
:END:


** I think free will is kind of like blue red or some other nonsensical concept.

You know what makes free will and determinism different, rewinding time and playing it forward to be a different result without any change.

I think free will would be somewhat satisfied by dualism but still, we have the illusion of free will so there would be no difference in function to the universe.

However what you describe isn't very much like free will, and just replacing your personality with another.
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1449963881.0
:DateShort: 2015-Dec-13
:END:


** What you could do is have a drug that allows self-hypnosis to actually work reliably allowing people to directly modify their own mental functions with repeated use, gradually making them more and more ideal. You would have people rewiring themselves to be constantly happy, to be completely dedicated to work, religion or a personal philosophy. It would make people free to seek their goals without suffering akrasia or cognitive dissonance. People would also become mechanical and single minded as people sacrifice things like their appreciation of art and entertainment.
:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1449966911.0
:DateShort: 2015-Dec-13
:END:

*** Although that sounds like an interesting jumping-off point for a premise; make that an explicit trade-off rather than a natural side-effect.
:PROPERTIES:
:Author: iamthelowercase
:Score: 2
:DateUnix: 1450052218.0
:DateShort: 2015-Dec-14
:END:


** What happens to a heuristic that I have previously consciously chosen to use?

For example, let us say that I have a heuristic that says "wear a white shirt on weekdays", consciously and deliberately chosen to make choosing clothes quicker so I can concentrate on other stuff. Do I have to re-rubberstamp this heuristic every time I take the pill?

--------------

About biases - let us say that someone with a /very strong/ bias against X race takes the pill. His every interaction with members of X race so far has been coloured and tinged by racism - what happens when he thinks back on those interactions? What happens to him when it wears off?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1450088739.0
:DateShort: 2015-Dec-14
:END:

*** 1. Possibly.

2. He knows that the plans which he made in Drug World (for lack of a better term) were the best that he could have made, given the time and information that he had. What happens next depends entirely on whether or not he's the sort of person to care about that.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1450159750.0
:DateShort: 2015-Dec-15
:END:

**** u/CCC_037:
#+begin_quote
  He knows that the plans which he made in Drug World (for lack of a better term) were the best that he could have made, given the time and information that he had. What happens next depends entirely on whether or not he's the sort of person to care about that.
#+end_quote

Is he the same person when he takes the drug as he is when it wears off?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1450163595.0
:DateShort: 2015-Dec-15
:END:

***** Depends on how you define it, but I would say no.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1450198178.0
:DateShort: 2015-Dec-15
:END:

****** Yeah, that's what I was thinking too. So, when he takes the drug, he temporarily becomes a different person, with potentially very different goals, and then when it wears off, he remembers the thoughts and plans of this possibly very different person. But since he is a different person, these plans may not help to achieve Non-Drug's goals. And if they do not, then he will remember what goal these plans serve and he may quite possibly not care for it.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1450211250.0
:DateShort: 2015-Dec-15
:END:

******* Yes. Presumably the drug will be most effective in the hands of people who want the upgrade and can precommit to following the plans of their drugged selves.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1450222430.0
:DateShort: 2015-Dec-16
:END:

******** Or just those whose /goals/ are comparatively free of bias, such that the plans made while on the drug still serve their (non-drugged) goals.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1450234327.0
:DateShort: 2015-Dec-16
:END:


** I have a more reasonable conceit you should feel free to steal.

"Off-label uses, or perfecting the body to perfect the mind":

Therma-3 is a designer drug, intended to aid in weightloss. It's mechanism of effect is that it drastically increases the propensity of the body to metabolize fat reserves in response to drops in bloodsugar, by knocking out several mechanisms that evolved to stop the body from doing exactly this at the drop of a pin.

It works fine. It's a prescription drug in order to stop anorexics from killing themselves with it, but as long as you are not actively selfdestructive, it's safe. If you want to take it and maintain weight, you end up eating a fair bit, but not to "Working in the arctic" levels of absurdity. a 50% increase in caloric intake will do it. It also results in users having nearly perfectly level bloodsugar for most of the day. Which drastically increases your average, effective, intelligence, because you spend a whole lot more of your day in peak operating form. The long term effects of use turn out to be quite drastic, because crystalized intelligence is mostly a measure of how much mental work you have put in over the years, and someone who has spent years studying without sugar crashes or food comas has just gotten a lot more learning done. At no point is there an "NZT moment" - it doesn't make you smarter, it just means you spend less time being stupid because you are low on fuel. But the accumulative effects on someone who has used it from childhood on is major.
:PROPERTIES:
:Author: Izeinwinter
:Score: 1
:DateUnix: 1450380844.0
:DateShort: 2015-Dec-17
:END:

*** Very interesting. Thank you.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1450392729.0
:DateShort: 2015-Dec-18
:END:
