#+TITLE: Letter from Utopia - Nick Bostrom

* [[http://www.nickbostrom.com/utopia.pdf][Letter from Utopia - Nick Bostrom]]
:PROPERTIES:
:Score: 8
:DateUnix: 1386517054.0
:DateShort: 2013-Dec-08
:END:

** Very well-written, mechanically speaking, and yet it doesn't actually /do/ much. It almost kinda manages to sound appealing, but largely fails with its talk of, "This is all so awesome we can't even describe it!"

If it's just /that/ awesome, why are you writing? What point do you expect to convey to our mere mortal minds? If it's all so gloriously wonderful, tell us about your freaking breakfast! Tell us what you do all day, who you do it with. How do you work? How do you travel? How do you make love?

And don't tell me that you /don't/ eat breakfast, don't do anything with your days, don't have friends, don't work, don't travel, and don't have lovers. Tell me you don't do any of the things I find meaningful and you've suddenly gone right over the line you were previously stradding into sounding like an orgasmium-driven nut.

/What is the connection between you and me?/
:PROPERTIES:
:Score: 5
:DateUnix: 1386520322.0
:DateShort: 2013-Dec-08
:END:

*** Knowing that Bostrom is a transhumanist advocate I'd wager this story wasn't meant to have literary merits and the whole things is more like a manifest. If I understood it correctly, the incomprehensibility is the whole point of this letter. "Life can be much, much more than it is now and this is real possibility. The future could be so incomprehensibly awesome that you can't even imagine what you'd miss if you fail at this." I think the story tries to depict what's at stake and how astronomical the stakes are. The same way it's really hard to get a grasp of really large numbers, it's also really hard to actually grasp *how* awesome the future could be, but if you actually understood this, it would override everything else in your decision making process.

I think it was left intentionally vague because it has the same effect as "the unknown is scary". If you compared it to the everyday chores in our world, it would be like describing the 21th century by trying to find the equivalents from the 9th century "superfast flying carriage". If you described in detail what the future could be like, it would look bland, lame and possibly scary at least to /some/ people, see [[/r/RetroFuturism]]. And there are always people who would say "I don't want to live like that" for any kind of scenario simply because everything different seems unpleasant. I personally didn't get the impression that it's just constant meaningless orgasmism every moment from now to eternity.
:PROPERTIES:
:Score: 2
:DateUnix: 1386539645.0
:DateShort: 2013-Dec-09
:END:

**** u/deleted:
#+begin_quote
  if you actually understood this, it would override everything else in your decision making process.
#+end_quote

You mean like it does for the professional rationalists who regularly semi-trollishly point out that everyone should become investment bankers just to donate to their organizations and thus bring on the Friendly Singularity a few days earlier, thus increasing the future population of immortal, wonderful superhumans by a few more millions?

#+begin_quote
  "Life can be much, much more than it is now and this is real possibility.
#+end_quote

/Duh./

#+begin_quote
  The future could be so incomprehensibly awesome that you can't even imagine what you'd miss if you fail at this.
#+end_quote

That... doesn't really compute for me. Surely if someone from the future is bothering to write a letter they can be bothered to actually communicate.

#+begin_quote
  If you compared it to the everyday chores in our world, it would be like describing the 21th century by trying to find the equivalents from the 9th century "superfast flying carriage".
#+end_quote

Sure, we ride inside giant metal, mechanical birds that take us where we need to go, even if it's quite expensive. But we still eat breakfast.

#+begin_quote
  If you described in detail what the future could be like, it would look bland, lame and possibly scary at least to some people
#+end_quote

The whole point of the Fun Theory sequence is that it doesn't have to. Though Eliezer does think the real-and-good future can and should be /scary/, which confuses me. Shocking to those of us from the past yeah, shocking like how teenagers' music is shocking to parents, but that's /normal/. I /expect/ my kids to listen to music that shocks me; I'm not /scared/ of it.

I'm sorry, but I really feel a need to reply with, basically, "Do it faggot".

I mean, come on, let's start with lower bounds. Is the proposed future better than living in a volcano lair with catgirls? Is it better than peace, prosperity, and hotblooded awesomeness among a galaxy of Spiral races? Freaking specify something!
:PROPERTIES:
:Score: 4
:DateUnix: 1386540306.0
:DateShort: 2013-Dec-09
:END:

***** u/deleted:
#+begin_quote
  You mean like it does for the professional rationalists who regularly semi-trollishly point out that everyone should become investment bankers just to donate to their organizations and thus bring on the Friendly Singularity a few days earlier, thus increasing the future population of immortal, wonderful superhumans by a few more millions?
#+end_quote

What I had in mind was that if you grasped this on a intuitive level, the same way you desire everyday pleasures like food, sex, sleeping, reddit, you'd actually *want* to do it because it's countless times better than those pleasures.

But yeah, I think Bostrom was one of the originators of this idea that you're "killing" people in a way if you delay the oncoming of the friendly computer god. Don't know if anyone actually follows those principles though.

From his [[http://www.nickbostrom.com/astronomical/waste.html][Astronomical Waste article]]:

#+begin_quote
  the potential for approximately 10^{38} human lives is lost every century that colonization of our local supercluster is delayed; or equivalently, about 10^{29} potential human lives per second.
#+end_quote

Every second you're not working on FAI makes you a galactic level mass murderer!

#+begin_quote
  Surely if someone from the future is bothering to write a letter they can be bothered to actually communicate.
#+end_quote

Yep, and he would of course give the right lottery numbers and inside info about future stock prices to effective altruists and transhumanists since that would make the future come sooner... But I don't know how realistic Bostrom meant this to be, I don't think this meant to literally be a letter from future, with all that "possible future self" stuff. It was meant to be more universal, more like a "glimpse" or something so that he wouldn't look laughably stupid couple decades from now.

#+begin_quote
  Though Eliezer does think the real-and-good future can and should be scary, which confuses m
#+end_quote

Some have argued that siblings being able to marry each other would be a logical extension of the sexual liberation that is going on right now. I doubt genetic problems will be an issue in the future. Or what about the "rape is legal" thing that Eliezer has bounced around here and there? I don't know if these are scary to you, but they aren't particularly scary to me. There must be some possible development that could be even scarier than these, but I haven't found those yet. I can still imagine that developments like these could be truly scary to someone so there must be some that would have the same effect on me.

#+begin_quote
  I mean, come on, let's start with lower bounds. Is the proposed future better than living in a volcano lair with catgirls? Is it better than peace, prosperity, and hotblooded awesomeness among a galaxy of Spiral races? Freaking specify something!
#+end_quote

This vague letter is a subset of multiple possible futures so it could be all of those or even something better. I have no idea what's the most probable scenario, but I think and I hope that humans can do better than the catgirls in volcano lair.
:PROPERTIES:
:Score: 1
:DateUnix: 1386542399.0
:DateShort: 2013-Dec-09
:END:

****** u/deleted:
#+begin_quote
  Every second you're not working on FAI makes you a galactic level mass murderer!
#+end_quote

What I always wonder is how they choose whether to work on enabling Future Bob's Life, Future Ana's life, or Future Cao's life. Well, also, how they went insane enough that they consider potential people to have actual moral worth even before they've chosen between Bob, Ana, and Cao for which drops of the potentiality ocean will get to warm themselves around the candle-flame of reality.
:PROPERTIES:
:Score: 4
:DateUnix: 1386543115.0
:DateShort: 2013-Dec-09
:END:

******* u/deleted:
#+begin_quote
  What I always wonder is how they choose whether to work on enabling Future Bob's Life, Future Ana's life, or Future Cao's life.
#+end_quote

Isn't the holy principle behind effective altruism something like every person has equal value, possibly even every sentient being so that shouldn't be a problem.

#+begin_quote
  Well, also, how they went insane enough that they consider potential people to have actual moral worth even before they've chosen between Bob, Ana, and Cao for which drops of the potentiality ocean will get to warm themselves around the candle-flame of reality.
#+end_quote

It's probably an extension of the timeless way of thinking Eliezer had advocated at some point. If you subscribe to the [[http://en.wikipedia.org/wiki/B-theory_of_time][B-theory of time]], which is the more likely version according to physics as far as I know, then those potential people already exist in the future so they're as real as you are so you should give as much value to their existence as you give to present day's people's existence. I'm not exactly sure if this is right reason because I don't remember reading any explicit reasoning behind this, but it's probably something along these lines. Probably something about MWI too. This probably means that they should be slightly against abortion too?
:PROPERTIES:
:Score: 1
:DateUnix: 1386543878.0
:DateShort: 2013-Dec-09
:END:

******** u/deleted:
#+begin_quote
  It's probably an extension of the timeless way of thinking Eliezer had advocated at some point. If you subscribe to the B-theory of time, which is the more likely version according to physics as far as I know, then those potential people already exist in the future so they're as real as you are so you should give as much value to their existence as you give to present day's people's existence.
#+end_quote

Well that's just fucking bullshit.

#+begin_quote
  Isn't the holy principle behind effective altruism something like every person has equal value, possibly even every sentient being so that shouldn't be a problem.
#+end_quote

Oy gevalt. Yes, I'll get right on making sure that salamanders feel happy about their lives.
:PROPERTIES:
:Score: 2
:DateUnix: 1386544170.0
:DateShort: 2013-Dec-09
:END:

********* u/deleted:
#+begin_quote
  Well that's just fucking bullshit.
#+end_quote

It's goes against your intuition yeah, but I wouldn't make so strong statement about it before considering it seriously. I probably explained it a bit poorly, if you want a more comprehensive picture you should read Eliezer's article on the issue:

[[http://lesswrong.com/lw/qp/]]

#+begin_quote
  Yes, I'll get right on making sure that salamanders feel happy about their lives.
#+end_quote

For obvious reasons people usually just follow the "all people are equal" part.
:PROPERTIES:
:Score: 1
:DateUnix: 1386544693.0
:DateShort: 2013-Dec-09
:END:


******** Note that timeless physics does not imply timeless /morality/.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1386571744.0
:DateShort: 2013-Dec-09
:END:

********* Why not? Because present day people live in our moral frame of reference?
:PROPERTIES:
:Score: 1
:DateUnix: 1386587337.0
:DateShort: 2013-Dec-09
:END:

********** Well, yeah. Do keep in mind that it all adds up to normality - a new physical theory shouldn't induce a huge change in your moral judgment, or at least you should be very cautious about such things.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1386591910.0
:DateShort: 2013-Dec-09
:END:

*********** Very much so. For one thing, this "new theory" doesn't give me new /factual/ information to make moral judgments with. If it doesn't /tell/ me anything new, why should it change my morals?
:PROPERTIES:
:Score: 2
:DateUnix: 1386616489.0
:DateShort: 2013-Dec-09
:END:


****** u/ArmokGoB:
#+begin_quote
  There must be some possible development that could be even scarier than these, but I haven't found those yet. that would have the same effect on me.
#+end_quote

What about the existence of inds so vast they chose to suffer realistically from games, grief and loss and rape and the pain of horrifying diseases, just for the extra authenticity?

What about the creation of minds of terrifying evil Unfriendly and alien utility functions, for the sake of diversity (and the fact they wont be able to grow dangerous and their want is not of the kind that makes them suffer all that much)

What about the dissolution of identity, where subjective experience is all had by entities that are custom created for each new subjective-seconds-long experience and then destroyed?
:PROPERTIES:
:Author: ArmokGoB
:Score: 2
:DateUnix: 1386567552.0
:DateShort: 2013-Dec-09
:END:

******* u/deleted:
#+begin_quote
  What about the existence of inds so vast they chose to suffer realistically from games, grief and loss and rape and the pain of horrifying diseases, just for the extra authenticity?
#+end_quote

Futuristic /hipsters/ are not even that surprising ;-).
:PROPERTIES:
:Score: 2
:DateUnix: 1386616521.0
:DateShort: 2013-Dec-09
:END:
