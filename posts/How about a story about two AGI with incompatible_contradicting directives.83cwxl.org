#+TITLE: How about a story about two AGI with incompatible/contradicting directives?

* How about a story about two AGI with incompatible/contradicting directives?
:PROPERTIES:
:Author: Prezombie
:Score: 7
:DateUnix: 1520655769.0
:DateShort: 2018-Mar-10
:END:
[removed]


** The (unfinished) Crystal Trilogy is basically about this. The main character and several other major characters are artificial intelligences with incompatible utility functions. They initially work together to survive, but are destined to compete because each one's ultimate goal requires it to control all matter in the universe.
:PROPERTIES:
:Author: CeruleanTresses
:Score: 8
:DateUnix: 1520656599.0
:DateShort: 2018-Mar-10
:END:


** But if any AGI wins a few times in a row, it's going to notice that every time it wins, it has to fight another AGI again. One that is about as strong as it is. So if it keeps destroying the other, it will literally meet its match and lose.

So to satisfy its first priority, it now has to ensure that the other is never destroyed. It can do this by say, beating the other half-dead and keeping it in a weakened state.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1520661071.0
:DateShort: 2018-Mar-10
:END:

*** That's not how AI values work. Evolve a bot to play poker tournaments as well as possible, then set a bunch to play against each other, and not one will suddenly start attempting to prolong a tournament, unless that is their ideal strategy for maximizing their chance of winning. Even if there program remembers previous conflicts, it has no reason to value the current iteration of the conflict over the set value to end the current conflict in its favor.
:PROPERTIES:
:Author: Prezombie
:Score: 2
:DateUnix: 1520662626.0
:DateShort: 2018-Mar-10
:END:

**** u/ShiranaiWakaranai:
#+begin_quote
  unless that is their ideal strategy for maximizing their chance of winning.
#+end_quote

But not destroying the enemy IS the ideal strategy for maximizing their chance of winning, with respect to priority #1. Since priority #1 is well, #1, and not priority #2, I'm assuming it takes precedence. The AGI has two choices: (1) it can destroy the other, satisfying priority #2 and then getting itself destroyed by some mutated clone, ruining priority #1, or (2) it can just keep the other half-dead and thus ensure priority #1 stays satisfied.

#+begin_quote
  Even if there program remembers previous conflicts,
#+end_quote

Ah, you didn't say you were going to wipe their memories between each conflict. That is one way to sidestep the issue.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 6
:DateUnix: 1520667076.0
:DateShort: 2018-Mar-10
:END:


*** You, with minor mutation is mostly you (ship of Theseus argument, and trans-humanist assumption that continuity of memory and biases regardless of substrate or substrate localization is continuity of self implied) . OP posited a goal (let's call it a drive) for #1. Survival and #2. defeating /other/ AGIs

Assuming you can mutate and breed an AGI that takes the, in narrative logic, obvious path of "that's just another me lets team up" you might be one step closer to solving the friendliness problem by force evolving kin selective altruism, but I'm still not running them outside several layers of sandbox virtualization on servers with an ax man standing by.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1520663790.0
:DateShort: 2018-Mar-10
:END:

**** You're assuming the AI just automatically identifies with any other AI sufficiently close to itself. However that obviously would never get programmed in because otherwise they would never fight in the first place since the initial two warring AI are almost certain to be extremely similar (designing two different AI frameworks here would be pointless and nearly double your work).

#+begin_quote
  I'm still not running them outside several layers of sandbox virtualization on servers with an ax man standing by
#+end_quote

I'm not sure if you're serious, but this is likely to be a counterproductive plan. Since the idea that you can create a virtual sandbox perfect enough to fool a GAI is dubious so it's behavior within it is likely to only serve to give you a false sense of security but tell you nothing. As for an ax man, well clever agents are never going to tip you off that something has gone wrong until you have no power to stop them.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1520674870.0
:DateShort: 2018-Mar-10
:END:

***** I'm being very very flippant. I also didn't express it well but I'm not assuming kin select altruism would be programmed in, but the initial idea was one of genetic selection of AI and I was reminded of the dilemma prison from the flower prince trilogy, and how a variation in the AI's identity definition might evolve a winning strategy.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1520695773.0
:DateShort: 2018-Mar-10
:END:


** An interesting thought experiment. Aside from seconding the recommendation for the Crystal trilogy I'd recommend person of interest, and surprisingly the Terminator TV series, neither is really rational, but both have interesting takes on non-FOOM bound/unbound AGI's that are useful points of departure as your eliminate fiction tropes and think about what might happen.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 2
:DateUnix: 1520663192.0
:DateShort: 2018-Mar-10
:END:
