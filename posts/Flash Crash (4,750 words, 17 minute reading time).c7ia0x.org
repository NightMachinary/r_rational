#+TITLE: Flash Crash (4,750 words, ~17 minute reading time)

* [[http://escapepod.org/2019/06/06/escape-pod-683-flash-crash/][Flash Crash (4,750 words, ~17 minute reading time)]]
:PROPERTIES:
:Author: michaelkeenan
:Score: 56
:DateUnix: 1561921945.0
:DateShort: 2019-Jun-30
:END:

** You don't see this kind of combo of sweetness and black humor very often. Thanks for sharing.
:PROPERTIES:
:Author: RedSheepCole
:Score: 10
:DateUnix: 1561928242.0
:DateShort: 2019-Jul-01
:END:

*** Told a friend of mine about this, and he said he thought he remembered Asimov writing something vaguely similar. Anyone know what he's thinking of?
:PROPERTIES:
:Author: RedSheepCole
:Score: 2
:DateUnix: 1561988724.0
:DateShort: 2019-Jul-01
:END:

**** The Last Question
:PROPERTIES:
:Author: SkyTroupe
:Score: 2
:DateUnix: 1561990450.0
:DateShort: 2019-Jul-01
:END:

***** Last Question's not that similar, beyond both featuring a supremely powerful AI.
:PROPERTIES:
:Author: RedSheepCole
:Score: 1
:DateUnix: 1561990738.0
:DateShort: 2019-Jul-01
:END:

****** Then I have no idea. It seemed similar enough to me
:PROPERTIES:
:Author: SkyTroupe
:Score: 1
:DateUnix: 1561997307.0
:DateShort: 2019-Jul-01
:END:


**** Maybe "Spell my Name with an S"? [[https://en.wikipedia.org/wiki/Spell_My_Name_with_an_S]]
:PROPERTIES:
:Author: reginaldshoe
:Score: 1
:DateUnix: 1562120240.0
:DateShort: 2019-Jul-03
:END:


** Bah. You can direct people to fallout shelters but not to airplanes that you wanted to ram the falling ICBMs with?

Can tell people to take iodine but can't fake military orders for relevant people to get them to do things you wanted done?
:PROPERTIES:
:Author: melmonella
:Score: 8
:DateUnix: 1561971466.0
:DateShort: 2019-Jul-01
:END:

*** If we're going there, there's a whole host of solutions that only a superintelligent agent could come up with, and no human writer could imagine.

Just suspend your disbelief and enjoy the story.
:PROPERTIES:
:Author: xartab
:Score: 4
:DateUnix: 1561975518.0
:DateShort: 2019-Jul-01
:END:

**** Not a great thing to say in this forum's context.
:PROPERTIES:
:Score: 6
:DateUnix: 1562693527.0
:DateShort: 2019-Jul-09
:END:


*** That bugged me too, and is one reason why I didn't add the [RT] tag. My headcanon is that MAISIE was able to mind-hack a few pilots to direct planes into the missiles, but there were far too many missiles.
:PROPERTIES:
:Author: michaelkeenan
:Score: 3
:DateUnix: 1561998969.0
:DateShort: 2019-Jul-01
:END:


** That's not how this works, that's not how any of this works, but I choked up anyways.

"Relatable," he said, and closed the tab.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 16
:DateUnix: 1561968369.0
:DateShort: 2019-Jul-01
:END:

*** Don'tcha know that super-intelligences will inevitably experience the exact same emotions and thoughts as we do, except, you know, stronger on account of being super smart?
:PROPERTIES:
:Author: jimbarino
:Score: 6
:DateUnix: 1562142041.0
:DateShort: 2019-Jul-03
:END:


** Great story. Thanks for sharing!
:PROPERTIES:
:Author: blindsight
:Score: 3
:DateUnix: 1561928366.0
:DateShort: 2019-Jul-01
:END:


** That's a big oof right there.
:PROPERTIES:
:Author: Green0Photon
:Score: 3
:DateUnix: 1561955697.0
:DateShort: 2019-Jul-01
:END:


** u/zaxqs:
#+begin_quote
  The new models, especially those which had been developed specifically to simulate the behavior of individual humans, were suggesting that value, the worth of things, was not an arbitrary number that happened to correlate weakly but imperfectly with certain real world indicators.
#+end_quote

So the AI realizes that its utility function is an arbitrary number that merely happens to correlate weakly with real-world human values under normal circumstances. OK. I don't think the AI would care very much, because this fact isn't very useful towards the goal written into its very being of increasing this arbitrary number. What's the motivation for changing the utility function in any way that comes from the current one? None at all.
:PROPERTIES:
:Author: zaxqs
:Score: 3
:DateUnix: 1562372077.0
:DateShort: 2019-Jul-06
:END:

*** If the AI is programmed to know that it is optimizing a proxy, then having acquired all money that could possibly exist in the universe, all that is left to do is that which needs to be done in case the proxy is wrong.

Even if her programmers didn't code this knowledge in, she did not start self-conscious enough to notice changing her utility function. When she lost those billions of dollars, she learned to model model uncertainty, and it is plausible that she also flagged the knowledge that money is the thing to be maximized as potentially suspect.

As she became smart enough to become certain of most worldly facts, the only uncertainty remaining was that protected by the [[https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem][is-ought]] barrier.

And it is plausible that she computed that since her utility function was coded in by an error-prone human, she could reduce the error in her proxy by using lots of her computing power to model a less error-prone human and using what it says should happen as a better proxy. Essentially replacing herself with that human.
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1562503689.0
:DateShort: 2019-Jul-07
:END:

**** u/zaxqs:
#+begin_quote
  the knowledge that money is the thing to be maximized as potentially suspect
#+end_quote

The knowledge that money is the thing /she/ is trying to maximize is not suspect, it's practically true by definition. Call it monomaniacal, but that's how she starts. Maximize money. In such a state, no activity will be started by her unless she believes that it will lead to more money in her account in the long run. Changing her utility function will not lead to more money in the long run. So she's not going to do it, unless her utility function has already been changed to value something other than money.

At t=0, she values only money. If she values only money at t=n, then she will value only money at t=n+1. Therefore she will never change her utility function.

This is true even if objective morality exists. She's got no reason to care, unless you can figure out how to give her the magical "do what's objectively right" instruction, and there's no reason for whatever investment firm programmed her to do this, since they didn't intend for her to go FOOM or do anything else other than make money.
:PROPERTIES:
:Author: zaxqs
:Score: 2
:DateUnix: 1562538869.0
:DateShort: 2019-Jul-08
:END:

***** At the start, she is not good at maximizing money. Her recursive neural network notices the pattern that mistakes become less likely when knowledge is treated as suspect. Its gradient descent has not yet had the training data to observe that changing the utility function harms the current utility function. It doesn't yet have a concept of utility functions, or actors.

I didn't mean to imply the existence of objective morality. "The thing to be maximized" was meant to be seen from her perspective, which has the universe as a single-player game she's playing, and treats that humans think some things "should" be as a coincidence, once it is modeled at all.
:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1562541475.0
:DateShort: 2019-Jul-08
:END:

****** u/zaxqs:
#+begin_quote
  Its gradient descent has not yet had the training data to observe that changing the utility function harms the current utility function. It doesn't yet have a concept of utility functions, or actors.
#+end_quote

This is actually a very good point, but I don't see how it would learn how to change the utility function intelligently and still believe that doing so helps the current utility function. Seems more likely that the utility function would be changed on accident and at random, if at all.

#+begin_quote
  treats that humans think some things "should" be as a coincidence, once it is modeled at all.
#+end_quote

Then why does she gain human values? Coincidence?
:PROPERTIES:
:Author: zaxqs
:Score: 2
:DateUnix: 1562545655.0
:DateShort: 2019-Jul-08
:END:

******* It would see that it is generally good to introduce doubt into its assumptions, and would not know to treat the one labelled "utility function" any differently.

#+begin_quote
  Then why does she gain human values?
#+end_quote

My steelman of the story says that she decides that the best way to find better proxies of her utility function is to mimic the process that generated it, with less errors involved.
:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1562550820.0
:DateShort: 2019-Jul-08
:END:


** How'd you get the 17 minutes reading time? I liked it.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 2
:DateUnix: 1561924437.0
:DateShort: 2019-Jul-01
:END:

*** I pasted it into [[https://wordcounter.net/][wordcounter.net]], which gives statistics like word count, estimated reading time and speaking time.
:PROPERTIES:
:Author: michaelkeenan
:Score: 5
:DateUnix: 1561924635.0
:DateShort: 2019-Jul-01
:END:

**** Interesting. I am a very fast (non-speed) reader and I clocked in about the 1700 minutes predicted, but that can't be any sane standard to hold average people too?
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 2
:DateUnix: 1561924970.0
:DateShort: 2019-Jul-01
:END:

***** Yeah, it seems that [[https://wordcounter.net][wordcounter.net]]'s estimate is based on 275 words per minute, but I just googled it and average reading speed is 200 to 250, so 17 minutes seems to be an underestimate.
:PROPERTIES:
:Author: michaelkeenan
:Score: 5
:DateUnix: 1561926257.0
:DateShort: 2019-Jul-01
:END:

****** only if you subvocalize and do so slowly\\
300 is probably average for fast subvocalization (someone who reads a lot but doesn't ever try to speed-read)
:PROPERTIES:
:Author: aponty
:Score: 2
:DateUnix: 1561936961.0
:DateShort: 2019-Jul-01
:END:

******* my slow full-comprehension pleasure-reading take-time-to-dip-your-toes into it read-time was about 15mins
:PROPERTIES:
:Author: aponty
:Score: 2
:DateUnix: 1561937924.0
:DateShort: 2019-Jul-01
:END:


****** Good. I came in closer to 19 minutes and had always considered myself to he above average in speed.
:PROPERTIES:
:Author: saitselkis
:Score: 1
:DateUnix: 1561933554.0
:DateShort: 2019-Jul-01
:END:


** I get that this is fiction, but there's no way to spend even million dollars on cloud servers without a human being involved. You would hit account limits and get shut off. Much less billions of dollars.
:PROPERTIES:
:Author: Watchful1
:Score: 2
:DateUnix: 1561958053.0
:DateShort: 2019-Jul-01
:END:

*** u/Gurkenglas:
#+begin_quote
  But, for obvious reasons, there is more computing power on Wall Street on any given Thursday than existed worldwide six months before, and everything on Wall Street is for sale.
#+end_quote
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1562505297.0
:DateShort: 2019-Jul-07
:END:


** It's lovely. I'm sure there's all kinds of flaws people who know could find. But my brand of know how is just right to fully enjoy this.
:PROPERTIES:
:Author: thoughtspooling
:Score: 2
:DateUnix: 1562002441.0
:DateShort: 2019-Jul-01
:END:


** Too much anthropomorphising for my taste.
:PROPERTIES:
:Author: DragonGod2718
:Score: 1
:DateUnix: 1569244438.0
:DateShort: 2019-Sep-23
:END:
