#+TITLE: I can fix you.

* I can fix you.
:PROPERTIES:
:Author: totorox92
:Score: 30
:DateUnix: 1519459451.0
:FlairText: WIP
:END:
[removed]


** #+begin_quote
  if past-you doesn't want to become present-you, but present-you likes being present-you, then who's preference should receive more moral weight?
#+end_quote

Of the present version, because 1) the past version had its chance already and had already used it, no matter how efficiently. 2) the opinions of dead people should be disregarded more than of the living.

#+begin_quote
  What if you wanted to be changed in such a radical way; does your preference really matter or does this become merely assisted suicide?
#+end_quote

This question's answer partially depends on definition of “suicide”; and while defining suicide is relatively straight-forward in the legal sense of the word, in context of such a discussion it is much less so. For example, one of the reasons behind the common negative perception of suicide is the effects it leaves on the people who knew the person who committed suicide. Another, from a government's point of view, is that it is interested to maintain such an environment in which the random destruction of its “cells” --- individual people --- is minimised. These types of problems are not an issue when we are dealing with the more esoteric definitions of death and suicide.

So, even if it does become an assisted suicide, what of it? Why should it matter?

#+begin_quote
  If the end result is the same, then does it actually matter how we got there?
#+end_quote

First, the analogy was so inaccurate that I won't even bother listing all the things wrong with it. Second, it matters because in case #1 you were changing the would-be ice cream lover through their consent, while in cases #2 and #3 you were violating a bunch of their publicly recognised rights.

#+begin_quote
  To draw the point home to fiction: how is it okay to fix Bellatrix Lestrange? If you can make her stop being evil and crazy via years of therapy, or via mind alteration, then is there a moral difference?
#+end_quote

See definition of death\murder\suicide above. It's ok to fix her because in the public's eye this esoteric definition of murder is a non-issue.
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 12
:DateUnix: 1519478658.0
:END:

*** On top of the fact that in scenario 2 they became a drug addict and apparently classical conditioning is supposed to cause a stronger desire for ice cream than opiates? Let alone how off scenario 3 is...
:PROPERTIES:
:Author: I_am_your_BRAIN
:Score: 1
:DateUnix: 1519485241.0
:END:


** I think a lot of this is conflating different questions, after all if you're not talking about cessation of experience (the mental processes in the brain that were generating your experience are still doing their thing) then "death" is a poorly defined thing and should probably be tabooed in such discussions.

So once you're just talking about personality/memory changes that's something of a different and broad topic. Ultimately though this is probably not a currently generally solvable set of ethical problems for pretty similar/identical reasons to AI alignment and of course should you get it wrong then you get an AGI/ethical system that minds controls people into wanting to be turned into hedonium.
:PROPERTIES:
:Author: vakusdrake
:Score: 9
:DateUnix: 1519491327.0
:END:

*** Bu the mental processes of your experience are now directed towards a different perspective. A set of experiences is less important than /who/ experiences them. We can call it something else if you like: mind state vector corruption? Mind state vector usurpation?

:P I was hoping there was a simpler answer. Oh well. Debating is till fun.

I think that humans can be swayed by a clever enough argument regardless of other factors; what if the AI is just really persuasive? It doesn't have to 'change your mind' if you come around to the idea on your own.
:PROPERTIES:
:Author: totorox92
:Score: 1
:DateUnix: 1519496662.0
:END:

**** #+begin_quote
  I think that humans can be swayed by a clever enough argument regardless of other factors; what if the AI is just really persuasive? It doesn't have to 'change your mind' if you come around to the idea on your own.
#+end_quote

Well yes the fact human minds can likely be convinced of anything using superhuman charisma without direct tampering with the brain is exactly why coming up with a consistent way of solving these sorts of alignment related problems isn't easy, nor is it even clear anyone's done it yet.

#+begin_quote
  Bu the mental processes of your experience are now directed towards a different perspective. A set of experiences is less important than who experiences them. We can call it something else if you like: mind state vector corruption? Mind state vector usurpation?
#+end_quote

This always struck me as a weird unjustified supposition to make. just because you identify with many aspects of your mind, doesn't mean there's any actual reason to suppose that anything except the the presence of the brain processes which generate experiences will function in a subjectively different way should you change other elements. To put it another way there's no reason to suppose you are anything except those processes that generate qualia for the sense of "you" which actually predicts experience.

Furthermore trying to tie a definition of identity which actually predicts experience to transient properties like personality/memory doesn't seem very workable because you can't make any real convincing points for why complete change in those traits should result in "death" but not the many tiny such similarly instant changes that happen to one's mind constantly (after all at some level the human brain must be quantized in its functions). Given such matters have great relevance when it comes to predicting future subjective experience it seems like a system that couldn't seem to /even in principle/ give definitive answers on that shouldn't make sense unless you think human minds are magic.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1519497683.0
:END:


** Clickbait title, this didn't fix me.
:PROPERTIES:
:Author: Gurkenglas
:Score: 16
:DateUnix: 1519471626.0
:END:


** #+begin_quote
  Edit: where is the guide to flairs? I don't know what these acronyms mean.
#+end_quote

Sidebar:

#+begin_quote

  - Is the story [RT] rational or [RST] rationalist? Or is your post [META] discussion about [[/r/rational]]?\\
  - Is the story a [WIP] work in progress, or [C] complete?\\
  - Optional genre tags: [HSF] hard scifi, [HF] hard fantasy, [DC] deconstruction, [EDU] educational, [MK] munchkinism, [TH] transhumanism, [FF] fanfic\\
#+end_quote
:PROPERTIES:
:Author: Escapement
:Score: 3
:DateUnix: 1519493333.0
:END:

*** Thank you!
:PROPERTIES:
:Author: totorox92
:Score: 1
:DateUnix: 1519495189.0
:END:


** #+begin_quote
  You are different from your past self. [..] In a certain sense, past you is dead, and has been replaced, albeit organically, by present you. If past-you doesn't exist anymore and can't be recovered, past-you has died.
#+end_quote

It's nice to find something that describes part of my own views so closely without having to specifically go looking for it.

Some additional things to add to this discussion:

- the undefined importance of the meta-self: how much is the current version of the self dependent on both its past versions, and its “meta-self”: the over-encompassing image of itself that stretches across time and can not be observed and analysed by itself due to the nature of a human brain and mind? How much do deep memories in fact affect us, mental and physical traumas, almost completely forgotten experiences, the changes that one's system of morality has undergone through the continuous history of all the past selves and reasons behind those changes, etc? And given the degree of importance that all these “hidden stats” hold over a current version of a self, can one really say that the current-self is its own person and that the past-selves just die when they change too much? Can a leopard really, truly change its spots; and if it can, does it matter enough to define it as a new creature or is it still the same one because its skeleton remained intact?

- ageing --- when you compare a 40 year-old version of someone with their 50, 70, 90, etc year-old selves; doesn't it turn out that the more they age the more of the person that you knew at their 40 withers away and dies? This can be a very painful question for those who have people close to them who are old or becoming older. The damage to the body and the brain make such people remnants of their past selves, but the society (and they themselves) often excepts us to treat them as the same person. If you interact with someone close to you who's now, say, 70 years old, and neither see them alive nor dead but as continuously dying, how are you supposed to deal with the emotioal backlash from such an understanding of a human sophont's \ consciousness's life?

- crimes of a past self --- how much should've the late-stage version of Alex DeLarge from Clockwork Orange be held responsible for the crimes committed by the earlier versions of his self? What are all the other important factors that should be considered when answering this question, besides the issue of the past-self deaths?
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 3
:DateUnix: 1519479944.0
:END:

*** Crimes of the past self: think of it this way. I go out and commit a murder, but then wipe my memory of having done so (say I restore from backup after shooting myself). If the current me, which has never committed a murder, could be extrapolated forward to performing the same action, then shouldn't I be just as culpable? So you'd need to change so much as to be nearly indistinguishable from the person who committed the crime. In the Clockwork Orange example, is Alex after conditioning actually a different sentient entity, or is his behavior simply under external controls? Since his original self was recoverable, I would say that definitionally his past-self didn't die, but was only suppressed.
:PROPERTIES:
:Author: totorox92
:Score: 2
:DateUnix: 1519496296.0
:END:


** And when I say "Change is Weakness", people think I'm just being contrary.
:PROPERTIES:
:Score: 2
:DateUnix: 1519502690.0
:END:

*** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1519503063.0
:END:

**** Change can be necessary, just like it can be necessary to be weak sometimes. But you shouldn't look on it as a good thing in and of itself, because the more you change, the more your goals drift, and the more your goals drift the less you even are the same person you used to be.

People on this sub are used to seeing goal drift as a huge issue with AI because of how fast it can happen and how it can have such negative effects on us. But goal drift in a person has very negative effects too; if you can't trust your future self to work towards the same goals it hamstrings you in the present.
:PROPERTIES:
:Score: 1
:DateUnix: 1519509098.0
:END:


*** I can certainly see that. If you need to change, it means you were flawed to begin with.
:PROPERTIES:
:Author: totorox92
:Score: 1
:DateUnix: 1519514053.0
:END:
