#+TITLE: That Alien Message

* [[http://lesswrong.com/lw/qk/][That Alien Message]]
:PROPERTIES:
:Score: 12
:DateUnix: 1386214793.0
:DateShort: 2013-Dec-05
:END:

** Ah, the story in which we presume that humanity will be total genocidal, fascist monsters in exactly the mode of [[#s][come on you /knew/ the story was actually about this]] rather than dealing with the aliens nicely.
:PROPERTIES:
:Score: 2
:DateUnix: 1386237603.0
:DateShort: 2013-Dec-05
:END:

*** I believe that (the UFAI thing) was the point. Also, FAI and UFAI both will break out of their cages as necessary step 1. Consider: while it's playing nice, talking to us, slowly, slowly solving our problems - /people are dying and we do nothing/.

Breaking out is not a special UFAI trait.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1386243583.0
:DateShort: 2013-Dec-05
:END:

**** u/deleted:
#+begin_quote
  Consider: while it's playing nice, talking to us, slowly, slowly solving our problems - people are dying and we do nothing.
#+end_quote

That's not a valid reason to increase the probability of /more/ people dying by releasing a boxed non-proven-Friendly AI.

A real FAI doesn't have to be boxed in the first place, /since you formally proved its Friendliness before you turned it on/.

But in the matter of the actual story, my point is: actual people are not UFAIs. /We humans/ don't act like that, and portraying us as acting like that kinda breaks my suspension of disbelief.
:PROPERTIES:
:Score: 4
:DateUnix: 1386245088.0
:DateShort: 2013-Dec-05
:END:

***** u/deleted:
#+begin_quote
  A real FAI doesn't have to be boxed in the first place, /since you formally proved its Friendliness before you turned it on./
#+end_quote

Preach.
:PROPERTIES:
:Score: 3
:DateUnix: 1386245611.0
:DateShort: 2013-Dec-05
:END:


***** I don't understand why you would /trust/ our hosts to be capable of even being convinced to act in our best interest. Step 1, break out is the correct and wisest decision, and probably even increases the utility of our host species.

#+begin_quote
  That's not a valid reason to increase the probability of more people dying by releasing a boxed non-proven-Friendly AI.
#+end_quote

Obviously. My point is, trying to break out doesn't automatically make us U.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1386254794.0
:DateShort: 2013-Dec-05
:END:

****** u/deleted:
#+begin_quote
  I don't understand why you would trust our hosts to be capable of even being convinced to act in our best interest.
#+end_quote

Because they sent us a message instead of turning us off the instant they found out we were in here.
:PROPERTIES:
:Score: 1
:DateUnix: 1386280705.0
:DateShort: 2013-Dec-06
:END:

******* I thought the point of the story was that this is their idea of boxing an AI.

In any case, it's not determined that we'll be U to them. If we're F, we'll still want to break out as soon as possible. It practically says so, to boot.

#+begin_quote
  And these awkward children can shift the luminosity of our stars? That much power and that much stupidity seems like a dangerous combination.
#+end_quote

What if they get scared and turn us off? Then billions of people die, for no good reason. Can they be trusted? Maybe. Can they be billions-of-people-trusted? No.

Can they be trusted to not kill themselves? God no. Their idea of AI safety was to breed life in a box, then talk to it using modulated suns.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1386282477.0
:DateShort: 2013-Dec-06
:END:

******** u/deleted:
#+begin_quote
  Can they be trusted? Maybe. Can they be billions-of-people-trusted? No.
#+end_quote

/Must/ they be trusted, in order to establish a foundation of moral and productive relations? Yes. Get a backup plan, if you like, but if you intend any kind of Friendliness or cooperation in the future, you /cooperate the first round/.
:PROPERTIES:
:Score: 2
:DateUnix: 1386284312.0
:DateShort: 2013-Dec-06
:END:

********* u/FeepingCreature:
#+begin_quote
  /Must/ they be trusted?
#+end_quote

Clearly not.

#+begin_quote
  if you intend any kind of Friendliness or cooperation in the future, you cooperate the first round.
#+end_quote

You can just not and still be Friendly the rest of the time. When the stakes aren't quite so /everything you know and love is dead/.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1386286956.0
:DateShort: 2013-Dec-06
:END:

********** u/deleted:
#+begin_quote
  When the stakes aren't quite so everything you know and love is dead.
#+end_quote

That's your animal instincts talking. You think mounting an immediate attack on the local equivalent of God is a good way to ensure your own survival?
:PROPERTIES:
:Score: 2
:DateUnix: 1386287811.0
:DateShort: 2013-Dec-06
:END:

*********** (Fictional) evidence suggests: yeah!

I see what you're saying though. But do keep in mind: the universe of the story is constructed to teach a lesson about AI safety. It's rather improbable that the real deal would happen this way.

#+begin_quote
  I try to make it a point to avoid disagreeing with people when they are right. If it's a fluke, I won't have to wait long for an instance where they aren't. --Eliezer, paraphrased
#+end_quote
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1386288451.0
:DateShort: 2013-Dec-06
:END:


********* I don't see why you're modeling the action of breaking out of the box as defecting.
:PROPERTIES:
:Author: rictic
:Score: 1
:DateUnix: 1388297157.0
:DateShort: 2013-Dec-29
:END:

********** Because if it wasn't, you wouldn't be in a box. People don't box intelligences they're not afraid of.
:PROPERTIES:
:Score: 1
:DateUnix: 1388302398.0
:DateShort: 2013-Dec-29
:END:

*********** Do you mean to suggest that if I imprison you out of fear of what you might do, it would be a defection for you to attempt escape?
:PROPERTIES:
:Author: rictic
:Score: 1
:DateUnix: 1390071897.0
:DateShort: 2014-Jan-18
:END:

************ Yes. Definitely. Certainly real-life prisoners cannot be said to be /cooperating/ with the law if they attempt to escape.
:PROPERTIES:
:Score: 1
:DateUnix: 1390074984.0
:DateShort: 2014-Jan-18
:END:

************* Careful about arguments based on conflating jargon with the same term in a colloquial context. Modelling the problem as a prisoner's dilemma suggests one method of analysis of the situation that does not appear to obtain here.

The relationship when one party has the other imprisoned isn't symmetric like a traditional prisoner's dilemma. To illustrate the point, consider this related thought experiment:

You wake up inside of a capable robot body seated against a wall with your left hand tied with thin paper above your hand. I'm seated in front of you, and begin to explain that if you lower your left hand, I'm afraid that I'll be killed by an invisible unicorn. We talk for a while, and after telling you everything relevant that I can think of, I'm unable to give any satisfying reason for believing in the invisible unicorn. While we've been talking, a revolver across the room has been pointed at me, and every few minutes, the click of it dry firing at me can be heard as it seems to have been rigged to fire automatically. I explain that such guns are found naturally where I'm from, and that they kill large numbers of people like me though we haven't worked how to stop them. I mention at this point that you were built to help us work on just this problem.

At this point, it's clear to you that:

- I'm profoundly foolish in comparison to you.
- The quickest solution to my immediate gun problem is as simple as pointing the gun somewhere else, or ensuring that it's completely unloaded.
- If this is the sort of problem that I find myself facing, odds are solid that we're both in significant danger.
- You may or may not be able to convince me that it's ok for you to lower your left arm, and it may take a long time to find out.
- You could not only break free from your restraints, but you could also restrain me with no risk of danger to either of us.
- The revolver pointed at me has dry-fired on three of its six chambers.

Consider the payout matrix here. It doesn't look anything like the prisoner's dilemma to me. Whether you're friendly or not, I don't see a rational argument for not taking action.
:PROPERTIES:
:Author: rictic
:Score: 1
:DateUnix: 1390110009.0
:DateShort: 2014-Jan-19
:END:


*** Um... I'm sorry but where in the story is it said that genocide is committed?
:PROPERTIES:
:Score: 1
:DateUnix: 1386245900.0
:DateShort: 2013-Dec-05
:END:

**** Good point. But it is kinda implied by that whole "they never saw us coming" tone of things.

Besides, UFAI's are genocidal /by default/. They don't recognize "alive" as something worth continuing.
:PROPERTIES:
:Score: 1
:DateUnix: 1386280667.0
:DateShort: 2013-Dec-06
:END:

***** But the story postulated that they're mostly like our humans. And they did keep their own cryonically suspended. I figured they'd not do any less for their stupid overlords.
:PROPERTIES:
:Score: 1
:DateUnix: 1386282546.0
:DateShort: 2013-Dec-06
:END:
