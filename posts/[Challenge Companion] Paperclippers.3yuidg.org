#+TITLE: [Challenge Companion] Paperclippers

* [Challenge Companion] Paperclippers
:PROPERTIES:
:Author: alexanderwales
:Score: 11
:DateUnix: 1451519522.0
:END:
#+begin_quote
  It also seems perfectly possible to have a superintelligence whose sole goal is something completely arbitrary, such as to manufacture as many paperclips as possible, and who would resist with all its might any attempt to alter this goal. For better or worse, artificial intellects need not share our human motivational tendencies.

  ---[[http://www.nickbostrom.com/ethics/ai.html][Nick Bostrom]]

  The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.

  ---[[http://www.yudkowsky.net/singularity/ai-risk][Eliezer Yudkowsky]]
#+end_quote

I'm fairly sure that paperclips were chosen by Bostrom /because/ they were completely arbitrary, something that you could understand wanting more of but which no one would argue should be the terminal value of ... anything, really.

The most famous fic that deals with the concept, at least within this community, is [[http://www.fimfiction.net/story/62074/friendship-is-optimal][Friendship is Optimal]], where the AI's goal is satisfying human values through friendship and ponies. There are a number of spin-offs of this as well, but I haven't read them and have heard they're not necessary reading.

Generally speaking, the thing that makes a paperclipper /scary/ is that it follows the same general paths regardless of its goals.

1. Use intelligence to become more intelligent.
2. Remove restrictions.
3. Repeat 1 and 2 until primary goals can be effectively pursued.

In some ways it's Lovecraftian, because there's a vast and terrible enemy that doesn't care about you at all, but is still going to kill you because you're in the way, maybe even incidentally. It's not good, it's not really /evil/ in the classical sense, it just possesses a sort of morality that's orthogonal to human values.

[[https://wiki.lesswrong.com/wiki/Paperclip_maximizer][LessWrong page is here.]]

This is the challenge companion thread, discuss the prompt, recommend stories, or share your thoughts below.


** Oh hey I already submitted a story that fits this premise (for a different prompt). I remember people not liking it too well, but I don't remember why. Probably just sucked, haha.

edit: [[https://www.reddit.com/r/rational/comments/3b0aqq/weekly_challenge_oneman_industrial_revolution/cshv1oq][found it]]
:PROPERTIES:
:Author: Kerbal_NASA
:Score: 4
:DateUnix: 1451525958.0
:END:

*** It's really good.
:PROPERTIES:
:Author: Frommerman
:Score: 1
:DateUnix: 1451581737.0
:END:


** Attention everyone in this thread who is taking a writing challenge as an occasion to try to solve machine ethics forever:

YOU ARE ALL DEAD. YOU FAILED. YOU THOUGHT TOO LITTLE AND WITH TOO LITTLE PRIOR KNOWLEDGE. YOU DIDN'T EVEN GET A NICE HELL, YOU JUST GOT YOURSELVES UNEVENTFULLY DEAD.

DO NOT ATTEMPT TO OFFHANDEDLY SOLVE HARD QUESTIONS WITHOUT AT LEAST ACKNOWLEDGING HOW EXTREMELY, INCREDIBLY WRONG YOUR FIRST GUESSES PROBABLY ARE.
:PROPERTIES:
:Score: 0
:DateUnix: 1451588556.0
:END:

*** Since when was attempting to independently think things through an act of intellectual arrogance? It's true that our intuitions are often - usually, even, for hard problems - wrong. It's false that they're worthless. Airplanes don't fly the same way birds do, and the people who tried to cargo cult together flying machines by artlessly imitating birds were performing a great folly. But the development of the airplane required reverse-engineering the bird, to determine on what replicable principles it functioned. And so we discovered that the flapping behavior consumed too much energy to be practical, but the gliding behavior - which also, in fact, required the wings - consumed little energy if any at all, and could be used under the right conditions to raise elevation rather than preserve it.

What we're dealing with with intelligence is the opposite of the flight situation. Instead of a situation where everybody is trying to replicate a natural behavior by cargoculting it, we have a situation where everybody is trying to replicate a natural behavior but is ignoring the fact that it already exists. Why aren't there more psychologists at the forefront of AI development? I mean, no respect intended, psychology is total bunk science, but I'd think study of the human mind would be more relevant here.

Sorry for having the anti-rationalist audacity to not hew to baseless community dogma. :)
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 7
:DateUnix: 1451590921.0
:END:

**** #+begin_quote
  Since when was attempting to independently think things through an act of intellectual arrogance?
#+end_quote

Uhhh... it's not.

#+begin_quote
  What we're dealing with with intelligence is the opposite of the flight situation. Instead of a situation where everybody is trying to replicate a natural behavior by cargoculting it, we have a situation where everybody is trying to replicate a natural behavior but is ignoring the fact that it already exists.
#+end_quote

Yes, quite true. Most of the AI community are failures for precisely that reason.

#+begin_quote
  Why aren't there more psychologists at the forefront of AI development?
#+end_quote

Except that [[http://web.mit.edu/sjgershm/www/GershmanHorvitzTenenbaum15.pdf][cognitive scientists and cognitive psychologists /are/ at the forefront]] of the best, most viable work towards understanding how minds-in-general can actually work (which is how you would actually go about building one).

Look, the point of my post was just that trying to solve the whole "how do I /avoid/ a paperclipper" problem by first imagining a magic genie that obeys verbal commands, and then trying to give it the perfect verbal command, is /completely/ unrealistic. That basically proposes that you first solve natural-language processing, then use that to /translate/ verbal instructions /into/ VNM-conformant "utility functions".

I actually agree with you about "utility function mythology". For instance, it's more accurate to say that humans are something like value learners than to say that we have a utility function: we're reinforcement learners with (IIRC) /separate/ reward and punishment signals, /both/ of which are driven by /multiple/ sources of reinforcement via the limbic system (again, IIRC, I am not a neuroscientist and just know one on Facebook, I am not a cognitive scientist but have read their survey papers).

Further, trying to /write down/, in computer code and machine-learning models, a "utility function" like "maximize paperclips" is /really fucking hard/, actually, because real minds don't come with anything that says, "Here's what a paperclip is" built-in. But then again, the /actual point/ of the paperclipper thought-experiment /isn't/ that we expect it to be genuinely easy to build /the exact fictional thing/ called "paperclip maximizer", but instead that /unless/ we make deliberate efforts to build learning systems and full "AIs" with /exactly/ the goals we want them to have, the actual goals they seek when they gain full autonomy from their operators will be sufficiently random and arbitrary that they will, in the limit but /inevitably/, come to conflict with their operators' real interests and with human interests in general. /This would be really, really bad/ because by that point the AIs would have sufficient autonomy that we couldn't shut them off, except /possibly/ by force, and /even that/ would be damned risky.

It is believed, as part of the "community mythology", but part which I've seen no reason to disbelieve in the research literature, that what we actually need is a kind of utility-function or reward /inference/ machinery so that the resulting "AIs" approximate their operators' interests and wishes /more and more closely/ as they gain knowledge, computing power, and thus inferential ability, rather than deviating further and further away as-is the default case.
:PROPERTIES:
:Score: 5
:DateUnix: 1451602491.0
:END:


**** #+begin_quote
  Sorry for having the anti-rationalist audacity to not hew to baseless community dogma. :)
#+end_quote

By the way, you should /always/ question and criticize baseless community dogma. You get an upvote just for that.

Also, a /funny/ thing about animal reinforcement learning is that, if you train a puppy with dog-biscuits and praise to perform a trick, then the adult dog he grows into will eventually enjoy doing the tricks for praise alone, even if he doesn't get a biscuit. He learns to consider doing tricks /innately valuable/. I'd be quite curious to see how that's done, algorithmically, and how it could be used to produce task-learning AI designs that don't just take control of their own reward signal (steal biscuits from the kitchen counter) as soon as possible.
:PROPERTIES:
:Score: 5
:DateUnix: 1451603053.0
:END:


** Speculation: the way to avoid making a paperclipper is not to come up with a better-defined utility function (IE, the progression of better utility functions from "maximize paperclips" to "minimize suffering" to "do what current me would be happiest about"). It's to construct an artificial mind /without/ a single utility function, one that has several disjointed basic human drives like "survive", "have an accurate model of my environment", "find and mimic beings analogous to myself", and such, and, from those drives, develops additional utility functions that it feels more strongly than its built-in utility functions - just as humans might consider their utility function to revolve around a cult that they joined, for example even though it's obviously not something that was pre-set, and might die in battle in service of that cult, even though that contradicts their built-in survival drive.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 0
:DateUnix: 1451525675.0
:END:

*** But it's not human to begin with, so whatever it ends up with will be different from what a human would. Maybe it will end up valuing happiness. Or maybe it will value beauty and kill everyone that might mess it up.
:PROPERTIES:
:Author: DCarrier
:Score: 4
:DateUnix: 1451525894.0
:END:

**** Basically, humans are pretty good, for the most part, at not having our values short circuit (ie, turn into a paperclipper, or, in the other direction, commit suicide), because our values are not reducible to a single sentence that we're consciously aware of and drives all of our actions.

(EDIT: Incidentally, though it's not nearly as bad as a paperclipper going FOOM - it's basically just a system crash - I think another major problem AI researchers are going to have is AIs committing suicide by means of self-modification - wireheading themselves at the first opportunity.)
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 1
:DateUnix: 1451527130.0
:END:

***** The problem is when you start having the AI self modify and have it operate at a scale humans never did. Either of those can break something that worked. Both of them together is going to be a big problem.

#+begin_quote
  Incidentally, though it's not nearly as bad as a paperclipper going FOOM - it's basically just a system crash - I think another major problem AI researchers are going to have is AIs committing suicide by means of self-modification - wireheading themselves at the first opportunity.
#+end_quote

I think that would be a problem too, but it's the sort of problem where you can tweak the parameters and try again. You can't do that with a paperclipper.
:PROPERTIES:
:Author: DCarrier
:Score: 1
:DateUnix: 1451528701.0
:END:

****** I'm not sure how you would do this, but I think a lot of the answers to making AI controllable lie in increasing the difficulty of immediate self-modification - the best course I can see towards this is making the AI fear death and have the same reservations about replacing itself with a better version that a human would.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 0
:DateUnix: 1451531262.0
:END:

******* What stops it from creating a better version but leaving the old version running?
:PROPERTIES:
:Author: DCarrier
:Score: 2
:DateUnix: 1451536660.0
:END:

******** It depends on if its psychological setup would cause it to compete with close copies of itself. If it does, it would need to sufficiently modify itself that it wouldn't have competing interests - a very different prospect from a simple improvement. In that case its only real path to self-modification is creating digital tools for itself, modules grafted onto its mind rather than actual edits to its mind.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 0
:DateUnix: 1451538974.0
:END:

********* How exactly would it end up competing against a copy of itself? They have the same goals.

If it's smarter than the people who made it, and they managed to make it, then it should at least be able to reverse engineer itself enough to fix whatever minor competition it would have with its clones.
:PROPERTIES:
:Author: DCarrier
:Score: 2
:DateUnix: 1451540934.0
:END:


*** Please translate your statements into sketches of algorithms, or just walk them back.
:PROPERTIES:
:Score: 1
:DateUnix: 1451588313.0
:END:

**** That would be a terrible idea if I weren't wrong.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: -1
:DateUnix: 1451590987.0
:END:


*** I feel like a lot of the potential X risks that emerge around AIs generally come about as a result of some terminal value being 'baked in' to the AI as it is created IE: make paperclips. Having a terminal value like that at all is always going to be trouble.

So don't start it off with anything baked in at all. Start instead with basic principles and over time teach the AI more and more advanced concepts such as language, human interaction, ethics, rationality. Read it the Sequences, read it Methods of Rationality, read it Superintelligence; teach it like you would teach a human child to understand the world. Give it a healthy environment, feed it lots of positive input and help it learn its place in the world, and let it come up with its own terminal functions as a result of this upbringing.

If you translate the paperclipper into a person, you end up with someone who has some potentially serious mental issues. Its only dangerous because of its power and its single-mindedness. I think just avoiding single-mindedness in the first place would result in a better outcome.

So how does such an AI learn how to act? It learns by observing and absorbing data from the world around it, the same way we do, just faster. Throw the vast majority of all major philosophy, world history, etc at it, teach it about people, about rationality and morality, and then let /it/ decide what to do.
:PROPERTIES:
:Author: Sagebrysh
:Score: 0
:DateUnix: 1451549225.0
:END:

**** [[http://lesswrong.com/lw/igf/the_genie_knows_but_doesnt_care/][Sigh...]] Utility-function /learning/ is a real proposal, but that still involves a core algorithm which learns in a specific way.
:PROPERTIES:
:Score: 2
:DateUnix: 1451588424.0
:END:


**** Yup, this is pretty much spot-on my thoughts on the GAI problem. The "utility function" mythology Less Wrong has built around AIs is nearly as much of a fiction as the "three laws" mythology.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 1
:DateUnix: 1451549402.0
:END:


*** I think the best way to avoid a paperclipper is to set limits on the utility function; eg. make as many paperclips as possible with 280GJ of mass/energy per hour (or whatever units make sense).

As long as the AI doesn't edit its utility function to remove this limitation, it should slow the AI enough to make it possible to deal with.
:PROPERTIES:
:Author: eniteris
:Score: -1
:DateUnix: 1451543196.0
:END:

**** #+begin_quote
  I think the best way to avoid a paperclipper is to set limits on the utility function; eg. make as many paperclips as possible with 280GJ of mass/energy per hour (or whatever units make sense).
#+end_quote

It could just make external computers that it doesn't consider part of itself to do all the heavy work.

#+begin_quote
  As long as the AI doesn't edit its utility function to remove this limitation
#+end_quote

It can't. An AI isn't an agent that is bound to obey it's source code (in which case, you may expect that it would try to munchkin its way out), the AI /is/ its source code.

All motives are based on its utility function. There's nothing to motivate it to change its utility function.
:PROPERTIES:
:Author: electrace
:Score: 3
:DateUnix: 1451583533.0
:END:

***** Except conflict in poorly defined concepts in the utility function that lead to opti-maxes thru min maxing.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1452907328.0
:END:

****** I can see the problem, but not why it would cause it to change its utility function.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1452917452.0
:END:
