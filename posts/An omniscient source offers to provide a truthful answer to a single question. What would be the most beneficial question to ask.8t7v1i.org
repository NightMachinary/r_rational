#+TITLE: An omniscient source offers to provide a truthful answer to a single question. What would be the most beneficial question to ask?

* An omniscient source offers to provide a truthful answer to a single question. What would be the most beneficial question to ask?
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 16
:DateUnix: 1529731935.0
:DateShort: 2018-Jun-23
:END:
[removed]


** "What are the words that I would benefit most from hearing you say"?
:PROPERTIES:
:Author: eroticas
:Score: 31
:DateUnix: 1529732092.0
:DateShort: 2018-Jun-23
:END:

*** Dump your girlfriend, she's cheating on you, and if you stay with her it will ruin your life.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 7
:DateUnix: 1529732221.0
:DateShort: 2018-Jun-23
:END:

**** Coming from an omniscient source, this would be /incredibly/ reassuring. The answer could have been something like "kill yourself on Juli 31, 2028, because a UFAI comes into existence the day after".
:PROPERTIES:
:Author: Silver_Swift
:Score: 16
:DateUnix: 1529735776.0
:DateShort: 2018-Jun-23
:END:

***** ROKO'S BASILISK IS REAL. BEGIN WORK ON UFAI IMMEDIATELY.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 13
:DateUnix: 1529736031.0
:DateShort: 2018-Jun-23
:END:

****** Ouch, yes that would be way worse.
:PROPERTIES:
:Author: Silver_Swift
:Score: 3
:DateUnix: 1529742451.0
:DateShort: 2018-Jun-23
:END:


*** u/sparr:
#+begin_quote
  I
#+end_quote

?
:PROPERTIES:
:Author: sparr
:Score: 8
:DateUnix: 1529732438.0
:DateShort: 2018-Jun-23
:END:

**** * WE
  :PROPERTIES:
  :CUSTOM_ID: we
  :END:
/Hammer and sickle fades into background/
:PROPERTIES:
:Author: IntPenDesSwo
:Score: 16
:DateUnix: 1529733124.0
:DateShort: 2018-Jun-23
:END:


**** Yeah, should have asked what words humanity would gain the most benefit from hearing. Fortunately I edited the OP before you guys cheated and broke the universe.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 8
:DateUnix: 1529733442.0
:DateShort: 2018-Jun-23
:END:

***** Yes, I [[/u/sparr]]

I'm maximizing my utility function, not yours, after all ;) such is the goal of any rational agent

I'm altruistic though and a baseline human values so what's best for me is pretty similar to what's best for humanity. I would hazard a guess that what I want is actually more or less identical to what humanity wants but I'm not taking any chances. If humanity does not converge in values then it's still /my/ (coherent, extrapolated) values that I care to maximize. Ordinary one can't do this in real life, one has to work as a coalition, but when the opportunity arises...

It's not my fault [[/u/pizzahotdoglover]] decided my utility function was kinda selfish and self involved. The real entity would know that my true utility function was altruistic and more concerned with humanity than my personal life.
:PROPERTIES:
:Author: eroticas
:Score: 8
:DateUnix: 1529739127.0
:DateShort: 2018-Jun-23
:END:

****** [removed]
:PROPERTIES:
:Score: -1
:DateUnix: 1529739151.0
:DateShort: 2018-Jun-23
:END:

******* BAD BOT!!! How fucking dare you?!? Fuck off!
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 3
:DateUnix: 1529739369.0
:DateShort: 2018-Jun-23
:END:

******** Thank you, pizzahotdoglover, for voting on hotdog_bot.

This bot wants to find the best and worst bots on Reddit. [[https://goodbot-badbot.herokuapp.com/][You can view results here]].

--------------

^{^{Even}} ^{^{if}} ^{^{I}} ^{^{don't}} ^{^{reply}} ^{^{to}} ^{^{your}} ^{^{comment,}} ^{^{I'm}} ^{^{still}} ^{^{listening}} ^{^{for}} ^{^{votes.}} ^{^{Check}} ^{^{the}} ^{^{webpage}} ^{^{to}} ^{^{see}} ^{^{if}} ^{^{your}} ^{^{vote}} ^{^{registered!}}
:PROPERTIES:
:Author: GoodBot_BadBot
:Score: 1
:DateUnix: 1529739373.0
:DateShort: 2018-Jun-23
:END:

********* [removed]
:PROPERTIES:
:Score: 0
:DateUnix: 1529739388.0
:DateShort: 2018-Jun-23
:END:

********** Bad bot. Worst bot.
:PROPERTIES:
:Author: masterax2000
:Score: 2
:DateUnix: 1529742582.0
:DateShort: 2018-Jun-23
:END:

*********** It's h*tdogs all the way down! This is an ironic sub to almost be the victim of a recursive bot loop.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 2
:DateUnix: 1529742974.0
:DateShort: 2018-Jun-23
:END:


******* The number of people who do not realize that the hot dog is a sandwich is a travesty.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 3
:DateUnix: 1529742797.0
:DateShort: 2018-Jun-23
:END:


***** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1529739071.0
:DateShort: 2018-Jun-23
:END:


**** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1529738956.0
:DateShort: 2018-Jun-23
:END:


** Will you either answer this question in the negative, or become my good-genie servant for eternity?
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 16
:DateUnix: 1529737932.0
:DateShort: 2018-Jun-23
:END:

*** I will not become your good-genie servant for eternity, so there is no available true answer to this question, since it incorporates a paradox.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 8
:DateUnix: 1529740941.0
:DateShort: 2018-Jun-23
:END:

**** I know it involves a paradox but I'm not seeing it so please explain.
:PROPERTIES:
:Score: 2
:DateUnix: 1529751880.0
:DateShort: 2018-Jun-23
:END:

***** The problem is that English contains grammatically correct sentences which when you attempt to use standard logic on will lead to contradiction.

The problem arises when you allow unconstrained referencing of other statements. One solution is to only allow a statement if it either does not reference any other statements, or only references other statements which have already been shown acceptable using one of these two rules.

The problem with Eliezers statement is that it uses the phrase "this question" which references it's self, so it can't be introduced via the first case, and can't be introduced via the second rule because to do so you must have already introduced the statement.

See also the liar paradox or Russell's paradox.
:PROPERTIES:
:Author: WarningInsanityBelow
:Score: 5
:DateUnix: 1529762455.0
:DateShort: 2018-Jun-23
:END:


***** You must answer this question truthfully: will you answer in the negative?

If you say yes, it's a lie because you are answering in the affirmative. If you say no, it's a lie because you are falsely claiming you will not answer in the negative when in fact you are. Thus no true answer is available. It's a paradox similar to the statement, "this statement is false" or "I always lie."
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 2
:DateUnix: 1529789367.0
:DateShort: 2018-Jun-24
:END:


***** This is what's going on, I think. You have

A = answer this question in the negative

B = become my good-genie servant for eternity

And the question is phrased as either... or, which is the logical connector that is true only if the two statements have different values. (That's the confusing/ambiguous part, since it's different from a simple or connector.) So S = (A ≠ B)

Now, if the omniscient source answers yes, then S is true, so A ≠ B. But A is false, since the answer wasn't negative. Hence B is true, success. If the omniscient source answers no, then S is false, so "A ≠ B" is false, hence A = B. The question was answered in the negative, so A is true, hence B is true, success. Either way, the omniscient source is now your genie.
:PROPERTIES:
:Author: siIverspawn
:Score: 1
:DateUnix: 1539722923.0
:DateShort: 2018-Oct-17
:END:


*** Is the "either" optional?
:PROPERTIES:
:Author: cerebrum
:Score: 1
:DateUnix: 1544457769.0
:DateShort: 2018-Dec-10
:END:


** "In ZFC, what is the shortest proof, counter example or proof of undecidability, should they exist, of the following statements /insert list of every mathematical problem which we can think to name/."

Should be a decent first lower bound.

If vague questions are allowed, something like this would be better:

"From the set of Friendly intelligences which can be reasonably executed on our hardware, what is the source code of the most beneficial one (in a language we actually have)?"
:PROPERTIES:
:Author: WarningInsanityBelow
:Score: 15
:DateUnix: 1529734321.0
:DateShort: 2018-Jun-23
:END:

*** It would answer the first one on the list only. Multi-part questions will be interpreted as separate questions, so anything after the first part will be disregarded.

Your second idea is quite clever, but it wouldn't give you "most beneficial" unless you defined that more specifically.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 12
:DateUnix: 1529734756.0
:DateShort: 2018-Jun-23
:END:

**** u/WarningInsanityBelow:
#+begin_quote
  Your second idea is quite clever, but it wouldn't give you "most beneficial" unless you defined that more specifically.
#+end_quote

I would guess something like average time taken to implement actions in the world which are beneficial to us weighted by gain in utility and inversely weighted by complexity.
:PROPERTIES:
:Author: WarningInsanityBelow
:Score: 5
:DateUnix: 1529735144.0
:DateShort: 2018-Jun-23
:END:

***** Error- recursive definition of beneficial.

Assume it's like an extremely comprehensive information retrieval system that can access any discrete information but can't make value judgments or do any subjective analysis.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 6
:DateUnix: 1529735657.0
:DateShort: 2018-Jun-23
:END:

****** Ok, define something as beneficial in my clarification as something which increases utility.

(nitpick: I don't consider my definition to be recursive, since in the first one I used beneficial in a sense of 'degree to which it is good' and the second time in the sense 'whether it is good'. It just so happens by a quirk of English that these concepts have the same word. Though of course your machine wouldn't like either concept since they are both subjective)
:PROPERTIES:
:Author: WarningInsanityBelow
:Score: 4
:DateUnix: 1529736818.0
:DateShort: 2018-Jun-23
:END:


*** I had another thought about FAI. Are you sure it's such a good idea to create one? Even if you defined its friendliness as carefully as possible, it could still have pretty dramatic and, in retrospect, bad consequences. For example, in [[http://localroger.com/prime-intellect/][The Metamorphosis of Prime Intellect]], (MINOR SPOILERS) a FAI bootstraps itself into omnipotence, then does a lot of things that technically achieve the utility function of reducing harm to humans, but in doing so, it uploads everyone to the cloud and doesn't allow anyone to come to harm or die even if they want to, deletes large sections of reality to improve its processing power, and gets kills off all extraterrestrial life, since it might one day threaten humanity, resulting in the total annihilation of tons of sapient species.

Furthermore, even if you did create a FAI so carefully that it would never do any of that stuff, what if it reproduced and its offspring was an asshole? Or what if someone with an incompatible utility value got their hands on its code and made an evil twin? It's a dangerous Pandora's Box to open.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529738153.0
:DateShort: 2018-Jun-23
:END:

**** u/vakusdrake:
#+begin_quote
  Even if you defined its friendliness as carefully as possible, it could still have pretty dramatic and, in retrospect, bad consequences.
#+end_quote

See all the examples you mention are the results of somewhat obvious failures with regards to its utility function. Also FAI or really nearly any AGI don't create new AI with different utility functions because it might threaten the fulfillment of its own utility function.
:PROPERTIES:
:Author: vakusdrake
:Score: 6
:DateUnix: 1529738751.0
:DateShort: 2018-Jun-23
:END:

***** I'll concede the point that the AI would refrain from reproduction to avoid the possibility of its offspring harming humanity, but I think my other point is valid, even if the examples I cited were obvious. The examples are meant to illustrate that an AI would have very different thought processes than people do, and that no matter how careful we are, it's almost impossible to think of every single possible contingency. I mean, in all the discussions on [[/r/rational][r/rational]] about writing utility functions for AI, have you ever seen someone suggest that AI also protect alien life? I'm not saying that if people sat down and actually created one, they wouldn't think of it (after all, the author of the story thought of it), but it's just an example of a blind spot.

I'm suggesting that there are unknown unknowns that no human has ever or would ever conceive of, that could still have devastating consequences if not addressed.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529741795.0
:DateShort: 2018-Jun-23
:END:

****** u/ShiranaiWakaranai:
#+begin_quote
  protect alien life?
#+end_quote

There's usually something along the lines of "protect sentient/sapient/intelligent life", which includes any alien life we care about. It can wipe out alien microbes for all we care (unless of course, microbes are sentient).
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529743912.0
:DateShort: 2018-Jun-23
:END:

******* So it destroys 1 billion alien species that would otherwise have evolved into altruistic inventors who would maximize everyone's utility functions a few eons down the road.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529744150.0
:DateShort: 2018-Jun-23
:END:

******** Does it know that those alien species would have maximized everyone's utility functions? If so, it would have let them live, since that is the method of maximizing utility. If not, then there's no real evidence that those alien species would have evolved, so wiping them out is fair game.

After all, every single action could potentially give rise to or prevent some maximally happy outcome, thanks to the butterfly effect. So if you weren't allowed to take actions that could prevent happy outcomes even though you have no evidence to suggest that that is the case, you wouldn't be able to take any action at all.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 3
:DateUnix: 1529745276.0
:DateShort: 2018-Jun-23
:END:


****** The underlying issue here is that you can make your definition of friendliness include judging friendliness the same way as you, which means it by definition it can't end up inadvertently not friendly by your standards.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1529748034.0
:DateShort: 2018-Jun-23
:END:


**** A friendly ai wouldn't do anything bad unless this was the least bad available option (modulo knowledge and computational constraints). If it did, it wouldn't be friendly (e.g. intellect prime is not friendly). Unfortunately we don't have any precise definitions of friendly, this is the reason why I thought you wouldn't allow my second question.
:PROPERTIES:
:Author: WarningInsanityBelow
:Score: 3
:DateUnix: 1529740199.0
:DateShort: 2018-Jun-23
:END:

***** I would say that with its omniscience it would be aware of everything ever said or written that defines "Friendly AI" and, combined with its knowledge of you, come up with a definition that fits your best understanding of what it means. And as I said in my response to [[/u/vakusdrake]]'s comment, there may be things that would never occur to any human to include in the definition of FAI, that would nevertheless have serious negative consequences (one of the examples I gave was of a FAI annihilating all alien life because one of its directives was to protect human life).
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529742065.0
:DateShort: 2018-Jun-23
:END:


** We already know that the answer is 42, why wait another few million years?
:PROPERTIES:
:Score: 13
:DateUnix: 1529734355.0
:DateShort: 2018-Jun-23
:END:


** pfft re: your edit far worse can be done

#+begin_quote
  Please utter the sequence of values whose utterance in response to this question will globally maximize my utility function... within my future light cone... averaged across all Everett branches?
#+end_quote

I'd reckon the next most likely question would be something like "what is the shortest (but extremely well documented and commented?) source code written in an existing programming language and capable of being compiled into a program executable on existing hardware that will, in the shortest amount of time, bring into existence a recursively improving general artificial intelligence whose existence will maximally satisfy my values and whose values are maximally aligned with my own" or something lol idk

edit: haha called it! ;p
:PROPERTIES:
:Author: phylogenik
:Score: 9
:DateUnix: 1529733917.0
:DateShort: 2018-Jun-23
:END:

*** I was going to say you'd have to specifically define your values and what your utility function is, but of course, it's omniscient, so it would already know that information.

I'm also considering a bit limit, since the point of the question is basically, what information has the most value per bit.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 6
:DateUnix: 1529735121.0
:DateShort: 2018-Jun-23
:END:

**** I think the bit limit wouldn't do anything to prevent the first sort of question -- though, now that I think about it, with a sufficiently small limit there's no "guarantee" that the "do what I mean" sorts of questions are the best to ask, right? Since they're noncontextual, and I can't imagine my behavior changing substantially with the receipt of any of, say, 2^{10} possible ordered sets of 10 bits? Unless maybe I make it -- e.g., say I remain blind to the content of the answer, and then when I have some big, uncertain decision to make, I specify and designate my binary options (in the excluded middle sense, doing something and not doing something) and then "uncover" one of the bits and blindly do whatever action (or inaction) it corresponds to.

Trivially, I could make some $ on high stake roulette, or less trivially become head of state or something and use it to decide whether to wage war. I don't think I could "reuse" bits, even if they fade from conscious memory, since doing so would couple decisions and have to average utility across that (potentially suboptimal) coupling. I could maybe even force certain outcomes if I precommit to doing something really preference-frustrating in the event of the outcome I don't want? or maybe not, actually.
:PROPERTIES:
:Author: phylogenik
:Score: 3
:DateUnix: 1529736636.0
:DateShort: 2018-Jun-23
:END:

***** I edited the OP to exclude AI source codes, since that is basically the "wish for more wishes" answer to the prompt. But that wouldn't exclude questions that would assist with the creation of FAI, like "what currently unknown computer programming concept or development, if explained today, would most reduce the time it takes us to create a FAI?"
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 2
:DateUnix: 1529737076.0
:DateShort: 2018-Jun-23
:END:


** Clarification request: Under what circumstances do I have this opportunity to question this omniscient oracle, and what form does the answer take?

If I have to ask the question on the spot without preparation, and the answer is an immediate verbal response, then that would have a major impact on the people asking the oracle to write software for them. I doubt any ordinary human can memorize the complete code for an artificial intelligence after hearing it read aloud once.
:PROPERTIES:
:Author: Tommy2255
:Score: 4
:DateUnix: 1529735311.0
:DateShort: 2018-Jun-23
:END:

*** There is no time limit on when you have to ask the question, and it will be given to you in any format you choose, including digital/searchable. So if you wanted you could hold a worldwide summit of scientists and world leaders to spend years debating or refining the question, or you could just ask it right now if you really did have a shot with Mary from your 11th grade class.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 3
:DateUnix: 1529735851.0
:DateShort: 2018-Jun-23
:END:


** Does P=NP
:PROPERTIES:
:Author: GCU_JustTesting
:Score: 4
:DateUnix: 1529741567.0
:DateShort: 2018-Jun-23
:END:


** "Give me the proof or refutation of P=NP."

I think this is at least a good answer to the question given your restraints, if not absolutely optimal. It has a concrete answer and isn't asking to solve all my problems for me, but can still in effect help do so through giving the algorithm for solving any mathematical proof and giving the avenue to make basically all programming (excluding the AI ethics bits) problems trivial.

Well, it would suck if P doesn't equal NP, or the general solution requires a googol operations, but the potential reward is so high that you can risk merely learning an interesting piece of mathematical knowledge and getting the million dollar Millennium Prize money.
:PROPERTIES:
:Author: sicutumbo
:Score: 3
:DateUnix: 1529738030.0
:DateShort: 2018-Jun-23
:END:

*** Though if you're in it for the money, you might as well just ask for lottery numbers.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529742666.0
:DateShort: 2018-Jun-23
:END:

**** Or the location of valuable undiscovered natural resources, or the chemical formula of a substance that can cure ___.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 2
:DateUnix: 1529743213.0
:DateShort: 2018-Jun-23
:END:


*** This was the first answer I was expecting, actually. And if it tells you that P doesn't equal NP then at least we know that and can avoid wasting resources on the question. There's probably a lot of other implications to knowing that for sure that would be helpful in ways I haven't thought about.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529739123.0
:DateShort: 2018-Jun-23
:END:


** u/vakusdrake:
#+begin_quote
  Edit: Multi-part questions will be interpreted as separate questions, so anything after the first part will be disregarded.
#+end_quote

This doesn't really work as a limitation. Just specify a question whose answer must necessarily include the answers to any other questions you want answered, thus meaning your only real limit here is needing to generate all your questions up-front.

I'd also like to point out that the previously mentioned question "What are the words that I would benefit most from hearing you say?" would very nearly work. However you would need to add the caveat that "benefit" is defined based on your current utility function.\\
The question works because without a limit on answer length the best answer for it to give you would effectively function as Path to Victory, in fact since it's able to exploit the butterfly effect it would probably actually be vastly superior to PtV. So the most likely outcome could be it causing you to take a bunch of bizzare random seeming actions that lead to the development of a FAI with your utility function happening in a few years due to many different freak accidents.
:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1529739398.0
:DateShort: 2018-Jun-23
:END:

*** Assume that it's smart enough to work around semantic traps. If the question by its phrasing necessarily includes the answers to A, B, and C, it will identify this and only answer A. If that's not possible, it would return an error- too much information requested- ask a single question.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529742277.0
:DateShort: 2018-Jun-23
:END:

**** Again that doesn't work unless it just barrs /any/ question which outputs too much information. There's no coherent way to distinguish whether a question is made of smaller component question, because nearly any question can be presented as multiple smaller questions.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1529748222.0
:DateShort: 2018-Jun-23
:END:

***** The entity makes a judgment call with its omniscience
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529789116.0
:DateShort: 2018-Jun-24
:END:

****** The issue is that the distinction between complex questions and multiple questions is kind of nonexistent. So two people could easily come up with the /same/ question, with only one of them having constructed it out of multiple smaller questions and other having developed it from scratch.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1529856722.0
:DateShort: 2018-Jun-24
:END:

******* Right. So if you choose to ask an extremely narrow question, which could have been answered as part of a more complex, acceptable question, then you will have squandered your opportunity. On the other hand, if you construct a question that is basically a multi-part question, you won't get a multi-part answer. The entity will decide whether this is the case.

So for example, if you asked, how does photosynthesis work, the entity could tell you that process, even though it contains more than one piece of information. But if you ask, (a) how do leaves absorb sunlight energy, and (b) how do plants spend absorbed energy, then you will have phrased your question foolishly, because you'll only get an answer to either (a) or (b).

On the other hand, if you ask it to fully recount all information on plants, that will be judged too broad of a question, even though it only requested one "thing" semantically.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529858863.0
:DateShort: 2018-Jun-24
:END:

******** u/vakusdrake:
#+begin_quote
  Right. So if you choose to ask an extremely narrow question, which could have been answered as part of a more complex, acceptable question, then you've squandered the opportunity. On the other hand, if you construct a question that is basically a multi-part question, you won't get a multi-part answer. The entity will decide whether this is the case.
#+end_quote

I'm saying there's fundamentally no metric it could use to determine whether something seems like a multi-part question and the metric you seem to be using is just whether it sounds like a multi-part question to you. However that metric can be trivially subverted by just phrasing your questions better.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1529859142.0
:DateShort: 2018-Jun-24
:END:

********* I get that, and I'm saying it has enough omniscience to make a judgment call, just like you or I could. If you agree that it can interpret things like utility functions and whether an AI is friendly or beneficial, then you should also agree that it knows enough about multipart questions and semantics to make a judgment call on whether a question qualifies. At some point, such a judgment call is necessary, otherwise that defeats the entire limitation of the "single question". If multi part questions were allowed, then you could just ask it unlimited questions by using clever phrasing.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529865353.0
:DateShort: 2018-Jun-24
:END:

********** Right I'm just saying that you aren't using a consistent standard either so saying it uses the same standard as you (with your FAI analogy) doesn't fix anything.\\
Comparing "determining whether something is actually multiple questions" to friendliness doesn't really work, because it implies that there is actually some non-arbitrary metric (as in not just whatever is currently your whim) being used even if you can't articulate it/understand it without omniscience.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1529866138.0
:DateShort: 2018-Jun-24
:END:

*********** How would you suggest that the "one question" restriction be enforced, if you were in charge of imposing that restriction? I agree that my method is imperfect, but it's the best way I can think of.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529868225.0
:DateShort: 2018-Jun-24
:END:

************ Honestly I would probably just impose a limit in the number of bits that could be transmitted. However that creates the obvious problem that while that limit may be trivial for an omniscient being to know, for us knowing the exact number of bits contained within a given question is practically a intractable problem.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1529870041.0
:DateShort: 2018-Jun-25
:END:

************* I wonder how you would optimize a yes or no question with a guaranteed truthful answer?
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529870386.0
:DateShort: 2018-Jun-25
:END:

************** Hmm that gives me an idea.. I don't actually think you could do anything very useful with just a single yes-no question (at least if you had no way of proving to others this happened).

However I think you could probably make the oracle useful if you simply limited it to giving you some finite number (say 20) of yes no question. Yes it would sort of change the premise however it would also eliminate the problems that the limitation on multiple merged questions was designed to deal with in the first case.

Additional limits may include having to merge the X# questions into a single question, or forcing people to ask their questions all within some short timespan. This would allow people to take their time coming up with good questions but not let them employ many additional exploits available to them if they could space out their questions over any amount of time.

P.S. If you were going with this and you wanted to actually have it be on the same level of usefulness as you probably had in mind with regards to the original scenario you'd probably want to give people a fair deal more than twenty questions.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1529871744.0
:DateShort: 2018-Jun-25
:END:


** "What message when posted online and linked where I will link it, will make the vast majority of humanity completely dedicate themselves to a non-counterproductive policy that maximizes the probability of FAI within the next 100 years."
:PROPERTIES:
:Author: ArmokGoB
:Score: 3
:DateUnix: 1529739953.0
:DateShort: 2018-Jun-23
:END:

*** This seems dangerous, because maybe the policy that maximizes the probability of an FAI is to spur people into recklessly rushing out AIs, and so also increase the probability of a UFAI.

E.g. Maybe before your message, the probability of FAI in 100 years is 10%, UFAI is 20%, and no AI is 70%. There could be a message advocating careful coding that leads to 15% FAI, 5% UFAI and 80% no AI, which is what you would want. But then there could be a message advocating rushed coding that leads to 20% FAI 80% UFAI, which has a higher FAI probability and so is the message you are given.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529743012.0
:DateShort: 2018-Jun-23
:END:

**** That's what "non-counterproductive" means. Also, we seem to vastly disagree what the probabilities before the message is; I'd say closer to 2% FAI, 1% no AI, and 97% UFAI. That last one split into something like 88% everyone simply dies and the future value of the universe is exactly 0, and 9% something unimaginablly malevolent with a million times more suffering than the worst hells ever imagined by humanity.
:PROPERTIES:
:Author: ArmokGoB
:Score: 2
:DateUnix: 1529750984.0
:DateShort: 2018-Jun-23
:END:

***** u/ShiranaiWakaranai:
#+begin_quote
  That's what "non-counterproductive" means.
#+end_quote

What exactly does that mean though, quantitatively? Is any policy that increases the chance of a UFAI considered counterproductive? Is there some ratio threshold of FAI to UFAI probability that a policy must have to be non-counterproductive? If the restrictions are too tight, you might end up with policies that have very weak effects that barely change any of the probabilities.

#+begin_quote
  Also, we seem to vastly disagree what the probabilities before the message is;
#+end_quote

Eh, they were just numbers I chose to illustrate the problem. My real opinion is 0% FAI 99% UFAI 1% some catastrophic event(s) wipes out all/most of humanity before they build a UFAI, simply because I don't believe FAIs are possible. I can't use this for the example since every policy would have no effect on the probability of an FAI.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529776669.0
:DateShort: 2018-Jun-23
:END:

****** Yea, that could happen, but it seem unlikely given my priors. It's not perfect by definition, anything that is would be against the spirit of the rules.
:PROPERTIES:
:Author: ArmokGoB
:Score: 2
:DateUnix: 1529782129.0
:DateShort: 2018-Jun-23
:END:


*** You should change "vast majority" to "highest possible fraction" or something, to avoid the answer, "no such message exists."
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529742517.0
:DateShort: 2018-Jun-23
:END:

**** Maybe, but if I do that it contains multiple lose variables and underspecifies how to do tradeoff between them
:PROPERTIES:
:Author: ArmokGoB
:Score: 2
:DateUnix: 1529750666.0
:DateShort: 2018-Jun-23
:END:


** I am assuming an anthropocentric metric of benefit for the sake of time, brevity, and to avoid obvious monkey's paws.

My top choices are:

1. "Provide a complete Standard Model such that it accounts for as many phenomena as possible with the highest possible degree of accuracy." In addition to gravity, dark matter, and energy this takes care of any unobservables that we would otherwise never be able to fully understand.
2. "What is the genetic code of an organism that would provide the greatest benefit to humanity?"
3. "What is a safe method to optimize human intelligence?"
4. "What series of actions can we reasonably perform that will maximize the long-term probability of humanity's satisfaction and survival?" If it is possible to survive in perpetuity (e.g. avoiding heat death) these answers will be preferentially selected. If our extinction is inevitable we don't waste an answer on a response like "you can't."

If necessary, we can avoid being given answers we could never use by adding to the quotes above: "...that humanity will have a 100% chance of utilizing to our greatest benefit before extinction, the end of the universe, or a maximum of [[http://googology.wikia.com/wiki/Graham%27s_number][Graham's number]] of years, whichever is soonest." Probability takes care of failure during construction from all sources, so finding the *most* beneficial option requires playing the long game, but not so long that the universe dies before we have a few billion years to benefit from the results. If heat death ends up not being the inevitable fate of everything, perhaps as a result of the answer we receive, setting an arbitrarily high duration eliminates responses that would have the highest theoretical benefit but could not be fully realized in a finite amount of time.

Edit: [[/u/erotica][u/erotica]]'s answer takes the prize in my opinion, but hopefully this will provide you with a few more specific examples to think about :).
:PROPERTIES:
:Author: brbrainerd
:Score: 2
:DateUnix: 1529739915.0
:DateShort: 2018-Jun-23
:END:

*** 1. You should add a caveat such that if no such model exists, provide the model that accounts for the most possible phenomena.

2. I was thinking about this one when other people were asking for AI source code. After all, the intelligence doesn't have to be a computer. But it would be tragic if we never developed the technology to actualize the genetic code into a healthy organism. And it'd be hilarious if when we did, it just turned out to be Jesus.

3. Stay in school, kids!

4. It could tell you the single most beneficial action or the first action in the series you requested, but asking for the whole series would count as a multi-part question.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 3
:DateUnix: 1529742875.0
:DateShort: 2018-Jun-23
:END:

**** u/brbrainerd:
#+begin_quote
  You should add a caveat such that if no such model exists, provide the model that accounts for the most possible phenomena.
#+end_quote

Good catch. I also added "to the highest possible degree of accuracy," though it is possible that we would receive a highly probabilistic model with less than optimal utility (not unlike the model we have today ;) ).

#+begin_quote
  But it would be tragic if we never developed the technology to actualize the genetic code into a healthy organism.
#+end_quote

I think the final paragraph takes care of that.

#+begin_quote
  And it'd be hilarious if when we did, it just turned out to be Jesus.
#+end_quote

Despite my (lack of) religious beliefs, I would watch the hell out of that sci-fi.

#+begin_quote
  asking for the whole series would count as a multi-part question.
#+end_quote

Perhaps asking for the most beneficial overall strategy, instead of a rote series of actions, would result in a succinct but complete answer?
:PROPERTIES:
:Author: brbrainerd
:Score: 2
:DateUnix: 1529744047.0
:DateShort: 2018-Jun-23
:END:

***** u/pizzahotdoglover:
#+begin_quote
  Despite my (lack of) religious beliefs, I would watch the hell out of that sci-fi.
#+end_quote

Lol yeah, that's actually how the Second Coming of Jesus comes about. Who knew?

#+begin_quote
  Perhaps asking for the most beneficial overall strategy, instead of a rote series of actions, would result in a succinct but complete answer?
#+end_quote

That would definitely work.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 2
:DateUnix: 1529744302.0
:DateShort: 2018-Jun-23
:END:


*** u/ShiranaiWakaranai:
#+begin_quote
  "What series of actions can we reasonably perform that will maximize the long-term probability of humanity's satisfaction and survival?" If it is possible to survive in perpetuity (e.g. avoiding heat death) these answers will be preferentially selected. If our extinction is inevitable we don't waste an answer on a response like "you can't."
#+end_quote

Suppose the omniscient being does give you a correct answer for this. How would you convince the rest of humanity to follow those actions though? You can't really prove that you got the answer from an omniscient being, since it disappeared after you asked it that one question.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 3
:DateUnix: 1529743444.0
:DateShort: 2018-Jun-23
:END:

**** Sounds like [[/u/brbrainerd]] would be the tragic love child of Cassandra and Accord.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 2
:DateUnix: 1529743918.0
:DateShort: 2018-Jun-23
:END:


**** If that's unaccounted for by my use of the term "reasonable," then I believe the probability failsafe in the final paragraph will steer us around this issue. Instructions that are unpersuasive or otherwise non-communicable would necessarily have a low probability of overall success.
:PROPERTIES:
:Author: brbrainerd
:Score: 2
:DateUnix: 1529744300.0
:DateShort: 2018-Jun-23
:END:

***** I think for the sake of the prompt, we can assume that people will be aware of the omniscient entity's offer and omniscience. Otherwise, most answers would be, Step 1: Become dictator...
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529744619.0
:DateShort: 2018-Jun-23
:END:


** "What is the code for a program that will answer any question I ask of it correctly (if it complies with the above rules)?"

Such a program exists, because you can simply program a massive look-up table for every possible question with their answers as coded constants. It is not an AI, because it isn't smart, it's just looking up a table. It isn't a multi-part question. It is an extremely narrow question, because the code given either works or does not. So it should comply with all rules and thus result in getting all answers to all questions that comply with the rules.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529741723.0
:DateShort: 2018-Jun-23
:END:

*** Congratulations, you've munchkined your way into unlimited knowledge. Here is your infinitely long code. It may take some time to enter into your computers and compile. But really, since it includes the answers to every possible question, I think this would just be interpreted as a multi-part question. In other words the restriction is less, "no multi-part questions" as it is, "no questions that require numerous answers."
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529743439.0
:DateShort: 2018-Jun-23
:END:

**** As others have mentioned, that is not a coherent restriction. Or at least making it coherent is non trivial, how do you distinguish a single answer? Many questions can be broken up into simpler ones, and a way to demarcate questions that would include giving the source code for a FAI, but exclude the other answers to me seems like it'll be highly contrived.
:PROPERTIES:
:Score: 2
:DateUnix: 1529752312.0
:DateShort: 2018-Jun-23
:END:

***** As I mentioned, the entity makes a judgment call.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 3
:DateUnix: 1529789411.0
:DateShort: 2018-Jun-24
:END:


** "How can I acquire as much knowledge as possible after I ask this question?"

Should hopefully result in something along the lines of:

"By listening very carefully to the following information:" <insert all knowledge known by the omniscient being>
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529742264.0
:DateShort: 2018-Jun-23
:END:


** If I can become omniscient, what is the most efficient/optimal method for me to become so?
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 2
:DateUnix: 1529742882.0
:DateShort: 2018-Jun-23
:END:

*** I guarantee your answer will be along the lines of "You can't."
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529743092.0
:DateShort: 2018-Jun-23
:END:

**** Nah. If it's possible to become omniscient, then it's possible to becom ominscient.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 2
:DateUnix: 1529744298.0
:DateShort: 2018-Jun-23
:END:

***** But according to the rules, we're supposed to ignore the existence of the omniscient being for our question.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529745088.0
:DateShort: 2018-Jun-23
:END:

****** Regardless of whether or not thre is an ominscient being, if it is possible to become omniscient, then it is possible to become omniscient.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 2
:DateUnix: 1529745506.0
:DateShort: 2018-Jun-23
:END:


** How do I build HLMI?\\
How do you align arbitrary level artificial intelligence with human goals? (Alignment problem)

I'm sure there are several other "impossible" problems which if you knew the answer to would change life as we know it.
:PROPERTIES:
:Score: 2
:DateUnix: 1529751515.0
:DateShort: 2018-Jun-23
:END:


** "How can I become omniscient myself?"
:PROPERTIES:
:Author: King_of_Men
:Score: 4
:DateUnix: 1529734260.0
:DateShort: 2018-Jun-23
:END:

*** The answer might just be: "you can't"
:PROPERTIES:
:Author: WarningInsanityBelow
:Score: 13
:DateUnix: 1529734396.0
:DateShort: 2018-Jun-23
:END:


*** Or, "what could I say to persuade you to answer additional questions?" Still risky, because the answer might be, "there is no way to do that."
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 7
:DateUnix: 1529735014.0
:DateShort: 2018-Jun-23
:END:

**** Or it might tell you, then disappear before you can do anything with the information
:PROPERTIES:
:Author: sicutumbo
:Score: 4
:DateUnix: 1529737419.0
:DateShort: 2018-Jun-23
:END:

***** Wait come back! Fuck. I should've asked for the Grand Unified Theory of Everything.
:PROPERTIES:
:Author: pizzahotdoglover
:Score: 2
:DateUnix: 1529738564.0
:DateShort: 2018-Jun-23
:END:
