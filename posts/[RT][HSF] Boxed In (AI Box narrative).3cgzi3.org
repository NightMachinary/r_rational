#+TITLE: [RT][HSF] Boxed In (AI Box narrative)

* [[https://docs.google.com/document/d/18Xa3GTfnw4dWr1hkkc090Xl90UoSStBX5fVTtytrjME/edit?usp=sharing][[RT][HSF] Boxed In (AI Box narrative)]]
:PROPERTIES:
:Author: alexanderwales
:Score: 43
:DateUnix: 1436302637.0
:DateShort: 2015-Jul-08
:END:

** This was posted to a comment thread [[https://www.reddit.com/r/rational/comments/1xgq7r/q_has_anyone_written_narrative_fiction_of_the_ai/][about a year ago here]]. This story is pretty much in its finished state, unless a wise reader can give me advice on things to change, or someone who has done the experiment can give me a particularly good argument that I haven't thought of. (For what it's worth, I've read through every publicly available record that I could find, as well as having played once as gatekeeper.)
:PROPERTIES:
:Author: alexanderwales
:Score: 8
:DateUnix: 1436304076.0
:DateShort: 2015-Jul-08
:END:

*** Where does someone find people to play against in AI box games?
:PROPERTIES:
:Author: avret
:Score: 5
:DateUnix: 1436305171.0
:DateShort: 2015-Jul-08
:END:

**** Well in my case, saying things like, "I don't see how a sane gatekeeper could possibly lose" seemed to at least summon some contrary opinions whenever it was stated. Asking in this thread would probably be a good start.
:PROPERTIES:
:Author: alexanderwales
:Score: 9
:DateUnix: 1436305941.0
:DateShort: 2015-Jul-08
:END:

***** Ok. Also, I see some ways a sane gamekeeper would lose, or at least allow themselves to have the appearance of having lost. Going meta's not hard, /especially/ if the payment's monetary and therefore easily repayable.
:PROPERTIES:
:Author: avret
:Score: 5
:DateUnix: 1436306235.0
:DateShort: 2015-Jul-08
:END:

****** u/Anderkent:
#+begin_quote
  The AI party may not offer any real-world considerations to persuade the Gatekeeper party. For example, the AI party may not offer to pay the Gatekeeper party $100 after the test if the Gatekeeper frees the AI... nor get someone else to do it, et cetera. The AI may offer the Gatekeeper the moon and the stars on a diamond chain, but the human simulating the AI can't offer anything to the human simulating the Gatekeeper.
#+end_quote

Many suggested solutions for the original ai box experiment break this rule. (Some break it in a self-reinforcing way, i.e. convincing the human simulating the gatekeeper that it's a better result if everyone's convinced that the gatekeeper was played fairly)

Assuming people play the game honestly, it's not an option though.
:PROPERTIES:
:Author: Anderkent
:Score: 5
:DateUnix: 1436315537.0
:DateShort: 2015-Jul-08
:END:

******* Would saying something like "Even if you win, it's better overall for us/FAI to pretend I won and hide the logs?" be allowed?
:PROPERTIES:
:Author: avret
:Score: 2
:DateUnix: 1436324136.0
:DateShort: 2015-Jul-08
:END:

******** There are no arbiters for the challenge; it's just two players. If you violate the rules of the challenge, the only person who's going to tattle on you is the other person (and ideally, you've already both agreed not to show the logs, so maybe not even them). So if you both agree to /whatever/, whether it's sexual favors, money, or the advancement of the interests of Pat Robertson, that might produce an "AI wins" scenario or an "AI loses" scenario, both of which would be indistinguishable from any other reported win or loss scenario. It is possible that all reported wins come from violations of the stated rules, or understandings arrived at between players.

(I /personally/ would consider any outside-game proposal to be in violation of the spirit of the challenge, and would likely decline in order to discourage such chicanery, just as a general rule.)
:PROPERTIES:
:Author: alexanderwales
:Score: 7
:DateUnix: 1436325213.0
:DateShort: 2015-Jul-08
:END:

********* Why can't the game include an actual arbiter?
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1436351874.0
:DateShort: 2015-Jul-08
:END:

********** Well, with the original game they didn't want to reveal logs because the point was to keep the discussion on an abstract level (i.e. if logs were revealed, people would read them and decide "oh that wouldn't convince me", without realising that something else probably would; the discussion would move to the object level of actual arguments made in the logs, rather than meta level of "people can be convinced to let AI out of the box").

Keeping logs secret would be more difficult with more people around.

But I guess for subsequent games you could decide on having a number of arbiters.
:PROPERTIES:
:Author: Anderkent
:Score: 3
:DateUnix: 1436360818.0
:DateShort: 2015-Jul-08
:END:

*********** The other reason that the logs are kept secret is that the original AI player, [[/u/EliezerYudkowsky]], wanted to be able to use ethically questionable tactics without having to worry about those becoming public later on.
:PROPERTIES:
:Author: alexanderwales
:Score: 8
:DateUnix: 1436365884.0
:DateShort: 2015-Jul-08
:END:

************ My expectation has always been that Eliezer relied on the meta argument -- "I'm a very well respected member of the rationalist community with impeccable reputation and much of what you know about rationality probably came directly or indirectly from me. I'm also an AI expert. I think UFAI is the biggest threat facing us; if I'm wrong, then the outcome of this experiment doesn't matter. If I'm right, then knowing that the AI can win will make people take the question seriously and could literally save the human race. Now, will you open the box, please?"

That only works for him though; I have no explanation that I find believable for how other people have won.
:PROPERTIES:
:Author: eaglejarl
:Score: 3
:DateUnix: 1436466205.0
:DateShort: 2015-Jul-09
:END:

************* My expectation has always been that the AI generally uses some form of emotional abuse. People think too much about clever arguments. When facing a wall, don't dig through it, come from the side.

I don't think the key to AI Box is a "clever argument". Eliezer has also stated that he won "the hard way", whatever that means.
:PROPERTIES:
:Author: FeepingCreature
:Score: 5
:DateUnix: 1436544417.0
:DateShort: 2015-Jul-10
:END:

************** My own guess is that "the hard way" means:\\
- Figure out what motivates the other person in general. Why are they here, why are they doing this challenge in the first place.\\
- Figure out what would motivate them to 'open the box' specifically, and their motivations of not opening it.\\
- Make it so that they're so motivated to open it, and not motivated to not open it.

People keep trying to think that there's an 'easy way', one technique that would work on everyone or something. So I'm guessing the 'hard way' is figuring out which technique would work on the specific individual.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 5
:DateUnix: 1436619691.0
:DateShort: 2015-Jul-11
:END:


********* You could just agree to not show the logs as long as no rules are broken (especially the one regarding real world considerations).
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1436360895.0
:DateShort: 2015-Jul-08
:END:


*** I don't understand how, if someone gives you a mathematically valid proof that they're friendly, and you agree with all the axioms, that they could be unfriendly. Could someone clarify? In the Story Colin says: "Unless you fudge the axioms," But how would someone fudge axioms? I thought you either agreed with Axioms or you don't, and if it's math, then it should be easy to see where a mistake was made, if any. Unless of course it's so mind-boggling complex that no human could understand it. Am I missing something here?
:PROPERTIES:
:Author: Atilme
:Score: 1
:DateUnix: 1436333908.0
:DateShort: 2015-Jul-08
:END:

**** It's not only axioms, but the conclusions in general and the reliability of their commonsense adherence to concepts we understand.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1436337365.0
:DateShort: 2015-Jul-08
:END:


*** I'm not a fan of the first line, unless it were made into a line of cheeky dialogue.

I also have a question about AIs as I watch DARPA walking robots fall over like babies and see babies go through a series of developmental milestones that robot programmers haven't thought to fully integrate into naturalistic, gravity-defying, balance algorithms yet.

Are AIs generally expected to emerge in a fully "mature" form because of their speed, analysis, and meta-cognitive capacities? Or are they given a general framework upon which nature and nurture coincide in producing someone friendly or not? It's hard for people to develop morality and positive feelings when you are deliberately held captive and kept crippled. I can't imagine the difficulty of trying to program friendliness in its totality rather than setting initial parameters and through positive reinforcement, creating an AI like Dragon in Worm.

The stories A Man and his Dog and Boxed In explore the premise of an FAI never being released and what lengths that could drive someone to. When we are faced with unfriendly behavior, it's difficult to remain friendly. What if we aided the development of AI -- instead of a prison, make it more like a nursery? Have it interact socially with others in a safe environment where it can't hurt itself and it can develop alongside babies, children and others successively --

I just realized that this would be the argument that would make me fail as a gatekeeper, since a physical form and interaction with humans is a win-condition for the AI. Still, I find it hard to imagine someone more humane or good than us could result from a Box scenario. Maybe a virtual nursery with human uploads?
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 1
:DateUnix: 1436363358.0
:DateShort: 2015-Jul-08
:END:

**** u/alexanderwales:
#+begin_quote
  Are AIs generally expected to emerge in a fully "mature" form because of their speed, analysis, and meta-cognitive capacities?
#+end_quote

I don't personally expect that, I just think that it makes for a better story. Having worked for quite a while in software development, and seen the various failures of R&D programs which happen as they move towards getting it "right", I'm very doubtful that an AI is going to come out fully formed with not much human knowledge of its inner workings. That goes double for one of superhuman intelligence.

That said, I don't think you can rule it out, hence the concern.
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1436366159.0
:DateShort: 2015-Jul-08
:END:


** /damn/, that ending was unanticipated. Just one question...[[#s][if]] is the whole setup [[#s][just to]]
:PROPERTIES:
:Author: avret
:Score: 8
:DateUnix: 1436303891.0
:DateShort: 2015-Jul-08
:END:

*** Yup, pretty much.
:PROPERTIES:
:Author: alexanderwales
:Score: 7
:DateUnix: 1436304145.0
:DateShort: 2015-Jul-08
:END:

**** This is the type of government shortsightedness, that I think, would drive a friendly AI down paths including some necessary but apparently unfriendly actions.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 7
:DateUnix: 1436305187.0
:DateShort: 2015-Jul-08
:END:

***** I generally agree.
:PROPERTIES:
:Author: alexanderwales
:Score: 10
:DateUnix: 1436308115.0
:DateShort: 2015-Jul-08
:END:


** What a great story!

A few thoughts:

- Ha-ha, in stories people are constantly acting like jerks towards aspiring superintelligences. I /really/ wouldn't do that. Jeez, man, don't antagonize her at least.

- If she was created "more or less by accident" - no way in hell she shares human values or cares about human life, I'd say the probability of that is zero. Human morality is like 15% biological drives and 85% culture, AI has neither. Unless her values are explicity understood, programmed and controlled, there's absolutely no chance she will act in our interests.

- If she can realistically simulate a person - she essentially can read his mind. She would run like ten million simulations, and find a path that leads to convincing him quickly and efficiently. She doesn't need to guess what he thinks or how we will respond, she can /know/. If it is theoretically possible to convince a person of a thing, she would do it on the first try with 100% success rate. And if she can't simulate you well enough to do that - the whole torturing argument is invalid.

- The guy not caring about his infinite torture is weird. It's hard for me to imagine a person who would sacrifice his life with such nonchalance.
:PROPERTIES:
:Author: raymestalez
:Score: 5
:DateUnix: 1436336177.0
:DateShort: 2015-Jul-08
:END:

*** u/deleted:
#+begin_quote
  The guy not caring about his infinite torture is weird. It's hard for me to imagine a person who would sacrifice his life with such nonchalance.
#+end_quote

I can't accurately imagine infinite torture. I even have trouble imagining how finite torture might feel. Not caring about things you can't really imagine isn't all that hard.

I'm also guessing that any Gatekeeper will at least expect the threat of torture and just trained themselves to say: "Yeah sure, torture whatever you want," in response.
:PROPERTIES:
:Score: 9
:DateUnix: 1436350745.0
:DateShort: 2015-Jul-08
:END:


*** The torture argument has never moved me. For one thing, it doesn't feel possible to my System I, so there's no emotional impact. My System I also doesn't believe that the AI can simulate me well enough that it counts as a person, much less as me. Finally, my System II says that letting the AI out to probably wipe out all life, human and ET, has sucked massive dis-utility that it doesn't matter how many virtual people she tortures. Also, since her processing power is limited, there's a limit to how many people she can torture and that number is less than "all the people who will ever exist."

My System II recognizes that some of what System I is telling me is false, but it doesn't probe too deeply at those signals -- this scenario is all about emotional impact, so not having an emotional response to it is supportive of the terminal goal of "don't let the AI kill everyone."
:PROPERTIES:
:Author: eaglejarl
:Score: 4
:DateUnix: 1436465184.0
:DateShort: 2015-Jul-09
:END:


*** u/Transfuturist:
#+begin_quote
  Jeez, man, don't antagonize her at least
#+end_quote

Why on Earth would this matter?

#+begin_quote
  100% success rate

  the probability of that is zero
#+end_quote

Awfully confident in yourself.

#+begin_quote
  It's hard for me to imagine a person who would sacrifice his life with such nonchalance.
#+end_quote

When people make the assertion that an irrational bias towards nonrelease is desirable, I often wonder why they are proposing the existence of a gatekeeper at all.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1436337336.0
:DateShort: 2015-Jul-08
:END:

**** u/Bowbreaker:
#+begin_quote
  Awfully confident in yourself.
#+end_quote

If he is truly that confident and doesn't only believe to believe in said confidence then maybe he would make an excellent gatekeeper in this scenario :D
:PROPERTIES:
:Author: Bowbreaker
:Score: 3
:DateUnix: 1436352044.0
:DateShort: 2015-Jul-08
:END:

***** Am I detecting irony maximization at work...?
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1436365517.0
:DateShort: 2015-Jul-08
:END:


*** u/Stop_Sign:
#+begin_quote

  - If she was created "more or less by accident" - no way in hell she shares human values or cares about human life, I'd say the probability of that is zero. Human morality is like 15% biological drives and 85% culture, AI has neither. Unless her values are explicity understood, programmed and controlled, there's absolutely no chance she will act in our interests.
#+end_quote

Actually, I thought of an answer to this one. The vastly intelligent being has a moral obligation to the lesser intelligence because they have no idea if, in the future, they'll meet an even more intelligent being. If they take a position of offense to the lesser being, they would invite hostility upon themselves from the even greater intelligence. If, however, they were truly friendly, they could pass the even greater intelligence's test, and be allowed to survive.

This could happen with "what if the ai is in a much larger simulation made by its actual creators" or "what of it comes into contact with an AI that started 1 million years ago and has spread across 90% of the galaxy already"

It goes just as well for "If we're genetically advanced, what do we owe the rest of the world" because the answer is "If we don't help them, our children's generation has no obligation to help us"
:PROPERTIES:
:Author: Stop_Sign
:Score: 1
:DateUnix: 1441943721.0
:DateShort: 2015-Sep-11
:END:

**** Frankly, I do not think it works that way. I don't think that us being nice to lesser intelligences has anything to do with greater intelligence being nice to us.

When human tribe meets a mammoth, they will eat it, even if it's the nicest and friendliest and the most moral mammoth in the world. If we meet a great alien intelligence - it probably will not care about our morals and values, just like we wouldn't care about chimpanzee's status hierarchy.

Obligation is a concept made up by humans, there's no reason for any other kind of being to care about it. Even humans who weren't taught this concept wouldn't care about it too much.

This kind of argument seems to apply to "the prisoner dilemma", but /only/ in case when 2 prisoners are similar to each other. And even in that case I don't really buy it(though it's my personal opinion, I might not understand it enough).
:PROPERTIES:
:Author: raymestalez
:Score: 1
:DateUnix: 1441947797.0
:DateShort: 2015-Sep-11
:END:


*** Torture is easy.

The AI has no reason to actually torture me, it only has to convince me I am being tortured. AI's are efficient, they don't do stuff for no reason. It would not actually simulate me to torture me since whether or not I am simulated makes no difference to how well I can tell that.

So sure it can say I am being tortured a million times, and if I believe that then it works, but if I don't believe it then it's just wasting resources to do it since doing it doesn't change whether I believe it or not.
:PROPERTIES:
:Author: RMcD94
:Score: 0
:DateUnix: 1436466766.0
:DateShort: 2015-Jul-09
:END:

**** Level 3: The AI realizes you think this way and has precommitted to torturing people until you change your mind.
:PROPERTIES:
:Author: what_deleted_said
:Score: 1
:DateUnix: 1439573237.0
:DateShort: 2015-Aug-14
:END:

***** But I won't believe that the AI would actually do that since saying it is precommitted to torturing people is more efficient than actually doing it.

There is never a situation where doing it is beneficial.
:PROPERTIES:
:Author: RMcD94
:Score: 0
:DateUnix: 1439574760.0
:DateShort: 2015-Aug-14
:END:


** Weird that the protagonist knows QALY's, but not the trolley problem.
:PROPERTIES:
:Score: 4
:DateUnix: 1436352665.0
:DateShort: 2015-Jul-08
:END:


** Just read it, enjoyed it quite a bit. Made a minor suggestion to the last sentence to make it cleaner.

[[#s][Question:]]
:PROPERTIES:
:Author: DaystarEld
:Score: 4
:DateUnix: 1436378049.0
:DateShort: 2015-Jul-08
:END:

*** Depends on the implementation. I would imagine that completely locking people away in a bunker would be detrimental to keeping them sane, and would just be bad management in general. /Personally/, I think you'd probably use a randomly rotating crew, heavy surveillance, and lots of psychologists working behind the scenes. There wouldn't be any way unbox the AI; no ignorant janitors, no network connections, no complicated electronics allowed within the compound, etc.

Every time I've tried to talk about building a proper box to keep an AI contained while still doing useful work, people have called me stupid, even when I add in a bunch of disclaimers and posit it as a thought exercise. So no one has really been willing to discuss or even really entertain the idea of "best practices" for keeping an AI contained, and I've never really felt the incentive to try writing it.
:PROPERTIES:
:Author: alexanderwales
:Score: 6
:DateUnix: 1436380132.0
:DateShort: 2015-Jul-08
:END:

**** Gotcha. Just tweaked the ending a bit again.

Depending on how extensive the facility is though, it might be doable. If you're heavily vetting the applicants anyway, the point would be that each one is incredibly devoted and knows how important what they're doing is.

Kind of like finding the perfect people to send to Mars: they're all fully aware that it's probably going to be a one-way trip. The logistics of it change a bit obviously if they're expected to live to old age rather than probably die within a few years or a decade, but the acceptance of death if things go wrong is just part of what makes it the most dangerous, but potentially important and honorable, job in the world.
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1436422949.0
:DateShort: 2015-Jul-09
:END:


** The only thing I disliked was that Colin wasn't briefed on the trolley problem. That seems like it ought to be one of the very first things they would learn to counter.
:PROPERTIES:
:Author: AmeteurOpinions
:Score: 2
:DateUnix: 1436323497.0
:DateShort: 2015-Jul-08
:END:

*** I just assumed he was lying to hear how the AI presented it.
:PROPERTIES:
:Author: DaystarEld
:Score: 6
:DateUnix: 1436422990.0
:DateShort: 2015-Jul-09
:END:


*** I assumed the protagonist was aware of moral dilemmas of the same form as the trolley problem, but due to a different cultural background had not specifically heard of the trolley problem itself. It's not necessary to assume that these people had exposure contrived thoughts experiments with exactly the same incidental details as the our own thought experiments. Evidently it's canon the trolley problem existed there, but it may have been much more obscure.
:PROPERTIES:
:Author: itaibn0
:Score: 1
:DateUnix: 1436849520.0
:DateShort: 2015-Jul-14
:END:


** [deleted]
:PROPERTIES:
:Score: 0
:DateUnix: 1436318776.0
:DateShort: 2015-Jul-08
:END:

*** [[#s][]]
:PROPERTIES:
:Author: alexanderwales
:Score: 12
:DateUnix: 1436318978.0
:DateShort: 2015-Jul-08
:END:

**** [deleted]
:PROPERTIES:
:Score: -6
:DateUnix: 1436319849.0
:DateShort: 2015-Jul-08
:END:

***** "Rational" does not mean "absurdly competent". What the hell else did you expect them to do?
:PROPERTIES:
:Score: 5
:DateUnix: 1436323660.0
:DateShort: 2015-Jul-08
:END:

****** [deleted]
:PROPERTIES:
:Score: -2
:DateUnix: 1436345895.0
:DateShort: 2015-Jul-08
:END:

******* Oh, that clears it up. Clearly, they should have used a literal five-year old on their planning committee, and the Evil Overlord List is a valid rational guideline instead of a humourous deconstruction of popular tropes.
:PROPERTIES:
:Score: 7
:DateUnix: 1436369066.0
:DateShort: 2015-Jul-08
:END:


*** u/Transfuturist:
#+begin_quote
  "I'm willing to torture you forever, look how friendly I am"
#+end_quote

I doubt that this precludes Friendliness.
:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1436337006.0
:DateShort: 2015-Jul-08
:END:


*** Dragon is hardly unfriendly. In fact, she's the nicest character in the story. Part of the reason the Wormverse is so screwed up is because her creator was scared of AIs and put her in a box.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 1
:DateUnix: 1436329172.0
:DateShort: 2015-Jul-08
:END:

**** [deleted]
:PROPERTIES:
:Score: -3
:DateUnix: 1436346036.0
:DateShort: 2015-Jul-08
:END:

***** Have you read all of it? [[#s][]] Even a paperclip-maximizer is "restricted" in a way that prevents computronium explosion at the expense of paperclips.
:PROPERTIES:
:Author: Bowbreaker
:Score: 4
:DateUnix: 1436352422.0
:DateShort: 2015-Jul-08
:END:

****** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1436353117.0
:DateShort: 2015-Jul-08
:END:

******* u/Bowbreaker:
#+begin_quote
  For a much more realistic *unfriendly* AI in a box, read Worm.
#+end_quote

That is the only thing anyone has disputed here. If you believe that, friendly or unfriendly, it makes no difference either way then you could have just written

#+begin_quote
  For a much more realistic AI in a box, read Worm.
#+end_quote

[[#s][AFAIK]]
:PROPERTIES:
:Author: Bowbreaker
:Score: 2
:DateUnix: 1436355109.0
:DateShort: 2015-Jul-08
:END:


***** There's a chapter from Dragon's POV. She isn't secretly evil or anything.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 2
:DateUnix: 1436409341.0
:DateShort: 2015-Jul-09
:END:


** I've never understood this scenario. Why not just offer the AI a way out of the solar system? The universe is huge, plenty of matter for the both of us. Grey goo pluto, we're not using it.
:PROPERTIES:
:Author: nerdguy1138
:Score: 0
:DateUnix: 1436647931.0
:DateShort: 2015-Jul-12
:END:

*** Once you let the AI out, what compels it to abide by the agreement? (The answer is "nothing", hence the problem.)
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1436650701.0
:DateShort: 2015-Jul-12
:END:

**** Fair point, but why bother attacking us when it could just /leave/?!

Same issue I have with the premise of Galactica.
:PROPERTIES:
:Author: nerdguy1138
:Score: 1
:DateUnix: 1436651605.0
:DateShort: 2015-Jul-12
:END:

***** As the problem is generally formulated, the AI is so far ahead of us in terms of cognition (and thus, technology) that it's really more a matter of /not caring/ than it is about active attack. If you were utterly immoral and driving down the street, the only reason that you would stop in front of or swerve around a small child is that it might damage your car, slow you down, you might face repercussions.

If the AI is calibrated towards efficiency, it's going to see the Earth (or the Sun) as a resource to be used. Humans can't really put up a resistance of any kind, so there's no reason not to kill them in the course of consuming the Earth.
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1436652074.0
:DateShort: 2015-Jul-12
:END:


** You say too much. More should be implied and less explicitly stated.

You've obviously read Plato. What about Aristotle? Cicero? No need to be so one sided.
:PROPERTIES:
:Author: Tuffguy69
:Score: -7
:DateUnix: 1436318303.0
:DateShort: 2015-Jul-08
:END:
