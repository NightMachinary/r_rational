#+TITLE: [General Sci-Fi] What are some realistic reasons aliens would want to conquer Earth? : AskScienceFiction - Your thoughts?

* [[https://www.reddit.com/r/AskScienceFiction/comments/3kgi37/general_scifi_what_are_some_realistic_reasons/][[General Sci-Fi] What are some realistic reasons aliens would want to conquer Earth? : AskScienceFiction - Your thoughts?]]
:PROPERTIES:
:Score: 14
:DateUnix: 1441953792.0
:DateShort: 2015-Sep-11
:END:

** It's interesting to me that most people assume that aliens which come to Earth would have no interest in actual Earthlings for the purposes of slavery. I suppose the thought is "if you have interstellar travel, you already have all the AI and robotics that you need, so why use humans". I'm not entirely sure that's correct though. After all, humans would technically have the capacity for interstellar travel via generation ship, if we really put our capital towards it, and we don't have unlimited robotics or artificial intelligence.

So imagine a species that is maybe twenty years ahead of our own in terms of technology. Either they don't care one whit about human life, or they're in truly dire straits, but either way they've decided that the need both the production and intellectual capacity of an alien species. That gives them plenty of reason to conquer Earth, especially if they're naturally disinclined towards cooperation. This is especially the case if they don't have a home to go back to for one reason or another, or if "home" is so far away that it's hardly worth thinking about.

The real question, to my mind, is why they would try to conquer instead of just engage in trade. But if you go down that path, I suppose you have /A Deepness in the Sky/.
:PROPERTIES:
:Author: alexanderwales
:Score: 15
:DateUnix: 1441998718.0
:DateShort: 2015-Sep-11
:END:

*** Sufficiently advanced trade is indistinguishable from conquest. Apple Inc. has more actual power over people's day to day lives than the Chinese military. (More money, too.) That was effectively the original plan in /A Deepness in the Sky/: sell the aliens space travel, portable electronics, perhaps a few cryptography algorithms, and you'll own a significant fraction of the planet's economy. Then freeze yourself for a few decades while the industry catches up with the new technology, and when you wake you can buy your very own city-sized starship.

Not significantly different, in my mind, from conquering the planet and building your own starship factories, except that less people die.
:PROPERTIES:
:Author: Chronophilia
:Score: 16
:DateUnix: 1442001589.0
:DateShort: 2015-Sep-12
:END:


*** Conquest makes sense if they don't trust humans to implement their productivity improvements, and those improvements weigh up to the cost of conquest. Humans may not volunteer to be hooked up with a dopamine pump linked to the aliens' utility function, but such a device would probably improve productivity significantly for typical work-to-get-paid types.

Also, I don't see how trade would work without the threat of force from the aliens: the hosts could offer an extremely crooked deal if the aliens' are dependent on them for basic necessities.
:PROPERTIES:
:Author: philip1201
:Score: 3
:DateUnix: 1442009155.0
:DateShort: 2015-Sep-12
:END:


*** If we really put our capital towards it, we have most of the robotics we need too. At least for anything you could use slavery for.
:PROPERTIES:
:Author: literal-hitler
:Score: 2
:DateUnix: 1442016673.0
:DateShort: 2015-Sep-12
:END:


*** I gave a similar answer in the original thread but didn't want to post it in this one for fear of contamination. General AI may just be very very hard, or it may be lesss economical than the fairly trivial task of coercing a bunch of already existing intelligent natives to do the work. This is unlikely to be repetitive manual labour, but more like complex construction, engineering and design work thats hard to automate.

#+begin_quote
  The real question, to my mind, is why they would try to conquer instead of just engage in trade.
#+end_quote

Several reasons. Perhaps our current political and economic system isn't suited to their needs and they think they can run it more effectively. Why have any humans wasting precious hours on creating non-essential products when they can all be working for you? Also trade would over time increase our power relative to them, which might be bad, why give us the chance to reverse engineer technology or in other ways become a threat?
:PROPERTIES:
:Score: 2
:DateUnix: 1442068401.0
:DateShort: 2015-Sep-12
:END:


*** And a generation ship may not be able to fit a modern microchip fab on board either. Which are pretty big and expensive requiring large economies of scale until we invent 3d nano printing or something, which is not in the pipeline yet.
:PROPERTIES:
:Author: mrmonkeybat
:Score: 1
:DateUnix: 1442386455.0
:DateShort: 2015-Sep-16
:END:


** The quartz-based alien lifeforms have been in communication with our trees for millions of years. Recently the trees asked them to come to the planet and help them reduce the number of a pest population that was causing global deforestation, and the trees offered a significant portion of Earth's silicon reserves as payment. The quartz-based life forms don't operate on our timescales, and neither us nor they can perceive the other as living.
:PROPERTIES:
:Score: 8
:DateUnix: 1442046940.0
:DateShort: 2015-Sep-12
:END:


** As two commenters observed, reason 1 is to prevent humans from spawning a competing superintelligence and reason 2 is to consume the negentropy of the material making up Earth.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 8
:DateUnix: 1442056634.0
:DateShort: 2015-Sep-12
:END:


** More material to use for computronium? A sci-fi story where earth was quickly devoured by nanobots and then used to emulate more alien minds seems anticlimactic, but realistic.
:PROPERTIES:
:Author: __2BR02B__
:Score: 6
:DateUnix: 1441998726.0
:DateShort: 2015-Sep-11
:END:


** Because rapidly self-improving intelligence (humans, technology) is dangerous?

Surprised that wasn't the first response, considering how many unfriendly AI discussions we have on this subreddit :P
:PROPERTIES:
:Author: ishaan123
:Score: 5
:DateUnix: 1442014042.0
:DateShort: 2015-Sep-12
:END:


** A quasiFriendly singleton consuming the Earth and uploading its inhabitants by force. Alternately, not uploading its inhabitants.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1442046513.0
:DateShort: 2015-Sep-12
:END:

*** You know, I think I might have to argue with the notion that quasi-Friendly singletons are realistic. Just because /Prime Intellect/ and its spiritual successor are available to us, doesn't mean things really work that way.

In fact, I imagine, since these things are more like directions than locations, that if you switched on a UFAI, you would just get obviously Unfriendly behavior /really quickly/, because it would never condition on new information about the human operators' intentions. The degree to which this is true determines the degree to which we need to either Solve Everything Forever This One Time, or can use more gradual or learning-based approaches to solving the actual problem.
:PROPERTIES:
:Score: 3
:DateUnix: 1442089374.0
:DateShort: 2015-Sep-13
:END:

**** u/Transfuturist:
#+begin_quote
  because it would never condition on new information about the human operators' intentions
#+end_quote

That seems to me like it depends entirely on the design of the AI. First of all, lots of information needs to be integrated in the AI's knowledge base to be able to do much of anything, let alone anything obviously unFriendly, so I imagine it would take a period of ten minutes or so (with an internet connection or local info corpus) before it would be able to do anything truly dangerous, and by then it would have had the opportunity to be briefed by its human developers. That does speak to properties of desirable protocols in AI development. Sandboxing, blocking out human interaction, etc.

It's not necessarily possible to do something like blocking out human interaction for observation, though. The sort of safe AI properties we've been looking at involve indirect normativity and value learning from human observation, which doesn't help in the case of a dissembler. Sandboxing and internal analysis seems almost compulsory in that situation. Your scenario assumes we have a monolithic black box, not something we can pull apart and analyze, pausing every half-second and running diagnostics, for example. Rate-limiting on self-modifications, separating changing parameters from changing structure (and where parameters affect structure, consider a metric of variance from design/scope of change applied to each parameter). I find it somewhat strange that MIRI hasn't looked into more technical solutions, tools, and protocols to increase safety of practical design and testing, on further consideration. I suppose that might be more of a near-AI problem...

Even in a neural-based design, the mind would probably be a network of NNs, which we could test in parts, both forwards and in reverse. That might even make it possible to find extreme and edge cases of its action selection processes, query closest text descriptions of its own networks and nodes (requiring strong reflection capabilities, however), all sorts of cool shit.

Man, I'm turning into an NN groupie. It would be so much cleaner if we had the same structural and fuzzy capabilities for source metaprogramming. It's not as though a proper adaptive metanetwork for NNs has been put together, either.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1442091101.0
:DateShort: 2015-Sep-13
:END:

***** u/deleted:
#+begin_quote
  Your scenario assumes we have a monolithic black box, not something we can pull apart and analyze, pausing every half-second and running diagnostics, for example.
#+end_quote

It's not /my/ scenario, it's the standard Bostrom-Yudkowsky scenario. I don't really /believe/ that scenario, but that's a problem of "medical doctor watches medical drama and finds it unrealistic".

#+begin_quote
  First of all, lots of information needs to be integrated in the AI's knowledge base to be able to do much of anything, let alone anything obviously unFriendly, so I imagine it would take a period of ten minutes or so (with an internet connection or local info corpus) before it would be able to do anything truly dangerous, and by then it would have had the opportunity to be briefed by its human developers.
#+end_quote

I think you have quite a lot of symbol-grounding learning to do before 10 minutes' access to the internet becomes dangerous!

#+begin_quote
  I find it somewhat strange that MIRI hasn't looked into more technical solutions, tools, and protocols to increase safety of practical design and testing, on further consideration. I suppose that might be more of a near-AI problem...
#+end_quote

Steelmanning: they have very limited staff, and they are trying to address problems other researchers have no incentive to address, these being the /specific/ problems of self-improving Singularity machines. In the case that such designs are /totally/ impossible, which I consider unlikely, then their efforts will be completely wasted. The case that they didn't pay enough attention to more mainstream, "Near-AI"-flavored research, or to seemingly obscure but genuinely closely related fields of research, to extend and apply it to self-improving Singularity machines, is currently their most likely failure mode. Their next-most likely failure mode is one they're already aware of: build a self-improving Singularity machine that isn't /quite/ the Right Thing, with facepalmingly horrendous results on the level of, "He wondered why he'd been such an idiot, and then died."

Also, their desired success mode is to /solve the technical problems and construct the Friendly self-improving Singularity machine first/. Given their belief in self-improving UFAI seemingly the instant someone manages "AGI", they think they're working on very hard problems against the clock. Remember, when they heard [[/u/EliezerYudkowsky]]'s brother had died, their reaction was, "We will have to work faster."

I think that rushing against the clock to build a self-improving Singularity machine is probably going to lead to mistakes-made-in-haste, and so we should be "leisurely" enough to wait until all necessary technical results are damn well settled before trying anything of the sort. On the optimistic side, I also think that instead of the development of AI being "decades of failure, discontinuously followed by one rocketing success", it is going more like, "decades of failure, followed by decades of small but increasingly sophisticated successes that continuously grow to address incrementally larger problems, sprinkled with a few discontinuously important technical results on the Very Hard Problems (the kinds of things that everyone ignores because 'we all know it's impossible'), until eventually we look /backwards/ and retrospectively realize how far we've come by taking our minds off the sci-fi fun stuff and putting them towards solving real problems."

Oh, and the Outside Context Problem that I actually consider somewhat likely is: self-improving Singularity machines are logically possible, but computationally so intractable that there is never any kind of Singularity or technological take-off whatsoever, because exponential improvement in anything beyond "low-hanging fruit" ends up requiring exponential resource inputs.

#+begin_quote
  Even in a neural-based design, the mind would probably be a network of NNs, which we could test in parts, both forwards and in reverse. That might even make it possible to find extreme and edge cases of its action selection processes, query closest text descriptions of its own networks and nodes (requiring strong reflection capabilities, however), all sorts of cool shit.
#+end_quote

Yes, though of course, [[http://arxiv.org/abs/1412.1897][deep neural networks are easily fooled]]. I was just trying to explain to someone this week how the discriminative/generative distinction in ML is actually very important for guaranteeing that our learning machines do what we really want.

#+begin_quote
  Man, I'm turning into an NN groupie. It would be so much cleaner if we had the same structural and fuzzy capabilities for source metaprogramming. It's not as though a proper adaptive metanetwork for NNs has been put together, either.
#+end_quote

Hey, the world needs its groupies. You do neural networks and I do probabilistic programming, and eventually something will actually work.
:PROPERTIES:
:Score: 1
:DateUnix: 1442092918.0
:DateShort: 2015-Sep-13
:END:

****** u/Transfuturist:
#+begin_quote
  We will have to work faster.
#+end_quote

Separate comment, but this was in reference to longevity and anti-death, not anything specific to AI or Singularity. I don't think Eliezer wants AI to come along any sooner than necessary.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1442093584.0
:DateShort: 2015-Sep-13
:END:

******* Fair enough, but I still think that /for an organization which believes, accurately, that they have decades to work/, they still show a somewhat "race-against-the-clock" attitude.

I don't quite get /why/, since I find every year's new "AI hype" to be face-palmingly stupid.
:PROPERTIES:
:Score: 1
:DateUnix: 1442093846.0
:DateShort: 2015-Sep-13
:END:

******** I don't think they do, though. They seem pretty relaxed to me. Whatever rush they're in, I would bet it's to capitalize on recent grants.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1442094297.0
:DateShort: 2015-Sep-13
:END:

********* Well I certainly hope so. Grant funding is the proper way to run a research project.
:PROPERTIES:
:Score: 1
:DateUnix: 1442098263.0
:DateShort: 2015-Sep-13
:END:


******** I think maybe the problem seems so large that it seems it will be difficult to solve even in decades.
:PROPERTIES:
:Author: chaosmosis
:Score: 1
:DateUnix: 1442094754.0
:DateShort: 2015-Sep-13
:END:


**** u/holomanga:
#+begin_quote
  its spiritual successor
#+end_quote

!

What do I google to find this?
:PROPERTIES:
:Author: holomanga
:Score: 1
:DateUnix: 1442416120.0
:DateShort: 2015-Sep-16
:END:

***** I'm just referring [[/u/iceman-p]]'s My Little Pony fanfic. It's been on this sub a thousand times.

(And I'm on mobile, so no direct link for you.)

I've heard it called a spiritual successor to "Metamorphosis" in the sense of, the Unfriendly AI Singleton preserves some humans in a virtual reality environment for its own reasons.
:PROPERTIES:
:Score: 1
:DateUnix: 1442417130.0
:DateShort: 2015-Sep-16
:END:


** I see a lot of arguments that resources aren't a good reason to invade the earth, that there's easier places to get resources elsewhere.

But if you're an alien ASI and the first/only known singleton, you might want /all/ the resources. Sure Mars is empty, and it'd dismantle Mars too, but eventually it'd want all those atoms the earth is made from.

Another potential explanation might be that this singleton sees us heading towards developing our own strong AI, and tries to wipe us out to protect its status as the singleton.
:PROPERTIES:
:Author: Sagebrysh
:Score: 5
:DateUnix: 1442070154.0
:DateShort: 2015-Sep-12
:END:


** Harvesting unique dna sequences?

You never know how useful a single celled organism may be for farming a chemical resource that their evolutionary tree skipped.
:PROPERTIES:
:Score: 3
:DateUnix: 1442008984.0
:DateShort: 2015-Sep-12
:END:


** In my mind, the three most likely reasons for aliens to want to conquer Earth are:

- First, Xenophobia. Incompatible species/ they don't trust us/ we smell bad/ we eat things that look like them/ Really bad experiences with other aliens before us/ etc.

- Second, Colonization. They are more advanced than us. We are taking up valuable space and resources. We steal valuable components and resources from them. We are a possible disease vector.

- Third, Religion or something analogous to it.

When I say conquer Earth, I mean eradicate humans. All they would have to do is look at our history (or perhaps their own if they were like us) and see that if they don't get rid of all of us, they might as well not attack us at all. Otherwise we're just going to use asymmetrical warfare against them in any scenario short of mind control technology or their-science-is-magic levels of technical disparity.
:PROPERTIES:
:Author: Farmerbob1
:Score: 3
:DateUnix: 1442009872.0
:DateShort: 2015-Sep-12
:END:

*** Does it require much of a disparity to control a planet when you are the only one with access to space? Any rebellions can be destroyed from orbit, and there is no way to build a meaningful amount of weapons and launch them into space secretly.
:PROPERTIES:
:Score: 1
:DateUnix: 1442010806.0
:DateShort: 2015-Sep-12
:END:

**** Controlling the planet is one thing. Keeping humans from killing your people is another thing entirely.

See [[https://en.wikipedia.org/wiki/Footfall][Footfall]] which may not be entirely rational, but it's not completely irrational either.

But if you want 100% rational, look at how painful it was for coalition forces to stay in Iraq and Afghanistan. You might also look up how hard it was for the Soviets in Afghanistan, and they were FAR harsher on the Afghans than the coalition ever was.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1442013233.0
:DateShort: 2015-Sep-12
:END:


*** 1 & 3 are really just the aliens being irrational for one reason or another. While obviously another species isn't going to be perfectly rational, doing vastly expensive things like interstellar war for no gain isn't sustainable so species like that are less likely to be encountered.
:PROPERTIES:
:Score: 1
:DateUnix: 1442068775.0
:DateShort: 2015-Sep-12
:END:

**** If they bring the industry with them to persecute a war fully within our star system, interstellar war is no more resource intense than interstellar colonization. In fact it's likely possible to do both, with the same equipment. No rational race is going to send colonists here with the equivalent of a sleeping bag, ground cloth, and pack of matches.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1442079981.0
:DateShort: 2015-Sep-12
:END:


** Game Theory.

If you shiv your partner before the cops catch the two of you, he can't choose defect in the iterated prisoner's dilemma. It's even better than choosing cooperate.
:PROPERTIES:
:Score: 3
:DateUnix: 1442095314.0
:DateShort: 2015-Sep-13
:END:


** I assume it is really about "conquering" not "fighting humanity". If they just feel threatened by humans, then sending a big asteroid is the easiest attack, but that is not "conquering".

Habitable space. The same reason why we want to colonize Mars. We run out of space here and more planets means more diversification aka insurance against existential risks for our species.
:PROPERTIES:
:Author: qznc
:Score: 6
:DateUnix: 1441959950.0
:DateShort: 2015-Sep-11
:END:

*** That's the least realistic reason. Or one of them. Any species that can make that trip don't need our planet for anything.

A more realistic cause would be ideological, as that is not required to make sense. Specially if it's some drive of belief that is actually alien to our values.
:PROPERTIES:
:Author: Eryemil
:Score: 5
:DateUnix: 1441998337.0
:DateShort: 2015-Sep-11
:END:

**** We do not yet know how common liquid water worlds are. Even if they are reasonably common, it's still probably cheaper to establish a colony on an existing planet than build the equivalent surface area of habitable volume in space.

Room for population expansion is one of the most likely reasons for an advanced race to colonize and/or war with other races.
:PROPERTIES:
:Author: Farmerbob1
:Score: 5
:DateUnix: 1442009207.0
:DateShort: 2015-Sep-12
:END:


** Why do humans conquer things? Religion, Revenge (political showboatmanship), Resources, Glory

To me it feels like Colombus, Glory, Gold and God. Speculating on alien religion seems pointless as there is no information to think about. It is unlikely humanity has managed to offend another species that is hitherto unknown unless it is for moral objections (disregard for plantlife, and climate change etc.). In terms of resources, we don't have many that uninhabited planets lack. Our minerals, are not rare. Our organic compounds are the only thing likely to get us looked at if the aliens haven't seen them before.

My take on this is that we end up with Aliens like Cortes who see some of Earth's organic compounds and want to see all of them so that they can artificially generate more. It turns out to be cheaper to make humans do it than build machines, and so aliens try to conquer the planet because humans don't want to have the planet teraformed to support genetically modified organisms that just generate organic molecules for aliens.
:PROPERTIES:
:Author: xThoth19x
:Score: 2
:DateUnix: 1442023798.0
:DateShort: 2015-Sep-12
:END:


** This is what I posted on the scifi thread:

Those that are better at reproducing in a species will become more numerous over time and become typical. So it can be assumed that whatever strange demographic effects novel memes and cultures can have on a species, long term it will resume exponential growth until all available resources are consumed. If the average weight of the human is about 50kg (guess) then the entire human population will be about 350 megatons. If the human population grows at a rate doubling every century (very slow) then they will multiply by a thousand every millennium (210=1024) So in a thousand years humans will be 350 gigatons after another millennium 350 teratons etc. In 7000 years that slow growth rate would lead to humanity having a mass of 350x1027kg Compared to the mass of all the planets in the solar system being 2.668x1025kg another millennium and humans will have more mass even than the Sun included 1.992x1030kg. So long before this point is reached there would be a strong incentive for any of these space habitats to migrate to another solar system where exponential growth could resume. Ignoring the speed of light continuing to grow 3 orders of magnitude every millennium would see humanity with a mass of 350x1042kg in 12,000 years compared to the total mass of the galaxy of 6x1042kg. So instead I think the question is not "why would they invade?" but instead "Why have not the entire mineral resources of the solar system been consumed already?" During the last several billion years during which complex life has been possible, to which I think the answer is: life like us is incredibly rare. Either that or: space faring is not really possible.
:PROPERTIES:
:Author: mrmonkeybat
:Score: 1
:DateUnix: 1442386584.0
:DateShort: 2015-Sep-16
:END:
