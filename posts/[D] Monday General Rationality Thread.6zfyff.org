#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 11
:DateUnix: 1505142393.0
:DateShort: 2017-Sep-11
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** In the broad spirit of [[https://www.reddit.com/r/slatestarcodex/comments/6z8grm/is_aphantasia_really_real_like_how_is_that_even/][here]] and [[http://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it/][here]]...

#+begin_quote
  I don't have a voice in my head, I have /four/, each of which speaks for a different part of me and who regularly talk to each-other socially.
#+end_quote

Now, that's a symptom of reading too much, and I know it. I'm fairly sure I saw the study saying that people who read or write too much develop this and only this dissociative symptom.

The universal human experience I think I'm missing though, is precisely dissociation. I've never really stood back from my actions, even when they've been deeply irrational or under the influence of drugs or mental illness, and said, "That wasn't me." I've never looked in the mirror and asked, "Is this really who I am?". Drunk-me tries to help out sober-me because I'm the same person drunk as sober.

This is more like my experience of the world, which is apparently so different from most people that it's worth noting as a character trait of /Granny Weatherwax/:

#+begin_quote
  Most people, on waking up, accelerate through a quick panicky pre-consciousness check-up: who am I, where am I, who is he/she, good god, why am I cuddling a policeman's helmet, what happened last night?

  And this is because people are riddled by Doubt. It is the engine that drives them through their lives. It is the elastic band in the little model aeroplane of their soul, and they spend their time winding it up until it knots. Early morning is the worst time -there's that little moment of panic in case You have drifted away in the night and something else has moved in. This never happened to Granny Weatherwax. She went straight from asleep to instant operation on all six cylinders. /She never needed to find herself because she always knew who was doing the looking./
#+end_quote

Emphasis mine. I often wonder that I sort of fail to communicate what's going on in my life with others because I can't put my psychologically abnormal experiences into their frame of reference.

So, uh, how /does/ that work, to step back from your experiences and have some gap between them and "you"?
:PROPERTIES:
:Score: 16
:DateUnix: 1505153058.0
:DateShort: 2017-Sep-11
:END:

*** u/eternal-potato:
#+begin_quote
  Now, that's a symptom of reading too much, and I know it. I'm fairly sure I saw the study saying that people who read or write too much develop this and only this dissociative symptom.
#+end_quote

How much reading is too much? Though I've hardly ever written anything, I estimate I read about 100k words of fiction a week on average, (ballpark, never actually cared to measure), but sometimes when I have nothing better to do I can read more in a single day. Yet I can't say I've ever experienced anything of the sort, let alone disassociation from it. I assume simulating fictional characters from behind the fourth wall doesn't count.

#+begin_quote
  Early morning is the worst time -there's that little moment of panic in case You have drifted away in the night and something else has moved in.
#+end_quote

Is this /really/ how it is for most people? Sounds mildly terrifying.
:PROPERTIES:
:Author: eternal-potato
:Score: 8
:DateUnix: 1505154990.0
:DateShort: 2017-Sep-11
:END:

**** u/callmesalticidae:
#+begin_quote
  Is this really how it is for most people? Sounds mildly terrifying.
#+end_quote

This only happens when I have a really, really disorienting dream. Is that how it is for most people, and Pterry is exaggerating its frequency for effect, or does it happen more often for most people?
:PROPERTIES:
:Author: callmesalticidae
:Score: 7
:DateUnix: 1505155291.0
:DateShort: 2017-Sep-11
:END:

***** Hm. For me dreams are like, well, really getting into a book or a movie. While I am engrossed in reading (dreaming) my internal narrative is kind of suspended and replaced by the narrative of the book (dream), but if I am interrupted by somebody (woken up) my narrative immediately resumes, with no disorientation or any need to remember who or where I am.

(I call my stream of consciousness 'narrative', but it is not really linguistic in the same way the book one is. I don't constantly describe what is happening, I just am.)
:PROPERTIES:
:Author: eternal-potato
:Score: 6
:DateUnix: 1505156017.0
:DateShort: 2017-Sep-11
:END:


**** u/deleted:
#+begin_quote
  How much reading is too much?
#+end_quote

Probably somewhere around the point where you've tried to step into the heads of people who do have mental conversations. Nita Callahan and God-Emperor Shinji Ikari are probably the ones that made me start doing that.

#+begin_quote
  I assume simulating fictional characters from behind the fourth wall doesn't count.
#+end_quote

Well my whole thing was that this seems to be a symptom of imagining other people a little too much, rather than an /actual/ dissociative symptom.

#+begin_quote
  Is this really how it is for most people? Sounds mildly terrifying.
#+end_quote

I know, right? But on the other hand, I find most people's discourse about personal identity completely, utterly baffling. Like, they /have a self-concept/ apart from whatever they happen to be at the time. I rarely develop one, and when I do, it's wildly divergent from reality, so it dissolves.

Maybe if you had one of those, you'd have to ask yourself where it is when you wake up, like remembering anything else you know?
:PROPERTIES:
:Score: 6
:DateUnix: 1505159552.0
:DateShort: 2017-Sep-12
:END:

***** u/PeridexisErrant:
#+begin_quote
  How much reading is too much?
#+end_quote

More than three or four /Culture/ novels in one day is probably too much, but the /Young Wizards/ series isn't... that I've noticed.

The closest I get to the descriptions above is something like (someone mentioned Discworld) Tiffany's First Thoughts, where I notice a thought that's more like me-as-I-want-to-be than I am. It's still me thinking it, though.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 1
:DateUnix: 1505175787.0
:DateShort: 2017-Sep-12
:END:

****** I mean, everything I think is still me. It's all just different trains of thought. It's kinda like:

#+begin_quote
  Fuck, missed the train!

  Well what do you expect, leaving the house less than 20 minutes before it?
#+end_quote
:PROPERTIES:
:Score: 1
:DateUnix: 1505215020.0
:DateShort: 2017-Sep-12
:END:


***** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1505235101.0
:DateShort: 2017-Sep-12
:END:

****** u/deleted:
#+begin_quote
  You don't have traits that you wish to show?
#+end_quote

Kind of, but only kind of. I know that there are certain ways I ought to act, but I don't really think of them as traits I ought to show. I always just figured, I am who and what I am, so whatever.

#+begin_quote
  I look at it as the person having a mental image of an ideal version of themselves.
#+end_quote

So identity is normative rather than descriptive?
:PROPERTIES:
:Score: 3
:DateUnix: 1505236560.0
:DateShort: 2017-Sep-12
:END:


**** u/gbear605:
#+begin_quote
  100k words of fiction a week on average
#+end_quote

I read similar amounts, perhaps even twice or thrice that. I have also never had that kind of symptom.

I occasionally will have moments where I look in the mirror and question myself being myself - whether the person I perceive myself as being is different from the person that others perceive me as being - but aside from that I have had none of the symptoms of dissociation that [[/u/eaturbrainz]] is talking about.
:PROPERTIES:
:Author: gbear605
:Score: 3
:DateUnix: 1505155399.0
:DateShort: 2017-Sep-11
:END:


**** I've become so...empathetic, I suppose, to other people's issues that I've forgotten I'm not them. I've literally tried to get off at other people's bus stops after having a conversation with them where they described doing so. Weird feeling of jolting back to being me.
:PROPERTIES:
:Author: Tinfoil_Haberdashery
:Score: 3
:DateUnix: 1505190739.0
:DateShort: 2017-Sep-12
:END:


*** When I wake up, I'm completely aware of who I am and (unless I wandered off somewhere last night and forget) where I am. I have one voice in my head, and it is mine, though I can use it to "read aloud" other voices and so on. I can narrate other voices as well. I read a very large amount of fiction and a good amount of nonfiction as well and do not have multiple internal voices. It's possible I'm just less imaginative or something though.
:PROPERTIES:
:Author: blazinghand
:Score: 6
:DateUnix: 1505161380.0
:DateShort: 2017-Sep-12
:END:


*** A close friend has jokingly referred to me as a "a spider hovering over a p-zombie".

Do you have an internal monologue, when you're not specifically trying to format your thoughts for communication? Or I suppose when you're not reading. Can you visualize things?

--------------

I suspect, in my case at least, that lack of disassociation is partly due to me not really making use of those internal feedback mechanisms. Serializing thought to text/speech is /slow/, and annoying, and I mostly don't do it except when I'm communicating with people.

--------------

Reading through your links after posting this, and yeah, that.
:PROPERTIES:
:Author: traverseda
:Score: 2
:DateUnix: 1505159001.0
:DateShort: 2017-Sep-12
:END:

**** u/deleted:
#+begin_quote
  Do you have an internal monologue, when you're not specifically trying to format your thoughts for communication?
#+end_quote

Yes, several.

#+begin_quote
  Can you visualize things?
#+end_quote

Usually I can visualize things in glimpses, or I can recall specific visual memories. I can't make a continuous movie in my head the way I can recall sound continuously.

EDIT: Actually, seems I can, it just takes more effort.
:PROPERTIES:
:Score: 3
:DateUnix: 1505160443.0
:DateShort: 2017-Sep-12
:END:

***** Huh, good to know.
:PROPERTIES:
:Author: traverseda
:Score: 1
:DateUnix: 1505161281.0
:DateShort: 2017-Sep-12
:END:

****** So what's the spider supposed to be?
:PROPERTIES:
:Score: 2
:DateUnix: 1505161603.0
:DateShort: 2017-Sep-12
:END:


*** I sometimes have a form of dissociation where I'm looking at the actions I'm doing like if the person performing them were someone else, a stranger that I find baffling and that I'm disappointed with. I'm an outside observer who tries vainly to influence my own actions. This usually correlates with periods of intense depression where I have to use a lot of will power to force that stranger to do anything.

The feeling is a bit similar to Camus l'étranger or at least what I remember of it (I read it 20 years ago) and that feeling is relatively recent (probably triggered by my divorce), it also rarely last for more than 4-5 days at a time.

When those period happen, I usually end up drowning myself in fiction, as a way to escape from reality and from this feeling, waiting for it to pass. But, yes, I wouldn't qualify this as being normal, nor would I wish it on anyone.
:PROPERTIES:
:Author: tomtan
:Score: 2
:DateUnix: 1505178141.0
:DateShort: 2017-Sep-12
:END:


*** u/DeterminedThrowaway:
#+begin_quote
  I don't have a voice in my head, I have four, each of which speaks for a different part of me and who regularly talk to each-other socially.
#+end_quote

This really fascinates me. If you wouldn't mind me asking, is this something you brought about purposely or did it just happen naturally for you?
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 1
:DateUnix: 1505179824.0
:DateShort: 2017-Sep-12
:END:

**** Overexposure to Nita and Shinji. Especially Shinji.
:PROPERTIES:
:Score: 1
:DateUnix: 1505214836.0
:DateShort: 2017-Sep-12
:END:

***** Shinji from Neon Genesis Evangelion? I haven't really watched it, and I'm not sure who Nita is. Is there any way you could elaborate if it's not too much trouble?
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 1
:DateUnix: 1505239181.0
:DateShort: 2017-Sep-12
:END:

****** Nita Callahan from /Young Wizards/. Shinji Ikari the /healthy/ version, from /Shinji and Warhammer 40K/.

(Yes, adding WH40K made him a healthier, happier person. Seriously. Adding 40K made things /better/. That's how fucked-up Eva is.)
:PROPERTIES:
:Score: 2
:DateUnix: 1505239456.0
:DateShort: 2017-Sep-12
:END:


*** Sometimes I can run myself on automatic.

It's only possible when doing something that I've done often enough that it becomes habit, and it's only possible /because/ it has become habit. Basically, I just... let the habit run on its own, more or less. I think about whatever catches my fancy while letting the body do as habit dictates.

I do not know if this counts as disassociation or not. I'm still doing stuff, I'm just not paying that much attention to what I am doing. Even then, I'm still technically in control - I was the one who set up the habits, and I am the one who can take back control at any point (such as if something unexpected happens).

--------------

As to what happens when I wake up - I'm most certainly not instantly firing on all cylinders. But it's not because I need to find myself. I'm just not very energetic in the mornings.

Sometimes, I'm aware enough to start moving before I'm awake enough to have kicked off sleep paralysis. That's weird, because I'm awake enough to know to move, and sometimes asleep enough to start dreaming about getting up for a short while before drifting to awake enough to realise that I have not in fact, actually moved at all... I've been known, on rare occasion, to go through a few cycles of this. And, sometimes, I'm just awake enough to realise that I'm /not/ moving... I have found that if I can find any part of me that can move, even a little, I can shake off sleep paralysis on that area of my body by literally shaking it. (Usually this is a hand, at first; once I can move my hand, it's not long before I can move my arm, and if I can manage to shake my head, that tends to wake the rest of me up really quickly...).

The other alternative to wake up quickly is to find something to panic over. "I'll be late!" is useful in this regard (because if it's necessary, it's generally true).
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1505308523.0
:DateShort: 2017-Sep-13
:END:


** Edit: See [[https://www.reddit.com/r/rational/comments/6zfyff/d_monday_general_rationality_thread/dmwfq4w/?st=j7hnugxd&sh=e01db591][my reply to ShiranaiWakaranai below]] for an overview of my endgame here...

--------------

A couple of weeks ago, I made a post here about Nassim Taleb, which did not accomplish what I had hoped it would. I still want to have that discussion with members of the rationalist community, but I'm not sure of the best place to go for that (this is the only rationalist forum that I am active on, at the moment, though it may not be the best place to get a full technical discussion going).

Anyway, Taleb has an interesting perspective on rationality that I would like people's thoughts about. I won't try to put words in his mouth like last time. Instead, the following two articles are good summaries of his position:

[[https://medium.com/incerto/how-to-be-rational-about-rationality-432e96dd4d1a][How to be Rational About Rationality]]

[[https://medium.com/incerto/the-logic-of-risk-taking-107bf41029d3][The Logic of Risk-Taking]]

I'll just add that when it comes to Taleb, /I notice that I am confused./ Some of his views seem antithetical to everything the rationalist community stands for, and yet I see lots of indicators that Taleb is an extremely strong rationalist himself (though he would never call himself that), strong enough that it is reasonable to trust most of his conclusions. He is like the Eliezer Yudkowsky of quantitative finance - hated or ignored by academia, yet someone who has built up an entire philosophical worldview based on probability theory.
:PROPERTIES:
:Author: LieGroupE8
:Score: 8
:DateUnix: 1505149717.0
:DateShort: 2017-Sep-11
:END:

*** Having read the two articles, I do not see anything that is antithetical to the rationalist community. I'd guess that you're thinking of claims like how Taleb does not think that science is useful for a lot of real-world problems. By his definition of science, I think Yudkowsky would agree. From what I can tell, Taleb's science is a specific subset of activities - academic science. Yudkowsky's science is "the ... kind of thought that lets us survive in everyday life." [1] Science to Yudkowsky is figuring out that the red berries are dangerous and that if you put a dead fish by your corn seeds, the corn will grow better. Taleb's science, however, is only the search for absolute truth.

This sentence [2] by Taleb sounds like something Yudkowsky could have said in fact. Taleb speaks about how you need to focus on the instrumental value of activity, Yudkowsky's rationalism is about doing whatever achieves your goal ("winning")

[1]: [[http://yudkowsky.net/obsolete/tmol-faq.html#theo_conflict]] (An old page, but I believe that Yudkowsky would agree with this part of it)

[2]: [[https://medium.com/incerto/how-to-be-rational-about-rationality-432e96dd4d1a]] "Your eyes are not sensors aimed at getting the electromagnetic spectrum of reality. Their job description is not to produce the most accurate scientific representation of reality; rather the most useful one for survival."
:PROPERTIES:
:Author: gbear605
:Score: 6
:DateUnix: 1505154628.0
:DateShort: 2017-Sep-11
:END:

**** The antithetical part is that "beliefs" have nothing to do with rationality, for Taleb. There is no such thing as epistemic rationality, only rationality of decisions. So Taleb finds religion perfectly agreeable if it causes people to not die. Most "rationalists" despise religion, in my experience.
:PROPERTIES:
:Author: LieGroupE8
:Score: 2
:DateUnix: 1505154934.0
:DateShort: 2017-Sep-11
:END:

***** I'd guess that this stems for Yudkowsky and most rationalists valuing truth for the sake of truth while Taleb does not. That's entirely a statement about personal preference, they just have different personal preferences.

I doubt that Taleb would claim that epistemic rationality does not help with finding the truth, instead he would claim that it is useless because finding the truth is useless unless it has some other benefit to him, in which case it is part of his rationality of decisions.
:PROPERTIES:
:Author: gbear605
:Score: 7
:DateUnix: 1505155201.0
:DateShort: 2017-Sep-11
:END:

****** I agree, although it's more than just religion. There are a whole set of issues where he would disagree with what I think that most rationalists think should be done in practice. (GMOs and Donald Trump, for example - see my post from a while back). Even though Taleb does not care about beliefs, he cares about decisions, and the things he considers optimal decisions do not seem like what rationalists would consider optimal decisions in certain settings. I could be mistaken about the degree of discrepancy though.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1505155973.0
:DateShort: 2017-Sep-11
:END:

******* (Link to the original post, for those who do not want to search through post history: [[https://www.reddit.com/r/rational/comments/6i6zfl/d_monday_general_rationality_thread/dj3z9d7/]])

As far as GMOs go, I recall that the rationality community is somewhat split for a number of reasons. I have heard the argument against GMOs that (you say) Taleb puts forth and the counter argument that I've heard in the past is that the risk from GMOs is likely low compared to the benefit. It's an equation that has lives on either side, so it just depends on what the risk and benefits actually are. If (cost from GMOs going bad) * (change of GMOs going bad) > (benefit from GMOs), then I think very few people would disagree with him. So this basically is a disagreement over the numbers.

In regards to Trump, I think that Trump's policies are likely good for people like Taleb (eg. rich, not female, not an illegal immigrant, etc.). His view about "most news stories as noise with no signal" seems like what Scott Alexander argues in [[http://slatestarcodex.com/2016/11/07/tuesday-shouldnt-change-the-narrative/]].

Some other points of his:

"talking like we're high-and-mighty empiricists while being too lazy to carry out actual experiments"

- [[https://www.gwern.net/][Gwern]] has done a number of actual experiments,
- there have been a number of surveys across LessWrong and SlateStarCodex collecting data,
- [[http://metaculus.com/][Metaculus]] is a startup that is part of the rationalist community that is collecting data to see if a prediction market works out
- Givewell and other Effective Altruism type groups are all about collecting data on what works and what does not
- many people in the rationalist community are professional scientists who work in labs where they collect real data

I would agree that the rationalist community needs to do more data collection though.

"learn the ultra-advanced theoretical statistics necessary to properly understand the data we have received"

- [[http://econlog.econlib.org/authorbcaplan.html#recent][Bryan Caplan]] is an economics professor who is part of the community
- [[http://www.overcomingbias.com/author/robin-hanson][Robin Hanson]] is another economics professor who is part of the community
- [[https://en.wikipedia.org/wiki/Julia_Galef][Julia Galef]], co-founder of the Center for Applied Rationality, has a degree in Statistics
- Gwern (again) appears to me to be very well educated in statistics
- The people at MIRI appear to know what they're doing with math
- The people at GiveWell definitely seem to know what they're doing with statistics

I can't evaluate this claim well because I definitely do not have the statistics knowledge.

Overall, I would guess that you're mainly mistaken about the degree of discrepancy.
:PROPERTIES:
:Author: gbear605
:Score: 9
:DateUnix: 1505157768.0
:DateShort: 2017-Sep-11
:END:

******** Good post, and thanks for adding the links (I was going to edit them in later when not on mobile). I could indeed be mistaken about the discrepancy. Part of the problem is that Taleb's community and Yudkowsky's community use different terminology and motivating examples. For example, when Taleb decries "rationalists," it is unclear if he is referring to the modern movement a la CFAR, or to the old-school philosophical rationalists, which have nothing to do with each other.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1505158198.0
:DateShort: 2017-Sep-11
:END:

********* It seems unlikely that Taleb even knows about rationality in regards to our group of rationalism a la CFAR - or if he does know about it, knows or cares enough to decry us. We're still a small community. Our biggest influence on the world could plausibly be HPMoR.

I do not know anything about the old-school philosophical rationalists though, so I'm not sure if he could plausibly be referring to them.
:PROPERTIES:
:Author: gbear605
:Score: 2
:DateUnix: 1505158385.0
:DateShort: 2017-Sep-12
:END:

********** I'd be surprised if he has /never/ encountered CFAR or modern rationalists, but he might have dismissed them purely by the name and not investigated further. I have in mind a specific Facebook post where someone who was clearly from the LessWrong-type rationalist community asks him what he thinks of "rationalists," at which point Taleb gets angry and goes on a tirade against rationalists, and I'm 50-50 on which type of rationalist he was talking about. There is a whole tradition of rationalism in philosophy which is [[https://plato.stanford.edu/entries/rationalism-empiricism/][contrasted with empiricism]], whereas LessWrong-type rationalists are all about empiricism. "Rationalist" is an unfortunate choice of label, in that sense.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1505159131.0
:DateShort: 2017-Sep-12
:END:


****** u/Veedrac:
#+begin_quote
  this stems for Yudkowsky and most rationalists valuing truth for the sake of truth
#+end_quote

Is this really true? I'd argue [[http://lesswrong.com/lw/go/why_truth_and/][this is him speaking to the contrary]].
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1505441033.0
:DateShort: 2017-Sep-15
:END:

******* One of the reasons he listed there, and one that I think applies to Yudkowsky, is for curiosity, which is essentially "valuing truth for the sake of truth."

And the rest of the post is Yudkowsky explaining that truth is valuable for helping make decisions, which is Taleb's point. I'd guess that the rest of the difference stems from disagreements about how useful truth is to understanding a situation.
:PROPERTIES:
:Author: gbear605
:Score: 1
:DateUnix: 1505441301.0
:DateShort: 2017-Sep-15
:END:

******** u/Veedrac:
#+begin_quote
  curiosity, which is essentially "valuing truth for the sake of truth."
#+end_quote

It's "valuing truth for the sake of enjoyment", which is different because it doesn't suggest any /intrinsic/ quality.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1505441478.0
:DateShort: 2017-Sep-15
:END:

********* If you value truth for the sake of enjoyment, you're going to seek out truth that has no other extrinsic benefit to you than enjoyment. Taleb would never do that (from my reading of him), so there's the crux.
:PROPERTIES:
:Author: gbear605
:Score: 1
:DateUnix: 1505441598.0
:DateShort: 2017-Sep-15
:END:

********** That matches my understanding, yes.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1505442924.0
:DateShort: 2017-Sep-15
:END:


***** There is nothing particularly strange happening here once you look at their goals.

Taleb's goal is the survival of the individual, and the collective. If that is your goal, the rational choice is to accept religion. To keep the status quo. Going against religion paints a target on your back for religious fanatics to go inquisition on you, lowering your survival odds. Abandoning a religion means adopting a different philosophy, which has higher chance of destroying society compared to just keeping the status quo. So again, keeping the status quo is the rational choice, if your goal is survival of the collective.

Most "rationalists" tend to not have survival as their goal. They tend to have utilitarian goals, i.e., they want to maximize happiness, even if it has a tiny chance of killing everyone in the process. In which case, religions are a hindrance, mainly because most religions are not utilitarian. Just about every major religion tells its followers to waste time praying and performing strange rituals when they could instead be out there saving lives or making the world a better place. They promote goals like "worshipping god", or "filial piety", or "honor and glory", instead of the utilitarian goal of maximizing happiness. Which means all the religious followers would frequently take actions which do not maximize happiness, simply because those actions maximize some other goal. So from a utilitarian perspective, religions should really be abolished to maximize happiness.

So even though their views on religion are opposing, neither is irrational. They just have different end goals.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1505188771.0
:DateShort: 2017-Sep-12
:END:


*** Also, the more I read about Taleb's views, the more worried I become. His views are not irrational. They are quite logical, and the actions he advocate truly are the best ways to achieve his goals.

The problem is his goals seem extremely susceptible to evil.

In "How to be Rational About Rationality", he states that his goals are about survival. Survival of the individual or the collective. And that any action taken that goes against survival is irrational.

Does he not see the potential for evil here? There are plenty of ways to improve your own odds of survival by hurting others. Stealing their stuff, murdering the competition, turning people into slaves, etc. Similarly, there are plenty of ways to improve the odds of survival for the collective by hurting individuals: rapes to increase birth rates, dictatorships and blind obedience so decisions can be made quickly, culling the old and weak so they don't drag down the species, etc. etc.

Now, last time, I was told that Taleb's philosophy has an exception: Follow the philosophy unless what it tells you to do infringes on ethics.

But this doesn't even work because Taleb's philosophy promotes willful ignorance. It tells you to perform actions even if you don't know the reasoning behind them, so long as other people are also doing said actions. For all you know, these actions could be committing major ethics violations without your knowledge. Yet you aren't allowed to wait and investigate whether your traditions are evil before obeying. You have to obey them *now*, because to do otherwise is to risk the survival of the collective.

It's really terrifying.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 5
:DateUnix: 1505194608.0
:DateShort: 2017-Sep-12
:END:

**** I'm going to respond to all your posts here, in one place. Just to tie things together, I'll tag the other people who responded to me (thanks): [[/u/eaturbrainz]] [[/u/696e6372656469626c65]] [[/u/gbear605]]

So here's my secret, ulterior motive for bringing up Taleb over and over: Taleb has intellectual tools that I covet for the rationalist community. We may not agree with everything he says and does, we may have different goals than he does, but if there are useful analytical tools that we could be using but aren't, we should greedily pluck them from wherever we can find them.

Logic and Bayes' theorem are great and all, but as Taleb would point out, the formal structures navigated by those tools are not sufficient for a certain class of problems, namely, the problem of reasoning about complex systems. Of course, logic /constructs/ the tools needed, because it constructs all of mathematics - but the direct application of modus ponens might not work out so well. Statements of the form "If A then B" for human-recognizable categories A and B will typically be useless, because by the nature of complexity, we can't get enough bits of shannon information about such propositions for them to be practically useful. Moreover, sometimes when it /seems/ like this sort of reasoning is trustworthy, it isn't.

For example, here's a mistake of reasoning that a starry-eyed young utilitarian might fall into:

1) If something is bad, then we should stop it from happening as much as possible

2) Wildfires are bad because they destroy property and hurt people and animals

3) Therefore, we should stop as many wildfires as possible

You might be thinking, "What's wrong with that?" But consider this: preventing small wildfires creates a buildup of dry brush and greatly increases the chance later on of a massive, even-worse wildfire. Thus it is better to accept the damages of small wildfires right away to prevent things from being worse in the long-term.

More generally, Taleb argues: many people make the mistake of trading short-term bounded risks for long-term existential risks. Quite often, preventing short-term disasters just sweeps problems under the rug until they all collapse at once. For example, bailing out big banks instead of letting them fail just maintains the status quo and ensures that there will be another market crash from corrupt practices. Polluting the atmosphere to generate electricity in the short-term has long-term environmental consequences. Using plasmid insertion to create super-crops that solve hunger in the short term could lead to an ecological disaster in the long term (hence the GMO issue from last time).

Talebs says: "Hey you guys. Stop naively applying modus ponens and bell curves to complex systems. Instead, here's a bunch of mathematical tools that work better: fractal geometry, renormalization, dynamic time-series analysis, nonlinear differential equations, fat-tailed analysis, convex exposure analysis, ergodic markov chains with absorbing states. It's a lot of math, I know, but you don't need to do math to do well, just listen to the wisdom of the ancients; practices that have survived since ancient times probably don't have existential risk. If you want to go against the wisdom of the ancients, then you'd better be damn careful how you do it, and in that case you'd better have a good grasp on the math."

Regarding survivability: it's not that surviving is Taleb's terminal goal so much as it's a prerequisite for all goals. If you don't survive, you can't do the utilitarian goal-maximization that you want to do. Therefore, maximizing your long-term survival chances should always be your first worry. You can never eliminate all risk, but you can choose which kind of risk you want to deal with. Fat-tailed risk (like non-value-aligned artificial intelligence!) virtually guarantees that everyone will die, it's just a matter of when. Thin-tailed risk (like specialized or friendly AI) is survivable long term.

So that's Taleb's general position, and I think a lot can be learned from it. That's why I recommend reading his books even if you don't agree with him. In the places where he is wrong, he is wrong in an /interesting and non-obvious way/.

P.S. I feel like these ideas will not have their maximum impact here on a weekly [[/r/rational]] thread. Suggestions of where to put them instead are welcome. An overview of these things would make a great State Star Codex article, for example, if Scott Alexander decided to investigate. This is why I wanted Eliezer Yudkowsky to weigh in last time. Part of my confusion is /why isn't the rationalist community talking about these important issues and techniques? Does the community have good reasons for disagreement, or are they just unaware?/
:PROPERTIES:
:Author: LieGroupE8
:Score: 2
:DateUnix: 1505223695.0
:DateShort: 2017-Sep-12
:END:

***** u/ShiranaiWakaranai:
#+begin_quote
  More generally, Taleb argues: many people make the mistake of trading short-term bounded risks for long-term existential risks. Quite often, preventing short-term disasters just sweeps problems under the rug until they all collapse at once. For example, bailing out big banks instead of letting them fail just maintains the status quo and ensures that there will be another market crash from corrupt practices. Polluting the atmosphere to generate electricity in the short-term has long-term environmental consequences. Using plasmid insertion to create super-crops that solve hunger in the short term could lead to an ecological disaster in the long term (hence the GMO issue from last time).
#+end_quote

But this mistake is what his philosophy also does. A lot of what he advocates is about keeping the status quo even if you don't know why. Going against the status quo is a short-term risk that he says you shouldn't take, even though keeping the status quo in the long term may be devastating.

The only way to prevent things from being worse in the long term is to actually think. Investigate. Analyze.

Willful ignorance and blind obedience like Taleb advocates are recipes for long term disasters with short term gains.

#+begin_quote
  just listen to the wisdom of the ancients; practices that have survived since ancient times probably don't have existential risk.
#+end_quote

I have discussed the perils of natural selection last time. Just because something is done a lot, doesn't mean it's safer. There are plenty of historical examples of natural selection leading to everyone dying. The very principles of natural selection advocate trading long-term advantages for short-term gains: half your lifespan in exchange for ten times the offspring now, create poison in your bodies which will eventually kill you in exchange for not being eaten by predators now, poison the environment in exchange for some boost to yourself now, etc. etc.

I also find it very inconsistent that Taleb is anti-pollution, anti-fossil fuels. Burning coal and gas is just burning stuff on a larger scale, and burning stuff is literally one of the most ancient human traditions. People have been burning stuff since they were cavemen, despite all the environmental risks, because fire = energy. Whoever burns stuff gains a short-term advantage of light and heat. Even though plenty of towns and nomadic groups have probably burned themselves to death in accidental fires, and groups of cavemen have probably suffocated themselves to death in caves from all their fires sucking all the oxygen, the practice of burning continues because natural selection only cares about the short-term gains. This ancient tradition of burning stuff for short-term gains is exactly why we are paying the price today with global warming, and precisely why I keep advocating against "monkey see monkey do".

Don't just blindly copy, THINK.

#+begin_quote
  Regarding survivability: it's not that surviving is Taleb's terminal goal so much as it's a prerequisite for all goals. If you don't survive, you can't do the utilitarian goal-maximization that you want to do. Therefore, maximizing your long-term survival chances should always be your first worry.
#+end_quote

This sounds dangerously like Knight Templar logic: *I AM THE FORCE OF GOOD. ALL WHO OPPOSE ME ARE THUS EVIL. ONLY I CAN SAVE THE WORLD, SO ONLY I MATTER!*

Only making me more worried here (x.x)...
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1505248242.0
:DateShort: 2017-Sep-13
:END:

****** I think you're just misunderstanding Taleb. Which is understandable, since he makes interpreting himself difficult. A lot of what you are saying is cleared up in his books, which I maintain are worth reading if only for some interesting methods of analysis to add to your mental toolbox.

He's not against reason and analysis; he just spends most of his time discussing how these are misused. He is very much in favor of mathematical analysis. But where you say "Investigate, Analyze," or "Don't just blindly copy, THINK," his point is that in /some/ cases, you /can't./ Like, literally, physically can't, unless you are actually a superintelligence. You can't get enough information about a complex dynamical system to make meaningful predictions (with important exceptions embedded in the mathematics). Like, can /you/ predict what the stock market will be in five years? But you still have to make a decision, and certain decision heuristics are better than others.

#+begin_quote
  Going against the status quo is a short-term risk that he says you shouldn't take, even though keeping the status quo in the long term may be devastating.
#+end_quote

Again, misunderstanding Taleb. If you can see devastation in the future of the status quo, then change, definitely change. It's just that for a certain class of old practices, /if/ the status quo were devastating, /then/ we would have already observed this devastation in the past and changed the status quo. Of course, this is not an automatic conclusion: we need /reasons/ to believe that this is the case, reasons based on the structure of the problem, the time horizon, the degree of devastation, etc. This maps directly into a set of factual questions: for particular issue X, according to our best statistical analysis, /should/ we have a bias towards the status quo for X? Taleb argues yes /for a certain set of issues./ The issue of fossil fuels you bring up requires reasoning about scale, for example. Burning campfires when the population of the planet was in the millions is not at all comparable to industrial pollution in a world with a population of billions. It's an order of magnitude difference that occurred in the last hundred years or so. Differences of that scale are things you actually /can/ reason about effectively. So the debate hinges on factual questions that differ depending on the issue, and as long as these factual questions are unspecified I will not debate this further.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1505252539.0
:DateShort: 2017-Sep-13
:END:


***** u/deleted:
#+begin_quote
  the problem of reasoning about complex systems
#+end_quote

Wargh. What do we mean by "complex systems"? As in complex-systems theory? Something else?

#+begin_quote
  Statements of the form "If A then B" for human-recognizable categories A and B will typically be useless, because by the nature of complexity, we can't get enough bits of shannon information about such propositions for them to be practically useful. Moreover, sometimes when it seems like this sort of reasoning is trustworthy, it isn't.
#+end_quote

Certainly. Verbalized sentences don't really pin down sensory observables very precisely, and we should try not to use them as if they do. Conceptual uncertainty is an important part of clear thinking: /accounting/ for the fact that words map to mental models only noisily, that mental models still generate sensorimotor uncertainty and error, and that when choosing actions we need to weight mental models up and down by how much /sensorimotor/ uncertainty and error they produce, /not/ by their verbal neatness.

This is why I'll tend to get in loud, vehement arguments with philosophy-types about methods: moving concepts around according to the rules of logic doesn't get rid of the inherent uncertainty and error /about the concepts themselves/.

#+begin_quote
  More generally, Taleb argues: many people make the mistake of trading short-term bounded risks for long-term existential risks. Quite often, preventing short-term disasters just sweeps problems under the rug until they all collapse at once. For example, bailing out big banks instead of letting them fail just maintains the status quo and ensures that there will be another market crash from corrupt practices. Polluting the atmosphere to generate electricity in the short-term has long-term environmental consequences. Using plasmid insertion to create super-crops that solve hunger in the short term could lead to an ecological disaster in the long term (hence the GMO issue from last time).
#+end_quote

Yep yep! One nasty bias in our decision-making, possibly even in /optimal/ decision-making, is choosing to control the events we can control /most precisely/, while siphoning risks into the inherently noisier part of the possible-worlds distribution, hoping that noise will save us. Well, the noise is in the map, not the territory, so /actually/ we probably need to marginalize out precision-of-control parameters to make good decisions.

#+begin_quote
  Talebs says: "Hey you guys. Stop naively applying modus ponens and bell curves to complex systems. Instead, here's a bunch of mathematical tools that work better: fractal geometry, renormalization, dynamic time-series analysis, nonlinear differential equations, fat-tailed analysis, convex exposure analysis, ergodic markov chains with absorbing states. It's a lot of math, I know, but you don't need to do math to do well, just listen to the wisdom of the ancients; practices that have survived since ancient times probably don't have existential risk. If you want to go against the wisdom of the ancients, then you'd better be damn careful how you do it, and in that case you'd better have a good grasp on the math."
#+end_quote

I really like that he actually proposes math. That's a very good thing.

I'm generally careful about the Wisdom of the Ancients, because the Ancients are dead. The thing about them is, one of the longest-running, most-repeating narratives about Ancient Civilizations is that they had some fatal flaw and destroyed themselves.

Which may render their advice counterproductive.

#+begin_quote
  Regarding survivability: it's not that surviving is Taleb's terminal goal so much as it's a prerequisite for all goals. If you don't survive, you can't do the utilitarian goal-maximization that you want to do. Therefore, maximizing your long-term survival chances should always be your first worry. You can never eliminate all risk, but you can choose which kind of risk you want to deal with. Fat-tailed risk (like non-value-aligned artificial intelligence!) virtually guarantees that everyone will die, it's just a matter of when. Thin-tailed risk (like specialized or friendly AI) is survivable long term.
#+end_quote

Sounds pretty intuitive, actually, but it also contradicts the principle above of marginalizing out the precision parameters that control whether tails are fat or thin.

#+begin_quote
  So that's Taleb's general position, and I think a lot can be learned from it. That's why I recommend reading his books even if you don't agree with him. In the places where he is wrong, he is wrong in an interesting and non-obvious way.
#+end_quote

Got a book you can recommend?

#+begin_quote
  An overview of these things would make a great State Star Codex article, for example, if Scott Alexander decided to investigate.
#+end_quote

You can suggest it in an open thread.

#+begin_quote
  This is why I wanted Eliezer Yudkowsky to weigh in last time.
#+end_quote

His reddit name is his real name, no spaces or underscores. You can just tag him and see if he responds.
:PROPERTIES:
:Score: 1
:DateUnix: 1505228878.0
:DateShort: 2017-Sep-12
:END:

****** u/LieGroupE8:
#+begin_quote
  What do we mean by "complex systems"? As in complex-systems theory?
#+end_quote

Yes, complex systems theory (the study of ecosystems, economies, chaotic systems, etc).

#+begin_quote
  Got a book you can recommend?
#+end_quote

If you read one book by him, read [[https://www.amazon.com/Antifragile-Things-That-Disorder-Incerto/dp/0812979680][Antifragile]]. [[https://www.amazon.com/Black-Swan-Improbable-Robustness-Fragility/dp/081297381X/ref=pd_bxgy_14_img_2?_encoding=UTF8&pd_rd_i=081297381X&pd_rd_r=GMNSRDVNQVEXP4R4V39H&pd_rd_w=jLuM1&pd_rd_wg=pMZTc&psc=1&refRID=GMNSRDVNQVEXP4R4V39H][The Black Swan]] and [[https://www.amazon.com/Fooled-Randomness-Hidden-Markets-Incerto/dp/0812975219/ref=pd_bxgy_14_img_2?_encoding=UTF8&pd_rd_i=0812975219&pd_rd_r=2526T36NHN4SG8VD1PZE&pd_rd_w=MUMRe&pd_rd_wg=TA0Rc&psc=1&refRID=2526T36NHN4SG8VD1PZE][Fooled by Randomness]] are also good.

#+begin_quote
  You can suggest it in an open thread.
#+end_quote

On [[/r/slatestarcodex]] or on the actual Slate Star Codex website?

#+begin_quote
  You can just tag him and see if he responds.
#+end_quote

I tried this last time, but he didn't reply. Here it goes again: [[/u/EliezerYudkowsky]]
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1505232984.0
:DateShort: 2017-Sep-12
:END:

******* u/deleted:
#+begin_quote
  If you read one book by him, read Antifragile. The Black Swan and Fooled by Randomness are also good.
#+end_quote

Thanks for the recommendation!

#+begin_quote
  On [[/r/slatestarcodex]] or on the actual Slate Star Codex website?
#+end_quote

Actual site.

#+begin_quote
  I tried this last time, but he didn't reply.
#+end_quote

Well, any given person only /has/ to reply if you say their name into a mirror thrice at midnight while offering the blood of their enemies and/or their favorite snack.
:PROPERTIES:
:Score: 2
:DateUnix: 1505233316.0
:DateShort: 2017-Sep-12
:END:


******* *Here's a sneak peek of [[/r/slatestarcodex]] using the [[https://np.reddit.com/r/slatestarcodex/top/?sort=top&t=year][top posts]] of the year!*

#1: [[http://slatestarcodex.com/2016/11/16/you-are-still-crying-wolf/][You Are Still Crying Wolf]] | [[https://np.reddit.com/r/slatestarcodex/comments/5ddf5i/you_are_still_crying_wolf/][948 comments]]\\
#2: [[http://slatestarcodex.com/2017/08/07/contra-grant-on-exaggerated-differences/][Contra Grant On Exaggerated Differences]] | [[https://np.reddit.com/r/slatestarcodex/comments/6s9zcn/contra_grant_on_exaggerated_differences/][457 comments]]\\
#3: [[http://slatestarcodex.com/2017/08/29/my-irb-nightmare/][My IRB Nightmare]] | [[https://np.reddit.com/r/slatestarcodex/comments/6wtylk/my_irb_nightmare/][136 comments]]

--------------

^{^{I'm}} ^{^{a}} ^{^{bot,}} ^{^{beep}} ^{^{boop}} ^{^{|}} ^{^{Downvote}} ^{^{to}} ^{^{remove}} ^{^{|}} [[https://www.reddit.com/message/compose/?to=sneakpeekbot][^{^{Contact}} ^{^{me}}]] ^{^{|}} [[https://np.reddit.com/r/sneakpeekbot/][^{^{Info}}]] ^{^{|}} [[https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/][^{^{Opt-out}}]]
:PROPERTIES:
:Author: sneakpeekbot
:Score: 1
:DateUnix: 1505233002.0
:DateShort: 2017-Sep-12
:END:


*** It seems to me that Taleb applies the same methods of reasoning used by rationalists, but he starts from a different set of assumptions. This doesn't seem particularly confusing to me, unless your confusion lies in why he chooses those assumptions as opposed to others (in which case he would probably reply "empirical evidence").
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 4
:DateUnix: 1505153892.0
:DateShort: 2017-Sep-11
:END:

**** I'm confused because two smart groups of people should not diverge so much in their views. Either a lot of "rationalists" are systematically wrong about a certain set of issues, or Taleb's community is. Or I'm mistaken about how much these views diverge, if they do at all.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1505154283.0
:DateShort: 2017-Sep-11
:END:

***** u/ShiranaiWakaranai:
#+begin_quote
  I'm confused because two smart groups of people should not diverge so much in their views.
#+end_quote

There's a strange tendency to believe that all smart people should agree on things, by virtue of their smartness leading them to eliminate the less intelligent choices. For example, if tasked to solve a difficult math problem, a bunch of average joes may give wildly different answers, while all the mathematically-smart people would give the one correct answer.

For better or worse, this is not how it works in reality. This is because intelligence only tells you: given a goal X and a set of assumptions S, how to achieve X. It doesn't tell you which goal X you should achieve, or which set S of assumptions reflects reality. (Well, technically, it can rule out some sets of assumptions, but a countless number of distinct sets are still possible.) In math, everyone agrees on S and X, so all smart people agree. In reality? Finding two people with the *exact* same S and X is nigh impossible.

And just like a computer program, all it takes is one bit of difference in the right place, to get drastically different behavior.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 3
:DateUnix: 1505183927.0
:DateShort: 2017-Sep-12
:END:


*** If he's in finance, how much money have his views made him? To what degree has he made money by /following those views/, as opposed to making money for other reasons, or by chance?

Do his beliefs pay rent?
:PROPERTIES:
:Score: 1
:DateUnix: 1505152229.0
:DateShort: 2017-Sep-11
:END:

**** Errrrrm... I really don't think you should judge beliefs by their financial gains. That promotes all kinds of evil like theft and fraud and corruption.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1505184170.0
:DateShort: 2017-Sep-12
:END:


**** Apparently he has made enough "fuck you" money from finance to be well-off, and he did it specifically by following his own advice, while the people who made money by chance usually went bust eventually (as he describes in any of his books, if anyone here bothered to actually do research before making judgements about him, and his Wikipedia page is consistent with his statements).
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1505152919.0
:DateShort: 2017-Sep-11
:END:

***** I don't think that [[/u/eaturbrainz][u/eaturbrainz]] was making a judgment about him, but was rather asking a question about his success to you, a relative expert. Making an assumption here, I doubt that he has time to research everything that catches his eye, so he was looking for more information.
:PROPERTIES:
:Author: Adeen_Dragon
:Score: 5
:DateUnix: 1505154239.0
:DateShort: 2017-Sep-11
:END:

****** Yeah, maybe ... sorry [[/u/eaturbrainz]] if I was projecting my own emotions...
:PROPERTIES:
:Author: LieGroupE8
:Score: 3
:DateUnix: 1505154420.0
:DateShort: 2017-Sep-11
:END:


****** Yeah, I'm always really confused about Taleb.
:PROPERTIES:
:Score: 1
:DateUnix: 1505154397.0
:DateShort: 2017-Sep-11
:END:


***** That sounds like it could just be the anthropic principle at work once again. If there are 20 coin flips in a row and a million people each guesses a different pattern then the one person who got it right would talk about how she has the correct strategy and everyone else might have made some guesses correctly but eventually messed up.

It could be that he really is better at gaming the stock market than anyone else, but it is much more likely that he has just been lucky.
:PROPERTIES:
:Author: gbear605
:Score: 1
:DateUnix: 1505154630.0
:DateShort: 2017-Sep-11
:END:

****** I don't think it is in this case, considering that his strategy is specifically "avoid ruin at all costs by having a strong filter on when to accept any deal", which allowed him to survive several market crashes.
:PROPERTIES:
:Author: LieGroupE8
:Score: 2
:DateUnix: 1505156170.0
:DateShort: 2017-Sep-11
:END:


*** u/ShiranaiWakaranai:
#+begin_quote
  How to be Rational About Rationality
#+end_quote

This was pretty helpful, I now understand his views better than the last time we discussed this subject.

Quote from that article: The only definition of rationality that I found that is practically, empirically, and mathematically rigorous is that of survival --and indeed, unlike the modern theories by psychosophasters, it maps to the classics. Anything that hinders one's survival at an individual, collective, tribal, or general level is deemed irrational.

I assume that, since you brought up Eliezer Yudkowsky specifically, you consider the views of the rationalist community to reflect Eliezer Yudkowsky's views. If I'm not mistaken, Eliezer Yudkowsky has roughly utilitarian goals. With that in mind, it's obvious why their views are so different: *they are trying to optimize different goals.*

Let me give a bit of an exaggerated example. Consider a town that practices slavery. A small part of the population are owners that live in luxury, while the remaining are slaves that lead unhappy lives serving the powerful owners. Depending on the goal, the rational choice of action to take is drastically different.

If your goal is utilitarian, that is, to maximize happiness, the rational choice should be to revolt. Free the slaves, even if at cost to the owners. The needs of the many (slaves) outweigh the needs of the few (owners). The expected utility of a revolt is far far higher than the expected utility of keeping the status quo.

If your goal is survival of the individual like Taleb advocates, your action would be to keep the status quo. If you are an owner, your individual survival odds are improved by having slaves, so why free them? If you are a slave, your survival odds are lower if you revolt, since the violence may result in your death. Your expected survival odds are much better if you just shut up and obey. You will live an unhappy life, but you will live.

Taleb also advocates survival of the collective. In this case, the rational choice is to again keep the status quo. A revolt has a small chance of resulting in everyone dead. Keeping the status quo has much better survival odds for the collective.

So you see, there's nothing particularly strange happening here. Eliezer and Taleb may choose opposing actions, but neither are being stupid. Their chosen actions truly are the rational ones for maximizing their own goals. They are opposing simply because their goals are different.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1505186614.0
:DateShort: 2017-Sep-12
:END:

**** u/deleted:
#+begin_quote
  If your goal is utilitarian, that is, to maximize happiness, the rational choice should be to revolt. Free the slaves, even if at cost to the owners. The needs of the many (slaves) outweigh the needs of the few (owners). The expected utility of a revolt is far far higher than the expected utility of keeping the status quo.
#+end_quote

Except that the utility-function formalism doesn't render utilities commensurable, and even if you go measure "hedons" in the slaves' and slaveowners' brains, either can just go ahead and reconfigure their brains to respond to the same events with more hedons, thus forcing a utilitarian to tip their balance.

Utilitarianism doesn't work without first establishing not only a common currency, but one that maps commensurably onto distal (not /just/ in-the-brain) world states.
:PROPERTIES:
:Score: 1
:DateUnix: 1505237551.0
:DateShort: 2017-Sep-12
:END:


** Just went hunting with family in northern Canada, right in the Yukon for 2 weeks.

We left tuesday 2 weeks ago, made it to canada, got off at whitehorse, then drove for 6 hours to watson lake, then took a bush plane into the middle of no where. The fourth day of the trip we hiked for 9 hours one way and posted camp. That night, i was walking like an old man. The next day, we hiked for six hours up a mountain. On the mountain was our quarry. Two mountain goats.

Typing on a phone is obnoxious, more on Friday. I'm at Vancouver airpt right now, waiting for my plane to seattle.
:PROPERTIES:
:Author: Dwood15
:Score: 3
:DateUnix: 1505174523.0
:DateShort: 2017-Sep-12
:END:


** Just finished my recent project, an ebook on instrumental rationality!

Link is [[https://mindlevelup.wordpress.com/][here]].

It's got stuff on planning, habits, and some assorted heuristic-y stuff.
:PROPERTIES:
:Author: owenshen24
:Score: 3
:DateUnix: 1505187905.0
:DateShort: 2017-Sep-12
:END:

*** Thanks for making this available, it's pretty fantastic :)

Question about the last chapter's disclaimer: How do you feel about the idea that, since "rationality is winning," then The True Rationality^{tm} would also involve a better understanding of those "useless" parts of yourself, including why they're not actually useless and how to respect their worth and their balance within your value framework?
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1505236053.0
:DateShort: 2017-Sep-12
:END:

**** Yep! I currently fully endorse this!

I think the common failure mode is to think that you need to beat those parts into submission via systems and habits and conditioning, which I claim isn't good for the long-term.
:PROPERTIES:
:Author: owenshen24
:Score: 2
:DateUnix: 1505263971.0
:DateShort: 2017-Sep-13
:END:

***** Gotcha :) The analogy of taking the lens off made it seem like there was a different lens to put on instead, which may well be true, but I've started to think of rationality as less of a lens and more of a lens crafting tool. You imply something similar with this:

#+begin_quote
  When you do take off those Rationality Glasses, it turns out that you can see even more clearly without them.
#+end_quote

Assuming you mean something like "because of them" rather than literally "without them."
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1505265195.0
:DateShort: 2017-Sep-13
:END:

****** Yep! Those are good nuances to point out, thanks for bringing them to light. I'll try to edit it to be clearer in the coming days.
:PROPERTIES:
:Author: owenshen24
:Score: 2
:DateUnix: 1505268583.0
:DateShort: 2017-Sep-13
:END:


****** u/tiny_smile_bot:
#+begin_quote
  :)
#+end_quote

:)
:PROPERTIES:
:Author: tiny_smile_bot
:Score: -1
:DateUnix: 1505265207.0
:DateShort: 2017-Sep-13
:END:


** Turns out, if you look really closely you can see stuff.

Turns out, human social interaction is a beautiful dance with a dynamic flow.

Turns out, you can enter a meditative state of artistic appreciation if you focus just so.

Turns out, the most important part of charisma is putting every last ounce of your focus into reading the conversation.

Turns out, getting your ears pierced is pretty sweet.

Turns out, with the right nootropic stack you can see into your own malladaptive mental processes and rewrite bits and pieces.

Turns out, you can approach that with just the right methods of meditative focus.

Turns out, sex isn't actually about nerve endings.

Turns out, multiplying 3 digit numbers together in your head is quite fun.

Turns out, the sequences are pretty decent, I probably should've read them earlier.

Turns out, what the FDA ain't made a ruling on, is pretty easy to buy online.

Turns out, your greatest enemy is usually yourself, hiding yourself from yourself.

Turns out, you can be whatever you want to be, if you can get the relevant hormones.

Turns out, the best of the best can gaze into the souls of men and see what they can hardly see themselves.

Turns out, humanity is insane.

Turns out, I want to be an angel when I grow up.

Turns out, the ballad of ancient earth is a grim one.

Turns out, we fight to save the world.

Turns out, you can be good if you try.

Turns out, a bachelors degree in CS and 5 years relevant work experience in a testing lab your college owns can sometimes be enough to net you a 6 figure income out of college.

Turns out, that only just allows you to break even when renting a 2 bedroom in Berkeley.

Turns out, tulpas are a pretty neat mental tool.

Turns out, the related discipline of hypnosis is too.

Turns out, turning off a mental process for a while can sometimes be as useful and enlightening as building a new one.

Turns out, you can churn credit cards for free air miles if you're careful.

Turns out, You can't always generalize among minds.

Turns out, you're all beautiful on the inside, but to see that you have to /see inside/.

Turns out, the attrition rate is over 7,000,000,000 per century.

Turns out, no mind deserves to have to suffer through this shit.

Turns Out.
:PROPERTIES:
:Author: gabbalis
:Score: 4
:DateUnix: 1505172089.0
:DateShort: 2017-Sep-12
:END:

*** Turns out, I enjoyed this :)
:PROPERTIES:
:Author: DaystarEld
:Score: 4
:DateUnix: 1505236234.0
:DateShort: 2017-Sep-12
:END:


*** Nice poetry for a series of bullet-points about your life.
:PROPERTIES:
:Score: 1
:DateUnix: 1505237614.0
:DateShort: 2017-Sep-12
:END:
