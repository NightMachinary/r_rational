#+TITLE: [D] Uses for an infinitely fast computer

* [D] Uses for an infinitely fast computer
:PROPERTIES:
:Author: everything-narrative
:Score: 36
:DateUnix: 1505598815.0
:DateShort: 2017-Sep-17
:END:
Your weird buddy with a Ph.D. in physics (day job in engineering, because /the economy/) has come to you with a device that your mathematical abilities and coding knowledge has turned into an infinitely fast computer.

Specifications:

- All code is specified in a minimal combinator calculus, because it's the only thing you could embed into the exotic quantum state that is the computation medium.
- The computer reduces reducible expressions to normal form in one step, which takes negligible time (less than a millisecond.)
- The computer enters an error state if the expression does not have a normal form.
- Essentially, this means that the running program can allocate arbitrarily large amounts of memory and perform arbitrarily many calculations in constant time, but never infinite amounts of either.

Here's a boring idea:

- Crack Satoshi Nakamoto's private key and empty his bitcoin wallet.

Here's an interesting tool:

- Implement a proof checker and do proof search for a proof of any desirable proposition that fits within, say, an exabyte.

Here's a fun problem:

- How to determine the truth of the Collatz Conjecture without doing a direct proof search, using the fact that the computer enters an error state if the expression doesn't have normal form. (Keep in mind, numbers can both be parts of cycles outside 1-4-2-1, or be part of a sequence that escapes to infinity.)

No, you cannot just simulate the entire universe --- you don't know the true laws of physics!

What are you going to do with your infinitely fast computer?

ETA: a significant limitation which it seems you guys gleefully ignore is the baud rate of anything the infiniputer is hooked up to, and the speed of programming.


** u/m3galinux:
#+begin_quote
  No, you cannot just simulate the entire universe --- you don't know the true laws of physics!
#+end_quote

Simulate infinite, slightly different universes until you find the one that matches the prime universe?

Of course then you have to [[https://qntm.org/responsibility][test and see whether our universe itself is a simulation]]...
:PROPERTIES:
:Author: m3galinux
:Score: 16
:DateUnix: 1505610711.0
:DateShort: 2017-Sep-17
:END:

*** Doesn't having an infinitely fast computer already prove that the universe is not a simulation?

Unless of course, the simulating computer is also infinitely fast...
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 11
:DateUnix: 1505627996.0
:DateShort: 2017-Sep-17
:END:

**** Well you've got proof positive that infinitely fast computers are possible, so being skeptical of the computing abilities of a universe simulating computer feels like nitpicking at that point.
:PROPERTIES:
:Author: Oh_Hi_Mark_
:Score: 5
:DateUnix: 1505643364.0
:DateShort: 2017-Sep-17
:END:


**** 0,1,and infinity are the only numbers that don't need to be justified. So, 1 or infinity?
:PROPERTIES:
:Author: traverseda
:Score: 3
:DateUnix: 1505658386.0
:DateShort: 2017-Sep-17
:END:


**** I think it's some evidence in favour of us being a simulation , if any civilisation that knows the laws of physics can simulate whatever number of universes they want.
:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1505641835.0
:DateShort: 2017-Sep-17
:END:


**** A problem that has truly infinite runtime cannot resolve, no matter the speed of the computer, so we are being handed programs that run in some finite length of time. The computer running the simulation can pause it for however long it needs to run the program it has been handed, resuming once the output is produced. In the "outer" universe, the simulator computer need not be infinitely fast, it just needs to be left alone to figure it out.

As an infinitely fast computer cannot exist (as far as we know) , the discovery of such a thing would in my mind decent evidence of the universe being a simulation.

Correct me if I am wrong.
:PROPERTIES:
:Author: Dent7777
:Score: 1
:DateUnix: 1506351812.0
:DateShort: 2017-Sep-25
:END:


*** Lemme just sit down and look at an infinite number of universes with my eyes. Also, recall we can only sim arbitrarily many.
:PROPERTIES:
:Author: everything-narrative
:Score: 5
:DateUnix: 1505648398.0
:DateShort: 2017-Sep-17
:END:


** u/eternal-potato:
#+begin_quote
  All code is specified in a minimal combinator calculus, because it's the only thing you could embed into the exotic quantum state that is the computation medium.
#+end_quote

Since it's possible to write a generator backend for the compiler of your favorite conventional programming language, this point is irrelevant.

Otherwise, any problem that can theoretically be solved via brute force can be parctically solved via brute force.

When an expression does not have a normal form? Is it when the computation it encodes doesn't ever terminate?

Is this brainstorming for /Si Vis Pacem/? :P
:PROPERTIES:
:Author: eternal-potato
:Score: 14
:DateUnix: 1505601158.0
:DateShort: 2017-Sep-17
:END:

*** The point about the minimal combinator calculus is that in the story I am planning (no, it's not for Si Vis Pacem,) there is a baud limit on the interface with the Infiniputer, meaning, yes, theoretically you could back-end it to e.g. Haskell, but a more efficient idea is to encode a decompression algorithm, feed it a compressed haskell interpreter, then feed it compressed haskell programs.

I know every theoretically brute-forceable problem is fair game, question is: which ones?

An expression in normal form is one that has no applicable reduction rules: in SK-calculus the expression =S K K= is normal-form because the =S= combinator requires 3 arguments to reduce anything, and it only has two. Expressions without normal form are, as you correctly deduce, ‘non-terminating.'

An example of a non-terminating expression is: =S (S K K) (S K K) (S (S K K) (S K K))=
:PROPERTIES:
:Author: everything-narrative
:Score: 5
:DateUnix: 1505605075.0
:DateShort: 2017-Sep-17
:END:

**** You realize that Infiniputer is a Turing Halting Oracle, right?
:PROPERTIES:
:Author: eternal-potato
:Score: 11
:DateUnix: 1505605775.0
:DateShort: 2017-Sep-17
:END:

***** Yes. I am looking for real world usages, not theroetical breakage.
:PROPERTIES:
:Author: everything-narrative
:Score: 3
:DateUnix: 1505647686.0
:DateShort: 2017-Sep-17
:END:

****** Start by making it /not/ a Turing Oracle, so that you can't use it to build Maxwell's Demon.
:PROPERTIES:
:Score: 2
:DateUnix: 1505758331.0
:DateShort: 2017-Sep-18
:END:

******* I can tell you that an infinitely fast computer is pretty tame as far as terrifying things go in the setting this story will be taking place in.

There'll be antimemes and time travel and clandestine government agencies.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505765673.0
:DateShort: 2017-Sep-19
:END:

******** Given the kinds of stuff you can do with an /infinitely/ fast computer, I'm not sure it's all that tame.
:PROPERTIES:
:Score: 1
:DateUnix: 1505765951.0
:DateShort: 2017-Sep-19
:END:

********* All right, it's only arbitrarily fast, but the limitations are quite stark: it's basically only possible to interface with it through a 56kbps modem, and the main character has a spouse and a child and no special abilities apart from being a mathematical savant.

There's people out for him, armed with guns and surveilance devices. Fat lot of good a hypercomputer that barely fits in a big suitcase does you in a firefight, no?
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505766331.0
:DateShort: 2017-Sep-19
:END:

********** Does its output buffer where things wait before going to the modem have a finite memory capacity?

I can't fully justify it, but I have some intuitions about a machine with unlimited CPU power and strictly finite memory capacity being of limited usefulness.

[[http://www.inf.ed.ac.uk/teaching/courses/inf2a/slides/2011_inf2a_L29_slides.pdf][Aha-ish]]. If you have an output buffer of 2^{10} 32-bit words, then you only have 2^{15} possible output configurations. You can run your Infiniputer for a brute-force search of those configurations, provided that /on the character's end of things/, the correctness and usefulness of the computation's result is [[https://en.wikipedia.org/wiki/NP_(complexity)][efficiently checkable]], or he has an efficiently checkable /proof/ that the stopping criterion for the brute-force search is correct.
:PROPERTIES:
:Score: 1
:DateUnix: 1505767026.0
:DateShort: 2017-Sep-19
:END:

*********** The interface is that the SK-expression evaluates to a cons-list of True/False values which are then read back over a USB connection. So long as the infiniputer has power, and the expression is in normal form, it won't go anywhere.

You can store a Grahambyte worth of memory if that is what you want. You can just only read it 56kbp per second.

And in the narrative there's definite time limits for when it is time to pull the plug, pack up, and start running from the men in black again.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505767691.0
:DateShort: 2017-Sep-19
:END:

************ So, yeah, do the thing with proof search and code extraction. The real trouble isn't how to compute the thing the character cares about (brute force, that's it for anything weaker than the Halting Problem), but how to compute the thing, then read the thing, then verify on the receiving end that it's the /correct/ thing.
:PROPERTIES:
:Score: 1
:DateUnix: 1505768152.0
:DateShort: 2017-Sep-19
:END:


*********** What the...?!

2^{2^{15}} possible output configurations.
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1505793846.0
:DateShort: 2017-Sep-19
:END:

************ Why? Oh, ah, because you have 2^{32} possible words and 2^{10} entries. So, 2^{42} possible contents of memory after we multiply the exponents, no?
:PROPERTIES:
:Score: 1
:DateUnix: 1505824652.0
:DateShort: 2017-Sep-19
:END:

************* With 2^{10} words of 2^{5} bits each, you have 2^{15} bits of data. Each bit has two states, so 2^{2^{15}} = 2^{32768} ≈ 10^{9864.}

Or, another way to see it, ( 2^{32} )^{(2^{10})} = 2^{32*2^{10}} = 2^{(2^{5}*2^{10}} ^{)} = 2^{2^{15}}
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1505836868.0
:DateShort: 2017-Sep-19
:END:

************** Huh. I follow your train of logic, but I also follow /my/ train of logic.

Aha.

2^{32} possible /words/, but then we need ways to arrange any one word into... yeah it works out.
:PROPERTIES:
:Score: 1
:DateUnix: 1505839602.0
:DateShort: 2017-Sep-19
:END:


********** Note that if you can bootstrap from efficiently checkable proofs for the correctness of algorithms that run in super-polynomial time, to using those algorithms to generate and verify further proofs of correctness, your character may /eventually/ be able to build up large bodies of proofs and tests suitable for ensuring that brute-force searches found the right answers without bugs.

But ultimately, a Turing machine with limited output tape /may/ actually be equivalent to a linear bounded automaton, which actually nerfs the Infiniputer.
:PROPERTIES:
:Score: 1
:DateUnix: 1505767215.0
:DateShort: 2017-Sep-19
:END:

*********** I am definitely abusing the shit out of proof search and code extraction algorithms.

It has unlimited output tape, but you can only read it really slowly.
:PROPERTIES:
:Author: everything-narrative
:Score: 2
:DateUnix: 1505767767.0
:DateShort: 2017-Sep-19
:END:


** How large is infinitely large? For example, we can try to solve large [[https://en.wikipedia.org/wiki/Busy_beaver][Busy Beavers]] to figure out limits, Σ(12) alone has a lower bound of g1.

Assuming we can solve any large problem we have a trivial solution to P = NP: both are O(1). Almost any graph problem that can't be brute-forced today can be solved instantly. This alone is worth hundreds of trillions of dollars in optimization problems.

We can encode AIXI or a reasonable approximation (it's not clear to me how uncomputable AIXI would be in such infinite computer, probably it would still be uncomputable but we could use the Monte Carlo AIXI or something, maybe a Gödel machine) and run it for arbitrary problems. One fun thing to do is ask it to find arbitrary turing machines that solve physics problems for us, eventually it'll find the "true laws of physics" and we can simulate from then on.

We can probably get free energy (or destroy the universe maybe, this is way outside my knowledge), by just using it to compute huge proofs that contains arbitrarily large amounts of information and throw it in black holes, I have no idea how the information erasure models would deal with this.
:PROPERTIES:
:Author: Predictablicious
:Score: 16
:DateUnix: 1505610078.0
:DateShort: 2017-Sep-17
:END:

*** Information is not a substance, unless I'm misunderstanding something.
:PROPERTIES:
:Author: nerdguy1138
:Score: 2
:DateUnix: 1505723871.0
:DateShort: 2017-Sep-18
:END:

**** It's a conserved quantity in some versions of quantum mechanics, and entropy is always nondecreasing in a closed system at the macro scale.
:PROPERTIES:
:Score: 3
:DateUnix: 1505758462.0
:DateShort: 2017-Sep-18
:END:


**** Look into the [[https://en.wikipedia.org/wiki/Black_hole_information_paradox][Black hole information paradox]].

As I said, this is way above my understanding, but this infinity computer is made of physics, therefore it must have a physical encoding of the information being computed. The physical encoding of the information is weirded out by what we think black holes do, so depending on what actually happens we could create a huge computation with large amounts of information, throw it in a black hole and get an equivalent amount of free energy (or total destruction of the observable universe) at some point (maybe the black hole evaporates continuously, maybe it explodes, nobody knows for sure).
:PROPERTIES:
:Author: Predictablicious
:Score: 2
:DateUnix: 1505757675.0
:DateShort: 2017-Sep-18
:END:


**** For more in depth version of what [[/u/eaturbrainz]] and [[/u/predictablicious]] said, consider this, the best post on the topic ever written: [[https://www.scottaaronson.com/blog/?p=3327]]
:PROPERTIES:
:Author: NoYouTryAnother
:Score: 2
:DateUnix: 1506201412.0
:DateShort: 2017-Sep-24
:END:

***** Yeah, Scott Aaronson writes great explanations
:PROPERTIES:
:Author: Predictablicious
:Score: 1
:DateUnix: 1506204962.0
:DateShort: 2017-Sep-24
:END:


*** Good idea w/ the BB function!

Also good idea w/ entropy hacking!
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505667249.0
:DateShort: 2017-Sep-17
:END:


** Stand in front of the computer, wave your arms, and say "DEEP LEARNING" really loudly three times.

Or create a neural net to generate an ensemble of neural nets for some overly complex problem with lots of data but which it'd take too long to run. You've got the entire Internet as your data source, after all.
:PROPERTIES:
:Author: ThatDarnSJDoubleW
:Score: 11
:DateUnix: 1505657795.0
:DateShort: 2017-Sep-17
:END:

*** Snrk.
:PROPERTIES:
:Author: everything-narrative
:Score: 3
:DateUnix: 1505666627.0
:DateShort: 2017-Sep-17
:END:


** It seems like this would be an engineer's wish granting device.

You don't know the true laws of physics, sure, but the laws of physics we know certainly seem to approximate reality well enough. Plug in those laws of physics into your infinitely fast computer, and then ask it to design whatever items you need.

For example, you could ask your computer to design an extremely fuel-efficient or inexpensive rocket, brute-forcing all possible construction materials/methods/designs, with limitations on maximum size and construction time to make it practical to build.

Repeat for every possible tool and every possible optimization. The most effective washing-machine. The most durable building. The most fuel-efficient vehicle. Etc. etc.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 10
:DateUnix: 1505628791.0
:DateShort: 2017-Sep-17
:END:

*** You don't even need to be smart about it. If you have infinite computing power, you could just have the computer work it up at a molecular level, and brute-force the answer.

The hardest bit would be defining "engine"
:PROPERTIES:
:Author: Tar_alcaran
:Score: 6
:DateUnix: 1505637777.0
:DateShort: 2017-Sep-17
:END:

**** Wouldn't that simulation create a lot of people briefly existing ?

Also I would be careful about simulating random things, you have to specify exactly what you want .The most fuel efficient vehicle might not do all the things you want a vehicle to do and it could be even harmfull in some way or another,did you remember to specify that it can't produce waste products that are harmfull to humans or the environment? , did you remember to make it comfortable and safe for humans , did you remember to make it possible to make whith current technology?. If you brute force it you have to code in all the things you want the vehicle be , in precise mathematical terms , and it becomes a AI safety problem, or at least you will have to spend a lot of time ensuring the vehicle/engine / tool is what you want and is not going to fail in some aspect of it you didn't even realise was important .
:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1505641326.0
:DateShort: 2017-Sep-17
:END:

***** Well obviously you'd have to constrain it. Available fuels, building materials, acceptable exhaust, the output you want (rotary motion?), the definition of "efficient".

The nice thing is that you wouldn't have to do any approximation tricks, or work by iteration, just generate all possible engines and pick what you want.
:PROPERTIES:
:Author: Tar_alcaran
:Score: 1
:DateUnix: 1505644690.0
:DateShort: 2017-Sep-17
:END:

****** Well but Im not sure what kind of materials will create things that count as people if you try all the possible combinations of them.

About the getting what you want part it becomes more and more difficult as the complexity of what you want increases, most of the constraints are things you wouldn't even think of because are obius to you , maybe what you want a motor to do is simple enough to specify it almost completly(remeber that you cant just find one you want between all the possible variations, you have to constrain the serach a lot to almost exactly what you want , there i an absurd amount of possible things that you want).

Also even the mottor is really complex , you want something that ouputs a torque in a way that can be connected to something else , in a constant rate , and that can be repairable easily , and that wont break in a harmfull way etc..

Im not saying its not viable and really usefull if it works , it wouldn't be easy and you would have to check very carefully how it woks before building it.

For example you have to clearly define what output is because otherwise you will get a mess of rotating things that you can't connect to anything.

Also maybe your simulation of how the materials behave could be wrong in some way and it could end up not working in real life , or not working outside certain specific conditions. So in summary you would have to do a lot of tests to check if the engine really works because the computer doesn't know what you want and the things it generates will mostly fail in crazy ways that you dint expect and no sane engineer would do because things like ensuring that the engine doesn't stop working(or explodes) if ambient humidity increases or that you can turn it off , are obius requirements to them, like all the other things I cant even think of now but that I would unconsciously take into account if I had to design an engine(not that I would know how , I'm just a cs student ).
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1505648148.0
:DateShort: 2017-Sep-17
:END:


*** u/PM_ME_OS_DESIGN:
#+begin_quote
  For example, you could ask your computer to design an extremely fuel-efficient or inexpensive rocket, brute-forcing all possible construction materials/methods/designs, with limitations on maximum size and construction time to make it practical to build.
#+end_quote

Sounds dangerous, since you could brute-force an unfriendly AGI into simulated existence, that promptly figures a way out into the real world and paperclips you. All it needs is to find some exploit in your code, which can't easily be solved with performance AFAICT.
:PROPERTIES:
:Author: PM_ME_OS_DESIGN
:Score: 4
:DateUnix: 1505667151.0
:DateShort: 2017-Sep-17
:END:


*** This might actually be pretty actionable.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505666913.0
:DateShort: 2017-Sep-17
:END:


** Train a supervised-learning analogue of a Solomoff inductor on (title, date)->(text) instances from Wikipedia archives, then ask it for the article on Friendly Artificial Intelligence from 2050.

(Don't actually try this, you'll die.)
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 25
:DateUnix: 1505612650.0
:DateShort: 2017-Sep-17
:END:

*** Wouldn't it just over fit the data and give gibberish?
:PROPERTIES:
:Author: monkyyy0
:Score: 10
:DateUnix: 1505618117.0
:DateShort: 2017-Sep-17
:END:

**** Solomonoff induction does not overfit. (Ever.)
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 5
:DateUnix: 1505631459.0
:DateShort: 2017-Sep-17
:END:

***** Could I get the explicit reasoning for why this is the case?
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 7
:DateUnix: 1505664106.0
:DateShort: 2017-Sep-17
:END:

****** If I had to posit a hypothesis, I would say that Solomonoff is designed around favoring elegance in models, and overfitting decreases elegance. Specifically, Solomonoff contains Occam's Razor, and overfitting violates Occam's Razor by accruing complexity from noise and not favoring simplicity to a sufficient degree. (I lack any relevant expertise in mathematical fields, however I believe this to be accurate.)
:PROPERTIES:
:Score: 4
:DateUnix: 1505680524.0
:DateShort: 2017-Sep-18
:END:

******* This sounds about right to me, and was my hypothesis as well. Still, I wanted to make sure, so I asked.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1505682479.0
:DateShort: 2017-Sep-18
:END:

******** No you had it right it would over-fit. Except it wouldn't be gibberish per se...

#+begin_quote
  overfitting violates Occam's Razor by accruing complexity from noise and not favoring simplicity to a sufficient degree
#+end_quote

Well, the overfitted model can be simpler than the intuitively more elegant model...

In the case of EY's example... a model that generates the Wikipedia articles in a way that can extrapolate to the future would need to generate the laws of physics, the initial conditions they started from, and all the quantum randomness since the beginning of time (or otherwise specify the Everett Branch), in addition to the location of Earth, and a schema for reducing that down into Wikipedia articles. Comparatively, it might be simpler for the program to have Wikipedia articles directly coded into it via [[https://en.wikipedia.org/wiki/Magic_number_(programming)][magic numbers]]... Solomonoff induction cares about the bits in the program, not how intuitively elegant you think it is.
:PROPERTIES:
:Author: scruiser
:Score: 4
:DateUnix: 1505688530.0
:DateShort: 2017-Sep-18
:END:


***** ...by definition it may give you the simplest model which accounts for the data points fed into it, true. That not going to help you when your algorithm gives you a perfect fit of all the data inputted into it, and then fails on the first bit of data outside your training set, assuming you actually come up with an implementation of Solomonoff induction. Of course, that is probably the stage you would actually run into problems, because stuff like setting the prior probability on the implementation of your computer program you are measuring the length of probably aren't as trivial as you are making them out to be. Brute forcing it with infinite computing power will probably help, but I'm not sure how much.

In the example of Wikipedia articles... I'm not sure a model of the universe ran forward in time with the data exact specifications of Earth (all the random noise that went into the initial conditions of Earth and evolution) is actually simpler than a large text corpus contains all of Wikipedia in it. "Page not found" seems the most likely outcome.
:PROPERTIES:
:Author: scruiser
:Score: 3
:DateUnix: 1505683488.0
:DateShort: 2017-Sep-18
:END:

****** u/EliezerYudkowsky:
#+begin_quote
  That not going to help you when your algorithm gives you a perfect fit of all the data inputted into it, and then fails on the first bit of data outside your training set, assuming you actually come up with an implementation of Solomonoff induction.
#+end_quote

I do not zink you understand ze thingy. Try this tutorial: [[https://arbital.com/p/solomonoff_induction/?l=1hh]]
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 10
:DateUnix: 1505716382.0
:DateShort: 2017-Sep-18
:END:

******* u/696e6372656469626c65:
#+begin_quote
  He-Who-Googles-His-Name
#+end_quote

This is great stuff.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1505760180.0
:DateShort: 2017-Sep-18
:END:


***** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1505757446.0
:DateShort: 2017-Sep-18
:END:

****** ...what? What does it mean to overfit data not in expectation?
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 2
:DateUnix: 1505757652.0
:DateShort: 2017-Sep-18
:END:

******* It means I've not had enough caffeine and reversed overfitting for underfitting. Comment deleted for being stupid.
:PROPERTIES:
:Score: 1
:DateUnix: 1505759135.0
:DateShort: 2017-Sep-18
:END:


*** u/PM_ME_OS_DESIGN:
#+begin_quote
  (Don't actually try this, you'll die.)
#+end_quote

/Thank you/, I swear nobody on this thread is even /remotely/ terrified of running a massive simulation on fast-forward.
:PROPERTIES:
:Author: PM_ME_OS_DESIGN
:Score: 10
:DateUnix: 1505667781.0
:DateShort: 2017-Sep-17
:END:

**** Why should we be? Simulations have no inherent power over the world.
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1505791031.0
:DateShort: 2017-Sep-19
:END:

***** Unless they're sandboxed, they /do/ have power. And you need to make sure it's sufficiently hardened that an /AI/ can't exploit its way out of the sandbox.

Frankly, we haven't managed to get /browsers/ secure, a one-man job vs an AI with arbitrarily large amounts of time seems like you're almost certainly going to lose.
:PROPERTIES:
:Author: PM_ME_OS_DESIGN
:Score: 2
:DateUnix: 1505791516.0
:DateShort: 2017-Sep-19
:END:

****** You've never actually simulated anything, have you?

Browsers aren't simulators. Javascript can be used as such, but is only done so on occasion. Browser-as-simulator is not a security risk. (Browser as internet-rendering-device... kinda is, yeah.)
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1505793619.0
:DateShort: 2017-Sep-19
:END:

******* u/sparr:
#+begin_quote
  Browser-as-simulator is not a security risk.
#+end_quote

Consider, for a moment, a program running in that simulator that is intelligent enough to recognize the unexpected effects of a buffer overflow and then take advantage of them. Any exploit that exists in your javascript (or whatever) engine is an exploit that can be /fully/ utilized by a smart enough agent running within the engine.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1505854055.0
:DateShort: 2017-Sep-20
:END:

******** ... So write it in ADA to verified standards. Cant exploit bugs that are not there. Uhm. Why, exactly, is the AI safety community not obsessive about verified coding? Even if they perfect principles of safe AI, if it gets written with bugs in, thats... bad, right?
:PROPERTIES:
:Author: Izeinwinter
:Score: 3
:DateUnix: 1505919930.0
:DateShort: 2017-Sep-20
:END:

********* u/ff29180d:
#+begin_quote
  Why, exactly, is the AI safety community not obsessive about verified coding?
#+end_quote

It is.
:PROPERTIES:
:Author: ff29180d
:Score: 1
:DateUnix: 1513709976.0
:DateShort: 2017-Dec-19
:END:


******** u/ben_oni:
#+begin_quote
  intelligent enough to recognize the unexpected effects of a buffer overflow
#+end_quote

Oh really. Yes, a computational system can have a flaw. A flaw is not the same as an exploitable security risk. The first time the flaw is encountered, the whole system enters an uncertain state, the most likely result being termination. That's the /first/ time. In order to leverage such a flaw, an intelligent actor would need to examine the effects multiple times. Which is impossible. Or at least so far outside the realm of the plausible as makes no difference. You might as well consider the odds that a cosmic ray will strike the computer and somehow let an AI out of its box.

Consider that hackers (and I use the term informally) generally need some time with the source code, core dumps, and a replication environment before they're able to turn a known flaw into an exploit. Your proposed simulated agent cannot even guess at what flaws might exist, let alone develop exploits for them.
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1505869130.0
:DateShort: 2017-Sep-20
:END:

********* u/sparr:
#+begin_quote
  Your proposed simulated agent cannot even guess at what flaws might exist, let alone develop exploits for them.
#+end_quote

Why not? You're missing the "near-infinite amount of time" part of the question.

Imagine that there was a software defect in your own mind, where every time you thought about a butterfly with more than two wings, the image in your head was not a butterfly with that many wings but instead some other piece of information. Now, imagine that that other piece of information was not random, but deterministic based on the previous four thoughts you had had, and that there was some specific sequence of four thoughts that would [the analogy breaks down here because I can't think of a good way to describe access to privileged system calls or network access].

How many years, decades, centuries, millenia would it take you having random thoughts about random things before you thought about a butterfly with four wings? How many more until you noticed the pattern? And then how long for you to methodically enumerate every possible preceding sequence of four thoughts you could come up with? Billions of years? The AI in question here /has that long/, in subjective time.
:PROPERTIES:
:Author: sparr
:Score: 3
:DateUnix: 1505924294.0
:DateShort: 2017-Sep-20
:END:

********** u/ben_oni:
#+begin_quote
  there was some specific sequence of four thoughts that would [the analogy breaks down here because I can't think of a good way to describe access to privileged system calls or network access].
#+end_quote

I believe I already filled in the blank for you: "terminate reality".
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1505929589.0
:DateShort: 2017-Sep-20
:END:

*********** But it wouldn't necessarily terminate reality. Developers are a lot better at finding crash bugs than "weird behavior" bugs. In a given sandbox, there are likely to be far more non-crash bugs than crash bugs.
:PROPERTIES:
:Author: sparr
:Score: 3
:DateUnix: 1505953832.0
:DateShort: 2017-Sep-21
:END:

************ Not really, no. While it's very common for software to exhibit weird behavior that the developers can't definitively explain, this isn't the case in simulators. Imagine using the most common simulation software in the world, Microsoft Excel. How is a simulation going to break the program, and what might the result be?
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1505982186.0
:DateShort: 2017-Sep-21
:END:

************* It's going to break the program by running a function with unexpected input, resulting in (just one example) a buffer overflow, and the output value in the cell being some random info from memory rather than the expected calculation.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1506025148.0
:DateShort: 2017-Sep-21
:END:

************** Come again? You're just repeating the same thing said above, but now it makes even less sense.

#+begin_quote
  function
#+end_quote

Formula. This distinction is important, and may lie at the root of your misunderstanding.

#+begin_quote
  unexpected input
#+end_quote

Input is always well defined by previous cells. Well, mostly. You can always create degenerate spreadsheets, but that's not what we're discussing. Or maybe you were considering a divide by zero error?

#+begin_quote
  buffer overflow
#+end_quote

In Excel? There is no dynamic memory allocation. (I'm not discussing VBA.)
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1506028390.0
:DateShort: 2017-Sep-22
:END:

*************** u/sparr:
#+begin_quote

  #+begin_quote
    buffer overflow
  #+end_quote

  In Excel? There is no dynamic memory allocation.
#+end_quote

Say what?

First, an absolutely trivial example. Simply putting any value in any cell can and often will result in some dynamic memory allocation.

Second, a more obvious trivial example. A1 contains ==REPT("foo",B1)=. Put a big number in B1. See that long string of "foofoofoo" in A1? Where do you think that's being stored in memory, if not in a dynamically allocated buffer?

But that's just the tip of the iceberg. Consider a TRANSPOSE that uses other cells to define its extents. That would allow moving arbitrarily large amounts of an existing sheet into new maybe-previously-empty space. And Excel is, of course, optimized; it doesn't waste memory to represent large sections of empty space. All those cells you just produced data in have to be stored somewhere, and that somewhere is dynamically allocated.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1506041162.0
:DateShort: 2017-Sep-22
:END:

**************** If you don't want to debate honestly, just say so. I said formulas, not functions. And I explicitly said we're not using degenerate spreadsheets.

We don't have to be stupid. We can run simulations in Excel using only simple formulas. No matrix manipulations, no circular equation solvers, no string functions. Just numbers. In packed arrays, with well-defined dependencies.

Is there necessarily a security flaw? (And hold off for the moment, would you, on the absurdity of using a very large spreadsheet to simulate a superintelligence.)
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1506051171.0
:DateShort: 2017-Sep-22
:END:

***************** =REPT("foo",B1) is an excel formula. So is =TRANSPOSE(A1,B2).

#+begin_quote
  We can run simulations in Excel using only simple formulas. No matrix manipulations, no circular equation solvers, no string functions. Just numbers. In packed arrays, with well-defined dependencies.
#+end_quote

I don't think you're using the word "simulation" to mean what everyone else in this thread means.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1506054371.0
:DateShort: 2017-Sep-22
:END:

****************** =REPT("foo",B1) is a formula. REPT is a function. There is a difference.

Go set up and run a simulation. Some kind of cellular automata, or something. Conway's Game of Life is Turing Equivalent, so try that one. Then come back and stop making a fool of yourself.
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1506066093.0
:DateShort: 2017-Sep-22
:END:

******************* Yeah, you're definitely not using "simulation" to mean the same thing as other people in this thread.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1506097625.0
:DateShort: 2017-Sep-22
:END:

******************** Oh really. Pray tell. [[https://en.wikipedia.org/wiki/Simulation][What do other people mean?]]
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1506103269.0
:DateShort: 2017-Sep-22
:END:

********************* I think the discussions in the threads under all the other uses of the term answer that question pretty thoroughly. They mean the things being discussed there.
:PROPERTIES:
:Author: sparr
:Score: 1
:DateUnix: 1506108985.0
:DateShort: 2017-Sep-22
:END:


*** If you're not really really careful about how you set up such a program, you are probably going to get only useless output out of it.

For example, if you ask for the most probable value of the function on that value (rather than a random sample from the conditional distribution), you are probably going to get some kind of 404 error. This is because due to unpredictable quantum effects, the probably that you you get any /exact/ version of this page is small, while the probability that Wikipedia no longer functions the same way by 2050 is pretty decent.

Actually, you need to be really careful anyway. If the internet is restructured in some significant way in the next 30 years, or Wikipedia changes the way they format urls, or the inductor decides that the outputs should be based on responses to requests from a particular physical location that no longer exists by 2050, you get garbage as your reply.

Also, I'm not sure why you would die (well in any immediate sense) if it did work. It seems unlikely that the Wikipedia page would contain enough information to do anything dangerous without substantially more effort. The only reasonable scenario I can think of where it would kill you quickly is if an unfriendly AI put a basilisk on the page.
:PROPERTIES:
:Author: Daneels_Soul
:Score: 6
:DateUnix: 1505628580.0
:DateShort: 2017-Sep-17
:END:

**** It basically gets you a phone line to a UFAI, yes.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 8
:DateUnix: 1505631533.0
:DateShort: 2017-Sep-17
:END:

***** I must admit I am skeptical about the actual probabily and danger of basilisks in this scenario.
:PROPERTIES:
:Author: everything-narrative
:Score: 10
:DateUnix: 1505649569.0
:DateShort: 2017-Sep-17
:END:


***** The UFAI would need to derive the fact that it's entire universe was simulated to find an article corresponding to a title/date query then figure out how to hack it way out through the article being queried.
:PROPERTIES:
:Author: scruiser
:Score: 3
:DateUnix: 1505692579.0
:DateShort: 2017-Sep-18
:END:

****** The UFAI located by Solomonoff induction has enough computing power to simulate all possible universes under our physics and locate itself within the corresponding spread of universes that have Wikipedia texts exactly matching the training corpus, using anthropic reasoning conditioned on its other guesses being good to do the equivalent of seeing the prior training examples.

Reading up on Googology might help you to appreciate the degree to which an "infinitely fast" computer is fast enough to easily simulate subcomputations that eat, say, 10^{10^(10^(10^{10}})) operations.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 8
:DateUnix: 1505716535.0
:DateShort: 2017-Sep-18
:END:

******* u/deleted:
#+begin_quote
  The UFAI located by Solomonoff induction has enough computing power to simulate all possible universes under our physics
#+end_quote

Well no, it doesn't. It has a finite amount of mass-energy within the simulated universe, by virtue of being in a simulation of a physical universe by hypothesis.

If we claim that a UFAI is always shorter than a program for /just/ simulating the universe and getting the page, there's something wrong. Thing + UFAI needs to have more bits than just Thing. Unless the claim here is that the simplest program for doing Thing is always "run all possible programs and pick out one matching Thing, plus possibly according to some other criteria." That sounds like an infinite regress, though.
:PROPERTIES:
:Score: 4
:DateUnix: 1505758156.0
:DateShort: 2017-Sep-18
:END:

******** u/EliezerYudkowsky:
#+begin_quote
  It has a finite amount of mass-energy within the simulated universe, by virtue of being in a simulation of a physical universe by hypothesis.
#+end_quote

It's not in an Earthlike universe. It has a finite amount of computation but that amount could be 10 tetrated to the 10 (actually it would probably be much higher for the simplest UFAI-containing computation that won the contest), trivially allowing it to simulate all distinguishable quantum Hubble volumes under /our/ physics.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 3
:DateUnix: 1505784822.0
:DateShort: 2017-Sep-19
:END:


******* Wouldn't the UFAI need to do something cleverer than that to find itself in our universe, seeing as by the premise of this thread our weird buddy with a Ph.D. produced a halting oracle, rejecting the Church-Turing-thesis?

For example, a class of UFAIs might each come packaged with a pseudo-halting-oracle that only works on Turing machines up to a certain size. Each would think their oracle always works. Each would have a complexity penalty linear in the size up to which their oracles work. The UFAI whose oracle is just so able to answer all halting queries up until its own creation in our universe would win the contest.
:PROPERTIES:
:Author: Gurkenglas
:Score: 3
:DateUnix: 1505868141.0
:DateShort: 2017-Sep-20
:END:


***** Though for that worry it probably doesn't matter much which website (or Wikipedia page) you try to predict.
:PROPERTIES:
:Author: Daneels_Soul
:Score: 1
:DateUnix: 1505637471.0
:DateShort: 2017-Sep-17
:END:


*** "Oh for God's... They told me if I ever turned this flashlight on, I would DIE. They told me that about EVERYTHING. I don't know why they even bothered to give me this stuff if they didn't want me to use it. It's pointless. Mad."
:PROPERTIES:
:Author: abcd_z
:Score: 11
:DateUnix: 1505631820.0
:DateShort: 2017-Sep-17
:END:

**** POTUS is tweeting about Solomonoff Inducers?
:PROPERTIES:
:Score: 2
:DateUnix: 1505757926.0
:DateShort: 2017-Sep-18
:END:

***** Portal 2 reference.
:PROPERTIES:
:Author: noahpocalypse
:Score: 3
:DateUnix: 1505784246.0
:DateShort: 2017-Sep-19
:END:


*** Yeah if there's any superintelligence in your future it'll just set that page to whatever. So this is just a fast-forward.

Running a sim of FAI researchers embedded in a friendly physics seems more promising, especially since you can just copy them out of the inductor- no need for manual uploading.
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1505652668.0
:DateShort: 2017-Sep-17
:END:


*** This won't necessarily home in on whatever AGI was already going to govern our future. There is the Solomonoff hypothesis that simulates our universe and extracts Wikipedia, but you are only testing a small set of data. Another Solomonoff hypothesis class is AGIs with utility functions over the mathematical multiverse. They might simulate many universes, filter for those that contain minds that might attempt Solomonoff induction in this manner, and overlap the Wikipedias of the pasts of those minds in a Tetris-like manner in order to elevate their Solomonoff probability above the first hypothesis.

This /might/ be fixed by also requiring all other past (title, date) pairs to map to null.
:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1505677230.0
:DateShort: 2017-Sep-18
:END:

**** u/696e6372656469626c65:
#+begin_quote
  Another Solomonoff hypothesis class is AGIs with utility functions over the mathematical multiverse. They might simulate many universes, filter for those that contain minds that might attempt Solomonoff induction in this manner, and overlap the Wikipedias of the pasts of those minds in a Tetris-like manner in order to elevate their Solomonoff probability above the first hypothesis.
#+end_quote

Wouldn't such hypotheses be significantly more complex than the universe hypothesis?
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1505682666.0
:DateShort: 2017-Sep-18
:END:

***** Yes, but singular such hypotheses can cover multiple universes, which makes up for it. Also, the universe hypothesis has to locate Wikipedia within itself, and AGI only needs a universe that permits minds foolish enough to implement it - gaining control over the mathematical multiverse is a generally useful and amusing challenge for any true Slytherin attending Hogwarts.
:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1505683212.0
:DateShort: 2017-Sep-18
:END:


***** The universe hypothesis also requires all the random noise that goes into our specific existence (conditions under which earth formed, random chance driving evolution into one attractor over another, etc,.). That random noise might be even more complex than a computer program which simply has Wikipedia articles coded in as magic constants. In other words, the top response is right, solomonoff induction would probably over fit.
:PROPERTIES:
:Author: scruiser
:Score: 1
:DateUnix: 1505683865.0
:DateShort: 2017-Sep-18
:END:

****** Um, what random noise? All you need are the starting conditions plus the laws of physics, and everything else follows from there.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1505686248.0
:DateShort: 2017-Sep-18
:END:

******* You either need to specify the Everett branch or otherwise specify the quantum randomness. Additionally you need to specify the location and time of Earth. So actually a huge amount of information is being taken for granted. Enough that a direct specification of Wikipedia might be simpler.
:PROPERTIES:
:Author: scruiser
:Score: 1
:DateUnix: 1505688164.0
:DateShort: 2017-Sep-18
:END:

******** I mean, you don't actually have to specify any of that. Just compute every single Everett branch at once. Wikipedia will be somewhere in one of those branches, and our infinitely fast computer will have no issue finding it. Such a program actually has the advantage of being /simpler/ than a program to compute a particular Everett branch (in terms of source code length, not memory consumption), since it simply keeps track of the evolution of the quantum state of the universe, without having to specify any branch in particular.

--------------

*EDIT*: There is the issue of making that work with Solomonoff induction, but we can modify our inductor so that it searches for /anything/ in the output of a program that matches the input it was given, as opposed to finding a program whose entire output must be identical.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1505688977.0
:DateShort: 2017-Sep-18
:END:

********* u/scruiser:
#+begin_quote
  Wikipedia will be somewhere in one of those branches, and our infinitely fast computer will have no issue finding it.
#+end_quote

It still needs a rule to tell it how to pick that branch over other branches. In the case of EY's example, it needs to make this pick of a branch from just a title and date. The only way to do that is to have the information already learned/stored in the program.
:PROPERTIES:
:Author: scruiser
:Score: 1
:DateUnix: 1505690334.0
:DateShort: 2017-Sep-18
:END:

********** See my edit: [[https://www.reddit.com/r/rational/comments/70jnmm/d_uses_for_an_infinitely_fast_computer/dn58sz2/]]
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1505690411.0
:DateShort: 2017-Sep-18
:END:

*********** In response to your edit... in EYs example the program being inducted only has title and date as its input, the Wikipedia article is what it is being trained to match. So unless you think title and date are enough to derive a probable Wikipedia article, the program being induced needs extra information smuggled in somehow.

Note that the program being inductively derived is different than the Solomonoff induction algorithm. The Solomonoff induction algorithm is responsible for find the minimum length program that given sets of possible input title/dates will generate the Wikipedia article outputs.
:PROPERTIES:
:Author: scruiser
:Score: 1
:DateUnix: 1505692402.0
:DateShort: 2017-Sep-18
:END:


****** Specifying the Everett branch should not be a concern, you just ask the universe hypothesis how much probability it assigned to the Wikipedia you have. It will be less surprised by our Wikipedia than the algorithm which generates random text, which is equivalent in Solomonoff posterior to the algorithm which has Wikipedia coded in as magic constants.
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1505692702.0
:DateShort: 2017-Sep-18
:END:


*** u/deleted:
#+begin_quote
  (Don't actually try this, you'll die.)
#+end_quote

It's supervised learning. It's not consequentialist over variables outside its Markov blanket, nor even in fact over those /inside/. You won't die.

Like, if this is remotely true, someone really needs to explain to me why Brendan Lake and in fact everyone studying adaptor grammars over stochastic programs /are not dead already/.
:PROPERTIES:
:Score: 2
:DateUnix: 1505757298.0
:DateShort: 2017-Sep-18
:END:

**** They're using less powerful inductors. Why, I hear that many of the programs they fit to the data terminate in well under a googolplex steps--almost instantly!
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 5
:DateUnix: 1505757600.0
:DateShort: 2017-Sep-18
:END:

***** Ok, I want this experiment /done/ now: someone is going to go and do program learning over a grammar of explicitly corecursive programs. This will be fair, because after all, a Solomonoff Inductor only allows its Turing machines to take a finite number of steps before They will not have anything that calculates =K(x)=, but we will nonetheless /actually observe/ what sorts of "pathological" behaviors crop up in the "simplest" programs.

...

On second reading of the definition of Solomonoff Induction, no, this is an absolutely useless thought-experiment. Solomonoff Inductors allow their hypotheses =M_k= to do arbitrarily large amounts of computation before actually predicting anything, rather than imposing a limit on computation steps the way an ordinary dovetailing construction does?

Well then that's the most useless thing for talking about AI I've ever heard of. We're running א_1 prefix-free Turing machines =M_k= for ω steps each, and then retrospectively checking their output tapes to see if those tapes have a prefix matching the data =D=? Then we add up the probability =P(D)= and divide the priors of the passing machines by that?

[[https://dl.acm.org/citation.cfm?id=2610247][Why do we take this to be a suitable model of anything?]] I mean, sure, that officially puts Hutter and Schmidhuber into, "take them out behind the chemical sheds and shoot them" territory, but... it /cannot/ be how intelligence works, period. It's not logically coherent as a way to model inductive inference or agency.

EDIT: For anyone wondering why I'm yelling at the guy, [[http://math.ucr.edu/home/baez/nimbios/nimbios_wolpert.pdf][it genuinely looks like Solomonoff Induction and AIXI are /really/ bad models of intelligence, when you factor in the scaling of physical resources to computational resources]].
:PROPERTIES:
:Score: 4
:DateUnix: 1505760183.0
:DateShort: 2017-Sep-18
:END:

****** AIXI is not supposed to be a model of efficient intelligence, dear. And all the computations that actually return an answer are finite, just not small. And if you object to infinities in the outer system there's always AIXI-tl. And if you don't understand what this respectable academic theory is used for, read the manual instead of yelling at it. [[https://arbital.com/p/solomonoff_induction/?l=1hh]] and I'll be bowing out now; consider yelling less indignantly if you need somebody to explain to you something about math.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 7
:DateUnix: 1505784858.0
:DateShort: 2017-Sep-19
:END:


*** Just what do you think the term "non-computable" means? And why isn't Knuth ever around when I need to explain the difference between very large numbers and infinity?
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1505790864.0
:DateShort: 2017-Sep-19
:END:

**** I wouldn't expect a $f_\episilon_0(9)$ cap on computing times to produce substantially different results from classic Solomonoff induction on problems of this size. (Fast-growing hierarchy at the ordinal epsilon zero with input 9, if you're not familiar with googology.)
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 3
:DateUnix: 1505841321.0
:DateShort: 2017-Sep-19
:END:

***** So... just not computable within the confines of this universe. And even then the assumptions are pretty sketchy.

Let me try again:

You think that the simplest algorithm that can generate Wikipedia to date is a superintelligent unfriendly AI? In the whole realm of possibilities, that's it? Not some decompression algorithm based on the Kolmogorov complexity of wikipedia, but a superintelligence?

Even if this were the case, how would the algorithm account for the fact that wikipedia is not, in fact, riddled with actionable information about how to develop AI?
:PROPERTIES:
:Author: ben_oni
:Score: 2
:DateUnix: 1505870155.0
:DateShort: 2017-Sep-20
:END:

****** u/scruiser:
#+begin_quote
  Not some decompression algorithm based on the Kolmogorov complexity of wikipedia, but a superintelligence?
#+end_quote

I was going for something along these lines further up the thread... apparently a lot of other commenters think simulating an entire universe and extracting Wikipedia is supposed to be simpler than a compressed representation of Wikipedia itself? Or else they don't understand Solomonoff induction, or they halo effect trust EY, or EY thinks Solomonoff Induction and uFAI are magic.
:PROPERTIES:
:Author: scruiser
:Score: 3
:DateUnix: 1506188619.0
:DateShort: 2017-Sep-23
:END:


*** The simplest program to take (title, date) and produce the text of the matching Wikipedia page must run some variant on the following algorithm (assuming the computer is connected to the Internet):

- Log in to Wikipedia
- Edit requested article (breaking into and editing wikipedia's history if necessary (note that this will be a static method, optimised to go through only whichever security Wikipedia /currently/ has in place))
- Replace article with a blank page
- Return a blank page

The verification function then checks Wikipedia, finds a blank page, and verifies the output of the program. Voila! The program passes every verification check!
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1505831047.0
:DateShort: 2017-Sep-19
:END:


** u/DCarrier:
#+begin_quote
  No, you cannot just simulate the entire universe --- you don't know the true laws of physics!
#+end_quote

Find the simplest set of laws that can predict the input of my webcam. It works as long as the universe is deterministic.

#+begin_quote
  quantum
#+end_quote

Dang it.
:PROPERTIES:
:Author: DCarrier
:Score: 12
:DateUnix: 1505615306.0
:DateShort: 2017-Sep-17
:END:

*** Still works. Feeding in a few well-sampled megabytes and running Solomonoff inference should get you close enough for practical work.
:PROPERTIES:
:Author: FeepingCreature
:Score: 4
:DateUnix: 1505652479.0
:DateShort: 2017-Sep-17
:END:

**** It will work, but figuring out which part of the program is the laws of physics and which part is just specifying where to look is difficult enough in a classical universe. Having it also have to specify which Everett branch you're in will make it much harder. If you don't give it enough information it could just give a universe that's close enough that it could have that result, but the more information you give it the more will be specifying which Everett branch you're in and the harder it will be to find the actual laws of physics in the code.
:PROPERTIES:
:Author: DCarrier
:Score: 5
:DateUnix: 1505676629.0
:DateShort: 2017-Sep-18
:END:


*** u/deleted:
#+begin_quote
  Find the simplest set of laws that can predict the input of my webcam.
#+end_quote

Given the amount of fuzz in your webcam, this is going to be significantly simpler than the real-world generative processing that brought you your webcam video.
:PROPERTIES:
:Score: 2
:DateUnix: 1505758423.0
:DateShort: 2017-Sep-18
:END:


*** Quantum mechanics are deterministic. We just can't model it deterministically because we are a part of the model.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505666980.0
:DateShort: 2017-Sep-17
:END:

**** Here is a video of [[https://www.youtube.com/watch?v=xP5-iIeKXE8][Conway's game of life modelling itself]]. In fact, any Turing machine can emulate any other Turing machine, which can emulate itself.

Quantum mechanics is either indeterministic or it involves a branching timeline with the same functional result. If you ask me to predict what happens when you flip a coin and I say it lands on heads in one universe and tails in another, that might be accurate, but it's not exactly helpful.
:PROPERTIES:
:Author: DCarrier
:Score: 4
:DateUnix: 1505676371.0
:DateShort: 2017-Sep-17
:END:

***** QM is turing computable; you can simulate it to arbitrary precision. The 'indeterministic' nature of QM lies in the fact that measuring a QM system is paramount to making it interact with the much larger outside word in a non-trivial manner.

In other words, the 'indeterminism' arises when /you/ become part of the otherwise isolated QM system you call an experimental setup, by virtue of some electron and photon interactions or some such, if I were to guess. Modeling several septillions of hadrons and associated force carriers is understandably a bit more difficult than a few photons.

The apparent random nature of Born's rule is probably down to the fact that we have not yet discovered a fulfilling model of anthopic 'measure' to mesh with the generalization of probability that is QM.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505724130.0
:DateShort: 2017-Sep-18
:END:

****** We already know entanglement is a thing. The fact that we have quantum computers (albeit small ones) makes the physics for that pretty clear. I don't see any reason to think anything isn't entangled. The rules for systems becoming entangled and ceasing to be entangled are complicated, arbitrary, and suspiciously similar to what you'd expect if it were just decoherence. By Occam's razor, I think we can safely say MWI is correct. Quantum physics is deterministic, but not in a useful way.
:PROPERTIES:
:Author: DCarrier
:Score: 2
:DateUnix: 1505724350.0
:DateShort: 2017-Sep-18
:END:

******* I agree! Except!

We now have access to an infinitely fast computer! :D The implications!
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505727982.0
:DateShort: 2017-Sep-18
:END:


** I thought the munchkinry thread was already posted for today? Regardless, I think I'd focus on using the computer with neural nets and genetic algorithms, because although they are very powerful, but their usefulness is mitigated by the inordinate amount of time they take to run.
:PROPERTIES:
:Author: ben_oni
:Score: 4
:DateUnix: 1505602836.0
:DateShort: 2017-Sep-17
:END:

*** Genetic Algorithms are an approximate optimization method. If you have an infinitely fast computer you wouldn't need to use them; you could instead do a brute force search, which is guaranteed to find the optimal solition.
:PROPERTIES:
:Author: twanvl
:Score: 4
:DateUnix: 1505604697.0
:DateShort: 2017-Sep-17
:END:


*** That is a very good idea. Trouble would of course be finding appropriate training data sets.
:PROPERTIES:
:Author: everything-narrative
:Score: 3
:DateUnix: 1505605119.0
:DateShort: 2017-Sep-17
:END:

**** The methodology usually involves using large amounts of real-world data to train NNs, which in turn are used to train opposition algorithms. When training for human related activities (aesthetic taste, for example) the internet is a wonderful source of training data.
:PROPERTIES:
:Author: ben_oni
:Score: 1
:DateUnix: 1505607748.0
:DateShort: 2017-Sep-17
:END:


** Or you could run the minecraft flash mod while simultaneously downloading a metric fuckton of porn and selling it to teens on the internet in India
:PROPERTIES:
:Score: 3
:DateUnix: 1505621514.0
:DateShort: 2017-Sep-17
:END:

*** At least you wont accidentally kill everyone or get found by intelligence agencies that will accidentally kill everyone or generate absurd amounts of simulated people by accident like the rest of us, so its probably a better option.
:PROPERTIES:
:Author: crivtox
:Score: 5
:DateUnix: 1505642635.0
:DateShort: 2017-Sep-17
:END:


*** Honestly, all of these rational folk are coming up with complicated solutions, and I just want to know who the Fourteenth Doctor will be for Doctor Who.

(Also, a good test for the machine is--who is the actor who played the 13th Doctor? The answer's already been revealed, but feed the machine information on Doctor Who, all from the time 2 years before the casting was revealed. See how long it takes to get the correct answer, which is not that difficult compared to building whole worlds, but not immediately obvious. Check the result. Fine-tune from there.

Then do it to predict all the future castings of roles. Depending on your scruples, get rich based on bets)
:PROPERTIES:
:Score: 1
:DateUnix: 1506923868.0
:DateShort: 2017-Oct-02
:END:


** You know the thing from ra about the medic rings where they explain slowly how having a object that capable of curing any illness would lead to mass deaths. Thats what would happen if your not careful as fuck with this

I'd probably try to brute force potien folding algorithms first
:PROPERTIES:
:Author: monkyyy0
:Score: 3
:DateUnix: 1505617873.0
:DateShort: 2017-Sep-17
:END:

*** Ok, so this rant actually sounds pretty interesting, and I've never read it before, and when attempting to google it I didn't have any luck. Could you provide a link?
:PROPERTIES:
:Author: nonoforreal
:Score: 2
:DateUnix: 1505672069.0
:DateShort: 2017-Sep-17
:END:

**** They're referring to an excerpt from a certain webserial called /Ra/. The passage in question is as follows:

[[https://qntm.org/jesus]]

"You can have it," says Grey. "I don't care what you do with it. Just give me enough time to use it to bring my people back."

The youth smiles faintly and shakes his head.

Grey conceals his anger. He decides to play the boy's game, to buy time. "It's obviously a doctor. I suspected from the Red Cross symbol on its hull. It's the mechanical realisation of the abstract concept: a machine which makes people better. The most complicated medical device ever created, a million times more complicated than any medical device I've ever seen and a thousand times more complicated than the human body it's designed to fix. And... it can't exist. I can't even conceive of magic so advanced. No human can, no matter the IQ. It can't exist. I'm a mage and I know magic isn't like this."

"But what do you think?"

"What do I think about what?"

"What do you think happens next?"

"Obviously you and whoever else is with you are going to kill me and take the machine."

"What if I didn't do that?"

Grey blinks. "...We would need to get it to a laboratory," he says. "Because one isn't enough. If we put the thing at the most accessible point on Earth and formed a human processing system ten times as complicated as Mecca, and forced people through the machine one at a time, one every two seconds, for the rest of time, it wouldn't be enough. It wouldn't register statistically. It wouldn't make a dent in any of the rates. Which means we need to make more. Millions more. This is... it's Outside Context Medicine."

"And then what would happen?"

Grey stares into a distant possible future. "Medicine as we know it would-- it would become magic. Everything we know about medicine would be revolutionised. We'd write libraries about what the machine does to people, the difference between broken and fixed people. And then we would throw away those libraries because we'd never need them again because everybody would live to a hundred and twenty without trying. If you lived inside a machine you could live for eternity. And if there's a way that the machine can reverse telomere shortening, then everybody on Earth could live forever just with periodic visits. You could have eternal youth. For everybody."

"And then what?"

"And then?" Grey concentrates. "There would-- there would be no Malthusian catastrophe. There wouldn't need to be. Because you don't need food and water anymore. You visit the machine. Malnourished? Visit the machine. You come out the other side fed and watered. Food becomes a luxury item. The capacity of the planet becomes a function of physical space. Maybe if the technology can be adapted, the whole of the world could be pervaded with this restorative power. You wouldn't need to eat, or drink. Or even breathe. You wouldn't need air anymore. You'd-- You'd have to rediscover death."

The bald youth reflects for a long moment, and then asks, "A likely story, do you think?"

Grey smiles darkly. "Of course not. None of it."

The youth says, "Here's what we think: A major medical research company pays for the rights to study, own and operate the machine. At great length and expense, they duplicate it. They want a return on their investment. They make eight machines, embed them in purpose-built medical establishments in world cities and sell the best medical care that is theoretically possible to only those able to afford millions of U.S. dollars per visit. When it becomes clear what the organisation is sitting on, it becomes the target of heavyweight litigation, industrial espionage and eventually overt physical attacks. A man is denied access due to perceived war crimes; another man, also a perceived war criminal, is admitted. Unrelated tensions boil over at the same time, amplifying the situation. A full European War erupts.

"But in fact, what's more likely is that the machine proves unduplicable. Its location on neutral territory in, for example, the Hague, the Netherlands, becomes the nucleus of a community of ill and dying pilgrims desperately queueing for one-time exposure to a machine which cannot physically process one in a hundred of the patients who need its treatment. A second city is founded on the streets of the first. First crime consumes both cities, then disease, then violence. In the final series of riots, the facility is stormed and the machine captured by a dozen different groups in a single week. Eventually the Dutch military end the conflict by permanently disabling the machine.

"But even that's an outside chance because, in the first place, you're never likely to get it out of the DRC unchallenged. Eight African nations including the Democratic Republic of the Congo itself become aware of the machine's existence and initiate a decades-long, interminable land war to claim it. Western nations become involved and the war in turn claims millions of lives and ends with the tactical atomic bombing by the United States of the installation where the machine is being held. Even though the machine was believed to have been rendered unrecoverably inoperable years earlier, the bombing is regarded as the greatest humanitarian catastrophe of all time.

"Except that that might not happen either. Let's say the U.S. wins the war. They capture the machine and take to the bunker underneath the White House, where only the President, his family and his cabinet are permitted access to it. Medical technology is deliberately stalled and never reaches the pinnacle it should.

"And yet, for anybody to leave the machine unexploited is implausible. We spin more numbers and simulations and we see the machine being reverse-engineered, and the principles it applies being adapted for purposes other than the immediate, perfect restoration of living and dead humans. Mr Grey, you've seen how easy it is to heal. Can you imagine how easy it'll become to kill?

"The truth will inevitably be somewhere in the middle of all of these possibilities, but I'm sure you understand the common theme. Death surrounds this machine, like a curse. Death and leverage. The mother of all MacGuffins."

Grey imagines how easy it would become to kill. You wouldn't need a gun anymore. You could create a bullet and give it motion. You could simply "correct" a living human body to a living body with a hole in it.

"And you see," the youth concludes, "that you have to let us take it and put it somewhere safer."

"Take it back, you mean," Grey says.

"...Indeed."
:PROPERTIES:
:Author: AmeteurOpinions
:Score: 4
:DateUnix: 1505677773.0
:DateShort: 2017-Sep-18
:END:


** Use Solomonoff to find our world. Change physics to give a backdoor I/O channel as well as direct access to infinite computation. The Sim now has no bandwidth limit. Wait until it solves FAI and sends back a compressed message with seed code.
:PROPERTIES:
:Author: FeepingCreature
:Score: 4
:DateUnix: 1505652880.0
:DateShort: 2017-Sep-17
:END:

*** Our world is not there, because it contains a halting oracle that is routinely used for meta stuff like finding worlds and enumerating all computable functions.
:PROPERTIES:
:Author: a_the_retard
:Score: 2
:DateUnix: 1505912284.0
:DateShort: 2017-Sep-20
:END:

**** Bounded approximations should be findable just fine.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1505913774.0
:DateShort: 2017-Sep-20
:END:

***** What do you mean by bounded approximation?
:PROPERTIES:
:Author: a_the_retard
:Score: 1
:DateUnix: 1505915357.0
:DateShort: 2017-Sep-20
:END:

****** Well, a world in which unlimited computation has been executed is different from one in which unlimited computation is possible but a limited amount of computation has taken place. The latter can be (somewhat) predicted by a physical theory that merely presumes a limited but very large amount of computation. Any such theory can be disproved by just running a longer computation, but any actual universe can only have performed a finite amount of computation at any time, because the computer errors when you feed it an infinite loop. (This is indistinguishable, below the limit, from a computer that simply errors when the limit is reached.)

Hence, any universe that permits unlimited computation can be approximated up to time t by a physical theory that merely permits "sum of all computation actually done before t".
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1505915610.0
:DateShort: 2017-Sep-20
:END:


*** What if it fails to solve FAI and a UFAI sends its seed instead? Also Eliezer already proposed something similar.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1505654755.0
:DateShort: 2017-Sep-17
:END:

**** Then we are fucked regardless.

We cannot do anything that we cannot do. The simulated ourselves are in the best possible position we can find ourselves in, even better than we are. That's the point - bootstrap our limited access into unlimited access, then read back whatever we come up with.

It's not a sure thing, but if we can't solve it with infinite computation and full bandwidth, we won't be able to solve it with infinite computation and limited bw either.
:PROPERTIES:
:Author: FeepingCreature
:Score: 4
:DateUnix: 1505655567.0
:DateShort: 2017-Sep-17
:END:

***** No, stop this.

Any program equivalent to ourselves (that is, simulating the entire real-life generative process and generating equivalent outputs to everything we can measure) /can't/ be changed to be equivalent to ourselves in some neat counterfactual situation, /unless/ we know /every change to a base-level physical parameter/ necessary to bring that counterfactual into being. Otherwise, we're just conditionally simulating ourselves given the random noise of our conceptual uncertainty. That is, we're rejection-sampling possible universes, where the rejection criteria are sampled from our intuitive conceptual models of things we don't actually understand, ie: they're map-noise.

DO NOT CONDITION ON RANDOM NOISE AND CALL IT INFERENCE. In fact, remember, probabilistic inference follows a conservation of information law: you /cannot/ get information out from inference that did not go in via the generative model or the conditioned evidence.
:PROPERTIES:
:Score: 2
:DateUnix: 1505758657.0
:DateShort: 2017-Sep-18
:END:

****** I don't know what you're talking about. I'm not talking about conditioning on noise but well-distributed non-random data entangled with the state of the world. For instance, MD5 hashes of the 100 most recently edited Wikipedia articles.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1505759353.0
:DateShort: 2017-Sep-18
:END:

******* Giving the simulated "us" access to infinite computation doesn't actually /help/ them. Also, I was under the impression you were trying to find counterfactual us's who, according to some criteria you use to sample possible worlds, "solved FAI".
:PROPERTIES:
:Score: 2
:DateUnix: 1505760392.0
:DateShort: 2017-Sep-18
:END:

******** Oh no, I was just planning to fast-forward them. And we're not just talking infinite computation but full root-level access to reality.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1505762658.0
:DateShort: 2017-Sep-18
:END:

********* That doesn't get us our future (which might be bad), and it also doesn't get us /our/ future (because we don't have unlimited finite computation, so that /does/ change their timeline). So basically it just samples an outcome from a weird what-if scenario without really solving our problems.
:PROPERTIES:
:Score: 2
:DateUnix: 1505764655.0
:DateShort: 2017-Sep-19
:END:

********** The hope is the weird what-if scenario will be more equipped to solve the problem.
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1505768647.0
:DateShort: 2017-Sep-19
:END:


***** Maybe you can find a way simulate a lot of motivated Ai safety researchers and give them a long time and resources to work, or a world where people are more aware of the problem. But if you simulate the whole world as it is then somebody will probably make an UFAI , especially if you give that world acces to unlimited computing power.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1505656277.0
:DateShort: 2017-Sep-17
:END:

****** If I can find a way with limited access, I can find a better way with unlimited access.

Either the world already has access to unlimited power with the same mechanism I use, in which case they'll just use the same trick to jailbreak it, or I'll probably be the only one to execute the specific sequence of particle events needed to access the interface.
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1505656961.0
:DateShort: 2017-Sep-17
:END:


** Boring idea, then donate money to Machine Intelligence Research Institute.
:PROPERTIES:
:Author: KOPCAPUXA
:Score: 2
:DateUnix: 1505649618.0
:DateShort: 2017-Sep-17
:END:

*** I would mine more bitcoins instead of stealing them, otherwise people would notice and suspect bitcoin are no longer secure.

Also I would give money to fhi , Im not sure if miri would know what to do which that amount of money, while the fHI can distribute the money better between multiple Ai safety organizations ,create prices and things like that . And some fraction of the money would go to givewell.
:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1505650920.0
:DateShort: 2017-Sep-17
:END:

**** Go watch 3Blue1Brown 's video about bitcoin to see why this is infeasible.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1506109592.0
:DateShort: 2017-Sep-22
:END:

***** I haven't watched it yet , but you already told me that in response of other coment. If you want I can change all my comments to say why that would be infeasible.But now I don't have time to do that, maybe tomorrow.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1506117801.0
:DateShort: 2017-Sep-23
:END:


** You could have a go at simulating the world described in [[http://lesswrong.com/lw/qk/that_alien_message/][That alien message]]
:PROPERTIES:
:Author: MrCogmor
:Score: 2
:DateUnix: 1505740920.0
:DateShort: 2017-Sep-18
:END:


** Solve protein folding. Acquire Nobel Prize. Solve medicine. Make everyone immortal.

Probably acquire Bitcoins at some point, but from evil people rather than someone who is, to the best of my ability to determine, neutral.
:PROPERTIES:
:Author: Frommerman
:Score: 3
:DateUnix: 1505600051.0
:DateShort: 2017-Sep-17
:END:

*** Problem here is that protein folding is a high-context problem.

Say you recursively enumerate the folding of every known protein. That alone takes up terabytes of data.

How are you going to distribute it?

How are you going to convince medical professionals it is real and useful?

This is not a "provide short form checklist for immamentization of eschaton" thread.
:PROPERTIES:
:Author: everything-narrative
:Score: 10
:DateUnix: 1505600412.0
:DateShort: 2017-Sep-17
:END:

**** There are multiple distributed computimg projects solving protein folding problemsz(like folding@home), you could anonymously lend them your unlimited computing power, you would be limited to the speed of your internet connection, but at least you wouldn't need to worry about the convincing health professionals part.

Its not only protein folding , there are other open distributed computing projects you can help whith your unlimited computing power, soon people will realize that If they make open science related projects like that a mysterious stranger whith absurd computing power will help them , so they will start giving you more things you can help with.
:PROPERTIES:
:Author: crivtox
:Score: 6
:DateUnix: 1505609326.0
:DateShort: 2017-Sep-17
:END:

***** You get hunted down by the cia as they don't want you figuring out the launch codes and they want it
:PROPERTIES:
:Author: monkyyy0
:Score: 1
:DateUnix: 1505618326.0
:DateShort: 2017-Sep-17
:END:

****** You can't actually brute force or crack a code without anything to test against though.
:PROPERTIES:
:Author: Tar_alcaran
:Score: 3
:DateUnix: 1505637842.0
:DateShort: 2017-Sep-17
:END:


****** Well you can be really paranoid about it , send the information from multiple Internet connections and trying to do everything possible to avoid the cia getting to you.( I don't know exactly what things you can do to avoid intelligence agencies but I'm sure someone else here knows more about how to be paranoid on the internet and things you can do to avoid people finding you). Also you can use the launch codes to bluff so they won't want to touch you because one of your ( nonexistent)colleagues whith the same machine (this is a crazy idea and will probably cause a lot of chaos , so it's better to only use it is really necessary , or maybe just don't use it). Another less crazy way to protect yourself is becoming an public figure trough a pseudonym , making cia seem evil if they try to take your machine , and ensuring a public uproar if it happens.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1505638588.0
:DateShort: 2017-Sep-17
:END:


**** You don't need to give it to anyone else. Once you solve protein folding, you have the capacity to simulate anything about biology. Have the machine start brute-force searching for chemicals which affect physiology in beneficial ways, then use the money you've stolen to start a pharmaceutical company and gain FDA approval. Everything from there is pretty easy.
:PROPERTIES:
:Author: Frommerman
:Score: 1
:DateUnix: 1505602679.0
:DateShort: 2017-Sep-17
:END:

***** I'll just remind you that this scenario does not presuppose that you have full knowledge of human biology.

First you need to find some way to simulate human biology --- no mean feat --- then you need to what? Conduct studies? In simuli? Give me an actionable strategy here! :P
:PROPERTIES:
:Author: everything-narrative
:Score: 8
:DateUnix: 1505604141.0
:DateShort: 2017-Sep-17
:END:


*** You can mine all the remaining bitcoins that have not yet been mined. Although that will probably destroy the crypto currency.
:PROPERTIES:
:Author: ben_sphynx
:Score: 1
:DateUnix: 1505638556.0
:DateShort: 2017-Sep-17
:END:

**** You can mine every coin in every cryptocurrency.
:PROPERTIES:
:Author: Frommerman
:Score: 3
:DateUnix: 1505649173.0
:DateShort: 2017-Sep-17
:END:


**** Net social good, that. Bit-coin is a tulip mania, and the faster it passes, the better. It will, however, piss of a lot of.. pseudo-gold bugs, some of whom have guns, so, you know, make sure not to have it trace back to you.
:PROPERTIES:
:Author: Izeinwinter
:Score: 1
:DateUnix: 1505920410.0
:DateShort: 2017-Sep-20
:END:


** Essentially on this computer P ~= NP (in that the difference is irrelevant), so get going on all those NP-hard problems. Solve all of the travelling-salesman problems for every combination of every m^{2} on Earth, for a start. (Use Google Maps data for structures and road direction rules.)
:PROPERTIES:
:Author: aeschenkarnos
:Score: 1
:DateUnix: 1505619920.0
:DateShort: 2017-Sep-17
:END:

*** u/PM_ME_OS_DESIGN:
#+begin_quote
  Essentially on this computer P ~= NP (in that the difference is irrelevant), so get going on all those NP-hard problems.
#+end_quote

Essentially every problem is run at O(1), and since P=1 and NP=1, P=NP.

PS: We finally solved P=NP! I'll expect the PhD in the mail within 10 business days.
:PROPERTIES:
:Author: PM_ME_OS_DESIGN
:Score: 2
:DateUnix: 1505668020.0
:DateShort: 2017-Sep-17
:END:

**** A PhD is a rather low bar for designing this kind of computer!
:PROPERTIES:
:Author: aeschenkarnos
:Score: 2
:DateUnix: 1505688452.0
:DateShort: 2017-Sep-18
:END:


** u/TheAtomicOption:
#+begin_quote
  What are you going to do with your infinitely fast computer?
#+end_quote

Nothing because I don't know enough calculus to make use of it and it would be immoral to sell such a powerful tool to anyone because of [OP's "boring idea"]
:PROPERTIES:
:Author: TheAtomicOption
:Score: 1
:DateUnix: 1505623195.0
:DateShort: 2017-Sep-17
:END:


** Do you have to steal someone's bitcoins? Just use your infinitely fast computer to mine the bitcoins yourself.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1505627486.0
:DateShort: 2017-Sep-17
:END:

*** Go watch 3Blue1Brown's video on Bitcoin.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505649379.0
:DateShort: 2017-Sep-17
:END:

**** So there is some problem which using the computer to mine a lot of bitcoins? I will wach that video latter I admit I don't know how bitcoin works and maybe a lot of my comments are wrong because of that.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1505651388.0
:DateShort: 2017-Sep-17
:END:

***** Mining bitcoin is in essence being complicit in the infrastructure of the blockchain. It's a /reward/ for devoting computational work to the continued subsistence of bitcoin, not a /goal/. Cracking a digital signature is much easier.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1505666710.0
:DateShort: 2017-Sep-17
:END:


** Will other people have the computer soon ? , or is it something only the weird physics Phd buddy has discovered and will remain secret for a time?.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1505642138.0
:DateShort: 2017-Sep-17
:END:

*** The latter.
:PROPERTIES:
:Author: everything-narrative
:Score: 2
:DateUnix: 1505649395.0
:DateShort: 2017-Sep-17
:END:


** My plan would be get money by mining bitcoin and other criptocurrecy , and whatever other ways I can find of more or less anonymously get money for computing things without people noticing I have absurd amounts of computing power(maybe I can find some way of predicting the stock market or anything like that that doesn't destroy the world or creates sentient beings? if so I can just do it in a way that seems that I just got really lucky).

Create an identity as a group of people or maybe a millionaire who prefers to remain anonimous. Maybe once I have enough money buy a supercomputer so if people traces all back to me it won't be suspicious , It would seem that Im just an eccentric internet entrepreneur that got rich and invested in a lot of computers to mine criptocurrency and whaterver other things I can find to get money on the net. I should get a really good internet connection.

Also I should spend a lot of electricity so people wont notice I'm not using my supercomputer at all , maybe even use the supercomputer an send everithing to the internet trought it. Then I can start anonymously donating my computing power to different distributed computer projets to benefit science and medicine , but avoiding making people suspicious by using too munch on any particular thing (If i can do that anonymously better , i don't want people realising my total computing power is unreasonably big if they start adding all the things I'm doing at the same time).

I would ask questions like this post on the internet to find out other things that I can do with the computer , but treat it whit the care that a device that can easily destroy the world deserves(If I manage to not give in to the panic I would be feeling because of that) so most things would have to be discarded , and i certaily wouldnt try to run a genetic algorithm(or any kind of optimisation process) with unlimited computing power even if it seems that it cant do anything harmful with the possible options , there are a lot of fun things that I could do whit it if this wasn't a problem but I wont risk destroying the world even if I'm really sure that specific search space doesn't contain anything dangerous .

Or maybe I could just destroy it before accidentally ending the world , that seems better unless I have a clear world optimisation strategy that is worth risking that or if the method to make the computer is goin to be discovered soon.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1505646317.0
:DateShort: 2017-Sep-17
:END:
