#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 23
:DateUnix: 1497884815.0
:DateShort: 2017-Jun-19
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** Alright, let's talk about Nassim Nicholas Taleb. If you're not familiar, he's the famously belligerent author of [[https://www.amazon.com/Fooled-Randomness-Hidden-Markets-Incerto-ebook/dp/B001FA0W5W/ref=sr_1_1?ie=UTF8&qid=1497580146&sr=8-1&keywords=fooled+by+randomness][Fooled by Randomness]], [[https://www.amazon.com/gp/product/B00139XTG4/ref=series_rw_dp_sw][The Black Swan]], and [[https://www.amazon.com/gp/product/B0083DJWGO/ref=series_rw_dp_sw][Antifragile]], among other works. I don't think Taleb's views can be fully comprehended in a single day, so I strongly advise going out and reading all his books.

--------------

** *Edit*: What I really want to know here is: of those of you who are familiar with Taleb's technical approach to decision theory and how he applies this to the real world, is his decision theory 1) Basically correct, 2) Frequently correct but mis-applied sometimes, or 3) basically incorrect?
   :PROPERTIES:
   :CUSTOM_ID: edit-what-i-really-want-to-know-here-is-of-those-of-you-who-are-familiar-with-talebs-technical-approach-to-decision-theory-and-how-he-applies-this-to-the-real-world-is-his-decision-theory-1-basically-correct-2-frequently-correct-but-mis-applied-sometimes-or-3-basically-incorrect
   :END:
On the one hand, I suspect that if he knew about the rationalist community, he would loudly despise it and everything it stands for. If he doesn't already know about it, that is: I remember seeing him badmouth someone who mentioned the word "rationalist" in Facebook comments. He has said in one of his books that Ray Kurzweil is the opposite of him in every way. He denounces the advice in the book "Nudge" by Thaler and Sunstein (which I admittedly have not read - is this a book that rationalists like?) as hopelessly naive. He considers himself Christian, is extremely anti-GMO, voted third-party in the election but doesn't seem to mind Trump all that much, and generally sends lots of signals that people in the rationalist community would instinctively find disturbing.

On the /other/ hand...

*Taleb the Arch-rationalist?*

Despite the above summary, if you actually look closer, he looks more rationalist than most self-described rationalists. He considers erudition a virtue, and apparently used to read for 30 hours a week in college (he timed himself). I remember him saying off-hand (in The Black Swan, I think) that a slight change in his schedule allowed him to read an /extra/ hundred books a year. When he decided that probability and statistics were good things to learn, he went out and read every math textbook he could find on the subject. Then he was a wall street trader for a couple of decades, and now runs a risk management institute based on his experiences.

He considers himself a defender of science, and calls people out for non-rigorous statistical thinking, such as thinking linearly in highly nonlinear problem spaces, or mis-applying analytical techniques meant for thin-tailed distributions on fat-tailed distributions. (Example of when thinking "linearly" doesn't apply: [[https://medium.com/incerto/the-most-intolerant-wins-the-dictatorship-of-the-small-minority-3f1f83ce4e15][the minority rule]]). He loves the work of Daniel Kahneman, and acknowledges human cognitive biases. Examples of cognitive biases he fights are the "narrative fallacy" (thinking a pattern exists when there is only random noise) and the "[[https://en.wikipedia.org/wiki/Ludic_fallacy][ludic fallacy]]" (ignoring the messiness of the real world in favor of nice, neat, plausible-sounding, and wrong, theoretical knowledge).

He defends religion, tradition, and folk wisdom on the basis of statistical validity and asymmetric payoffs. An example of his type of reasoning: if old traditions had any strongly negative effects, these effects would almost certainly have been discovered by now, and the tradition would have been weeded out. Therefore, any old traditions that survive until today must have, at worst, small, bounded negative effects, but possibly very large positive effects. Thus, adhering to them is valid in a decision-theoretic sense, because they are not likely to hurt you on average but are more amenable to large positive black swans. Alternatively, in modern medical studies and in "naive scientistic thinking", erroneous conclusions are often not known to have bounded negative effects, and so adhering to them exposes you to large negative black swans. (I /think/ this is what he means when he casually uses one of his favorite technical words, "ergodicity," as if its [[https://www.facebook.com/nntaleb/posts/10152335020578375][meaning]] were [[https://www.facebook.com/nntaleb/posts/10152867756623375][obvious]]).

Example: "My grandma says that if you go out in the cold, you'll catch a cold." Naive scientist: "Ridiculous! Colds are caused by viruses, not actual cold weather. Don't listen to that old wive's tale." Reality: It turns out that cold weather suppresses the immune system and makes you more likely to get sick. Lesson: just because you can't point to a chain of causation, doesn't mean you should dismiss the advice!

Another example: Scientists: "Fat is bad for you! Cut it out of your diet!" Naive fad-follower: "Ok!" Food companies: "Let's replace all the fat with sugar!" Scientists: "JK, sugar is far worse for you than fat." Fad-follower: "Well damn it, if I had just stuck with my traditional cultural diet that people have been eating for thousands of years, nothing all that bad would have happened." Lesson: you can probably ignore dietary advice unless it has stood the test of time for more than a century. More general lesson: applying a change uniformly across a complex system results in a single point of failure.

For the same sorts of reasons, Taleb defends religious traditions and is a practicing Christian, even though he seems to view the existence of God as an irrelevant question. He simply believes in belief as an opaque but valid strategy that has survived the test of time. [[https://www.quora.com/What-did-Taleb-mean-by-his-criticism-of-Richard-Dawkins-in-Talebs-Reddit-Ask-Me-Anything-Q-A][Example 1]]. [[https://www.facebook.com/nntaleb/posts/10152548740888375][Example 2]]. Relevant quote from example 2:

#+begin_quote
  Some unrigorous journalists who make a living attacking religion typically discuss "rationality" without getting what rationality means in its the decision-theoretic sense (the only definition that can be consistent). I can show that it is rational to "believe" in the supernatural if it leads to an increase in payoff. Rationality is NOT belief, it only correlates to belief, sometimes very weakly (in the tails).
#+end_quote

His anti-GMO stance makes a lot of people immediately discredit him, but far from just being pseudoscientific BS, he makes what is probably the strongest possible anti-GMO argument. He only argues against GMOs formed by advanced techniques like plasmid insertion, and not against lesser techniques like selective breeding (a lot of his detractors don't realize he makes this distinction). The argument is that these advanced techniques, combined with the mass replication and planting of such crops, amounts to applying an uncertain treatment uniformly across a population, and thus results in a catastrophic single point of failure. The fact that nothing bad has happened with GMOs in the past is not good statistical evidence, according to Taleb, that nothing bad will happen in the future. There being no good evidence against /current/ GMOs is secondary to the "precautionary principle," that we should not do things in black swan territory that could result in global catastrophes if we are wrong (like making general AI!). I was always fine with GMOs, but this argument really gave me pause. I'm not sure what to think anymore - perhaps continue using GMOs, but make more of an effort to diversify the types of modifications made? The problem is that the GMO issue is like the identity politics of the scientific community - attempt to even entertain a possible objection and you are immediately shamed as an idiot by a facebook meme. I would like to see if anyone has a /statistically rigorous/ reply to taleb's argument that accounts for black swans and model error.

Taleb also strongly advocates that people should put their "skin in the game." In rationalist-speak, he means that you should bet on your beliefs, and be willing to take a hit if you are wrong.

To summarize Taleb's life philosophy in a few bullet-points:

- Read as many books as you can
- Do as much math as you can
- Listen to the wisdom of your elders
- Learn by doing
- Bet on your beliefs

Most or all of these things are explicit rationalist virtues.

*Summary*

Despite having a lot of unpopular opinions, Nassim Taleb is not someone to be dismissed, due to his incredibly high standards for erudition, statistical expertise, and ethical behavior. What I would like is for the rationalist community to spend some serious time considering what Taleb has to say, and either integrating his techniques into their practices or giving a technical explanation of why they are wrong.

Also, I would love to see Eliezer Yudkowsky's take on all this. I'll link him here ([[/u/EliezerYudkowsky]]), but could someone who knows him maybe leave him a facebook message also? I happen to think that this conversation is /extremely important/ if the rationalist community is to accurately represent and understand the world. Taleb has been mentioned occasionally on LessWrong, but I have never seen his philosophy systematically addressed.

[[https://www.youtube.com/channel/UC8uY6yLP9BS4BUc9BSc0Jww][Taleb's Youtube Channel]]

[[https://medium.com/@nntaleb][Taleb's Medium.com Blog]]

[[https://medium.com/incerto/the-intellectual-yet-idiot-13211e2d0577][His essay on "Intellectuals-yet-idiots"]]

[[http://www.fooledbyrandomness.com/][His personal site, now with a great summarizing graphic]]
:PROPERTIES:
:Author: LieGroupE8
:Score: 19
:DateUnix: 1497887804.0
:DateShort: 2017-Jun-19
:END:

*** u/ShiranaiWakaranai:
#+begin_quote
  He defends religion, tradition, and folk wisdom on the basis of statistical validity and asymmetric payoffs. An example of his type of reasoning: if old traditions had any strongly negative effects, these effects would almost certainly have been discovered by now, and the tradition would have been weeded out. Therefore, any old traditions that survive until today must have, at worst, small, bounded negative effects, but possibly very large positive effects. Thus, adhering to them is valid in a decision-theoretic sense, because they are not likely to hurt you on average but are more amenable to large positive black swans. Alternatively, in modern medical studies and in "naive scientistic thinking", erroneous conclusions are often not known to have bounded negative effects, and so adhering to them exposes you to large negative black swans. (I think this is what he means when he casually uses one of his favorite technical words, "ergodicity," as if its meaning were obvious).

  Example: "My grandma says that if you go out in the cold, you'll catch a cold." Naive scientist: "Ridiculous! Colds are caused by viruses, not actual cold weather. Don't listen to that old wive's tale." Reality: It turns out that cold weather suppresses the immune system and makes you more likely to get sick. Lesson: just because you can't point to a chain of causation, doesn't mean you should dismiss the advice!
#+end_quote

NO NO NO! This argument is one of my worst triggers. It's my firm belief that this is biggest reason why the world we live in is the hellhole we know today. Let me break down this argument for you, he's claiming that if everyone takes some action X, X must be positive. If it was negative, people doing X would slowly die off from the consequences of X until no one does X. That sounds plausible, but it's only half of the story.

The thing you need to realize is that for many actions X, X can not only kill you, it can also cause more people to start doing action X. There's an actual term that describes this process: natural selection.

Given any system of objects that can produce (slightly different) copies of themselves, what kinds of objects will dominate? A naive thinker would go "OH OH I KNOW: *survival of the fittest*!" and then talk about how the objects that are strongest, the objects that are healthiest, the objects that take the least self-harming actions, would dominate the system over time. Oh happy happy world.

The truth is, the phrase "*survival of the fittest*" may have been the single worst scientific marketing blunder in the history of science. And that's saying something since they make other kinds of shitty blunders like "global warming" all the time. Descriptions of scientific phenomena that give laypeople ideas that are completely off the mark. For example, the layperson that hears global warming thinks "oh no the earth is getting hotter everywhere", when actually its the average temperature that is getting hotter, and some places may actually become colder. And so you end up with politicians throwing snowballs around claiming that debunks global warming. *facepalm*.

The same thing is happening here. Fittest, does *not* mean the best at surviving. That is part of it, but a much much larger part of it is best at *reproducing*. Frankly, if there's a way to trade half your lifespan for several times more children, natural selection would welcome it with open arms. For example: an impotent human with the healthiest habits in the world will be removed from the system in a generation. Meanwhile, all kinds of rapists, adulterers, playboys, gigolos, prostitutes and what not continue to linger in the system, even if they have a whole host of behaviors that tend to harm themselves. In a sense, rape and adultery ARE traditions. They are actions that a significant fraction of the population do and have been doing for eons past, and will likely continue to do generations into the future.

Are these actions positive? Do they help you survive? Hell freaking no. They are crimes, so you get caught by police and punished, and such punishments tend to reduce your lifespan significantly. And even if there are no police, these actions still earn people's hatred, and may then cause you to be murdered in your sleep. But they help produce children. Children with your genes. And while yes, environmental factors can easily cause the child to abandon the way of the rapist or the adulterer (so you certainly shouldn't demand children be hanged for the sins of their parents), they now have a genetic push towards them, as well as a push from every idiot that says "TRADITIONS ARE ALWAYS GOOD". And so rapists and adulterers continue to make up a significant fraction of the population. It's the miracle of natural selection! Woohoo (sarcasm)!

Now you might be thinking, "well okay, I'll just stay away from the traditions that involve having sex then. Surely they must be all good for survival now?" Still wrong. Because you can be a gene protector even without having sex. Consider racism. Racism was (and probably still is in many places) quite literally a tradition. A whole set of traditions even. Traditions you might not even think are associated with racism, yet have racist effects. Racism, from a natural selection point of view, is extremely good. When you oppress and kill people who don't have your genes, people who do have your genes have less competition for resources. But is racism good for you on a personal level? No. Racism prompts you to fight. Fighting involves risk to life and limb. You could easily get yourself killed or permanently crippled in these fights. Yet it is still everywhere because of natural selection.

Natural selection rejoices in making suicidal idiots for its cause. Kind of like bees really. There are bees that don't reproduce at all, and basically perform suicide attacks on any creature that attacks their hive. You know, suicide attacks: bad for personal survival, good for gene survival! And these suicidal bees are everywhere. Truly a great tradition (sarcasm)!

And the worst part is, actions can reproduce in ways other than genes. Memes are a thing. You see this happening in real life all the time: successful people go around writing books about the actions they took to become successful, and people follow those actions to try and also become successful. In a sense, religious wars are the meme version of racism. If you oppress and kill the people who don't have your memes, people with your memes have less competition. Natural selection and tradition prompts you to be the suicidal bee, sacrificing your personal wellbeing (along with the wellbeing of people who don't have your memes), for the sake for the people who do have your memes.

Frankly natural selection just loves evil and self-harm. There's just so much stuff you can do for your genes/memes by being evil and suicidal that it's the overwhelming favorite of natural selection. Hence reality being the hellhole that it is today.

So the next time you see a tradition, or something everyone else is doing. Stop for a moment and think: do I know the logic behind these actions? Can I point to a chain of causation? Otherwise, there's a significant chance that chain of causation is some kind of suicidal evil that protects/generates genes/memes.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 23
:DateUnix: 1497894368.0
:DateShort: 2017-Jun-19
:END:

**** This is a strawman of Taleb's views, which I cannot possibly do justice to in a single post. I do not fully agree with Taleb, but his argument is subtler than "it has survived natural selection so we might as well keep doing it." Taleb has explicitly said that he makes exceptions to his arguments for any practices that infringe on ethics. He defends religious practice mostly on a ceremonial and aesthetic basis. So, for example, fasting and prayer are good, but killing apostates is definitely bad. He is against extremism and literalism.

Your point on the trade-off between individual survival and mass replication is good, though.
:PROPERTIES:
:Author: LieGroupE8
:Score: 13
:DateUnix: 1497900223.0
:DateShort: 2017-Jun-19
:END:

***** u/ShiranaiWakaranai:
#+begin_quote
  This is a strawman of Taleb's views, which I cannot possibly do justice to in a single post. I do not fully agree with Taleb, but his argument is subtler than "it has survived natural selection so we might as well keep doing it." Taleb has explicitly said that he makes exceptions to his arguments for any practices that infringe on ethics.
#+end_quote

That is good to hear, but is still problematic, especially because *everything* "infringes on ethics" to some extent. After all, ethics includes *lies*.

For example, if you perform an action X, you either have to hide it or others will know you have performed X. If you hide it, that almost certainly will involve lying, infringing on ethics. If it's revealed, others will be curious why you perform X. Many will suspect that X has some kind of positive effect, since you are doing X and you haven't died or suffered significant harm from doing X. (Otherwise why would you still be doing X?) And so by doing X, you will be implicitly suggesting to others, that X is a good thing to do. But if you aren't sure that X is a good thing to do, then that is an implicit lie. It infringes on ethics to lead people to do something you aren't sure is good for them.

In effect, saying "obey some rule X unless it infringes on ethics" really says nothing at all, and is the kind of thing you say when you're tired of listening to people tell you how horrible rule X is yet still refuse to acknowledge that rule X is horrible. And it's utterly terrifying when I see "smart", "rational" people say stuff like this.

A few years ago, I stumbled upon a rationality website where the author went on and on about his system of ethics and how wonderful it was. And then, he had a page that said "hey guys, I know this system of ethics sometimes tells you to kill people in certain situations. You should just treat those cases as exceptions, and always obey the system unless it tells you to kill people."

...

Are you freaked out by this? Because I certainly am. Any system of ethics that tells you to kill people in some situations is almost certainly going to tell you to beat people to an inch from death in some cases, two inches in others, three inches in yet more others, and so on. Which of these are exceptions and which aren't!? And why?! The author sadly, did not explain this.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 7
:DateUnix: 1497905287.0
:DateShort: 2017-Jun-20
:END:

****** Let me put it another way. In every decision, you can do one of two things: 1) Keep doing what you've been doing, or 2) Do something else. Taleb says you should have a strong bias in favor of (1), /unless/ there is a strong reason for (2). The set of strong reasons for (2) includes ethical violations caused by doing (1). Taleb backs up his arguments with lots of math about complex systems and stochastic processes. The thing is, I don't know enough of this type of math to tell how much he is BSing (and I majored in math!)

#+begin_quote
  Are you freaked out by this?
#+end_quote

Yeah, sort of, because the system sounds too ad-hoc to work.

#+begin_quote
  Any system of ethics that tells you to kill people in some situations is almost certainly going to tell you to beat people to an inch from death in some cases, two inches in others, three inches in yet more others, and so on.
#+end_quote

This is also true of basically any plausible system of ethics, though.
:PROPERTIES:
:Author: LieGroupE8
:Score: 6
:DateUnix: 1497907632.0
:DateShort: 2017-Jun-20
:END:


****** u/OutOfNiceUsernames:
#+begin_quote
  if you perform an action X, you either have to hide it or others will know you have performed X. If you hide it, that almost certainly will involve lying, infringing on ethics
#+end_quote

That's a pretty far-fetched statement. If you decide to be hiding some of your actions and reasons behind them it doesn't automatically (or “almost certainly”) mean that you're lying. You're not obliged to be explaining your actions to some random people --- or any people at all, really. You can /choose/ to give them explanations, but you may as well decide that it's none of their business and refuse to give answers.

#+begin_quote
  And then, he had a page that said "hey guys, I know this system of ethics sometimes tells you to kill people in certain situations. You should just treat those cases as exceptions, and always obey the system unless it tells you to kill people." [...] Are you freaked out by this? Because I certainly am. Any system of ethics that tells you to kill people in some situations is almost certainly going to tell you to beat people to an inch from death in some cases, two inches in others, three inches in yet more others, and so on.
#+end_quote

Same with this. I don't know what the mentioned person was arguing in favour of, but just because his ethics allows (or even dictates) murder in specific cases doesn't mean that it'll automatically degrade into a slippery slope of human right abuse in general.

There are many real-world scenarios (e.g. limited choice of actions against someone who's about to kill a hostage or trigger a bomb, etc) where killing someone would be the more ethical thing to do (compared to inaction, for instance).

Maybe if this ethics system was being applied to some nearly omnipotent creature that could stop time and just fix things, murder would never be an acceptable choice (in systems where murder by itself is deemed bad, at least). But since we can neither predict everything nor prevent it all in non-violent manner, we have to make do with what we have, including justifiable homicide in some cases.

Also, I feel like I'm misinterpreting your comment in general, so apologies if that's the case.
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 1
:DateUnix: 1497908175.0
:DateShort: 2017-Jun-20
:END:

******* u/ShiranaiWakaranai:
#+begin_quote
  You can choose to give them explanations, but you may as well decide that it's none of their business and refuse to give answers.
#+end_quote

Ok, I guess you could avoid hurting anyone by avoiding interaction with anyone, going into the wilderness and becoming a hermit or something, but if you continue to live among other humans, it's really inevitable that someone will eventually discover you doing X and start suspecting that doing X is a good thing. And while yes, it's not really your responsibility if people start copying you and end up hurting themselves, you have to admit your actions can influence the wellbeing of others in a negative way as a result. It's an ethical gray area.

Take the example given by [[/u/LieGroupE8]], prayer. When you pray according to some religion X, and other people see you praying according to X, that's an advertisement for religion X. They may then become curious about your prayer activities, look up X, and then join X as well. And if X turns out to have terrible self-harming religious practices, those people may get hurt. Now that's not exactly your responsibility per se, since you didn't explicitly tell them to join X, but you must admit your prayer activities did give them a push in that direction. So to some extent, its ethically questionable to pray according to X when you're not sure whether X is good or bad, especially when your main argument for supporting prayers to X it is just that lots of other people are also praying according to X.

#+begin_quote
  Same with this. I don't know what the mentioned person was arguing in favour of, but just because his ethics allows (or even dictates) murder in specific cases doesn't mean that it'll automatically degrade into a slippery slope of human right abuse in general.
#+end_quote

Erm, I guess I explained this badly since both replies I got were missing the point I was trying to make. The problem isn't that the system dictates murder. If your system of ethics says you should kill all murderers, that's okay with me. I don't really agree with capital punishment, but it doesn't freak me out, and its a consistent ethical system that makes sense.

The problem here is how the author says always obey the system unless it tells you to kill someone (in which case you're supposed to ignore the system and don't kill that someone). In saying that, the author is basically admitting that his system has a fundamental flaw and is utterly wrong in situations where it tells you to kill. Yet despite this major flaw, we are still expected to obey this system in all other cases, including cases that are extremely similar: like if the system tells you to beat a man to an inch from death or beat a man until he has a 99.999% chance of dying instead of 100%, it sounds like you should go ahead and obey the system.

The complete 180 in the decision making process over the slightest difference in conditions makes no sense to me, it sounds like the 99.999% chance should still be horrible and you still shouldn't obey the system in this case. But then that would also apply for the 99.998% chance, and so on, so you get, at best, a gradual slope of system exceptions: i.e., the more it tells you to hurt people, the more you shouldn't obey it.

But then by induction, the author's entire system of ethics becomes irrelevant, the only important part becomes the exceptions: avoid actions in proportion to how much they hurt people. The fact that the author advertises the original flawed system throughout the website as if its perfectly good and demands everyone obey it, while the horribly important exception is only in one small webpage so hidden away that I can't even find it anymore, freaks me out because it suggests the author is still going to ask for obedience to a system he knows is fundamentally flawed.

And a similar situation is happening here: the rule given is "Traditions are good!" and I suspect the exception rule is only ever given once we start pointing out how horrible it is ethically. And even then, it just goes ok ignore that particular case, and doesn't delve into all the generalizations of that case and how the system should still be fundamentally broken to various extents in those cases.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1497913770.0
:DateShort: 2017-Jun-20
:END:

******** u/CCC_037:
#+begin_quote
  Take the example given by [[/u/LieGroupE8]], prayer. When you pray according to some religion X, and other people see you praying according to X, that's an advertisement for religion X. They may then become curious about your prayer activities, look up X, and then join X as well. And if X turns out to have terrible self-harming religious practices, those people may get hurt. Now that's not exactly your responsibility per se, since you didn't explicitly tell them to join X, but you must admit your prayer activities did give them a push in that direction. So to some extent, its ethically questionable to pray according to X when you're not sure whether X is good or bad, especially when your main argument for supporting prayers to X it is just that lots of other people are also praying according to X.
#+end_quote

If I'm praying according to X, then I am (presumably) already following X, including all the self-harming practices. That is, I /already/ think it's a good idea to follow X.

Now, I could be wrong. It's possible that it's not a good idea to follow X and I am in some was misled or deluded. But... then I don't know that. I still think it's a good idea to follow X. If I am misled or deluded to that extent, then anything that I could do may be leading someone down the wrong path. (In fact, since I think that X is the right path, I'll likely be doing a lot more than just praying to try to encourage others to follow it, too)

There are two ways to handle this dilemma:

1. Regularly examine and re-evaluate my own choices. Be willing to change my mind in public, and to seriously consider arguments against my current path.

2. Become a hermit, lest I accidentally persuade someone to do something I only think is a good idea.

I don't really see any other options...
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1497956220.0
:DateShort: 2017-Jun-20
:END:

********* u/ShiranaiWakaranai:
#+begin_quote
  There are two ways to handle this dilemma: Regularly examine and re-evaluate my own choices. Be willing to change my mind in public, and to seriously consider arguments against my current path.
#+end_quote

Yes, that's the whole point of this discussion. We started off by discussing the virtues of choosing to follow the Tradition Rule: "Old things that are done by lots of people are good to do".

I pointed out that natural selection means there are plenty of old things that are done by lots of people that are outright suicidal and evil. At which point I was told that there's an exception to the rule: if it infringes on ethics, don't do it.

So we have the revised Tradition Rule: "Old things that are done by lots of people are good to do, unless they infringe on ethics."

I was then told, that under this rule, praying and fasting are good things because they are old things that are done by lots of people and don't appear to infringe on ethics. Therefore, according to the revised Tradition Rule, you should pray because lots of other people are doing it. So *not* because you follow X, and *not* because you think it's a good idea to follow X. You are praying to X *only* because you know lots of other people are doing it, because it's a tradition.

So my last post was saying that that too could be considered an infringement on ethics. Which is why the end result is that the Tradition Rule has to be revised again, to make more exceptions in all kinds of generalizations of ethically infringing cases, to the point where it becomes utterly irrelevant because by induction, you derive that you should just do things in accordance to how little they infringe on ethics, regardless of how old it is or how many other people are doing it.

In other words, you *should* regularly examine and re-evaluate your own choices, *not just blindly follow whatever tradition tells you*. If you can't see the logic behind a tradition, then even if it doesn't appear to infringe on ethics, keep thinking, because it might still be doing so in some way that's not apparently visible.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 3
:DateUnix: 1497973842.0
:DateShort: 2017-Jun-20
:END:

********** I think you might be mischaracterising the rule. It's not "Always do what tradition says" - at least not in my understanding of it. It is, rather, "if this is how it was traditionally done, then that fact alone should /bias/ your decision-making in favour of the traditional solution".

Nothing in there says that that bias should, on its own, be enough to counter other, relevant factors. (And we could probably have quite a debate about the size of that bias).

So no, you should not be *blindly* following tradition. But if tradition says to do X, then you should not stop doing X /unless/ you can provide a good reason (enough to overcome the Tradition Bias) for stopping doing X. (It's basically the same argument as Chesterton's Fence, just phrased differently; just because you can't /see/ a reason for the tradition, doesn't mean that there /isn't/ one, and the probable existence of that as-yet-unknown reason should be folded into your decision-making algorithm).
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1498034181.0
:DateShort: 2017-Jun-21
:END:

*********** u/ShiranaiWakaranai:
#+begin_quote
  I think you might be mischaracterising the rule. It's not "Always do what tradition says" - at least not in my understanding of it. It is, rather, "if this is how it was traditionally done, then that fact alone should bias your decision-making in favour of the traditional solution".
#+end_quote

Ok, I guess we could quantify the rules by making them add bias to a decision, and arrive at a decision based on the total sum of bias from different rules. In this case, our above arguments demonstrate that the size of the bias from the tradition rule should not be large, since if you can provide a good reason to not do something, that reason should overrule the tradition rule.

However, I shall now argue that the bias from the tradition rule shouldn't even be a positive value. The reason is actually precisely what you stated at the end:

#+begin_quote
  just because you can't see a reason for the tradition, doesn't mean that there isn't one, and the probable existence of that as-yet-unknown reason should be folded into your decision-making algorithm
#+end_quote

I am in full agreement with this. Just because you can't see a reason, doesn't mean there isn't one. Now that reason could be good, but it could also be bad, given the previous arguments on natural selection. This bad reason could be more than self-harm, it could also involve hurting others. And while yes, the good reason may also involve helping others, the point is: if you don't know the reason for doing X yet do X anyway, that's essentially gambling with the wellbeing of yourself and everyone around you.

And, well this might just be my pessimism at work, but given the rules of natural selection, I can't help but think the odds are really stacked against you if X is something many people are doing. Either way, without a good reason for doing so, I don't believe we have the right to gamble with other people's wellbeing, and the Tradition reason is nowhere good enough.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1498062597.0
:DateShort: 2017-Jun-21
:END:

************ u/CCC_037:
#+begin_quote
  Ok, I guess we could quantify the rules by making them add bias to a decision, and arrive at a decision based on the total sum of bias from different rules.
#+end_quote

Yes, precisely.

#+begin_quote
  However, I shall now argue that the bias from the tradition rule shouldn't even be a positive value.
#+end_quote

Hmmm. To summarise your argument; you're saying, in short, that something being Tradition might have either good or bad effects, and you think that it is likely to be bad, on the whole. (Is this right?)

Let me, therefore, provide a counter-argument. The positive bias given to Tradition is not from minor positives or negatives; it is because the odds of an /extremely/ negative Black Swan event are much lower when doing something that has been done for generations.

Let me take an example. It is said that, once, long long ago, in China, there was a bit of a sparrow problem. The sparrows would come into the rice fields and eat the rice; and no matter what the farmers did, they could not keep the sparrows out of their rice.

So, they proceeded upon a program of /extermination/. Throughout the entire country, they killed all the sparrows they could find, by any and every means they had to hand (including, apparently, playing loud drums at all hours so the sparrows could not rest and just collapsed of exhaustion, sometimes mid-fllight). There were a lot of sparrows; but China also has a lot of rice farmers, and they were reasonably successful in their quest; they sharply reduced the number of sparrows.

Then winter hit, and so did the locust swarms. With no sparrows to eat the locusts, well... the result of /that/ was known as the Great Chinese Famine, and it was a pretty terrible time for all concerned.

--------------

The positive bias applied to Tradition isn't because of /minor/ positive or negative effects. It's because we know that there is a /limit/ to the severity of the possible negative effects of following Tradition. If we've been farming using a traditional method for the last hundred years, and we have not in that time had major famines, then that implies that the traditional farming methods are rather unlikely to lead to a famine next year, either. (It doesn't mean we can't improve our farming yields. It just means that if we try to do so, then we must think about the new method carefully first, maybe test it in one field before doing it on the entire farm, just in case).
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1498065815.0
:DateShort: 2017-Jun-21
:END:

************* I guess I should clarify something here: when I say don't blindly follow traditions, I don't mean you should then blindly try something new. I just mean stop doing whatever tradition you're doing until you figure out a good reason why you should do it.

Blindly trying new things is also dangerous, as your example well proves. Since the farmers did not properly understand the role of sparrows in the ecosystem, but blindly went ahead and killed them all anyway, they paid the price for it.

In other words, you should never blindly do anything, whether that thing is old or new. Only do things if you have good reasons for doing them. Not because other people are also doing said things or because they are traditions.

Also, when you say there's a limit on the negative effects on tradition, that is true. But that limit is only the incredibly weak reassurance that if tradition X was going to kill us all, it just seems odd that it hasn't yet. The limit does not prevent the tradition from eventually still killing us all (for example, the tradition of waging war with more and more powerful weapons would almost certainly kill us all if we continued it). It also does not prevent the tradition from causing severe harm to a minority of individuals (for example, ritual human sacrifice traditions, or racism traditions). Nor does it prevent the tradition from hurting the group as a whole in ways other than death. It could for example, corrupt the whole group into becoming more evil, committing more human rights atrocities left and right and being completely happy with it. Considering the principles of natural selection discussed earlier, it actually seems quite likely that many traditions will include such negative effects.

So I'm inclined to believe that given an arbitrary tradition X, if you can't think of a good reason for doing X, X is probably mired in all kinds of evil and self-harm so you should stop doing X.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1498073733.0
:DateShort: 2017-Jun-22
:END:

************** u/CCC_037:
#+begin_quote
  I guess I should clarify something here: when I say don't blindly follow traditions, I don't mean you should then blindly try something new. I just mean stop doing whatever tradition you're doing until you figure out a good reason why you should do it.
#+end_quote

An excellent principle, if you have the time to carefully reason out everything before you do it.

Let's consider a couple of hypotheticals here:

1: Tradition dictates that you complete activity X at the same time every day. You've recently run across this argument, and have decided to thoroughly question all your traditions. However, the time for Activity X is five minutes from now; you cannot complete an analysis in that time. Is it better to blindly follow, or blindly not follow the tradition? Assume that the tradition carries no /immediately/ obvious positive or negative effects.

2: You have thoroughly analysed a certain tradition, as compared to alternative task Y. Your analysis shows no expected difference in the desirability of the result. Should you stick to the traditional way, or try the new way instead?

3: As (2), but this time the analysis shows that the new way is /very slightly/ better than the old. Should you stick to the traditional way, or try the new way instead?

#+begin_quote
  Also, when you say there's a limit on the negative effects on tradition, that is true. But that limit is only the incredibly weak reassurance that if tradition X was going to kill us all, it just seems odd that it hasn't yet.
#+end_quote

Yes, exactly.

#+begin_quote
  The limit does not prevent the tradition from eventually still killing us all (for example, the tradition of waging war with more and more powerful weapons would almost certainly kill us all if we continued it). It also does not prevent the tradition from causing severe harm to a minority of individuals (for example, ritual human sacrifice traditions, or racism traditions). Nor does it prevent the tradition from hurting the group as a whole in ways other than death. It could for example, corrupt the whole group into becoming more evil, committing more human rights atrocities left and right and being completely happy with it. Considering the principles of natural selection discussed earlier, it actually seems quite likely that many traditions will include such negative effects.
#+end_quote

True. And if any tradition can be /shown to have/ such negative effects, then it should be abandoned. Those are all reasons which, /if applicable/, should easily outweigh the Tradition Bias.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1498075236.0
:DateShort: 2017-Jun-22
:END:

*************** u/ShiranaiWakaranai:
#+begin_quote
  1: Tradition dictates that you complete activity X at the same time every day. You've recently run across this argument, and have decided to thoroughly question all your traditions. However, the time for Activity X is five minutes from now; you cannot complete an analysis in that time. Is it better to blindly follow, or blindly not follow the tradition? Assume that the tradition carries no immediately obvious positive or negative effects.
#+end_quote

Blindly not follow it. Just use the time you would have spent on activity X to instead analyze/research activity X. This will almost always be better than blindly doing X.

#+begin_quote
  2: You have thoroughly analysed a certain tradition, as compared to alternative task Y. Your analysis shows no expected difference in the desirability of the result. Should you stick to the traditional way, or try the new way instead?
#+end_quote

First, I assume this means you have found good reasons for performing either the tradition or Y, otherwise as in case 1, you should just not do either of them. In this case, assuming your analysis is really thorough (with experiments and everything), I don't feel strongly either way. You should probably do both tradition X and alternative task Y (or have some people do X while others do Y), that way if either method fails, at least you got the backup. Although the viability of this method really depends on what X and Y are.

#+begin_quote
  3: As (2), but this time the analysis shows that the new way is very slightly better than the old. Should you stick to the traditional way, or try the new way instead?
#+end_quote

Then I would very slightly be more in favor of performing alternative task Y, but still not really feel strongly either way. But again, this assumes the analysis has been really thorough, and that you have good reasons for doing tradition X or alternative task Y.

Going back to the farmers vs sparrows example, a thorough analysis would have first experimented with only killing the sparrows in a small area, and only gradually performing this sparrow extermination task on a wider scale after confirming that there are no bad effects in smaller experiments.

#+begin_quote
  True. And if any tradition can be shown to have such negative effects, then it should be abandoned. Those are all reasons which, if applicable, should easily outweigh the Tradition Bias.
#+end_quote

My point here is that given any arbitrary tradition, even if you have not been able to show that it has such negative effects, there is still a significant chance that it does due to the principles of natural selection. This is not something you can just ignore and say /"oh I don't know if I'm doing something horrible, but I'm just going to do it anyway until someone shows me that it is horrible!"/

If a tradition X has no good reason to be continued, *don't* wait until someone can prove it is horrible, abandon it right away because it could be horrible, the chances are *not negligible*, and people could end up seriously hurt if you continue blindly following it.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1498079344.0
:DateShort: 2017-Jun-22
:END:

**************** u/CCC_037:
#+begin_quote
  Blindly not follow it. Just use the time you would have spent on activity X to instead analyze/research activity X. This will almost always be better than blindly doing X.
#+end_quote

So, you'd tear down Chesterton's Fence?

I guess that's your choice; though, personally, I'd recommend keeping doing X until you have an actual reason to stop (reason being, some traditions are there for /very good reasons/).

#+begin_quote
  First, I assume this means you have found good reasons for performing either the tradition or Y, otherwise as in case 1, you should just not do either of them.
#+end_quote

I am assuming that Y is the best possible thing that you could be doing if you were not following the tradition (if there is something better, then substitute it for Y).

#+begin_quote
  You should probably do both tradition X and alternative task Y (or have some people do X while others do Y), that way if either method fails, at least you got the backup.
#+end_quote

This is sensible. Where both are possible, I do agree that this sort of experimental approach is good. Sometimes, of course, it's not possible.

#+begin_quote
  Going back to the farmers vs sparrows example, a thorough analysis would have first experimented with only killing the sparrows in a small area, and only gradually performing this sparrow extermination task on a wider scale after confirming that there are no bad effects in smaller experiments.
#+end_quote

This is, unfortunately, a case where the experimental approach is not really going to help much. Both locusts and sparrows are very mobile - you'd have to desparrow a very /large/ area to prevent the sparrows around the edge of the area from doing quite a lot of damage to locusts passing through the area in any case.

But none of this is the main point of contention.

#+begin_quote
  My point here is that given any arbitrary tradition, even if you have not been able to show that it has such negative effects, there is still a significant chance that it does due to the principles of natural selection.
#+end_quote

This, here, is the main point of contention.

I don't see how this works. The principles of natural selection in this case can be summarised as "that which keeps me and my close family alive, tends to survive as tradition"; and I'm not seeing the correlation between "keeps me and my family alive" and "does something horrible" (in fact, I'd assume those two were negatively correlated, because deliberately not keeping my and my family alive is pretty horrible on its own).

So, let me ask, then - why do you think that natural selection leads to horrible outcomes?
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1498223452.0
:DateShort: 2017-Jun-23
:END:

***************** u/ShiranaiWakaranai:
#+begin_quote
  I don't see how this works. The principles of natural selection in this case can be summarised as "that which keeps me and my close family alive, tends to survive as tradition"; and I'm not seeing the correlation between "keeps me and my family alive" and "does something horrible" (in fact, I'd assume those two were negatively correlated, because deliberately not keeping my and my family alive is pretty horrible on its own).

  So, let me ask, then - why do you think that natural selection leads to horrible outcomes?
#+end_quote

Did you not read [[https://www.reddit.com/r/rational/comments/6i6zfl/d_monday_general_rationality_thread/dj45bjh/][my first reply]] at all?

Natural Selection = Survival + *Reproduction* + a bunch of other stuff.

I.e., if a certain behavior adds enough reproduction ability, then even if it seriously hurts survival ability, natural selection will cause it to dominate the system.

You want to see why natural selection is utterly horrible? Walk into a jungle/mountain/tundra/etc. Not one of the manmade ones where we intelligently decide which species to raise. I'm talking about an actual wilderness full of wildlife, where natural selection has full reign. There you will see all kinds of animals performing behaviors that kill themselves.

You will see herbivores bashing theirs skulls against each other for the right to reproduce, even though the head is obviously an important component for survival, and many herbivores end up dying from this every year.

You will see insects performing suicide attacks EVERYWHERE, just to protect their queens. Meanwhile their queen is practically crippled, since natural selection has selected insect queens whose bodies produce so many eggs that the queen just turns into an immobile baby factory. Completely helpless and unable to do anything other than lay eggs and eat the food that is carried to her. Not exactly good for its own survival.

You will see male spiders pursue female ones so they can have sex, even though the female will EAT the male spider immediately after sex. So again, sex > survival. Incidentally, the female spiders that eat their males? Also selected by natural selection! Natural selection weighs the life of that one male spider, versus the amount of nutrition it would give the female spider's numerous eggs, and the eggs are the clear winner. This act of murder by the female spider is their natural tradition.

And, you will see all kinds of evil and depravity. Fiction often portrays humans as the most sinful creatures on the planet... these people just haven't taken biology. There are plenty of animals out there whose behaviors are far worse when put in human terms. I'm not even talking about how cats sadistically torture their food. That is plenty horrible and happens on a larger scale in the wild, but there are actually even more evil things. I'm talking about parasites. Creatures that sneak into the bodies of other creatures and slowly steal their nutrients.

Now you might be thinking, what's so evil about parasitism? Isn't it just like predators eating prey? Well it turns out, natural selection has favored a certain ability in parasites: *MIND CONTROL*. There are many parasites that can alter the behaviors of their hosts. It wasn't bad enough to eat someone from the inside out, these parasites also turn them into zombies that then proceed to perform various self-destructive actions. Or worse, actions that help the parasites infect more of their friends and family. That's right, a zombie apocalypse is a very real thing that happens in nature, thanks to natural selection. (Fortunately, human brains appear much harder to mind control. Or at least, they seem to be.)

When you walk into the wilderness and see such horrors (or just open a biology book), all the horrors chosen by natural selection, you really think it won't choose horrible things for humans? Really? Because we're so so special? Just so you know, for every one of the above behaviors, there are human ones that eerily mirror them.

Herbivores bashing their skulls against each other just so the winner can have sex? Humans have a strange phrase "all's fair in love and war", suggesting all kinds of horrible evils are allowed in the name of love, a concept that generally leads to reproduction. And there's no shortage of "romance idiots" that will act on these principles, forming everything from stalkers, rapists, adulterers, to murder suicides. Yes, strangely enough, murder suicide is a naturally selected behavior. If you can't have someone's good genes, sacrifice yourself to eliminate their genes from the gene pool, so that the people who have your genes have less competition.

Also, female spiders eating their mates to provide nutrients for their eggs? Humans have had, for a very long time, a tradition that's basically this. [[/s][Any guesses what it is?]] And I don't think I need to elaborate on the depraved behaviors of wars and religious frauds.

So yeah, that's why I think natural selection leads to horrible outcomes. Sure it also sometimes leads to good outcomes (maybe), but considering how horrible the bad outcomes can be, you really shouldn't take the risk unless you have a good reason for doing so.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1498238839.0
:DateShort: 2017-Jun-23
:END:

****************** I did read your first reply, but I don't think I understood it properly at the time.

I think I have a significantly better understanding now, thanks.

To summarise the main point, as I understand it; survival is not the only reason that traditions survive, reproduction (i.e. having a large family) is also a factor, and some things done in the name of reproduction are pretty horrible.

Very well, then. Let us broadly consider traditions as falling in one of two groups. One group is traditions which make it more likely that the people following these traditions will survive. The second group is those traditions that make it more likely that the people following the tradition will have lots of descendants.

This divide is important, because a lot of my points about tradition apply mostly to the /first/ group, while your point about negative reproduction behaviours only applies to the /second/ group of traditions.

I will contend that survival, in the group sense, is a virtue; and thus that the total virtue of traditions involving survival is a net positive. The reproductive traditions, on the other hand, I expect to have somewhat random virtue - some positive (like birdsong), some negative (you've given plenty of examples). Perhaps the total virtue, summed across all the reproductive traditions, is negative - but either way, I expect the total (positive or negative) magnitude of the virtue to be significantly lower than for the survival traditions.

Therefore, I contend that the /average/ virtue over all traditions - which is equal to the /expected/ virtue of a randomly selected tradition - is a positive value. Thus, a tradition should be considered to have a selection bias in its favour proportional to this expected virtue.

But, having seen your argument, I will add a caveat. If the tradition is inspected, and found to be of the reproductive type, then its selection bias must drop significantly. (Some traditions will fall under both types; but many will fall under only one, so that if a tradition has no clear survival utility but does have a clear reproductive utility, then it is reasonable to consider that it may very well not have survival utility at all).
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1498241733.0
:DateShort: 2017-Jun-23
:END:

******************* u/ShiranaiWakaranai:
#+begin_quote
  I will contend that survival, in the group sense, is a virtue; and thus that the total virtue of traditions involving survival is a net positive.
#+end_quote

I got bad news for you. Survival traditions aren't clean either. I mean, this should be quite obvious, but there are plenty of selfish/evil behaviors that help your survival. Or the survival of your family. For example, parents that resort to burglary/robbery so they can feed their children is naturally selected, because it increases the children's survival. For example, racism is naturally selected (among whichever race has the majority), because it increases the survival of people of the same race (sure some will die fighting the minorities, but strength in numbers means overall this is a winning strategy for your race genes).

In fact, there was once a survival behavior that almost wiped out all life on Earth. A long time ago, everyone was just tired and lethargic, floating around and just slowly drinking nutrients to stay alive and reproduce. There wasn't a lot of food or methods to use that food, so people just didn't have the energy to really do stuff. Then one day, by some lucky mutation, someone gained a biological process that generates huge amounts of energy, in exchange for producing a tiny amount of poison gas.

This biological process quickly became the favorite of natural selection. After all, huge amounts of energy means healthier people that can use that energy to repair damage to their bodies. It means stronger people, since they have more energy to push things around. It means faster people since they can use that energy to move themselves around. It was essentially a massive boost to every aspect of every organism that gained the genes needed to perform this biological process.

And so that one someone quickly became two. And four. And eight. And then spread all over the world. Now remember the part about the poison gas? Natural selection doesn't really care about that, after all, the poison affects everyone equally. Or rather, it actually affects the poison producers less, since the poison producers are getting massive amounts of energy which they can then use to heal their wounds and run away from the areas they poisoned. And so there kept being more and more people that produce poison gas.

And eventually, the entire Earth was covered in poison gas and pretty much everyone died. (Other than the lucky few that were in isolated caves or had lucky poison resistance genes or something.) Can you guess the name of this poison-gas-producing biological process? [[#s][It's]]. That's right, we are still surrounded by these poison gas producers, and our ancient ancestors were one of the lucky few that had adapted to the poison gas [[#s][called]]. You can read more about it [[https://en.wikipedia.org/wiki/Great_Oxygenation_Event][here]].

The fun part? This wasn't even a Black Swan event. It's poison gas. It's obviously poisonous! Poison kills, there was nothing unexpected about it. But natural selection went for it anyway because the poison problem was too long-term for natural selection to care, until it was too late.

Now you might be thinking, what does all that have to do with humanity? That's just a biological process, not a conscious behavioral decision. Well... if you believe in Climate Change, there's something happening *right now* that eerily parallels this story. Someone discovered you could gain massive amounts of energy from burning coal/oil/gas/etc, though that also produces gas in the process. That someone got filthy rich, and so gained a massive boost to his survival. By the principles of natural selection, tons of people started copying that someone and also became filthy rich. Burning coal/oil/gas/etc became a tradition that massively boosts survival, at the cost of producing gases. So now the atmosphere is filling up with those gases, gases that (as far as I know) most climate change scientists believe will kill us all if we don't stop producing so much of it.

Unless we artificially use our brains and consciously choose to stop this tradition, natural selection ensures it will continue to dominate the system until we all die from climate change. (Well almost all, because once again, the people who practice coal/oil/gas/etc burning are getting flithy rich, and that money can be used to run away from areas of the world that turn into hell from climate change, so those guys will mostly survive. Have I mentioned natural selection is like anti-karma?)

Then again, we will probably all die from AI first.

Anyway, that's not the point. The point is, survival traditions? Not safe either.

#+begin_quote
  Therefore, I contend that the average virtue over all traditions - which is equal to the expected virtue of a randomly selected tradition - is a positive value.
#+end_quote

I contend that the average virtue over all traditions *that we do not see good reasons for continuing* is negative. Hence I advocate the abandonment of all traditions until you figure out why they are good, if they are.

However, I realize that this will be hard to prove. I could go on and on about all the horrible things that have happened because of natural selection. I could talk about how many traditions are more subtle than rape and murder and could indirectly help the more evil traditions despite looking as innocent as sending children on a field trip. But if you claim that these are just outliers and not a pervasive pattern of natural selection, there's nothing I can do to prove that they are.

So instead I'll talk about why I can't prove this: [[https://en.wikipedia.org/wiki/Survivorship_bias][Survivorship bias]]. The reason why whenever an optimist tells a pessimist "But I see no bad things in my life! Show me all the bad things!", the pessimist then repeatedly bashes his head against the wall.

What is this survivorship bias? Well, pick a random tradition X. Now go around asking people "How has X impacted your life?" There may be a lot of different responses, but not one of them will be "I died because of tradition X". That is survivorship bias. If X was good for 10 people, and killed another 10 people, the only people who answer your survey are the 10 that X was good for, because the other 10 are dead and dead people can't talk. So when you tally your survey results: "Oh look! everyone loves X! X is awesome!"

And that's the essence of the problem: Bad things, by their very nature, are hidden. The victims of bad things are often too dead, too crippled (physically, mentally, socially, financially, etc), or too oppressed to talk about the bad things or even know what bad thing hurt them. The culprits of bad things are often going to hide the fact that they did bad things. The witnesses of bad things will often cower in fear and hope no one notices. And so whenever you take a sample of anything, especially your life experiences, survivorship bias means you're likely to get a completely underwhelming representation of the amount of badness in the actual population.

So if you ask me to show you how the average tradition has negative virtue, I probably can't. I certainly can't go survey all the people that have died from terrible traditions. And I sure as heck won't walk up to the evil people that have benefited from evil or evil-enabling traditions and ask them to admit their crimes. I like living. All I can do is keep bashing my head on the wall, and hope that I can convince at least one more person to think things through carefully before performing potentially murderous traditions.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1498281506.0
:DateShort: 2017-Jun-24
:END:

******************** Well, naturally, there /are/ survival strategies that are zero-sum or even negative-sum - that allow one person to survive at the expense of other people. (And yes, such negative-sum survival traditions should be ended, once identified).

#+begin_quote
  [oxygen catastrophe]
#+end_quote

I must disagree with you on one minor (and somewhat tangential) point here. The oxygen catastrophe was a classic Black Swan; not because it wasn't obvious, but because it was not foreseen. (Admittedly, this was more because natural selection is incapable of foresight than anything else, but nonetheless).

Climate change is a different story. The negative effects of climate change /have/ been foreseen.

#+begin_quote
  [Survivorship bias]
#+end_quote

An excellent point, but there is one problem - while the ten dead people don't say anything, there are still *ten dead bodies* there for a competent researcher to uncover.

It is important to keep this bias in mind when studying traditions to see whether or not a given tradition is good or bad. But it does not prevent a bad tradition from being notices and identified as bad by a competent researcher. (Yes, a competent researcher needs to go to those dark, shadowy corners and find out why people are hiding in them).

#+begin_quote
  So if you ask me to show you how the average tradition has negative virtue, I probably can't.
#+end_quote

This, I believe, is the crux of our disagreement. We are agreed that a tradition with positive virtue should be continued, and a tradition with negative virtue ended. We simply disagree on what value is best ascribed to a tradition whose total virtue, on first glance, appears neither positive nor negative. I'm inclined to think positive, you're inclined to think negative, and neither of us can prove our guess.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1498319457.0
:DateShort: 2017-Jun-24
:END:


****** u/CCC_037:
#+begin_quote
  For example, if you perform an action X, you either have to hide it or others will know you have performed X. If you hide it, that almost certainly will involve lying, infringing on ethics. If it's revealed, others will be curious why you perform X. Many will suspect that X has some kind of positive effect, since you are doing X and you haven't died or suffered significant harm from doing X.
#+end_quote

Not necessarily. Let us say that X is juggling.

I might be juggling in order to receive praise from fellow humans, or in an attempt to impress a person of opposite gender. I might do so in order to show off my coordination or my hours of practice. I might do so simply because I enjoy it.

But I don't think that would necessarily suggest to the average person that he should learn to juggle. He may see it as a neutral activity, or an activity whose benefits are not worth the cost for himself.

#+begin_quote
  A few years ago, I stumbled upon a rationality website where the author went on and on about his system of ethics and how wonderful it was. And then, he had a page that said "hey guys, I know this system of ethics sometimes tells you to kill people in certain situations. You should just treat those cases as exceptions, and always obey the system unless it tells you to kill people."
#+end_quote

So, the author has two ethical systems; and the one that says "never kill" overrides the one he described in so much detail. It is possible that his wonderful system of ethics needs a little adjustment.

(Mind you, there are plenty of well-considered and well-used ethical systems that say it is occasionally, under rare conditions, alright to kill people. Generally as a penalty (for some crime) ordered by a judge or jury.)
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1497955673.0
:DateShort: 2017-Jun-20
:END:

******* u/ShiranaiWakaranai:
#+begin_quote
  (Mind you, there are plenty of well-considered and well-used ethical systems that say it is occasionally, under rare conditions, alright to kill people. Generally as a penalty (for some crime) ordered by a judge or jury.)
#+end_quote

Yes I'm realizing I phrased this badly. The killing part isn't the problem. The problem is that the author is advocating a system of ethics he admits is literally fatally flawed in one particular case: the case where the system tells you to kill someone.

The problem is such flaws can never truly be contained in one special case. Flaws leak. There are plenty of ways to generalize that special case so that the flaws spread over the entire system. For example, you could consider all the situations where the system tells you to gamble on someone's life. So rather than 100% kill them, you instead put them at a 99.999% risk of death, a 99.998% risk of death, and so on. If, as the author puts it, you should only ignore the system in the special case where it tells you to kill someone, then what do you do in the 99.999% case? Go ahead and put them at risk of almost certain death?

Either you would have to keep making exceptions for all these generalizations until your original system becomes irrelevant because of all the exception rules, or you insist that we obey the system even in these generalizations. In the latter case, we end up in the weird situation where the slightest change in conditions results in a complete 180 in our decision making process, where 99.999% chance of murder is perfectly good but 100% chance of murder is absolutely bad.

Either way, the fact that there are so many problems suggests that the system is just fundamentally flawed, and that the author should stop advocating it. Yet because he still does, and the exception rules aren't made clear, the followers are going to go around gamble killing people (including innocent people) when the system tells them to and the situation doesn't neatly fall under the exception. Heck, two followers of the same system may even make opposing decisions because of the ill-explained exception rule, causing them to fight and spread more death and destruction.

Bottomline: if you know a system of ethics is so horribly flawed that you have to go around making exceptions to avoid murder fests, *maybe don't advocate that system at all.*
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1497975012.0
:DateShort: 2017-Jun-20
:END:

******** I would have phrased that differently; if your system of ethics has to have an exception, then your system of ethics is not yet properly described and, ideally, should be polished up and fixed before being presented as complete, which it clearly is not).

But I think we're broadly in agreement here, then.
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1498034369.0
:DateShort: 2017-Jun-21
:END:


**** u/DiscyD3rp:
#+begin_quote
  Meanwhile, all kinds of rapists, adulterers, playboys, gigolos, prostitutes and what not continue to linger in the system, even if they have a whole host of behaviors that tend to harm themselves.
#+end_quote

Somewhat tangential to your main point, but I think it's incredibly unfair to include prostitutes in that list. Sex work is the opposite of bad or evil behavior and I grow tired of seeing people disparaging it so readily.
:PROPERTIES:
:Author: DiscyD3rp
:Score: 9
:DateUnix: 1497900553.0
:DateShort: 2017-Jun-19
:END:

***** I apologize for that, I did not mean to imply prostitution was in any way evil like rape. I put it there as an example of self-harming behavior. Offering sex services is like painting a huge target mark on yourself for sexual violence, and that can have serious effects on your health and wellbeing. It is really not a good action to take for your personal survival unless all your other options are worse.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 9
:DateUnix: 1497903972.0
:DateShort: 2017-Jun-20
:END:


**** I wish I could upvote this more than once. This one sentence fragment encapsulates so many bad ideas that I wanted to reach through the Internet and slap someone.

#+begin_quote

  #+begin_quote
    if old traditions had any strongly negative effects, these effects would almost certainly have been discovered by now, and the tradition would have been weeded out
  #+end_quote
#+end_quote
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 12
:DateUnix: 1497900953.0
:DateShort: 2017-Jun-20
:END:

***** This is a claim that can be operationalized and tested, perhaps via simulation. And note that Taleb is not talking about ethical badness, which he makes an exception for, but about badness in terms of individual death or adverse health effects.
:PROPERTIES:
:Author: LieGroupE8
:Score: 2
:DateUnix: 1497902686.0
:DateShort: 2017-Jun-20
:END:

****** I'm kind of surprised that you would complain about [[/u/ShiranaiWakaranai]]'s post being a straw man when you're doing the same thing.

See, the thing here is, you don't get to pick and choose what parts of religion other people (the ones that are propagating and actualizing these memes) are going to act out. So, sure, there's lots of things in religion that are ethically neutral or good but they're inextricably bound in with the evil and self-harming stuff.

Or, put another way, if you simplify the problem space to religious traditions that aren't harmful, you don't get to use that to prove that religious traditions aren't harmful. Because you still have the harmful ones as proof that "old traditions with string negative effects" aren't "weeded out".

And you don't need a simulation to test it, you can observe it in the real world.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 12
:DateUnix: 1497905697.0
:DateShort: 2017-Jun-20
:END:

******* u/LieGroupE8:
#+begin_quote
  I'm kind of surprised that you would complain about [[/u/ShiranaiWakaranai]]'s post being a straw man when you're doing the same thing.
#+end_quote

Strawmanning whom? ShiranaiWakaranai? I didn't make any effort to refute that post, though. I just pointed out that Taleb's views are more sophisticated than what anyone is replying to here.

#+begin_quote
  See, the thing here is, you don't get to pick and choose what parts of religion other people (the ones that are propagating and actualizing these memes) are going to act out.
#+end_quote

True, this is a problem, and part of where I disagree with Taleb.

#+begin_quote
  Or, put another way, if you simplify the problem space to religious traditions that aren't harmful, you don't get to use that to prove that religious traditions aren't harmful.
#+end_quote

Of course, but Taleb wants to refute the sort of people who argue against the /benign/ traditions for bad reasons.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1497909580.0
:DateShort: 2017-Jun-20
:END:


***** Edit: I want to point out I don't agree with the method, but i can't let go of someone misinterpreting the method.

Black swan events are catastrophic things. City breaking events. Civilization ending events.

I'm not sure why everyone is missing on this.

Black swan events are results that break the fundamentals that they were based on.

Fat < Sugar /but actually/ Sugar < Fat is a black swan event because the previous theory was producing the worst possible outcome to be had in choosing a diet.
:PROPERTIES:
:Author: ZedOud
:Score: 1
:DateUnix: 1498890258.0
:DateShort: 2017-Jul-01
:END:

****** Not sure what that has to do with my comment. I'm not claiming that all old traditions are bad, I'm claiming that you can't expect all the bad old traditions to have been weeded out as the text I quoted states.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1498904244.0
:DateShort: 2017-Jul-01
:END:


**** Crimes don't end civilization, they don't break cities.

Plagues break cities. That's a black swan result.

I don't understand how you see mere crimes as something that would need to be weeded out on the scale of the discovery (Germ Theory etc) that wiped out all previous hygiene traditions across the world (non-plagued parts of the world, that is).
:PROPERTIES:
:Author: ZedOud
:Score: 1
:DateUnix: 1498890593.0
:DateShort: 2017-Jul-01
:END:

***** u/ShiranaiWakaranai:
#+begin_quote
  I don't understand how you see mere crimes as something that would need to be weeded out on the scale of the discovery (Germ Theory etc) that wiped out all previous hygiene traditions across the world (non-plagued parts of the world, that is).
#+end_quote

I have two reasons for this: one, the chance of a black swan event is much, much smaller. You need to weight outcomes by their probabilities, otherwise you should never ever take any action since every action has a tiny chance of killing everyone on earth.

So the small decrease in the probability of a Black Swan event does not justify the blind following of traditions, which I argue greatly increases the probability of evil/self-destructive behavior.

The second reason is that there is nothing "mere" about crimes. It only looks "mere" for the people who aren't on the receiving end. Whenever an act of evil is committed, especially one that goes unpunished, everyone who so much as hears about it is stained. They now have evidence, that crime can pay. That is a corrupting influence. It's all too easy to think "X got away with major crimes! Surely I can get away with a little one! Surely it's forgivable to commit smaller ones!" And so it spreads, like a mental version of a plague, eating away at our morals and ethics instead of our physical bodies.

So yeah, crimes may not end cities or civilizations, but they can turn them into such horrible dens of evil that it honestly would have been better for them to go extinct.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1498903901.0
:DateShort: 2017-Jul-01
:END:


*** Put here because original comment was too long:

*Taleb the Libertarian Anti-Transhumanist*

Taleb's political views are somewhat difficult to figure out. (Actually a lot of his personal beliefs are difficult to figure out, either because he forms no beliefs out of epistemic humility, or because he explicitly considers it a virtue to be opaque, to the great frustration of every rationalist who has tried to understand him. See his cryptic April 30th facebook post, [[https://www.facebook.com/nntaleb/?fref=nf]["Never explain why something is important"]]. Notice how he stays true to this advice regarding the advice itself.). As far as I can tell, he is not a Trump supporter (because he voted third-party according to at least one interview), but he considers a lot of Trump's policies as a step in the right direction due to axing blanket legislation that acts as a single point of failure and a black swan attractor. Taleb's nonchalance in the presence of Trump is due to the fact that he (rightly) considers most news stories as noise with no signal which ultimately won't affect anything. [[https://www.youtube.com/watch?v=kKW0LbeiWio][See this interview]]. He ignores the way Trump talks and claims that just looking at actions, he is not much different than other politicians (really, Taleb???). He despises Hillary Clinton, not only because he doesn't like her policies, but apparently because he considers her as utterly devoid of morality and skin in the game. He dislikes labelling himself, but I would guess that he is mostly a libertarian, believing that small local governments and redundant economies are more robust (and "antifragile") than large governments.

Toy example of a globalistic over-optimization that leads to non-redundancy and fragility: Country A has a comparative advantage in food production and country B has a comparative advantage in machine production. So B produces all the farming equipment and A produces all the food, the two countries trade, everyone is happy. Whoops, country A's regime collapsed in a brutal civil war - now everyone in B starves to death because they have no redundant farming economy of their own. But country C depended entirely on B for mining equipment, so their economy collapses too, and so on. The errors propagate until the whole world economy collapses. Lesson: interdependent globalism without local error-absorption barriers is a ticking time-bomb.

I expect Taleb would dislike the rationalist community because he would consider us to be over-optimizers who have fallen prey to overconfidence bias, who are unaware of asymmetric payoffs, and who apply linear statistical thinking where it doesn't work. In other words, he would denounce us for talking like we're high-and-mighty empiricists while being too lazy to carry out actual experiments or learn the ultra-advanced theoretical statistics necessary to properly understand the data we have received.

If Taleb delved further into the rationalist community, he would likely commend some of our people on their willingness to bet on their beliefs and on their approach to scientific rigor, for rationalists have a philosophy closer to his own beliefs than he realizes. But he would still strongly condemn transhumanism. This is because he views risk-taking as a virtue and an inseparable part of life, and he views transhumanism as wanting to remove all risk from existence. If there is no /real/ risk of death, then nothing is exciting anymore! Transhumanism just makes everything fragile and removes a critical aspect of the environment that we evolved to flourish in, or so he would argue.

The most frustrating thing I find about Taleb (aside from his unnecessary combativeness) is that he can be very difficult to understand when he is making an argument. Sometimes he gives examples without explanation, simply saying that the general principle of the example should be clear. Other times he doesn't even give the whole example, but makes cryptic references and allows the reader to fill in the details. I wonder if he does all this on purpose - I remember him saying something in Antifragile about how you learn more from teachers who are hard to understand, simply because you are forced to pay more attention. It took me a while to comprehend his worldview, but I think I've accurately represented it.
:PROPERTIES:
:Author: LieGroupE8
:Score: 7
:DateUnix: 1497887855.0
:DateShort: 2017-Jun-19
:END:

**** Has he actually said as much about transhumanism? The goal is to remove the risk of death but life will still hold many, many risks. You can still put your money on the line or do things that risk being a colossal waste of time or do things like enter romantic relationships not knowing if they'll work out. Risk of death is a narrow subset of all risks out there and it's the just the one with the worst penalty for losing.
:PROPERTIES:
:Author: InfernoVulpix
:Score: 7
:DateUnix: 1497899399.0
:DateShort: 2017-Jun-19
:END:

***** I can't recall him ever mentioning the term transhumanism directly, but in some places he seems to refer to that general set of ideas indirectly. I'm inferring what I think his reflex response would be.
:PROPERTIES:
:Author: LieGroupE8
:Score: 4
:DateUnix: 1497901763.0
:DateShort: 2017-Jun-20
:END:


**** u/deleted:
#+begin_quote
  He dislikes labelling himself, but I would guess that he is mostly a libertarian, believing that small local governments and redundant economies are more robust (and "antifragile") than large governments.
#+end_quote

That comes across as weird to me. I see "libertarians" as directly enabling the economy to over-optimize itself into extreme fragility, and discouraging the robustness that comes from social democracy.
:PROPERTIES:
:Score: 5
:DateUnix: 1497901581.0
:DateShort: 2017-Jun-20
:END:

***** I don't know how much he actually counts as a libertarian. I thought he called himself that once, but I can't remember for sure.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1497903272.0
:DateShort: 2017-Jun-20
:END:


**** u/buckykat:
#+begin_quote
  This is because he views risk-taking as a virtue and an inseparable part of life, and he views transhumanism as wanting to remove all risk from existence. If there is no real risk of death, then nothing is exciting anymore!
#+end_quote

Handy thing about deathists is that they die off.
:PROPERTIES:
:Author: buckykat
:Score: 2
:DateUnix: 1497904075.0
:DateShort: 2017-Jun-20
:END:

***** So far, evidence suggests that /everyone/ dies off (minus a statistically insignificant sample who have not quite died /yet/)
:PROPERTIES:
:Author: CCC_037
:Score: 5
:DateUnix: 1497958266.0
:DateShort: 2017-Jun-20
:END:


*** I am not familiar with Taleb, but only commenting on the arguments as presented in your post.

#+begin_quote
  He considers erudition a virtue [...]
#+end_quote

I do agree that knowledge is important.

#+begin_quote
  (Example of when thinking "linearly" doesn't apply: the minority rule).
#+end_quote

I read the linked article, and found it devoid of insight, but rather a collection of anecdotes. Some seemed quite forced, compounded by the fact that it tried to argue multiple theses depending on the previous ones. The chain of logic went from obvious statements to false ones quite nicely.

#+begin_quote
  If old traditions had any strongly negative effects, these effects would almost certainly have been discovered by now, and the tradition would have been weeded out.
#+end_quote

I do not agree with this argument at all. The length of time something has been around for is not a strong indicator of usefulness. Many traditions (e.g. not washing your hands) have survived for thousands of years, yet abolishing them has yielded the most substantial improvement's in quality of life. (Also note, that this argument is not falsifiable by presenting some currently ongoing tradition.)

For any tradition to be continued, it is only necessary for public belief to support its continuation. This is a weak indicator of any actual effects, but due to the huge influence of cognitive biases not a strong indicator. The process which produces the best predictions of reality (that are available to us) is called science (by definition). Things, with potentially huge downsides, you need to investigate carefully (/including/ a variety of sources, like historical data) and apply error bars generously. And after you have done so, and the results are in, /you update your probabilities and move on/.

#+begin_quote
  Alternatively, in modern medical studies and in "naive scientist thinking", erroneous conclusions are often not known to have bounded negative effects, and so adhering to them exposes you to large negative black swans.
#+end_quote

Traditions are not known "to have bounded negative effects", only to have had bounded negative affects in the past (even that statement is generous). Everything changes over time, and even knowledge that hold true for a long time may become outdated. It is, of course, possible to extrapolate from previously collected data in a reliable fashion. This is also called science.

#+begin_quote
  Example: "My grandma says that if you go out in the cold, you'll catch a cold." Naive scientist: "Ridiculous! Colds are caused by viruses, not actual cold weather. Don't listen to that old wive's tale."
#+end_quote

Actual scientist: "Let me do a study on this and get back to you."

#+begin_quote
  Reality: It turns out that cold weather suppresses the immune system and makes you more likely to get sick.
#+end_quote

Actual scientist: "You're welcome."

This is (obviously) arguing a straw-man, of course you should not be naïve.

#+begin_quote
  Scientists: "Fat is bad for you! Cut it out of your diet!"
#+end_quote

Somehow I doubt that there were many scientists expressing that sentiment. (Feel free to drop the link to any paper you might have cited this from, however.)

As far as I know, the evidence points in the direction of a balanced diet having no significant disadvantages (for an average person). Claims in the media tend to be exaggerated. As there is evidence that having a balanced diet has no significant disadvantages, and there is a lack of evidence for any change having advantages, being conservative regarding your nutrition is only rational (without any appeal to tradition).

#+begin_quote
  For the same sorts of reasons, Taleb defends religious traditions and is a practicing Christian, even though he seems to view the existence of God as an irrelevant question. He simply believes in belief as an opaque but valid strategy that has survived the test of time. [...]

  #+begin_quote
    Some unrigorous journalists who make a living attacking religion typically discuss "rationality" without getting what rationality means in its the decision-theoretic sense (the only definition that can be consistent). I can show that it is rational to "believe" in the supernatural if it leads to an increase in payoff. Rationality is NOT belief, it only correlates to belief, sometimes very weakly (in the tails).
  #+end_quote
#+end_quote

I agree with the sentiment expressed in the quote. Rational actions, by definition, are the one with the highest payoff. Neither the practice nor the belief of religion is necessarily incompatible with a belief in rationality. However, I find it unlikely that the /methods/ of religion (a part of the beliefs) are effective (i.e. compatible with a belief in rationality).

#+begin_quote
  The argument is that these advanced techniques, combined with the mass replication and planting of such crops, amounts to applying an uncertain treatment uniformly across a population, and thus results in a catastrophic single point of failure.
#+end_quote

The logic depends on these techniques, which have been studied extensively, being more uncertain than traditional agriculture in a changing environment. I see no reason to believe that more advanced techniques are somehow more dangerous, but also able to---coincidentally---hide this fact under investigation.

#+begin_quote
  The fact that nothing bad has happened with GMOs in the past is not good statistical evidence, according to Taleb, that nothing bad will happen in the future.
#+end_quote

The fact that nothing bad has happened with traditional agriculture in the past is not good statistical evidence that nothing bad will happen in the future. Scientific research, however, is good evidence.

#+begin_quote
  There being no good evidence against current GMOs is secondary to the "precautionary principle," that we should not do things in black swan territory that could result in global catastrophes if we are wrong [...]
#+end_quote

Doing nothing may also lead to disaster. There are no safe choices.

#+begin_quote
  Taleb also strongly advocates that people should put their "skin in the game." In rationalist-speak, he means that you should bet on your beliefs, and be willing to take a hit if you are wrong.
#+end_quote

This is excellent advice.
:PROPERTIES:
:Author: suyjuris
:Score: 8
:DateUnix: 1497906522.0
:DateShort: 2017-Jun-20
:END:

**** u/LieGroupE8:
#+begin_quote
  I am not familiar with Taleb, but only commenting on the arguments as presented in your post.
#+end_quote

Maybe I should have asked people not to comment unless they had read all of Taleb's books, plus his personal website and facebook posts. Not that your comment is bad (it isn't), but a lot of the stuff that people are bringing up is addressed very thoroughly in his writing. I assumed that more people here would have read Taleb on the general principle of reading lots of different viewpoints, so that they would be on the same page as me, but either I was mistaken or those people are not commenting.

#+begin_quote
  I read the linked article, and found it devoid of insight, but rather a collection of anecdotes
#+end_quote

Yeah, that's one of the things that really frustrates me about Taleb. His arguments are filled with disjointed, half-baked examples.

#+begin_quote
  The length of time something has been around for is not a strong indicator of usefulness.
#+end_quote

Eh, sort of. See the other comments here addressing this.

#+begin_quote
  Actual scientist: "Let me do a study on this and get back to you."
#+end_quote

Taleb would defend the actual scientist here. But I have seen plenty of people who think they are smart act like the naive scientist.

#+begin_quote
  Doing nothing may also lead to disaster. There are no safe choices.
#+end_quote

Simulated Nassim Taleb replies: "That's like saying that even regular driving carries a risk of death, so I might as well drunk-drive! It completely misses the point of /asymmetric/ risk! Traditional agriculture does not end the world with any serious probability, because if it did, we would already be dead (this is the principle of ergodicity). GMOs, on the other hand, have not been tested for long enough to rule-out fat-tails."
:PROPERTIES:
:Author: LieGroupE8
:Score: 5
:DateUnix: 1497908514.0
:DateShort: 2017-Jun-20
:END:

***** u/suyjuris:
#+begin_quote
  Maybe I should have asked people not to comment unless they had read all of Taleb's books, plus his personal website and facebook posts.
#+end_quote

That would be an unreasonable burden on the commenters and is unlikely to yield more useful comments. I am willing to spent a few hours reading an opinion I find flawed, but after some time there just is no expected utility in it. (At some point the probability of me being unable to understand the argument drops too low, compared to the probability of the author's argument being flawed. That is just a general heuristic.)

Also beware of being in an echo chamber; people who have read all his books are likely to agree with him.

#+begin_quote
  Eh, sort of. See the other comments here addressing this.
#+end_quote

I only saw others addressing the ethics of traditional behaviors. Mind dropping a quote?

#+begin_quote
  Simulated Nassim Taleb replies: "That's like saying that even regular driving carries a risk of death, so I might as well drunk-drive! It completely misses the point of asymmetric risk!
#+end_quote

This is backwards. The point was /not/, that in an absence of safe choices the most dangerous one was preferable. But that risks have to be assessed and the assumption of a risk-free alternative does not hold.

Applied to your metaphor: "There is no point in wearing a seat belt! I drove around for decades without one, and I'm fine! This means that not wearing a seat belt does not kill me with serious probability, since I would have been long dead by now. But who knows what might happen if I put it on? After all, it could cut me, maybe trap me inside the car, or provide a false sense of security. No, driving without is perfectly safe and will always be."

#+begin_quote
  Traditional agriculture does not end the world with any serious probability, because if it did, we would already be dead (this is the principle of ergodicity).
#+end_quote

Citing [[https://en.wikipedia.org/wiki/Ergodicity][Wikipedia]]: "In probability theory, an ergodic dynamical system is one that, broadly speaking, has the same behavior averaged over time as averaged over the space of all the system's states in its phase space."

This is a simplifying assumption (when applied to the system earth), that does not hold in reality. (Just look at a graph of surface temperatures.)

As I understand the concept (and please correct me if I am wrong) the argument goes like this: When a system is ergodic, a measurement of a probability over a long period of time automatically gives the probability of that behavior in a random state. Meaning that any tradition is automatically safe, since it has previously exhibited a probability of extinction in the vicinity of 0.

In a mathematical sense, this is a correct deduction. But please note (some of) the implicit assumptions:

- The earth is an ergodic system.
- 100 years is a long time (the time we have been doing traditional agriculture without it causing an extinction).
- The only way to measure extinction-level risk of technologies is by employing these technologies on a large scale.
:PROPERTIES:
:Author: suyjuris
:Score: 8
:DateUnix: 1497912524.0
:DateShort: 2017-Jun-20
:END:

****** u/LieGroupE8:
#+begin_quote
  I only saw others addressing the ethics of traditional behaviors. Mind dropping a quote?
#+end_quote

You're right, the other comments aren't that related this particular issue. Let me respond here.

/Simulated Nassim Taleb/ replies:

The long-term survival of a practice is evidence that there are no (probable, fat-tailed) terminal or absorbing states. Here we model the evolution of a practice as a Markov chain with possible absorbing states, where in reality, an absorbing state corresponds to anything that ends the practice. This could be total extinction, the deaths of the practitioners, or just something like the societal recognition that the practice has a bad effect. The case of hand-washing is an example of this last effect, where unsanitary practices hit the absorbing barrier of falsification. A "bounded" negative effect is any bad effect that is not an absorbing state in the chain. Certainly, bad practices can remain for a long time due to belief, but /really bad/ practices tend to be falsified with time. The great contribution of science is that it strongly improved our ability to discover and falsify bad practices.

In general, in the case of fat-tailed distributions, the fact that something has happened in the past is not good evidence that it won't happen in the future. The is the /Black Swan/ problem, which I wrote a whole book about! Alternatively, you only need /one/ example of something happening to falsify it! This leads us to the principle of /via negativa/: traditional practices that are /negative/, that is, tell you /not/ to do something, are generally more trustworthy than /positive/ practices. This is because at worst, not doing a particular thing is usually neutral, and at best, that particular tradition arose in opposition to previous falsified practice. So, for example, if your grandma tells you not to go out in the cold, it might be superstition, or it might be because people noticed a legitimate black swan problem with the opposite advice.

#+begin_quote
  Applied to your metaphor: "There is no point in wearing a seat belt! I drove around for decades without one, and I'm fine!
#+end_quote

/Simulated Nassim Taleb/ replies:

This is precisely backwards with regards to the GMO problem. Transgenic GMO advocates are telling us to /take off/ the seatbelt after wearing it for years, because car crashes don't happen that often anyway. Here, the "seatbelt" is local, incremental, bounded modification. Traditional cross-breeding practices are strongly unlikely to propagate errors globally (this practice has occurred for thousands of years!). Whereas GMOs correspond to large, global modifications, serious black swan territory without local absorption barriers for errors!

--------------

I don't necessary agree with how far (simulated) Nassim Taleb takes his conclusions. Regarding the assumptions you list at the end, the most important one is the ergodicity of earth and human culture as a system. I think Taleb would argue that these complex systems tend to be ergodic over thousands of years as a rule, but I would want to see more evidence of this. Regarding the 100 years assumption: Taleb would say that 100 years is not long at all, and that modern agriculture is probably /already/ very fragile. Regarding the third assumption, Taleb would say you don't need to /deploy/ anything, just analyze the systemic properties of the practice.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1497918615.0
:DateShort: 2017-Jun-20
:END:


**** What do you mean by "conservative with your diet"?

Are you saying there has been a valid, optimal diet recommendation this last century?

The newest dietary guidelines limited added sugar intake to 10% of daily caloric intake. So not even a full soda.

Fat has been banned for a while, with carbs the only alternative. But now high-glycemic index carbs are being vilified.

I'm not sure how we can ignore the heavy-handed influence on the market that Science has enjoyed this last century.

Wether it be a false recommendation of safety, a lack of warning, or a false warning, "science" and /science/ does not have clearly, presently, valid conclusions all the time. There is no alternative to "conservatively" examining the hell out of the expressed theories; *I don't think there exists a conservative middle ground to safely navigate.*
:PROPERTIES:
:Author: ZedOud
:Score: 1
:DateUnix: 1498891549.0
:DateShort: 2017-Jul-01
:END:

***** u/suyjuris:
#+begin_quote
  What do you mean by "conservative with your diet"?
#+end_quote

I meant that a rational person with a balanced diet should not spend much time optimizing further. (This obviously does not apply to persons with a medical condition.)

#+begin_quote
  The newest dietary guidelines limited added sugar intake to 10% of daily caloric intake. So not even a full soda.
#+end_quote

According to my back-on-the-envelope calculation that recommendation would evaluate to 2.17 cans of Cola for an active, 30 year old male, per day. That aside, I do not see your problem with that recommendation.

#+begin_quote
  Fat has been banned for a while, with carbs the only alternative. But now high-glycemic index carbs are being vilified.
#+end_quote

Please provide a source.

#+begin_quote
  I'm not sure how we can ignore the heavy-handed influence on the market that Science has enjoyed this last century.
#+end_quote

Science gains credibility because it works. It provides increasingly accurate descriptions of reality. Facts are often able to influence a market. This is good, and I don't think you are arguing against /that/.

Notice, however, that it is beneficial for a product to /appear/ as if based on scientific evidence, even if the actual science does not warrant the underlying conclusions. In general, scientists are very careful in their statements (they have to be, otherwise they would not pass peer-review), and claims tend to be exaggerated by other parties. Additionally, what you are calling Science consists of numerous independent research groups, distributed all over the planet. The coordination necessary for the 'heavy-handed influence' you imply simply does not exist.

#+begin_quote
  Wether it be a false recommendation of safety, a lack of warning, or a false warning, "science" and /science/ does not have clearly, presently, valid conclusions all the time.
#+end_quote

Please provide sources to the examples you are referring to.

Are you saying that science is not infallible? That is obviously true, but also misleading. It is self-correcting, highly accurate and the best tool we have.

#+begin_quote
  There is no alternative to "conservatively" examining the hell out of the expressed theories; I don't think there exists a conservative middle ground to safely navigate.
#+end_quote

I am not sure what exactly your point is. As long as there are multiple competing theories and there is an actual scientific debate, you might want to hold out on making substantial changes. Instead, adopt recommendations based on the scientific consensus, which changes very rarely.
:PROPERTIES:
:Author: suyjuris
:Score: 1
:DateUnix: 1498911361.0
:DateShort: 2017-Jul-01
:END:


*** So, here's a question that I think is very relevant to Taleb: is it rational to always accept an argument that you can't fault, even if you suspect that the source of the argument is biased or untrustworthy? I don't think that's a question with an obvious answer, but I'd argue no.

Suppose you Googled a well-established conspiracy theory- 9/11 truthers, UFOs, whatever. You'd almost certainly encounter arguments and apparent evidence that you couldn't immediately debunk based on first-hand knowledge. You could, of course, also Google facts and articles to debunk those claims- but if you consider only the facts and reasoning presented and not the trustworthiness of sources, doing so would appear to be motivated reasoning. These conspiracy theories are built up from decades of motivated reasoning, so why should using the same method yourself produce better results?

I think the answer has to be that the sources of these theories aren't trustworthy enough to support their extraordinary claims. We know that the people who come up with these kinds of theories tend to rely on fact-gathering and rhetorical methods that introduce an enormous amount of bias; we know that their arguments are usually contradicted by more trustworthy sources; and we know that they're often not all that rational.

So, is it rational to discount the arguments of conspiracy theorists on no other basis than that mistrust? Maybe in a perfect world, we'd all have the time to independently test the arguments that can be tested, and the education to judge the arguments that can't. In a world with limited time, in which we encounter vastly more claims than we can independently verify, however, I think that mistrust can be a valid reason for disbelief.

Nassim Taleb appears, at least to me, to be an extremely intelligent pathological narcissist. He's made a lot of extraordinary arguments, a small number of which I can find fault with, but most of which I can't. I think he's my intellectual superior, in both education and intellect, but I don't find him trustworthy. I know from experience that people who behave like he does have problems with self-delusion, and I don't think he does a good job of taking the ideas and criticisms of others into account.

Is that mistrust sufficient reason to dismiss his arguments, even when I can't personally fault them? Maybe not entirely- he's not some rocker-adjacent conspiracy theorist, and he could turn out to be right about everything- but I think it's sufficient reason to be extremely skeptical.
:PROPERTIES:
:Author: artifex0
:Score: 3
:DateUnix: 1497893020.0
:DateShort: 2017-Jun-19
:END:

**** u/deleted:
#+begin_quote
  is it rational to always accept an argument that you can't fault, even if you suspect that the source of the argument is biased or untrustworthy?
#+end_quote

The /always/ part is a trivial no. If a hostile superintelligence puts forward an argument I can't fault in, I try my best to behave as if that damned thing had never spoken to me. Even changing my behavior in the opposite direction from where the argument points is most likely letting the interlocutor manipulate me. This also applies well below "superintelligence" to people who just happen to have cached arguments I've never heard before.

Arguments /just are/ social manipulation. That is their chief function. That's /why/ the discipline of logic, and thence mathematics, evolved separately from rhetoric.
:PROPERTIES:
:Score: 10
:DateUnix: 1497901723.0
:DateShort: 2017-Jun-20
:END:


**** u/LieGroupE8:
#+begin_quote
  I don't think he does a good job of taking the ideas and criticisms of others into account.
#+end_quote

Agreed. He makes himself almost unapproachable in this regard, at least online. Dissenters in the comments sections of his facebook posts are ridiculed.

#+begin_quote
  So, is it rational to discount the arguments of conspiracy theorists on no other basis than that mistrust?
#+end_quote

I don't think Taleb should be put in the same bucket as conspiracy theorists. Also, your question has an equal and opposite, namely: Is it rational to trust the arguments of someone established to be a strong rationalist even if you don't fully understand them?

#+begin_quote
  I think the answer has to be that the sources of these theories aren't trustworthy enough to support their extraordinary claims.
#+end_quote

Taleb doesn't care about epistemology so much as he cares about decision-making, and the interesting thing is that his main arguments tend to mirror the idea of distrusting theories that can't produce extraordinary evidence. Namely, he argues that under many cases of real-world uncertainty, your "default" behavior should be tradition and well-established heuristics, and you should only depart from these if you have a very strong reason.
:PROPERTIES:
:Author: LieGroupE8
:Score: 3
:DateUnix: 1497901448.0
:DateShort: 2017-Jun-20
:END:


*** u/ShiranaiWakaranai:
#+begin_quote
  Edit: What I really want to know here is: of those of you who are familiar with Taleb's technical approach to decision theory and how he applies this to the real world, is his decision theory 1) Basically correct, 2) Frequently correct but mis-applied sometimes, or 3) basically incorrect?
#+end_quote

I think you have answered this question yourself pretty well. He is basically correct but only in some cases, because he has a strong bias towards not doing new things. For example, when he promotes caution in doing things like genetic modification, that's great. Caution is always good. And he does correctly point out several dangers in scientific research like spurious correlations that are just the result of random chance, which is always important to watch out for. Yet he doesn't promote that same caution for old things, like greenhouse gases and climate change, because burning coal and what not is what we have already been doing for years, its old!

Which is strange because if he was born about hundred years ago, back before we started burning up all the coal and oil and what not, his old-things-good ideology would almost certainly make him promote the same extreme caution against dumping strange chemicals into the air as he now promotes against dumping strange genes into plants.

The bias towards doing whatever old thing we're doing now instead of some new thing doesn't really make sense to me, because whatever old thing we're doing now was a new thing at some point in the (usually quite recent) past, and, by his own argument, the fact that this previously new thing hasn't catastrophically killed us all yet is no proof that it won't do so in the future.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 4
:DateUnix: 1497914996.0
:DateShort: 2017-Jun-20
:END:

**** u/LieGroupE8:
#+begin_quote
  Yet he doesn't promote that same caution for old things, like greenhouse gases and climate change, because burning coal and what not is what we have already been doing for years, its old!
#+end_quote

I'm pretty sure he is a strongly in favor of curtailing emissions. Only 100 years of emissions is not enough to make the practice "old" - thousands of years would be better. And emissions have global effect, resulting in a single point of failure. It's not just about being old - one needs to consider the error propagation mechanics and the dynamic time horizon of the given process.

#+begin_quote
  The bias towards doing whatever old thing we're doing now instead of some new thing doesn't really make sense to me, because whatever old thing we're doing now was a new thing at some point in the (usually quite recent) past,
#+end_quote

See my second reply to [[/u/suyjuris]] above.
:PROPERTIES:
:Author: LieGroupE8
:Score: 2
:DateUnix: 1497919695.0
:DateShort: 2017-Jun-20
:END:


*** u/KilotonDefenestrator:
#+begin_quote
  or the same sorts of reasons, Taleb defends religious traditions and is a practicing Christian, even though he seems to view the existence of God as an irrelevant question. He simply believes in belief as an opaque but valid strategy that has survived the test of time.
#+end_quote

Religion is always bad because it promotes the meme of accepting things without proof, indeed without the /possibility/ of proof ^{1} . It is a harmful meme that opposes reationality and makes people vulnerable to other comfortable falsehoods.

^{1} : I can place a high level of trust scientific results because I understand the steps I need to perform to become a scientist and verify them for myself. For religion there is no possible path to verifying the supposed facts.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 3
:DateUnix: 1497946358.0
:DateShort: 2017-Jun-20
:END:

**** I mean, I /agree/ with this sentiment. But Taleb doesn't really think that religion has epistemic content anyway; he even compares believing in God to believing in Santa Claus in one of the links. He considers religious belief as something different altogether, a /non-epistemic/ mode of thinking, a myth you tell yourself to motivate good behaviors. This is, of course, an extremely alien mode of thinking to most people on [[/r/rational]], including myself, and I don't really agree, but was using it as example of how he applies his decision theory.

/Sigh/ maybe I should have stuck with Taleb's stuff about quantitative finance.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1497957612.0
:DateShort: 2017-Jun-20
:END:

***** u/KilotonDefenestrator:
#+begin_quote
  a myth you tell yourself to motivate good behaviors.
#+end_quote

How do you differentiate between the myths that motivate good behavior and those that motivate bad behavior? All myths claim that their mandated behavior is good.

If you are able to differentiate between good and bad based on something other than what religion tells you, then what do you need religion for?

How does adding a layer of myths improve anything?
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 2
:DateUnix: 1497959002.0
:DateShort: 2017-Jun-20
:END:

****** u/LieGroupE8:
#+begin_quote
  How do you differentiate between the myths that motivate good behavior and those that motivate bad behavior?
#+end_quote

There's good in the sense of ethics, and then there's good in the sense of health. We can differentiate good ethics purely on principle, but it is harder to get powerful enough statistical evidence for health, in which case we default to antifragility analysis.

#+begin_quote
  How does adding a layer of myths improve anything?
#+end_quote

Presumably because it is better at motivating "healthy" behaviors than anything else. Abstract reason is not as emotionally appealing as a vivid myth.

Though I would argue that having epistemically true beliefs is good as a matter of ethics and long-term strategy, and therefore there is an ethical imperative to work around this motivation problem.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1497963222.0
:DateShort: 2017-Jun-20
:END:


*** u/OutOfNiceUsernames:
#+begin_quote
  He considers himself a defender of science, and calls people out for non-rigorous statistical thinking [...] He defends religion, tradition, and folk wisdom on the basis of statistical validity and asymmetric payoffs. [...]
#+end_quote

** 
   :PROPERTIES:
   :CUSTOM_ID: section
   :END:

#+begin_quote
  [[https://www.quora.com/What-did-Taleb-mean-by-his-criticism-of-Richard-Dawkins-in-Talebs-Reddit-Ask-Me-Anything-Q-A][the Quora post]]
#+end_quote

** 
   :PROPERTIES:
   :CUSTOM_ID: section-1
   :END:

#+begin_quote
  What I would like is for the rationalist community to spend some serious time considering what Taleb has to say, and either integrating his techniques into their practices or giving a technical explanation of why they are wrong.
#+end_quote

Wouldn't this mean that any analysis or criticism regarding his views would have to come from people who have proven to understand statistics --- and mathematics in general --- without having strayed off into [[/r/badmathematics/]] territory? And the arguments themselves would have to be based on stat\math related concepts, so essentially they'd be made by and for people who know their math?

And if that's the case, then I guess the ending request in your comment should also be to /first/ prove that the commenter knows their math or go learn it (“BRB!”) and only /afterwards/ make their opinions known regarding this mr. Taleb's stances, in this discussion tree (or any future ones related to it).
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 2
:DateUnix: 1497901351.0
:DateShort: 2017-Jun-20
:END:

**** You can criticize him on general principles without a full math background, sure, but having technical explanations is preferable. Taleb, after all, produces highly mathematical academic papers to back up his views. No one needs to go into the math right here and now, but having someone make a series of blog posts would be good. I expect members of the rationalist community are more likely than average to have mathematical experience.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1497902052.0
:DateShort: 2017-Jun-20
:END:

***** Well, here are some additional possible angles of criticism, besides what ShiranaiWakaranai has said higher.

1.1) (this one can be seen as a follow-up to ShiranaiWakaranai's comment) if he can criticize Dawkins for “not understanding probability”, then Dawkins can criticise him for not understanding evolution and the core idea of memetics. Especially since these concepts serve as pretty good counter-arguments against what you've described in your original comment (unless he does address this somewhere else, and you didn't include it due to space limitations).

1.2)

#+begin_quote
  any old traditions that survive until today must have, at worst, small, bounded negative effects,
#+end_quote

Unless the negative effects are such that they can't easily be traced back to their source. Or ones that are so overwhelming that we can't even notice them and imagine an alternative society where they don't exist. Or ones that the old traditions themselves are presenting as not negative effects at all, and maybe even as positive ones.

#+begin_quote
  but possibly very large positive effects
#+end_quote

As ShiranaiWakaranai's said, this doesn't necessarily follow from the previous statement.

1.3)

#+begin_quote
  "My grandma says that if you go out in the cold, you'll catch a cold." Naive scientist: "Ridiculous! Colds are caused by viruses, not actual cold weather. Don't listen to that old wive's tale." Reality: It turns out that cold weather suppresses the immune system and makes you more likely to get sick.
#+end_quote

What is omitted from here is that once the naive scientist finally figures out exactly how are the cold and the viral infections related, they update their advice to be more accurate and helpful. Meanwhile, if you ask the grandma /why/ it is that you'll catch a cold if you go out in the cold, she'll likely be unable to provide a deeper explanation (due to various reasons, including the limited amount of information that can be passed through generations as traditions and common sense). This lack of deeper insight, among other things, is also bad because it can easily be hijacked by third parties if they give plausible-enough sounding explanations. Best case scenario, this will be the naive scientist themselves (prior to updating their understanding of the link between cold and infections), and worst case scenario it will be someone who's motivated in the hijacking because of nefarious self-interest (e.g. a politician pandering to the crowd, a cult member, etc).

1.4)

#+begin_quote
  Things that have endured for a long time are, by probability, likely to endure - otherwise they would have died out already. It is hard to see The Odyssey, The Bible, The Iliad and similar works being forgotten, whereas last year's bestseller is unlikely to be remembered in 1000 years.
#+end_quote

[[https://www.goodreads.com/book/show/27068734-but-what-if-we-re-wrong-thinking-about-the-present-as-if-it-were-the-pa][/But What If We're Wrong? Thinking About the Present As If It Were the Past/]] --- haven't read it yet, but I think it's relevant here. The point being that “it is hard to see The Bible being forgotten” because it both had a better (earlier) opportunity to get itself established in the public awareness /and/ is designed to be propagating itself throughout the generations. Imagine a society where people have to wait for the children to become adults before they can be talking with them about religions or classical literature --- all the “endurance” of these memes would greatly suffer in such a world. Facebook is a shitty social media platform, but it's hard to get rid of it, because it had the opportunity to garner a very large userbase for itself.

--------------

2)

#+begin_quote
  He defends religion, tradition, and folk wisdom on the basis of statistical validity and asymmetric payoffs. [...] Alternatively, in modern medical studies and in "naive scientistic thinking", erroneous conclusions are often not known to have bounded negative effects, and so adhering to them exposes you to large negative black swans.
#+end_quote

This looks like an example of false dichotomy: it is possible to both get rid of the inefficient [[https://en.wikipedia.org/wiki/Noncoding_DNA#Junk_DNA][(and\or placebo)]] traditionalist rituals /and/ minimise the risks of [[https://en.wikipedia.org/wiki/Quackery_involving_radioactive_substances][unknown negative effects]] from new scientific discoveries (e.g. through tighter regulations, more thorough research on the technologies before they are released to open market, addressing the [[https://en.wikipedia.org/wiki/Replication_crisis][replication crisis,]] etc).

--------------

3.1)

#+begin_quote
  Religion is a prime example of the 'antifragile'.
#+end_quote

What if we-as-a-civilisation have reached the point where the current flavours of widespread religions are soon to lose their “antifragile” property, like it has already happened with greek mythology, etc? In other words, [[https://en.wikipedia.org/wiki/Normalcy_bias][just because the Abrahamic religions have managed to survive for so long, doesn't mean that they won't decline in popularity and perish on their own some time soon.]]

3.2) If he supports traditional religious rituals at least in some manner because they're “antifragile”, doesn't that make his argument into an example of circular reasoning?

p.s. A person can have a high IQ and\or erudition and /still/ manage to hold to false beliefs and inconsistent worldview. E.g. if the operating system itself is buggy, it doesn't matter how powerful is the machine it's running on, it will still pop out errors.

p.p.s. I feel like /there are/ some very good notions among all the stuff you've described mr. Taleb saying, but they have to be dug out of all the faulty reasoning and burnished, much like some ideas that the ancient philosophers had to share.
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 5
:DateUnix: 1497922129.0
:DateShort: 2017-Jun-20
:END:

****** u/CCC_037:
#+begin_quote
  Unless the negative effects are such that they can't easily be traced back to their source. Or ones that are so overwhelming that we can't even notice them and imagine an alternative society where they don't exist. Or ones that the old traditions themselves are presenting as not negative effects at all, and maybe even as positive ones.
#+end_quote

In all of these cases, the negative effects in question:

- are not extinction-level
- do not result in a society significantly worse than our current society

Those are the bounds by which the potential negative effects seem to be bound. Yes, there may be some tradition out there with /massive/ negative effects which will become obvious once that tradition is discontinued - but society has existed with those negative effects for so long already, that it doesn't seem they're going to make society worse if continued for a bit.

So, yeah, I can see where the idea that traditions should have bounded negative effects comes from, and it seems sensible.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1497958984.0
:DateShort: 2017-Jun-20
:END:


****** Good responses in general, though I don't know enough to assess how they stack up to Taleb's technical arguments. I'll just say one or two things.

1.2) See my responses to suyjuris about survivability and "boundedness"

1.3) /Simulated Nassim Taleb/ replies: Complex systems generally do not have discoverable causal pathways; that is, the causal complex behind any specific phenomenon is often not going to be captured in any quickly describable or statistically testable way. Thus, we must evaluate decision-making in such settings empirically, without resorting to explanation (which only introduces model error). By all means, refute old wisdom with empirical evidence, and if you can find /strong/ evidence for a causal pathway, great. But otherwise, don't pretend you understand the phenomenon, or that describing a plausible-sounding causal mechanism automatically makes you smart.

2) Sure

3.1) Interesting. [/Simulated Nassim Taleb/ replies: Bodily evolution is slow, on the timescales of hundreds of thousands to millions of years. Therefore, it is not likely that the basic antifragile health benefits of religious practices (at least, the /via negativa/ practices), which are tailored to the complex system of the human body, have changed over so short a period of time as thousands of years.] Is simulated Taleb's argument misleading? Perhaps.

3.2) I.e., they're traditional because they're antifragile, but we know they're antifragile because they're traditional? /That/ is circular, though the actual argument is just traditional ==> antifragile, I think.

#+begin_quote
  p.s. A person can have a high IQ and\or erudition and still manage to hold to false beliefs and inconsistent worldview.
#+end_quote

Of course, though strong indicators of intelligence should at least give us pause.

#+begin_quote
  p.p.s. I feel like there are some very good notions among all the stuff you've described
#+end_quote

I recommend reading Taleb, even if you disagree with him, because he has some extremely useful thinking tools that I've never seen anywhere else. His ideas exposed me to complex systems theory and fat-tailed analysis, which I had never seen anyone address before, apparently because those topics are just hard to work with in the real world due to not providing answers as satisfying as neat thin-tailed analysis.
:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1497962368.0
:DateShort: 2017-Jun-20
:END:


** I've recently read a number of articles/posts/stuff which proclaim a general despair of the "culture war", "social media", "mainstream media", etc. One thing which can be agreed on, is that this problem is created an enabled by modern communications technology, whether you consider that the Internet, TV, radio, or printing press.

For the sake of assume this /is/ is a technological problem (as opposed to an alien brain parasite), and that there /is/ a technological solution to said problem. What does this solution look like? I suppose mass wireheading would solve it, but that's the most brute-force approach. Actually, no, the most brute-force solution is planetary extinction via de-orbited celestial body. We should try to come up with a somewhat less harmful solution. Our victory condition is a sufficient reduction in perceived negativity that people don't feel compelled to blog about public negativity.

A few ideas to get started:

You could ban media which exceeds some arbitrary limit of negativity. This would require control of media to enforce said ban, so that's out.

You could genetically modify people to be happier (CRISPR?) bit that would take multiple generations to achieve the necessary scale.

You could create Social_Media_But_Better which has active or passive countermeasures against increasing negativity. More feasibly, invent such tech and get an existing media company to buy and integrate it.
:PROPERTIES:
:Author: AmeteurOpinions
:Score: 4
:DateUnix: 1497887714.0
:DateShort: 2017-Jun-19
:END:

*** I would say it's more enabled than created by the media. There's a reason that clickbait exists, and it's that human brains are primed for it. Same with "if it bleeds, it leads", which has been a guiding principle of yellow journalism for a long time. Changing the human brain is (mostly) right out, unless you're a world-class writer/thinker who can sway people away from negativity, or you want to muck around in gray matter, which isn't technically feasible.

Large companies like Facebook, Reddit, and Google are entirely capable of doing sentiment analysis and directing people away from the things that make them angry, upset, sad, etc. They actually do this, to a limited extent, but there are some rightful (and wrongful) free speech and bias concerns. It's more difficult to figure out which things make people angry/upset/sad for the right reasons, whatever those are, and to steer them away from things like righteous indignation or political action, but it's probably doable. If people knew (or found out) you would have to worry about evasion, which would be a whole problem by itself. I don't think it's really the right way to go because of the pushback it would get.

A better method is probably just a change in the cultural zeitgeist so that people focus themselves on spreading positivity and warmth in the world, but I sort of doubt that's going to happen unless it can gain some countercultural traction. You see it a little bit in the "wholesome" subreddits, I guess.
:PROPERTIES:
:Author: alexanderwales
:Score: 11
:DateUnix: 1497899882.0
:DateShort: 2017-Jun-19
:END:


*** For the last one: [[https://www.youtube.com/watch?v=rE3j_RHkqJc]] makes me think that the emotion that spreads a meme could be identified by graph analysis. Tagging each post with a corresponding icon would let people actively choose what emotions to spend their time on.
:PROPERTIES:
:Author: Gurkenglas
:Score: 3
:DateUnix: 1497890002.0
:DateShort: 2017-Jun-19
:END:


*** What is the problem exactly? Being upset by negative news?
:PROPERTIES:
:Author: eternal-potato
:Score: 3
:DateUnix: 1497901313.0
:DateShort: 2017-Jun-20
:END:

**** No, it's being sad/angry/upset by overexposure to negative news out of proportion to how much that negative news actually impacts your life and/or reflects statistical reality. For a non-current example, things like satanic panic or "super-predators" are extreme cases. Mostly it's about people walking around being sad or angry or afraid because the multimedia landscape, and to a greater or lesser extent, societal forces, have incentives to make them that way.
:PROPERTIES:
:Author: alexanderwales
:Score: 4
:DateUnix: 1497901822.0
:DateShort: 2017-Jun-20
:END:


**** I think the most general case is "advanced communications technology discourages cooperation instead of encouraging it."
:PROPERTIES:
:Author: AmeteurOpinions
:Score: 2
:DateUnix: 1497901533.0
:DateShort: 2017-Jun-20
:END:


*** Nationalise all media, social or otherwise, strictly censor it in favour of the official correct opinions, enact increasingly nuisance punishments for violators (e.g., user-unfriendly interfaces, access limitations and restrictions, notifying your relatives, friends and acquaintances of your politically-incorrect opinions and punishing them in proportion to their proximity to you, loss of formalised social status, monetary fines) and gamification strategies for rewarding continued and active obedience, effectively making people "fake it until they make it".

Everyone is addicted to Grindr and Candy Crush? Make "being an obedient subject" the next viral killer app. The last one, if you will.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: -2
:DateUnix: 1497896033.0
:DateShort: 2017-Jun-19
:END:

**** This will quickly give you a "War of the Worlds" problem in which hoax broadcasts or just regular hacks enable precise, deterministic social control... for rando hackers on another continent. Or just whoever happens to care, on the entire planet, including people who don't care for your ideology.
:PROPERTIES:
:Score: 4
:DateUnix: 1497902014.0
:DateShort: 2017-Jun-20
:END:

***** If you're the dominant military and economic power, this can easily be combined with economic, military and cultural imperialism to ensure your ideology is the government-sponsored norm worldwide, willingly or otherwise, or near enough so that the outliers can be branded as "rogue states" and subjected to crippling sanctions and targetted killings by superior military technology to which they have zero forceful or judicial recourse. This, in turn, may result in asymmetric warfare attempts from those countries, which you can label as terrorism to further delegitimise them, their ideology, and anyone who subverts yours.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: -2
:DateUnix: 1497902528.0
:DateShort: 2017-Jun-20
:END:

****** Everyone point and laugh at the guy who thinks American imperialism /works well/ at suppressing all other ideologies.
:PROPERTIES:
:Score: 6
:DateUnix: 1497904303.0
:DateShort: 2017-Jun-20
:END:

******* He does have an /interesting/ post history.

Edit: usually I feel bad about handing out downvotes, but not this time.
:PROPERTIES:
:Author: AmeteurOpinions
:Score: 1
:DateUnix: 1497924756.0
:DateShort: 2017-Jun-20
:END:


******* It obviously doesn't, but it could if the power of the state were wielded in a more consistent manner and the state had more long-term control over its and its subjects' ideology.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: -1
:DateUnix: 1497904398.0
:DateShort: 2017-Jun-20
:END:

******** I'm pretty sure it couldn't. Repressing hard enough can't change the world, and the dissent then comes from people just responding rationally to the world.
:PROPERTIES:
:Score: 2
:DateUnix: 1497904848.0
:DateShort: 2017-Jun-20
:END:

********* I doubt it's impossible. If fucking cellphone apps can get people addicted to unfulfilling serial monogamy and frivolous spending in the most literal sense of the word, there's obviously a reward signal there that can be hijacked for just about any purpose. We're not talking about rational actors, we're talking about facebergian bugmen. This is just one possibility, but a state that truly controlled all mass communications and did so effectively could essentially get people addicted to obedience in large enough numbers that repression for the rest wouldn't even have to come from the state. Now more than ever, the normie can be made to believe what he has to.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: -1
:DateUnix: 1497905720.0
:DateShort: 2017-Jun-20
:END:

********** u/deleted:
#+begin_quote
  fucking cellphone apps can get people addicted to unfulfilling serial monogamy and frivolous spending in the most literal sense of the word
#+end_quote

Wait a minute, they can?

#+begin_quote
  a state that truly controlled all mass communications and did so effectively could essentially get people addicted to obedience in large enough numbers that repression for the rest wouldn't even have to come from the state.
#+end_quote

I guess I have three actual objections here:

- I really, sincerely don't expect it to work.

- I really, sincerely expect it to blow up in the face of anyone who tried it. I really don't think you can make people suppress their own needs and desires to such an extent.

- It just seems kinda boring. Ultimate sociological control -- for what? What /could/ I indoctrinate people into that I actually want or need to? How does this make the world a more enjoyable place to live, especially for me, the Emperor of Mankind ;-)?
:PROPERTIES:
:Score: 5
:DateUnix: 1497909014.0
:DateShort: 2017-Jun-20
:END:

*********** u/BadGoyWithAGun:
#+begin_quote
  I really, sincerely don't expect it to work.
#+end_quote

China is giving something similar a try. I guess we'll see. It probably helps to have a society without individualism and accustomed to violent suppression of independent thought, so maybe we should work on that first.

#+begin_quote
  It just seems kinda boring. Ultimate sociological control -- for what? What could I indoctrinate people into that I actually want or need to? How does this make the world a more enjoyable place to live, especially for me, the Emperor of Mankind ;-)?
#+end_quote

To crush your enemies, see them driven before you, and hear the lamentations of their women. We can work on the good stuff like converting the future light cone into computronium after it's assured that it won't be misused by the enemy.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 0
:DateUnix: 1497909261.0
:DateShort: 2017-Jun-20
:END:

************ Again with your failures to mentalize! Neither I nor most other people you'll have to market yourself to /have/ any desire to blah blah crush blah blah lamentation, at least not a desire capable of driving months to years of concerted group action. You're in a world where being an effective barbarian warlord is difficult and costly, so most prospective barbarians are too lazy to do it. Like, ok, I physically put the bourgeoisie /and/ tankies (my enemies personally) on barbecues and eat them. Then what?

You know what? Then I feel dirty and have to go take a years-long shower to get the Lone Power's filth off me.

/It/ is the real enemy /anyway/, so why even bother with the barbecue? I may not be good at it, but I'm at least aiming at the powers and principalities I genuinely want to overthrow on behalf of my own desires and motivations.
:PROPERTIES:
:Score: 3
:DateUnix: 1497919224.0
:DateShort: 2017-Jun-20
:END:

************* u/BadGoyWithAGun:
#+begin_quote
  You know what? Then I feel dirty and have to go take a years-long shower to get the Lone Power's filth off me.
#+end_quote

Wow, that's pretty pathetic. And I mean that in the strictest observational sense.

#+begin_quote
  I may not be good at it, but I'm at least aiming at the powers and principalities I genuinely want to overthrow on behalf of my own desires and motivations.
#+end_quote

I would prefer the "overthrow" to be as permanent as possible.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: -1
:DateUnix: 1497939884.0
:DateShort: 2017-Jun-20
:END:


** Advice and guides for overlearning academic material? I want to be able to go back to coursework and get consistent A's rather than even a single B, without having taken the course previously.
:PROPERTIES:
:Score: 5
:DateUnix: 1497901398.0
:DateShort: 2017-Jun-20
:END:

*** Take the course "Learning How to Learn" on coursera. [[https://www.reddit.com/r/GetMotivated/comments/5950tm/text_i_just_finished_the_online_coursera_course/?st=j44qoq5x&sh=7280341a][Here]] is a link to a reddit post summarizing the content.
:PROPERTIES:
:Author: LieGroupE8
:Score: 5
:DateUnix: 1497913028.0
:DateShort: 2017-Jun-20
:END:

**** Score!
:PROPERTIES:
:Score: 1
:DateUnix: 1497919268.0
:DateShort: 2017-Jun-20
:END:


*** I use anki flash cards for all my classes, after each lecture I make them up. I get mixed results: very good recall for multiple choice questions (like, I can almost get the slides word-for-word), but writing long, detailed answers is a lot harder as rote memorisation doesn't help with synthesis.

However, there are some things it's perfect for: doctors who have to learn the names of all the bones in the hand and things like that.
:PROPERTIES:
:Author: MagicWeasel
:Score: 2
:DateUnix: 1497914682.0
:DateShort: 2017-Jun-20
:END:

**** Rote memorization is probably good for symbol sequences as well, so thank you!
:PROPERTIES:
:Score: 3
:DateUnix: 1497919679.0
:DateShort: 2017-Jun-20
:END:

***** Anki is very icebergian. It seems like a simple enough program but there are extensions, shared decks, etc all over the place.

For example, as well as my studies and french vocab decks, I have decks for the nations of the world (identifying them on a map, identifying their flag, their capitals). It's pretty useful/useless knowledge.

Plus it's great on a quiz night: "What do Zambia, Kazakhstan, Papua New Guinea and Moldova have in common?" (they all have birds on their flags)
:PROPERTIES:
:Author: MagicWeasel
:Score: 2
:DateUnix: 1497919941.0
:DateShort: 2017-Jun-20
:END:


*** I'm not sure what you mean by overlearning but I've found that a logarithmic rehearsal schedule is effective for memorization. As a heuristic, if you have flash cards, instead of moving the card to the end after rehearsing it, move it back a number of times proportional to how confident you are that you'll remember it next time.
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1498263448.0
:DateShort: 2017-Jun-24
:END:

**** What's the formula for that?
:PROPERTIES:
:Score: 1
:DateUnix: 1498263772.0
:DateShort: 2017-Jun-24
:END:

***** I don't have anything universal. Number of rehearsals = O(log t). Constant varies based on how easy it is to remember.

The key is to rehearse just before you forget so that your brain's garbage collector sees that it needs to be stored in more long term memory.
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1498287568.0
:DateShort: 2017-Jun-24
:END:


*** I second this request, for reasons of my wanting to get into a reasonable college.

For the little that it is worth, the most consistently academically successful person I know gave the advice of simply refusing to do anything - including sleep - until you have committed the important points of the day's materials to memory. He is not what I would call superlatively social or physically healthy, and he also has an excellent memory, but that is the best I have.
:PROPERTIES:
:Score: 1
:DateUnix: 1497902721.0
:DateShort: 2017-Jun-20
:END:

**** u/deleted:
#+begin_quote
  committed the important points of the day's materials to memory
#+end_quote

I feel like your person left out the operational part of the advice. How do you commit things to memory? How do you /know/ you've committed them to memory in a long-term way?

My particular thing is that I want to be able to retain and use the material long after I take the course, since core material often comes up again and again in different contexts -- and there's a lot of it.
:PROPERTIES:
:Score: 5
:DateUnix: 1497904384.0
:DateShort: 2017-Jun-20
:END:

***** Have you considered Spaced Repetition Systems? It covers both memorizing and - to an extent - tracking progress.

For retention the trick seems to be as EY says: make the knowledge a part of you. Don't favor learning specific declarative facts, and instead favor learning trends, laws, methods etc.. Then context should give you at least much of the rest of the equation. A good example is mathematics. For reason of your being a moderator on this sub I am presuming you understand the universality of the methods of algebra, etc. Generalize from that principle.

(I apologize if this is not all that helpful)
:PROPERTIES:
:Score: 1
:DateUnix: 1497905522.0
:DateShort: 2017-Jun-20
:END:

****** u/deleted:
#+begin_quote
  Have you considered Spaced Repetition Systems? It covers both memorizing and - to an extent - tracking progress.
#+end_quote

I've heard of them but not tried them. Lemme go look them up more thoroughly, thank you.

#+begin_quote
  For retention the trick seems to be as EY says: make the knowledge a part of you. Don't favor learning specific declarative facts, and instead favor learning trends, laws, methods etc..
#+end_quote

The funny thing is, I'm /good/ at learning laws and methods. I'm really, /really/ good at internalizing the "feel" or concept to something. The trouble is to do that with sufficiently exact strings of symbols that I can just rattle off the actual formal content of the concept, since I can already "feel" the concept.

(Seriously, once a concept has been mapped to proprioceptive and motor imaginations, it /doesn't/ go away. Sensorimotor intuition is /really solid/ in our brains, and translating things into that space /works/.)

For example, despite not having used it since... high school, I /almost/ remembered the exact quadratic formula. I can /almost/ entirely remember beginning calculus. Vector-matrix multiplication is still totally there. Matrix-matrix multiplication is there once I remember that the second matrix is treated as column vectors, the first as a row-matrix. Row reduction on matrices needed a lookup just now.

But I'm not sure I've ever had it under deliberate control what got committed to which extent, with how much formal content versus how much intuitive content.

#+begin_quote
  For reason of your being a moderator on this sub I am presuming you understand the universality of the methods of algebra, etc.
#+end_quote

I can't tell if you mean elementary algebra or Universal Algebra ;-).
:PROPERTIES:
:Score: 5
:DateUnix: 1497909064.0
:DateShort: 2017-Jun-20
:END:


*** I've been thinking about this. In principle, it seems like it should be pretty straightforward.

1. Convert the book to a number. This could be a sequence of ASCII codes, with escape characters to LaTex as needed.
2. Checksum the number so you can verify you have the right number in the future.
3. Memorize the entire number. Since human memory can hold tons of stuff as long as it's in story/vivid imagery form, you just need to turn the number into a long detailed story with lots of vivid imagery.
4. Decode the number back to text when you want to peruse it. Use the checksum to make sure you are doing it accurately.
5. Learn to lucid dream. Spend 8 hours every night reading the book(s).
6. Speed up your time perception in the dreaming phase as much as possible so you can study for years on end in a matter of weeks.
:PROPERTIES:
:Author: lsparrish
:Score: 1
:DateUnix: 1498093344.0
:DateShort: 2017-Jun-22
:END:

**** I had meant things you can do in real life.
:PROPERTIES:
:Score: 1
:DateUnix: 1498093622.0
:DateShort: 2017-Jun-22
:END:


** I recently came across the concept of [[https://en.wikipedia.org/wiki/Chekhov%27s_gun][Chekhov's gun]]. It's an old idea but this time when I read about it, it really appealed to me.
:PROPERTIES:
:Author: _o_O_o_O_o_
:Score: 5
:DateUnix: 1497887608.0
:DateShort: 2017-Jun-19
:END:

*** What I find really interesting is that there's some counter play with the audience. The author doesn't introduce a gun in the first act unless it will be fired in the third act, but since the audience knows that then the gun firing in the third act becomes less unexpected/thrilling. So authors are in a way encouraged to leave unfired guns and red herrings laying around, but that undercuts the tightness of the plot.
:PROPERTIES:
:Author: alexanderwales
:Score: 14
:DateUnix: 1497892108.0
:DateShort: 2017-Jun-19
:END:

**** I've observed myself noticing Chekhov's guns before, and then almost entirely forgetting them soon afterwards as I follow the rest of the story. The true value of a Chekhov's gun is in how easy it is to, when the plot moves to another scene, let the gun slip to the level of remembered factoid, at which point the use of it in act 3 not only comes by surprise just as if it came out of nowhere, but has /bonus/ thrill due to the connection to the first act.

Intellectually, the reader can review what's happened and conclude the gun's going to be used, but when you're immersed in the story it's really hard to keep that in mind in the moment as you approach where it's used, so it works out just fine.
:PROPERTIES:
:Author: InfernoVulpix
:Score: 8
:DateUnix: 1497896766.0
:DateShort: 2017-Jun-19
:END:


**** If you want to talk audience counterplay, there's always [[http://tvtropes.org/pmwiki/pmwiki.php/Main/SchrodingersGun][Schrodinger's Gun]]. It's mainly used in tabletop games, but long form TV series use it as well. When The Master was killed and cremated on Doctor Who, there was a shot of someone reaching in and grabbing his ring from the ashes. Russell T Davies didn't even know who that person was going to end up having been, he just wanted to leave a hanging plot thread for later use.
:PROPERTIES:
:Author: mg115ca
:Score: 5
:DateUnix: 1497911981.0
:DateShort: 2017-Jun-20
:END:


**** u/neshalchanderman:
#+begin_quote
  So authors are in a way encouraged to leave unfired guns and red herrings laying around, but that undercuts the tightness of the plot.
#+end_quote

These two (red herrings, unfired guns) differ. By way of example Percy skulking around in Harry Potter and the Chamber of Secrets is a red herring but not an unfired Chekov's Gun. Act 3 finds the gun fired, the snag sewn: he has a girlfriend.

Things that draw our attention, but are not part of the main plot, can be either distractors that go nowhere and mean nothing (unfired guns), or part of some other story strand (red herrings). Red herrings need not undercut the tightness of the plot. Side stories may add to the main narrative by imparting context and nuance.

Both can generate surprise, an unsureness as to how the story will unravel, but be careful not to overwhelm your reader with detail.
:PROPERTIES:
:Author: neshalchanderman
:Score: 3
:DateUnix: 1497904393.0
:DateShort: 2017-Jun-20
:END:


**** u/_o_O_o_O_o_:
#+begin_quote
  since the audience knows that then the gun firing in the third act becomes less unexpected/thrilling
#+end_quote

Yes. Thats an interesting perspective. The author has to walk a fine line between this balance
:PROPERTIES:
:Author: _o_O_o_O_o_
:Score: 1
:DateUnix: 1497929619.0
:DateShort: 2017-Jun-20
:END:


*** I recently read a book that featured the concept heavily. Steelheart by Brandon Sanderson. From the beginning it makes clear that every small element of the first chapter will be pivotal in the final chapter. And it features a few explicit gun forms of checkhovs gun narrative elements throughout the story.
:PROPERTIES:
:Author: Terkala
:Score: 6
:DateUnix: 1497890533.0
:DateShort: 2017-Jun-19
:END:
