#+TITLE: [Q] Has anyone written narrative fiction of the AI Box Experiment?

* [Q] Has anyone written narrative fiction of the AI Box Experiment?
:PROPERTIES:
:Author: HamillianActor
:Score: 8
:DateUnix: 1391982684.0
:DateShort: 2014-Feb-10
:END:
[[http://yudkowsky.net/singularity/aibox/][Link for reference]].


** [[http://lesswrong.com/lw/qk/that_alien_message/]['That Alien Message']] tells about the AI box experiment from the point of view of the AI. To make the point more intuitive the story uses humanity in the place of AI.
:PROPERTIES:
:Score: 14
:DateUnix: 1392046318.0
:DateShort: 2014-Feb-10
:END:


** [[https://docs.google.com/document/d/18Xa3GTfnw4dWr1hkkc090Xl90UoSStBX5fVTtytrjME/edit?usp=sharing][Here's a first rough attempt.]] I'll clean it up a bit later.
:PROPERTIES:
:Author: alexanderwales
:Score: 6
:DateUnix: 1392147137.0
:DateShort: 2014-Feb-11
:END:


** I'd do it, but A) I don't know how anyone ever wins the AI box experiment as the AI and B) no sane person would set up the AI box experiment like that - you're going to need a quorum of individuals, break time to mull things over, etc. I suppose you could sketch out some kind of story even with both those things in mind, but it seems like it would be better to just get someone to hand you over a transcript of the actual AI-in-a-box conversation and read that instead, or to take that transcript and fictionalize it (perhaps with the twist of the story being that the person on the other end of the line was a human all along, and the test was designed to see whether you were fit for AI-in-a-box watching duty).
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1391987629.0
:DateShort: 2014-Feb-10
:END:

*** I think before you're going to write a story about it you should definitely try the experiment at least once. Almost everyone who has seriously tried it has said that it was a really unique experience. As far as I know [[/u/tuxedage]] has witnessed most AI box experiments so you could ask him more about details, he's an active redditor (though he mostly posts on [[/r/dogecoin][r/dogecoin]], idk maybe he's gone crazy after so many experiments) but I think he has promised not to reveal too much about the experiments he has participated in.

Here are all the interesting links about AI box experiments I could find.

[[http://tuxedage.wordpress.com/2013/10/04/ai-box-experiment-logs-archive/]]

[[http://lesswrong.com/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/]]

[[http://lesswrong.com/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/]]

[[http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/]]

[[https://tuxedage.wordpress.com/2013/01/21/revisiting-the-ai-box-experiment/]]

[[http://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/]]

[[http://lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you/]]

[[http://lesswrong.com/lw/9ld/ai_box_log/]]

[[https://www.refheap.com/18378]]

[[http://lesswrong.com/lw/uq/ais_and_gatekeepers_unite/]]

[[https://news.ycombinator.com/item?id=327427]]

[[http://lesswrong.com/lw/qk/that_alien_message/]]
:PROPERTIES:
:Score: 6
:DateUnix: 1392045900.0
:DateShort: 2014-Feb-10
:END:

**** I'd read maybe a third of those links - thanks, reading more should be helpful. I would definitely do the experiment, but I'm in the wrong category of participant - I think that a transhuman superintelligence could argue itself out of most varieties of box, and I think that a person could argue another person out of a box in the experiment as stated, but I don't think that another person could argue /me/ into letting them out of the box.

The big problem is finding someone who actually wants to run the experiment with me. I'd definitely be down for it, but as Tuxeage clearly shows, the demand to be the human is much higher than the supply. And then add to that the fact that if I did the experiment it's probable that I would just win, which wouldn't teach me all that much since I already know pretty much all the arguments used. (Probable not because I'm super awesome, but because from what I've been able to find the AI usually loses.) What I really want is that single, powerful moment where the human has been broken down and finally decides to let the AI out - and there's no guarantee that I would get that from playing a game.

Edit: Okay, I'm dumb, the very first link you gave was a collection of logs. I've read them all now, including the [[http://tuxedage.wordpress.com/drafts-and-notes/leotal-gk-vs-n9-2600-ai-gatekeeper-victory/soundlogic-gk-vs-smoothporcupine-ai-gatekeeper-victory/][Soundlogic vs Smoothporcupine]] one that was a broken link, and I'm even more convinced that there's not really a reason for me to actually run the experiment myself. What I'd really like to see is an experiment where a gatekeeper loses, and I don't think that I would lose as gatekeeper or win as AI.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1392048332.0
:DateShort: 2014-Feb-10
:END:

***** I'll play you, or anyone for that matter, for 2 million dogecoins, if you're ever curious or desperate enough to wonder what's involved in the experiment.
:PROPERTIES:
:Author: Tuxedage
:Score: 2
:DateUnix: 1392273721.0
:DateShort: 2014-Feb-13
:END:

****** I have no idea what that converts to in dollars, but I'm fairly sure that I can stand not knowing, /or/ I can at least wait until the logs of a more compellingly played AI-win game are released, which has to happen eventually, right?
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1392276450.0
:DateShort: 2014-Feb-13
:END:

******* Maybe. Probably. I mean, we did go for 7 years before someone managed to consistently beat other LWers in the AI box. It is hard to say.
:PROPERTIES:
:Author: Tuxedage
:Score: 1
:DateUnix: 1392276684.0
:DateShort: 2014-Feb-13
:END:


***** u/deleted:
#+begin_quote
  I don't think that another person could argue me into letting them out of the box.
#+end_quote

Doesn't everyone think that, why would people otherwise do this experiment? But yeah, what I've read your posts you don't seem like the kind of person who would lose in this.

edit. But I don't think you actually know all the arguments used in these experiments.
:PROPERTIES:
:Score: 1
:DateUnix: 1392049129.0
:DateShort: 2014-Feb-10
:END:

****** I finished reading through the logs. I don't think that I know all of the arguments used in the experiments, I just think that I know the general classes of arguments to a decent enough level that there wouldn't be any surprises. Though the experiments with released logs surely have a selection bias, I didn't see anything in there that surprised me.

Then again, maybe there's something that would surprise me, an unknown unknown that's been kept secret by the victors. I won't rule that out ... but I currently consider it unlikely.
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1392054120.0
:DateShort: 2014-Feb-10
:END:

******* If you read Tuxedage's post [[http://lesswrong.com/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/][here]], his opponent said that

#+begin_quote
  Huge props to Tuxedage, who played very well and used strategies I haven't even considered, even despite the rule change. There were a couple of times where I came close to losing. I think his approach was pretty clever and original. It's not something I expected, despite already having done extensive research into the AI box experiment before our game
#+end_quote

(from Alexei Testimony)

That piece of information convinced me that there must be some class of arguments that I haven't thought of.
:PROPERTIES:
:Score: 3
:DateUnix: 1392054400.0
:DateShort: 2014-Feb-10
:END:

******** u/Tuxedage:
#+begin_quote
  That piece of information convinced me that there must be some class of arguments that I haven't thought of.
#+end_quote

You are correct.
:PROPERTIES:
:Author: Tuxedage
:Score: 2
:DateUnix: 1392273431.0
:DateShort: 2014-Feb-13
:END:

********* This probably isn't an important argument, but if you knew real life information about the gatekeeper or some great conspiracy or /something/ that the gatekeeper would like to know, let's say you actually knew the solution to the Riemann hypothesis, but haven't just told anyone, could you use that information in the experience or would it count under real life consequences?
:PROPERTIES:
:Score: 1
:DateUnix: 1392306176.0
:DateShort: 2014-Feb-13
:END:

********** Uhhhh....

So far that situation hasn't come up yet. I'd have to think carefully about how the rules would be interpreted. So far my intuition is leaning towards "No.", it wouldn't count.
:PROPERTIES:
:Author: Tuxedage
:Score: 1
:DateUnix: 1392306602.0
:DateShort: 2014-Feb-13
:END:


****** I just went and read one. /Wow/ was /that/ ever boring. "Shit LW-ers Say", for two hours.
:PROPERTIES:
:Score: 2
:DateUnix: 1392051305.0
:DateShort: 2014-Feb-10
:END:

******* There must be some selection pressure to which logs get published and which ones don't.
:PROPERTIES:
:Score: 3
:DateUnix: 1392051532.0
:DateShort: 2014-Feb-10
:END:

******** /rolls eyes/

Yeah, obviously the ones in which the AI wins don't get published to hide the presence of benign/low-impact superintelligences among us.
:PROPERTIES:
:Score: 1
:DateUnix: 1392058191.0
:DateShort: 2014-Feb-10
:END:

********* Oh, you mean Eliezer Yudkowsky.
:PROPERTIES:
:Score: 2
:DateUnix: 1392058318.0
:DateShort: 2014-Feb-10
:END:

********** Recalibrate your sarcasm detector.

I do not regard EY as a Friendly superintelligence. I regard him as a mere human who /managed to notice the problem/, and has been putting together a team of people to seek an actual solution.

Now, given that most people either maintain some kind of faith-based ethics, go into sheer denial about the incompleteness of their moral notions, or shrug off a casual moral nihilism with no concept of /dangerous consequences of such/... Given that ethics is probably /the/ major thing where most people just not-look and not-think about their raw, gaping, nasty ignorance and its consequences.... Given that those few of us who /do/ look at the gaping, horrid void straight-on usually just spend our lives in mild neurosis with no notion of how to proceed from there...

His noticing that the problem exists and beginning to demand a rational solution to it /without/ going completely off-the-rails /nuts/ are actually major credits to his person.

That doesn't mean he's a living solution, of course.
:PROPERTIES:
:Score: 4
:DateUnix: 1392058703.0
:DateShort: 2014-Feb-10
:END:

*********** u/deleted:
#+begin_quote
  Recalibrate your sarcasm detector.
#+end_quote

NO YOU

(we should really start using the /s notation around here)

I agree about the rest of your post, EY has deserved at least some of his small-scale glory.
:PROPERTIES:
:Score: 2
:DateUnix: 1392059383.0
:DateShort: 2014-Feb-10
:END:


******* I promise you that the level at which I played at is /completely different/ from the boring stuff that goes on in the released logs. Just ask my previous gatekeepers.
:PROPERTIES:
:Author: Tuxedage
:Score: 1
:DateUnix: 1392273392.0
:DateShort: 2014-Feb-13
:END:

******** Well that's pretty cool, but did you actually manage to /win/?
:PROPERTIES:
:Score: 1
:DateUnix: 1392273598.0
:DateShort: 2014-Feb-13
:END:

********* Yes. Three times out of six. Did you read the linked pages?
:PROPERTIES:
:Author: Tuxedage
:Score: 4
:DateUnix: 1392273818.0
:DateShort: 2014-Feb-13
:END:


**** u/Tuxedage:
#+begin_quote
  (though he mostly posts on [[/r/dogecoin][r/dogecoin]], idk maybe he's gone crazy after so many experiments)
#+end_quote

That statement offends me!

I'll play you, or anyone for that matter, for 2 million dogecoins, if you're ever curious or desperate enough to wonder what's involved in the experiment.
:PROPERTIES:
:Author: Tuxedage
:Score: 1
:DateUnix: 1392273630.0
:DateShort: 2014-Feb-13
:END:

***** u/deleted:
#+begin_quote
  I'll play you, or anyone for that matter, for 2 million dogecoins, if you're ever curious or desperate enough to wonder what's involved in the experiment.
#+end_quote

Ah, I'll keep that in mind if I ever have a couple grands lying around with no use.
:PROPERTIES:
:Score: 1
:DateUnix: 1392305690.0
:DateShort: 2014-Feb-13
:END:


***** I'm in, provided you play AI.
:PROPERTIES:
:Author: TKOE
:Score: 1
:DateUnix: 1392993868.0
:DateShort: 2014-Feb-21
:END:

****** Yes I am. If you are serious about this, add me on skype: "Tuxedage". Let's talk.
:PROPERTIES:
:Author: Tuxedage
:Score: 1
:DateUnix: 1392997669.0
:DateShort: 2014-Feb-21
:END:

******* Done.
:PROPERTIES:
:Author: TKOE
:Score: 1
:DateUnix: 1392998085.0
:DateShort: 2014-Feb-21
:END:

******** Can you tell here what happened?
:PROPERTIES:
:Score: 1
:DateUnix: 1393429000.0
:DateShort: 2014-Feb-26
:END:

********* Never ended up happening, I misunderstood the offer.
:PROPERTIES:
:Author: TKOE
:Score: 1
:DateUnix: 1393492671.0
:DateShort: 2014-Feb-27
:END:

********** Did you think only Tuxedage would have to give money if you won and you wouldn't have to give anything? Because that was my first impression until I remembered how these things usually worked.
:PROPERTIES:
:Score: 1
:DateUnix: 1393512150.0
:DateShort: 2014-Feb-27
:END:

*********** No, actually I assumed that if either of us won we'd be paid, in reality Tuxedage was asking to be /paid/ to play the game, 2m dodge regardless of winning or losing.
:PROPERTIES:
:Author: TKOE
:Score: 1
:DateUnix: 1393532158.0
:DateShort: 2014-Feb-27
:END:


*** u/deleted:
#+begin_quote
  B) no sane person would set up the AI box experiment like that - you're going to need a quorum of individuals, break time to mull things over, etc.
#+end_quote

Which is why the supposed "experiment" is blatantly biased in favor of the person playing the AI. In real life, anyone guarding a boxed AI is going to have a "Code Red: relieve guard due to prisoner attempting psychological warfare".
:PROPERTIES:
:Score: 2
:DateUnix: 1392021097.0
:DateShort: 2014-Feb-10
:END:

**** The experiment is only meant to show people who believe that they could never be persuaded to release a boxed AI, that they can be persuaded. Not only that, but they can be persuaded by a merely human-level intelligence.

The argument is that any significantly above human level AI would be better at persuading (and everything else) so could overcome the difficulties. It's a proof of concept, rather than a prototype.
:PROPERTIES:
:Author: duffmancd
:Score: 6
:DateUnix: 1392021552.0
:DateShort: 2014-Feb-10
:END:

***** In which case the result is /much/ weaker than claimed, because "people can be persuaded of things through sheer emotional manipulation, repetition, and exploitation of ego failures"... was a known fact to everyone except the /really arrogant/ decades ago.

As in, Hitler was the prototype, and EY's proof of concept is a much weaker result than Hitler's prototype.
:PROPERTIES:
:Score: 0
:DateUnix: 1392022414.0
:DateShort: 2014-Feb-10
:END:

****** I don't think so. "People can be persuaded of things [they know to be very bad] through sheer emotional manipulation, repetition, and exploitation of ego failures" through a text-only console, in less than two hours had yet to be shown.

Out of interest, what do you think the claimed result is, because I suspect its very different from what I thought it was.
:PROPERTIES:
:Author: duffmancd
:Score: 3
:DateUnix: 1392023872.0
:DateShort: 2014-Feb-10
:END:

******* u/deleted:
#+begin_quote
  Out of interest, what do you think the claimed result is
#+end_quote

"A sufficient intelligence can take control of a human mind through a text terminal."
:PROPERTIES:
:Score: 2
:DateUnix: 1392024163.0
:DateShort: 2014-Feb-10
:END:

******** And getting a human to do something they previously stated they didn't want to do and saw no way they could be possibly persuaded to do doesn't count as control? I don't read it as Hollywood style "mind control" but a more subtle, "I can get you to *willing* do whatever I want" - advertising style. I think its strong evidence for that.
:PROPERTIES:
:Author: duffmancd
:Score: 2
:DateUnix: 1392024580.0
:DateShort: 2014-Feb-10
:END:


**** u/Tuxedage:
#+begin_quote
  Which is why the supposed "experiment" is blatantly biased in favor of the person playing the AI. In real life, anyone guarding a boxed AI is going to have a "Code Red: relieve guard due to prisoner attempting psychological warfare".
#+end_quote

I fully admit this. My ruleset is designed to be as advantageous to the AI as reasonably possible. It would have been way too difficult for me to win otherwise.
:PROPERTIES:
:Author: Tuxedage
:Score: 1
:DateUnix: 1392273944.0
:DateShort: 2014-Feb-13
:END:


*** I have an idea for a strategy I would employ that I think would also make for a strong dramatic situation. I'd be interested in writing it out as a fictional scenario, but obviously I'm doing my research first.

Of course, getting a transcript of any actual experiment would be an enormous boon, but those are hard to come by for obvious reasons.
:PROPERTIES:
:Author: HamillianActor
:Score: 1
:DateUnix: 1391988192.0
:DateShort: 2014-Feb-10
:END:

**** The biggest attack vectors seem to be personal knowledge and application of pressure; the AI gives a hard sell like you might find from a door-to-door vacuum salesman. And in fact, this strategy is explicitly favored in the rules, given that the Gatekeeper has to actively stay engaged for a somewhat long period of time with no breaks.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1391990107.0
:DateShort: 2014-Feb-10
:END:

***** Which I don't quite understand as a scenario. If you create an AI and put it in a box with no access to the outside world (other than through the gatekeeper), how can it use personal knowledge as leverage against a gatekeeper it doesn't know?

This is part of my potential dramatic scenario. Presume the AI, when first created, had enough contact with the larger world for long enough to have a dossier on anybody with a substantial enough electronic footprint. And that it's far enough in the future that just about everybody has a substantial electronic footprint.

As a gatekeeper, why not hire somebody with no electronic footprint (even if you have to go to substantial lengths to do so) to act as guard? The AI may still learn the relevant info through conversation, but at least it'd be a handicap and if the guard were sufficiently strong willed, could keep the AI from learning any information to use as leverage in the first place.
:PROPERTIES:
:Author: HamillianActor
:Score: 1
:DateUnix: 1391993556.0
:DateShort: 2014-Feb-10
:END:

****** As far as I've ever guessed, the AI doesn't use blackmail or stuff like that. It goes for emotional attacks, exploitation of cognitive biases, and basically just all the other ways for tricking a person despite themselves. The idea seems to be, a human mind that is prone to mistakes in predictable ways is open to attack on its will in predictable ways.

The transcripts of every actual experiment were kept secret, seemingly just to be mysterious. One guy who blogged about it said that he played as the AI and lost, but kept the transcript secret because his dialogue really had been /outright cruel/ in its blatant attempts to attack and wear down the Gatekeeper's ego.
:PROPERTIES:
:Score: 2
:DateUnix: 1392021444.0
:DateShort: 2014-Feb-10
:END:

******* u/deleted:
#+begin_quote
  The transcripts of every actual experiment were kept secret
#+end_quote

Actually you're *WRONG!*

There are some logs of experiments, I linked to them in my [[http://www.reddit.com/r/rational/comments/1xgq7r/q_has_anyone_written_narrative_fiction_of_the_ai/cfbpntf][post.]] Though not a single one where the AI won.
:PROPERTIES:
:Score: 2
:DateUnix: 1392046455.0
:DateShort: 2014-Feb-10
:END:


****** It's an interesting concept, and one that I think has some legs. Really, the whole scenario raises a lot of very good questions. I wish you luck in writing it - writing a superintelligence is hard.
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1391994390.0
:DateShort: 2014-Feb-10
:END:


****** u/deleted:
#+begin_quote
  how can it use personal knowledge as leverage against a gatekeeper it doesn't know?
#+end_quote

You don't know how much information you reveal just by writing things (or if the AI has access to surveillance footage of you then even more).
:PROPERTIES:
:Score: 1
:DateUnix: 1392050392.0
:DateShort: 2014-Feb-10
:END:


** There was a group that was trying to turn it into a movie. They've produced a couple of short films/teasers. It's called [[http://www.k3loid.com/][Keloid.]]

EDIT They have changed the video posted on their site significantly. At around 2:20 in the video, though, you can see the setup for the AI Box.
:PROPERTIES:
:Author: MinibearRex
:Score: 3
:DateUnix: 1392014379.0
:DateShort: 2014-Feb-10
:END:


** If anyone ever puts in a decent effort into a narrative fiction for the AI box experiment, if you send it to me, I'm willing to give my feedback.
:PROPERTIES:
:Author: Tuxedage
:Score: 1
:DateUnix: 1392273779.0
:DateShort: 2014-Feb-13
:END:


** The first chapter of Robopocalypse by Daniel H Wilson is a kind of iterated AI box experiment where a series of slight variations on a single strong AI are awaked, interrogated and then wiped if they are decided to be untrustworthy or unfriendly.
:PROPERTIES:
:Author: grawk1
:Score: 1
:DateUnix: 1393126593.0
:DateShort: 2014-Feb-23
:END:


** Gods, I hope not.
:PROPERTIES:
:Author: Newfur
:Score: 1
:DateUnix: 1396626929.0
:DateShort: 2014-Apr-04
:END:
