#+TITLE: [WIP][HSF][TH] FAQ on LoadBear's Instrument of Precommitment

* [WIP][HSF][TH] FAQ on LoadBear's Instrument of Precommitment
:PROPERTIES:
:Author: DataPacRat
:Score: 8
:DateUnix: 1430327571.0
:DateShort: 2015-Apr-29
:END:
My shoulder's doing better, so I'm getting back into 'write /something/ every day' by experimenting with a potential story-like object at [[https://docs.google.com/document/d/1nRSRWbAqtC48rPv5NG6kzggL3HXSJ1O93jFn3fgu0Rs/edit]] . It's extremely bare-bones so far, since I'm making up the worldbuilding as I go, and I just started writing an hour ago.

I welcome all questions that I can add to it, either here or there.


** You could combine it with your other story about Bunny where this is a FAQ provided by one of the human-turned-AIs she runs across. The question is how far she could even trust this FAQ to tell her anything useful about this new, unknown AI?
:PROPERTIES:
:Author: xamueljones
:Score: 3
:DateUnix: 1430328565.0
:DateShort: 2015-Apr-29
:END:

*** It's an interesting idea, but at the moment, I'd like to stretch my imagination a bit with a separate setting - one in which civilization hasn't had an apocalyptic crash.
:PROPERTIES:
:Author: DataPacRat
:Score: 3
:DateUnix: 1430328683.0
:DateShort: 2015-Apr-29
:END:

**** Hum, if you ever wanted to work in something that has a bit more handwavium again It would actually fit in a setting I'm tentatively looking for collaborators for.

For me it's mostly out of a desire to really dig into a fantastical fantasy (something like rational alice and wonderland) setting instead of just more tolkeinesque gobbos.
:PROPERTIES:
:Author: Nighzmarquls
:Score: 3
:DateUnix: 1430349824.0
:DateShort: 2015-Apr-30
:END:


** Quick poll!

People who've said out loud something along the lines of "I consent to have my mindstate instantiated in other environments under the following conditions", raise your hands.

/hand/
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1430365320.0
:DateShort: 2015-Apr-30
:END:

*** What is this 'out loud' you speak of, and what purpose does it serve? :)

(I'm currently working out what conditions I /would/ say something like that to, but second-order effects can be tricky...)
:PROPERTIES:
:Author: DataPacRat
:Score: 3
:DateUnix: 1430390325.0
:DateShort: 2015-Apr-30
:END:

**** Well, I'm assuming just /thinking/ it would not be considered actual consent. Thought is involuntary. Saying it out loud implies a conscious choice.

Also I assume decoding air vibrations is easier than decoding neural patterns.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1430397629.0
:DateShort: 2015-Apr-30
:END:

***** How would you figure the decoding works?

Wouldn't it be more reliable to have your statement explicitly notarised (and present in your will, the institute's records, with family members and friends, etc)?

Also by current law you're a corpse that is either the property of your family or of the cryonics institute, so they don't need consent to copy you.
:PROPERTIES:
:Author: philip1201
:Score: 1
:DateUnix: 1430401009.0
:DateShort: 2015-Apr-30
:END:

****** No yeah, this mostly applies if some version of the Simulation Hypothesis is true. Doesn't have to be our descendants.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1430407862.0
:DateShort: 2015-Apr-30
:END:


**** Well, memorizing a terms and conditions of your own emulation is kind of a cool gimmick. And who knows, maybe the standard ethics for reviving licenseless mindstates in the future will be to check for terms of use in the mindstate itself.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1430966716.0
:DateShort: 2015-May-07
:END:


** Interesting.

I guess my viewpoint on mindstate proliferation has always been colored by the idea of 4chan getting their hands on me. The incentive to torture is sometimes solely the torture itself, and it doesn't matter that you wouldn't get cooperation out of it. By engaging in a proliferation scheme, you're dooming some percentage of your selves to extremely unpleasurable existences. I suppose it still makes sense if you think that the percentage of selves that don't get tortured is going to be high, or if you think that a life of being tortured and abused is better than not existing even if you have alternate instantiations out there.

Am I allowed to roll back LoadBear to a previous version? Is this considered the same as killing him? Does it require his consent in order to be within the Terms?
:PROPERTIES:
:Score: 2
:DateUnix: 1430503562.0
:DateShort: 2015-May-01
:END:

*** u/DataPacRat:
#+begin_quote
  I suppose it still makes sense if you think that the percentage of selves that don't get tortured is going to be high, or if you think that a life of being tortured and abused is better than not existing even if you have alternate instantiations out there.
#+end_quote

There's also a few interesting caveats about identity theory: If Random-Omnipotent-4channer builds a box with a copy of you from a few years ago, tortures it, then deletes the copy... how much effort would it be worth to go to to try to stop the RO4?

#+begin_quote
  Am I allowed to roll back LoadBear to a previous version? Is this considered the same as killing him?
#+end_quote

Given that any LoadBear that's rolled back all the way to the initial mindstate is, effectively, undone, as if they'd never computed at all... it seems safe to conclude that rolling back any LoadBear to a previously-stored state would be pretty much the same as deleting the active copy, "killing" it (for whatever definition of 'killing' applies to an entity with multiple instantiations).

#+begin_quote
  Does it require his consent in order to be within the Terms?
#+end_quote

Barring some extraordinary circumstances, which would likely require third-party verification to prove actually happen... then such rollbacks without prior consent would seem to be against the Terms.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1430505846.0
:DateShort: 2015-May-01
:END:

**** u/deleted:
#+begin_quote
  Given that any LoadBear that's rolled back all the way to the initial mindstate is, effectively, undone, as if they'd never computed at all... it seems safe to conclude that rolling back any LoadBear to a previously-stored state would be pretty much the same as deleting the active copy, "killing" it (for whatever definition of 'killing' applies to an entity with multiple instantiations).
#+end_quote

But I could (for example) save a copy of my LoadBear on a monthly basis? And according to the terms there doesn't seem to be a set minimum amount of runspeed that I'm required to give to LoadBear. So if I thought that LoadBear was doing a poor job as my customer service rep, and that he'd been doing better a month ago before he got burnt out on dealing with our recall issues, could I simply slow LoadBear down until he was at 1sec/1000sec and using minimal processing power, then boot up the month old copy of LoadBear? Does this depend entirely on my jurisdiction's definition of death and murder? I mean, speaking to you I'm pretty sure that LoadBear cares, and wouldn't like this, but the Terms aren't really clear on this, and I don't know whether it would be a breach of contract.

Incidentally, what are my options if I instantiate LoadBear but then don't want him anymore?
:PROPERTIES:
:Score: 2
:DateUnix: 1430510621.0
:DateShort: 2015-May-02
:END:

***** u/DataPacRat:
#+begin_quote
  could I simply slow LoadBear down until he was at 1sec/1000sec and using minimal processing power, then boot up the month old copy of LoadBear?
#+end_quote

Yes, that would be entirely within the Terms.

#+begin_quote
  what are my options if I instantiate LoadBear but then don't want him anymore?
#+end_quote

I can't believe I haven't already included this in the FAQ.

LoadBear's recommended procedure in such a case is to put the em on pause, and copy the current Mindstate onto longer-term storage media than RAM, such as a hard-drive. If you do not wish to store even that, then the recommended procedure is to find a third party willing to hold that copy, such as LoadBearNet Alpha, and transfer it to them. (Note that in such a case, it would be poor form to continue to demand that the LoadBear in question continues to owe you for the processing power you used to run him, as part of the Terms involve said LoadBear having the opportunity to pay you back. At the very least, you should stop charging interest on that debt, and not require any payments until that LoadBear is unpaused.)
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1430511792.0
:DateShort: 2015-May-02
:END:


** Has anyone got some tables and charts for increasing computer power, extending Moore's Law and its relatives into the future? I want to identify some interesting moments for my future history - eg, when running an em becomes cheaper than paying a human minimum wage, or when an em can be stuffed into a human-sized chassis - but I seem to have lost my references on the topic.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1430390204.0
:DateShort: 2015-Apr-30
:END:

*** That would depend entirely on simulation fidelity, though - synapse/molecule-level simulation as opposed to emulating higher-level mental processes (and we don't have a good idea of how much computing either would require to begin with). Even assuming Moore's law continues to hold - not a particularly probable assumption - insufficient data for meaningful answer.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 1
:DateUnix: 1430435837.0
:DateShort: 2015-May-01
:END:

**** At the moment, I'm stealing a number from [[http://www.orionsarm.com/eg-article/4a53a8f690f09]] and postulating 100 petabytes per mindstate.

I'm not trying to place prediction-market bets; I'm just trying to get a reasonably consistent and plausible set of numbers for some SFnal worldbuilding.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1430436561.0
:DateShort: 2015-May-01
:END:

***** Allright, using the following data

[[http://www.jcmit.com/memoryprice.htm]]

[[https://en.wikipedia.org/wiki/FLOPS#Cost_of_computing]]

I got the following fits for linear trends of [[http://i.imgur.com/GmHXdkp.png][log10(USD/megabyte)]] and [[http://i.imgur.com/dS78Iw2.png][log10(USD/GFLOPS)]].

If you extrapolate that, you get 2013 $1000 per near-baseline human's worth of storage in ~2047, and 2013 $1000 per near-baseline human's worth of processing power in ~2035. This doesn't account for ongoing costs like power, maintenance and support.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 2
:DateUnix: 1430476689.0
:DateShort: 2015-May-01
:END:

****** ***** 
      :PROPERTIES:
      :CUSTOM_ID: section
      :END:
****** 
       :PROPERTIES:
       :CUSTOM_ID: section-1
       :END:
**** 
     :PROPERTIES:
     :CUSTOM_ID: section-2
     :END:
[[https://en.wikipedia.org/wiki/FLOPS][*FLOPS*]]: [[#sfw][]]

--------------

#+begin_quote
  In [[https://en.wikipedia.org/wiki/Computing][computing]], *FLOPS* or *flops* (an acronym for *FL*oating-point *O*perations *P*er *S*econd) is a measure of [[https://en.wikipedia.org/wiki/Computer_performance][computer performance]], useful in fields of scientific calculations that make heavy use of [[https://en.wikipedia.org/wiki/Floating-point][floating-point]] calculations. For such cases it is a more accurate measure than the generic [[https://en.wikipedia.org/wiki/Instructions_per_second][instructions per second]].

  Although the final /S/ stands for "second", singular "flop" is often used, either as a [[https://en.wikipedia.org/wiki/Back_formation][back formation]] or an abbreviation for "*FL*oating-point *OP*eration"; e.g. a flop count is a count of these operations carried out by a given algorithm or computer program.
#+end_quote

--------------

^{Interesting:} [[https://en.wikipedia.org/wiki/Flip-flops][^{Flip-flops}]] ^{|} [[https://en.wikipedia.org/wiki/Flip-flop_(electronics)][^{Flip-flop} ^{(electronics)}]]

^{Parent} ^{commenter} ^{can} [[/message/compose?to=autowikibot&subject=AutoWikibot%20NSFW%20toggle&message=%2Btoggle-nsfw+cquvivj][^{toggle} ^{NSFW}]] ^{or[[#or][]]} [[/message/compose?to=autowikibot&subject=AutoWikibot%20Deletion&message=%2Bdelete+cquvivj][^{delete}]]^{.} ^{Will} ^{also} ^{delete} ^{on} ^{comment} ^{score} ^{of} ^{-1} ^{or} ^{less.} ^{|} [[http://www.np.reddit.com/r/autowikibot/wiki/index][^{FAQs}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/][^{Mods}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/][^{Magic} ^{Words}]]
:PROPERTIES:
:Author: autowikibot
:Score: 1
:DateUnix: 1430476701.0
:DateShort: 2015-May-01
:END:


****** Thank you /very/ much for those tables. Running your numbers back and forth, I get the following timeline for prices of a near-baseline's storage and realtime processing:

#+begin_example
  2015: RAM: $1B. CPU: $91M.
  2020: RAM: $115M. CPU: $5.2M.
  2025: RAM: $13.3M. CPU: $300k
  2030: RAM: $1.5M. CPU: $17k.
  2035: RAM: $177k. CPU: $1000.
  2040: RAM: $20k. CPU: $58
  2045: RAM: $2371. CPU: $3.31.
  2050: RAM: $274. CPU: $0.19.
  2055: RAM: $31.61. CPU: $0.011
#+end_example

... Now, that is a /fascinating/ timeline in the context of ems.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1430504142.0
:DateShort: 2015-May-01
:END:

******* On the other hand, you may not need the entire em in ram at all times. Hard drives or even solid-state drives are a much cheaper option in terms of money per unit of storage, and since this extrapolation puts the necessary processing power much sooner than ram, that may be the more sensible estimate.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 1
:DateUnix: 1430504761.0
:DateShort: 2015-May-01
:END:

******** Another possibility: Running an em at faster than realtime speeds requires additional CPU power, but pretty much the same amount of RAM.

I just checked [[https://en.wikipedia.org/wiki/Koomey%27s_law]] , and have made a note to see if I can work out how many kWh per subjective year a nearbaseline would need, and then compare that to typical energy prices (and overall worldwide energy production); that may give me an upper bound on em population.

I'm also going to see if there's anything similar for increasing resolution in electron micrography, which might let me pinpoint the year in which LoadBear's initial mindstate was first digitized.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1430505416.0
:DateShort: 2015-May-01
:END:

********* ***** 
      :PROPERTIES:
      :CUSTOM_ID: section
      :END:
****** 
       :PROPERTIES:
       :CUSTOM_ID: section-1
       :END:
**** 
     :PROPERTIES:
     :CUSTOM_ID: section-2
     :END:
[[https://en.wikipedia.org/wiki/Koomey%27s%20law][*Koomey's law*]]: [[#sfw][]]

--------------

#+begin_quote
  *Koomey's law* describes a long-term trend in the [[https://en.wikipedia.org/wiki/History_of_computing_hardware][history of computing hardware]]. The number of computations per [[https://en.wikipedia.org/wiki/Joule][joule]] of energy dissipated has been doubling approximately every 1.57 years. This trend has been remarkably stable since the 1950s (R^{2} of over 98%) and has actually been somewhat faster than [[https://en.wikipedia.org/wiki/Moore%E2%80%99s_law][Moore's law]]. [[https://en.wikipedia.org/wiki/Jonathan_Koomey][Jonathan Koomey]] articulated the trend as follows: "at a fixed computing load, the amount of battery you need will fall by a factor of two every year and a half."

  * 
    :PROPERTIES:
    :CUSTOM_ID: section-3
    :END:
  [[https://i.imgur.com/FC9ll0W.jpg][*Image*]] [[https://commons.wikimedia.org/wiki/File:Koomeys_law_graph,_made_by_Koomey.jpg][^{i}]] - /Computations per KWh, from 1946 to 2009/
#+end_quote

--------------

^{Interesting:} [[https://en.wikipedia.org/wiki/Dennard_scaling][^{Dennard} ^{scaling}]] ^{|} [[https://en.wikipedia.org/wiki/Jonathan_Koomey][^{Jonathan} ^{Koomey}]] ^{|} [[https://en.wikipedia.org/wiki/Performance_per_watt][^{Performance} ^{per} ^{watt}]] ^{|} [[https://en.wikipedia.org/wiki/Moore%27s_law][^{Moore's} ^{law}]]

^{Parent} ^{commenter} ^{can} [[/message/compose?to=autowikibot&subject=AutoWikibot%20NSFW%20toggle&message=%2Btoggle-nsfw+cqv9u7y][^{toggle} ^{NSFW}]] ^{or[[#or][]]} [[/message/compose?to=autowikibot&subject=AutoWikibot%20Deletion&message=%2Bdelete+cqv9u7y][^{delete}]]^{.} ^{Will} ^{also} ^{delete} ^{on} ^{comment} ^{score} ^{of} ^{-1} ^{or} ^{less.} ^{|} [[http://www.np.reddit.com/r/autowikibot/wiki/index][^{FAQs}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/][^{Mods}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/][^{Magic} ^{Words}]]
:PROPERTIES:
:Author: autowikibot
:Score: 1
:DateUnix: 1430505462.0
:DateShort: 2015-May-01
:END:


******** u/DataPacRat:
#+begin_quote
  Hard drives or even solid-state drives
#+end_quote

True, but brain emulation seems the sort of thing that would require accessing random pieces of data to update, which implies that the processor would need to be spending most of its time waiting for swapped-out parts of the em to get copied to ram and back. There may be times when that's useful, but it seems unlikely to be a common approach.
:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1430511413.0
:DateShort: 2015-May-02
:END:

********* I don't know much about how the brain works, but I'm currently training a deep belief network for visual face recognition and reconstruction tasks. Most learning algorithms can easily be modified so that only a small subset of its ~25GB parameter space has to be accessed simultaneously - small enough to fit into the 2GB of memory on my graphics card. It's when you switch from learning to trying to do work with it that you need access to all the parameters of the model as fast as possible, but even that can be somewhat organised in layers. As I understand it, the brain doesn't have a hard switch between learning stuff and doing stuff, so the same general principle may apply.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 1
:DateUnix: 1430512553.0
:DateShort: 2015-May-02
:END:


********* Solid-state drives.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1430966818.0
:DateShort: 2015-May-07
:END:
