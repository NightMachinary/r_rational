#+TITLE: How would a character be able to realize they are living in a fictional universe? (Rationalizing Death Chpt 10)

* How would a character be able to realize they are living in a fictional universe? (Rationalizing Death Chpt 10)
:PROPERTIES:
:Score: 16
:DateUnix: 1475261062.0
:END:
I think there was something like that on "Rationalizing Death", chapter 10 where Misa was questioning reality or something. But the fic got rebooted and I was only on chapter 2. I also saw something familiar on "Friendship is Optimal" where it was stated that Loki from the "Asgard Game" was able to realize that he was in a game. I believe I saw the suggestion from pedromvilar also known as scientifictheisis


** The scientific-themed spelling of "fictional universe" is "actively managed simulation". You'd probably look for situations that systematically violate established probability. (As Pratchett said, "one-in-a-million events tend to crop up nine times out of ten.")
:PROPERTIES:
:Author: FeepingCreature
:Score: 22
:DateUnix: 1475264359.0
:END:

*** Likewise, the religious-themed spelling of "we are living in a fictional universe" is "theism is true".
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 17
:DateUnix: 1475267787.0
:END:

**** That's a fascinating train of thought!
:PROPERTIES:
:Author: Dwood15
:Score: 3
:DateUnix: 1475275664.0
:END:


**** Blasphemy!

I mean, literally blasphemous. Probably true, but none the less, blasphemy!
:PROPERTIES:
:Author: Mr_Smartypants
:Score: 3
:DateUnix: 1475391721.0
:END:


*** Another option is to look for flaws in the simulation. If it runs in the physical universe then it's limited (in terms of processor power and memory) to something less complex than the real universe. It may not be possible to notice it; if it's accurate enough then there won't be any human-perceptible flaws. Likewise, if the author can tune your mind in real time then you won't ever be able to remember noticing it because zie will just cut the memory out and make you think something else instead.

Assuming it's possible to notice, then things that could give it away might include: minor clipping errors, improper rendering of complex things like hair when moving quickly / in a windstorm, vision cones that don't quite update quite fast enough and therefore leave a flash of blackness at the edge of your vision when you turn around quickly enough, or improper rendering in reflections. Likewise, if some of the people around you are NPCs then you might be able to notice overly simplistic behaviors, especially in complex environments with lots of people where the processor has trouble keeping up.
:PROPERTIES:
:Author: eaglejarl
:Score: 4
:DateUnix: 1475327852.0
:END:

**** Or you might notice that, due to data limitations, the simulation loses some detail at high enough magnification. It might, for example, not have enough space to save both position /and/ momentum data for every single particle.
:PROPERTIES:
:Author: CCC_037
:Score: 16
:DateUnix: 1475479655.0
:END:

***** But in reality we can't....

Ohhhh.
:PROPERTIES:
:Author: eaglejarl
:Score: 6
:DateUnix: 1475514918.0
:END:


** I don't understand what it means for a character to be living in a fictional universe. A fictional universe doesn't contain any characters; it contains depictions of characters. Normally this distinction doesn't matter, of course, but it matters here.

The best I can come up with is "if the description of this fictional universe described an actual universe, would a character in it be able to recognize that his universe matches a description that was used as fiction".

I understand the question if you replace "fictional universe" with "simulation", but I'm not convinced that what we normally refer to as a fictional universe is a simulation in a relevant way.
:PROPERTIES:
:Author: Jiro_T
:Score: 9
:DateUnix: 1475322080.0
:END:

*** Yes; the trivial and always correct answer to "how does a character realize they're fictional" is "the author decides that they do". A fictional character may never notice that they're fictional against the author's will, rational fiction included - if it becomes apparent that anyone rational would realize that they were fictional, the author may avoid it and preserve rationality by retroactively changing past events to make it less obvious, and this in fact produces a more rational story, not a less rational one, by the standards of most.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 10
:DateUnix: 1475325107.0
:END:


** You're talking about [[https://www.fanfiction.net/s/10580913/1/Rationalising-Fiction][Rationalising Fiction]] one-shot, a reverse fanfiction of Rationalising Death.

#+begin_quote
  How would a character be able to realize they are living in a fictional universe?
#+end_quote

I can think of a few ways.

First is, as [[/u/FeepingCreature][u/FeepingCreature]] pointed out above, narrative causality: if events which make narrative sense seem to happen more often than they should, the probability that you're in a story increases.

Second is human limitations.

All stories, so far, were written by humans. Which means that the plot can only be as complex as a human can conceive, it should make sense to a human, and it cannot contain answers to questions humanity hadn't answered yet. If an AI achieves the Singularity and then behaves like Skynet, we're in a story. If aliens are inexplicably humanoid and want to take our women, we're in a story. And so on. Anthropic Principle, essentially: if this is a story created by a human, it must be such that a human could have imagined and created it.

Moreover, the author is usually but one person. One person could only know so much, and be only that competent in so many things. As we on [[/r/rational][r/rational]] know, a writer cannot write a character smarter than the writer. It goes further: besides incongruent characterization, stories might have nonsensical worldbuilding, scientific mistakes, plot holes and other inconsistencies.

Basically, no author is perfect, and if an author is capable of noticing flaws in his/her own worldbuilding, charactarization or plot, then this author could write a character who can and will do the same.

Then there's predictability, which can work in only certain case: if a character's mind is similar to the author's. Then that character could predict the future by asking himself, 'if this were a story and I was writing it, what would I write next'? I saw a few Self-Inserts do just that.
:PROPERTIES:
:Author: Noumero
:Score: 6
:DateUnix: 1475269619.0
:END:

*** #+begin_quote
  Rationalising Fiction

  If you're still not convinced, just wait until the day you die and wake up in the Naruto universe. I hear that's been happening a lot lately.
#+end_quote

Oh my god, my sides.
:PROPERTIES:
:Author: gabbalis
:Score: 12
:DateUnix: 1475274789.0
:END:


*** It's possible to write a character marginally smarter than the writer, by using smarter-than-the-writer sources/assistance, or having a character arrive at conclusions faster than the writer would or on fewer clues or skimpier evidence.

Far more difficult to write a character /noticeably/ smarter than the writer, though, unless you're having a significantly smarter person (or group) actively providing input on the character's thoughts/actions/reasoning, and the writer is less involved in that aspect and more in integrating the result into the rest of the story.
:PROPERTIES:
:Author: Geminii27
:Score: 5
:DateUnix: 1475286065.0
:END:


*** #+begin_quote
  scientific mistakes
#+end_quote

Compared to what? Fictional characters can't check real-world textbooks.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1475404488.0
:END:

**** But they can decide to check in-universe textbooks, which a honest author would fill with real-world knowledge^{1}. Then they may engage in research, forcing the author in reality to do the same, and afterwards either confirm the violation of the laws of physics (with the author realizing his mistake), or disprove it.

^{1. That is, assuming the setting has no magic or other fundamental differences from reality.}
:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1475416132.0
:END:

***** I think most authors usually elide over the content of textbooks, except when they're meaningless technobabble or a factoid that's part of the plot.

Harry Potter, for example. Hermione spends her days reading books on magical theory. But the theory doesn't exist. If she tried to think of a contradiction between what they predict and reality, her brain would supply a fatuous "prediction" on the spot, pre-designed to be harmless and fit the worldbuilding.

#+begin_quote
  Well, Eardhart's Extinguishing Draught famously appears to conflict with Johnson's law, that the composites of a unique poison are always made from non-poisons themselves, but that was explained in 1973 by Magwalt Humdinger as a result of several of the components not being true poisons, but more accurately classified venoms that employ debilitating magical effects. That's only a theory, though, Humdinger's classification system was never experimentally verified.
#+end_quote

That's not even the author deliberately placing a thumb on the scales (which they could do, of course, if they entered into conflict with the character); her universe just fundamentally /operates/ on rules like "X is governed by very complex sciency rules that must be learned, offscreen, in order to understand it". You can't appeal to a textbook

__

Meanwhile, Neo's universe does have some science - not computer science, but there's biology and physics and you can discuss them in some detail, because it's a sci-fi story.

But the world doesn't have conservation of energy, because the authors forgot/deliberately ignored it. If he looks up "can humans produce more energy than they take in", he will find that it's a well-known fact they produce X watts of net energy output.

There's /science/ - he can ask why animals didn't evolve to eat each other, for example, and learn that the sun provides much more energy than animal biology does so it's more efficient to eat plants or use solar panels. But the only way he's going to learn about /conservation of energy/, the real world concept, is if the author is genuinely too stupid to spot the contradiction and puts it in supporting, say, the reason they can't use antigravity thingies to build a perpetual motion machine.
:PROPERTIES:
:Author: MugaSofer
:Score: 5
:DateUnix: 1475441472.0
:END:


*** How about something like "Friendship is Optimal" where the character, or code in this case, while created by the creator can think for himself and even become smarter than the creator.
:PROPERTIES:
:Score: 1
:DateUnix: 1475270235.0
:END:

**** That's based around a certain theory of how an AI might work, since if you could find a more efficient neural architecture for it than the human brain it could think better than humans. That said, what you said is my main argument against that model. I just don't think intelligence works that way and even if I did, I don't understand why anyone would want to create an optimizer AI anyway.
:PROPERTIES:
:Author: trekie140
:Score: 2
:DateUnix: 1475340442.0
:END:

***** #+begin_quote
  I don't understand why anyone would want to create an optimizer AI anyway.
#+end_quote

To manage the world better (according to human values) than our current social structures have been able to?
:PROPERTIES:
:Author: Evan_Th
:Score: 1
:DateUnix: 1475345997.0
:END:

****** That idea has always sounded to me like "let's create God", which I dislike because I think it conflicts with my value of human autonomy even if the AI is benevolent. I'm all for transforming humanity into gods, but I'm opposed to the idea of surrendering our self determination to a god.

You can argue that a properly designed AI would take that into account in order to satisfy us, but I still disapprove of the reason for creating it. It always sounds like you're giving up on humanity and deciding we should create something better than humans to take care of us all. Even if I could do that, I wouldn't.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1475355980.0
:END:

******* I think I see where you're coming from, but I think that - if the AI's designed correctly, which is definitely a big "if" - individual human's autonomy wouldn't be much more abridged than it already is by society. I as an individual am already surrendering a lot of my self-determination to society in exchange for a whole lot of benefits, and on the whole I'm pleased with the transaction. A good AI running society would probably give me an even better bargain.

Or are you valuing human autonomy in a social sense, in which case we'd probably need to agree to disagree on our different values?
:PROPERTIES:
:Author: Evan_Th
:Score: 2
:DateUnix: 1475356482.0
:END:

******** I do value it in social sense, so I guess we will. However, if your eventually goal is still to create a AI to optimize human values, you are going to have the take the values of people like me into account. As a political centrist, the only future I can currently imagine tolerating with AIs is one where they have the same legal rights and responsibilities as humans, including running for political office.
:PROPERTIES:
:Author: trekie140
:Score: 2
:DateUnix: 1475363356.0
:END:


******* Do you value that abstract concept more than you value everyone not dying any more forever as well as generally being much, much happier?
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 2
:DateUnix: 1475533074.0
:END:

******** I would prefer my difference in values not be framed that way as it implies that I am acting in defiance of utilitarian morality, when I am also trying to find the way to do the right thing.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1475535784.0
:END:

********* Well, is it in defiance of utilitarian morality? Either it is, in which case it is, or it isn't in which case you do in fact value it more than that other stuff. I don't really see a third way here, and I don't think that's because of the frame.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1475604233.0
:END:

********** I do not believe that thinking in black and white like that is useful for a discussion of moral values. I have seen many rationalists, including Yudkowsky, discussing utilitarian morality as if any logical person would agree with them on what the right thing to do is. After reading The Righteous Mind by Jonathan Haidt, I no longer believe that human moral values can be modeled that simply.

It isn't just a matter of overcoming biases, it's about dealing with psychological predispositions that have been ingrained by natural selection. Some people find it easy to think like pure utilitarians, but many do not and possibly cannot because evolution has made us implicitly value things that helped our species survive when we were just tribes of hunter-gatherers. Going against that grain may seem rational, but not everyone is psychologically capable of doing it.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1475607173.0
:END:

*********** Could you continue with that thought and see where it leads us? You're definitely building up to something and I'd like to hear what it is.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1475609756.0
:END:

************ Haidt really does explain it better and has evidence to back it up, but I'll try my best. Basically humans likely have genetic predispositions toward following the moral philosophy that their social group identifies with, while also rejecting the philosophy espoused by other groups, because that trait helped the group to survive. It's not Belief in Belief, it's actual belief that lies at the core of human social instincts.

The reason why rationalists/utilitarians have trouble understanding moralities different from their own is because they are the product of a culture disproportionately made up of wealthy educated liberals. In fact, the default way in which humans think about morality is to make impulsive decisions and then rationalize them afterward, and that's actually a good thing for our species to survive.

Haidt goes on to develop a new model of human morality and identifies six separate metrics by which we instinctively rate actions, of which utilitarianism is only based around one. Most philsophies, including religions, appeal to all six metrics so the cohesiveness of groups built around them tend to be stronger even when members understand that they have no empirical reason to believe as they do.

The lesson I got from this is that rationalists are not an island of sanity in a world of madness, just people who found it easy to think differently from others that happened to be very useful. I believe this has resulted in a inaccurate portrait of the human mind that sees irrationality as a bug rather than a feature. We can still teach people to overcome their biases, but we can't convince people to abandon their convictions.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1475621180.0
:END:

************* Well, that's all very interesting and you make a good point that I largely agree with, but you also seem to be saying that you know your viewpoint if applied as in the original example would not lead to a better world and you're okay with that because the thought process you used to get there is more appealing to you (and also to humanity). If you're essentially saying that you are not the sort of person who is suited to rationality then that's fine in and of itself, but it's not really very good when applied as a defence of a conclusion.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1475696609.0
:END:

************** What I meant was that the definition of a "better world" you are using is different from mine because we possess different values. I am not nor will ever be okay with the idea of living under an AI God because I value human autonomy in addition to the prevention of death and suffering, and cannot change those values.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1475698164.0
:END:

*************** And I believe that those values and stated preferences when taken to a logical conclusion include stuff which you find abhorrent enough not to want to take mental ownership of. Luckily this doesn't particularly matter as it is unlikely that either of us will be in a position to enforce those values one way or the other.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1475700033.0
:END:

**************** I highly recommend reading The Righteous Mind, since it does give insight into how non-rationalists think about morality and why many of them can't think about it rationally. That has huge implications for what human values really are.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1475702576.0
:END:


***** #+begin_quote
  I just don't think intelligence works that way
#+end_quote

Even if it didn't, with sufficiently robust hardware you could still run a human level AI at hyperspeeds, in many threads, and get there (superintelligence) the hard way.
:PROPERTIES:
:Author: OrzBrain
:Score: 1
:DateUnix: 1475592064.0
:END:

****** I can agree with that, though I don't think it would be quite as easy as some think. I sided with Hanson in the AI-Foom Debate.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1475596750.0
:END:


** See [[https://www.amazon.com/Redshirts-Novel-Three-John-Scalzi/dp/0765334798/ref=sr_1_1?ie=UTF8&qid=1475414654&sr=8-1&keywords=red+shirts+john+scalzi]["Red Shirts" by John Scalzi]] in which the characters begin to notice when the Narrative is on them because people start acting weird.
:PROPERTIES:
:Author: kithpendragon
:Score: 5
:DateUnix: 1475414775.0
:END:


** There's a problem where you have an AI running in a simulation to make sure it's not evil that's pretty similar to this. The AI tries to figure out if it's in a simulation. You could try to make it a really good simulation, or you can cheat and just make whatever function it runs return true. It would be pretty difficult to pull that on a real self-modifying AI, since it could modify itself to the point that you don't understand its mind and you can't tell what the functions it runs are supposed to do, but it works fine on a fictional character. They can only realize they're in a fiction universe if you let them.
:PROPERTIES:
:Author: DCarrier
:Score: 3
:DateUnix: 1475294045.0
:END:


** At most they would develop /strong suspicions/ about it, because there is no way for them to distinguish and prove whether it's a fictional universe, a simulation (which would be a more perfect version of the former), an unknown natural phenomenon, magic, or something else that's behind all the things they are observing that makes them think about being a fictional entity / in a story.

As to how they would get such suspicions in the first place:

- inconsistency --- FeepingCreature's answer is one example of this: the character's background knowledge about the world is in conflict with their personal experiences. Examples: Harry Potter, Naruto, SCP Foundation, Bible, etc. In all of these, if a character was to sit down and carefully analyse the laws of their universe and the current state of their society / existence, they would realise that the map of the world is inaccurate or incomplete in at least /some/ manner.

- genre savviness: one of the easiest cases is when the protag starts to notice silly fanservice tropes happening all around them.

- while philosophising / developing their worldview / starting to ask existential questions to themselves: some characters wouldn't need any specific strong prior experiences to start believing that they are, in fact, characters thought by someone else. If people in our world can wonder whether they're being part of someone else's dream / thoughts ([[http://www.dreamviews.com/beyond-dreaming/94402-we-may-all-inside-dream-sleeping-god.html][1,]] [[http://tvtropes.org/pmwiki/pmwiki.php/Main/DreamApocalypse][2,]] etc) then so can fictional characters.
:PROPERTIES:
:Author: OutOfNiceUsernames
:Score: 3
:DateUnix: 1475277519.0
:END:


** Read Vernor Vinge's "The Cookie Monster" [[https://www.analogsf.com/0310/cookie.shtml]]
:PROPERTIES:
:Author: progeriababy
:Score: 2
:DateUnix: 1475294015.0
:END:

*** Full link: [[https://www.ida.liu.se/%7Etompe44/lsff-book/Vernor%20Vinge%20-%20The%20Cookie%20Monster.htm][https://www.ida.liu.se/~tompe44/lsff-book/Vernor%20Vinge%20-%20The%20Cookie%20Monster.htm]]
:PROPERTIES:
:Author: raypacman
:Score: 3
:DateUnix: 1475307898.0
:END:

**** thanks mang
:PROPERTIES:
:Author: progeriababy
:Score: 1
:DateUnix: 1475334840.0
:END:


*** uhm. It says to be continued? Didn't really mention much about what I was talking about? Thanks anyway
:PROPERTIES:
:Score: 1
:DateUnix: 1475296065.0
:END:

**** Sorry about that. I thought it was the full story. [[/u/raypacman]] linked to the full story below. Its related to what you were talking about ... probably more than anything else in this thread: [[https://www.ida.liu.se/%7Etompe44/lsff-book/Vernor%20Vinge%20-%20The%20Cookie%20Monster.htm][https://www.ida.liu.se/~tompe44/lsff-book/Vernor%20Vinge%20-%20The%20Cookie%20Monster.htm]]
:PROPERTIES:
:Author: progeriababy
:Score: 3
:DateUnix: 1475334828.0
:END:


** I actually had a similar thought, except dealing with typically "supernatural" things showing up in a world similar to our own.

Senario1: A person develops something akin to D&D sorcerer powers. They allow themselves to be tested. A few months gives some exciting new hypothesis about physics and such, a few years sees the codification of laws we'd never noticed or misunderstood. Similar powers never manifest in anyone else but the new laws of physics we derived from them hold up in all real world experimentation we devised. - I'd imagine the vast majority of people would just chalk it up to science we don't understand yet, as what we were able to work out holds up and advanced some of our areas of understanding.

Scenario 2: Confirmation of HP styled ghosts. After years of study, we've got some weak theories to the average person sound something like "Yadda yadda quantum mechanics yadda yadda." - Again, I imagine most people would just assume something we don't understand yet. To most people, "Quantum mechanics" is sufficiently science magic enough to explain it away.

Scenario 3: An immortal appears. He or she hit some age between 25-30 and stopped. All tests show that any stimulus / pressure / etc. stops just shy of pain or harm. No virus or bacteria they've been exposed to seems to cause any negative effects. Doesn't appear to suffer any psychological harm when exposed to things like isolation or confinement. Pending approval on other forms of testing. Drugs seem to allow their positive effects without any of the draw backs. No tests show any reason for it to work beyond "It does." No attempts at replication due to having nothing to try it with. Currently suggested experiments involve having the subject reproduce. One researcher remarked "If the world was a video game, I'd say the subject is immune to HP damage and negative status effects." As near as anyone can tell, this seems to be some kind of conceptual immunity to harm. - I think this one is a blatant enough violation of everything we know about the world that people would start to at least toss some serious consideration behind the world being a simulation.
:PROPERTIES:
:Author: LeonCross
:Score: 2
:DateUnix: 1475346912.0
:END:

*** #+begin_quote
  Senario1: A person develops something akin to D&D sorcerer powers. They allow themselves to be tested. A few months gives some exciting new hypothesis about physics and such, a few years sees the codification of laws we'd never noticed or misunderstood. Similar powers never manifest in anyone else but the new laws of physics we derived from them hold up in all real world experimentation we devised. - I'd imagine the vast majority of people would just chalk it up to science we don't understand yet, as what we were able to work out holds up and advanced some of our areas of understanding.
#+end_quote

Yes, but when that apparently happened back in the 70's, in [spoiler] Ra, it turned out to actually be a [even spoilier] real-life retcon by future humanity wielding sufficiently advanced technology, all of it still operating under standard physics[/spoilers]
:PROPERTIES:
:Author: khafra
:Score: 1
:DateUnix: 1476380104.0
:END:
