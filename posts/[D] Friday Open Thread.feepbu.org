#+TITLE: [D] Friday Open Thread

* [D] Friday Open Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 26
:DateUnix: 1583507125.0
:END:
Welcome to the Friday Open Thread! Is there something that you want to talk about with [[/r/rational]], but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with [[/r/rational]] instead of going over to [[/r/japanesegameshows]], but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could (possibly) be found in the comments below!

Please note that this thread has been merged with the Monday General Rationality Thread.


** When my post is 11 hours old, the folks from [[https://www.doofmedia.com/deep-in-pact/][Deep in Pact]], the We've Got Worm-style podcast analysis of [[https://pactwebserial.wordpress.com/][Pact]], will start the recording of their last episode, where they discuss the epilogue.

Right afterwards, they'll begin a 24 hour livestream called [[https://www.doofmedia.com/2020/02/04/all-pact-up-details/][All Pact Up]], which will be livestreamed on their [[https://www.twitch.tv/doofmedia][twitch]], where they'll be collecting donations for a homeless kids charity.

Their 24 hour marathon willl include fun stuff like a short game of Pactdice, live writing, fanart and fanfic reviews, Tarot readings, and a live interview with Wildbow himself. This is probably the first time Wildbow has ever done an audio interview, so I'm really looking forward to it.
:PROPERTIES:
:Score: 16
:DateUnix: 1583511468.0
:END:

*** So you're saying I have ~2 hours to read Pact if I want to hear a live Wildbow interview without risks of spoilers ðŸ¤”
:PROPERTIES:
:Author: Roxolan
:Score: 10
:DateUnix: 1583544860.0
:END:


** I got drawn into yet another dumb argument about the teleporter problem (in case you aren't familiar, this is the idea that teleporters kill you and create an identical copy at the destination) in yesterdays post on the front page post about teleporters.

I'm curious about how the people in this community see it. Here is my view:

Any coherent definition of life/identity that does not require a soul or other non-physical component will lead to one of 2 conclusions: either teleportation is not death or else death happens many times to every individual to the point that it is not something worth worrying about.

I come to this conclusion via the following:

The main two arguments about why teleportation /would/ be death is the fact that it isn't the same physical atoms/molecules and there isn't continuity of consciousness.

However, physical atoms are being replaced constantly within the human body and I am sure there is complete turnover multiple times over the course of someones life. Under this definition, everyone is slowly dying all the time and will die multiple times across their lifetime.

As for continuity of consciousness, there is decent evidence that consciousness is not continuous under any circumstances and that the appearance is a lie told by our brains. But even ignoring that (it certainly isn't proven), continuity is broken by comas, anesthesia, and, arguably, sleep. Meaning that, in the worst case scenario, continuity is broken multiple times every single day and under the best case scenario, almost everyone in modern society will experience it at least once or twice over the span of their existence.

I assume this community has a higher incidence of people who have thought deeply about this (probably much more so than I have) and would love any thoughts.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 15
:DateUnix: 1583521962.0
:END:

*** #+begin_quote
  However, physical atoms are being replaced constantly within the human body and I am sure there is complete turnover multiple times over the course of someones life. Under this definition, everyone is slowly dying all the time and will die multiple times across their lifetime.
#+end_quote

Personally, I think the rate of change is relevant. If you're okay with your personality slowly shifting over time, the same thing happening to your body is probably acceptable. But similar to how you wouldn't want your personality completely changing overnight, I wouldn't want my brain being replaced all at once, even if it was with an identical copy.

#+begin_quote
  As for continuity of consciousness, there is decent evidence that consciousness is not continuous under any circumstances and that the appearance is a lie told by our brains. But even ignoring that (it certainly isn't proven), continuity is broken by comas, anesthesia, and, arguably, sleep.
#+end_quote

I'd disagree with sleep being a break in continuity of consciousness, since your brain is still active to some degree. But yes, under my current theory of consciousness going into a coma or being put under via anesthesia would effectively kill me.

Also, the fact that many people would already be dying from the use of anesthetics isn't necessarily an argument against that being the case. If anything, it means you should be cautious to never use them on the off chance that your theory of consciousness is incorrect.
:PROPERTIES:
:Author: paradoxinclination
:Score: 9
:DateUnix: 1583534014.0
:END:

**** #+begin_quote
  But similar to how you wouldn't want your personality completely changing overnight, I wouldn't want my brain being replaced all at once, even if it was with an identical copy.
#+end_quote

I'm curious. If scientists discover tomorrow that due to quantum /mumble/ multiple universe /mumble/, our atoms are, in fact, switching places with other atoms with identical properties from across the universe approximately every 3.49 seconds.

Would this bother you? I can say personally that this wouldn't even make me blink.

#+begin_quote
  But yes, under my current theory of consciousness going into a coma or being put under via anesthesia would effectively kill me.
#+end_quote

Your brain is still active in a coma and under anesthesia. That's how doctors differentiate between "coma" and "death."
:PROPERTIES:
:Author: electrace
:Score: 8
:DateUnix: 1583549047.0
:END:


**** #+begin_quote
  If you're okay with your personality slowly shifting over time, the same thing happening to your body is probably acceptable. But similar to how you wouldn't want your personality completely changing overnight, I wouldn't want my brain being replaced all at once, even if it was with an identical copy.
#+end_quote

Your personality changing over time influences you (you're slowly becoming a different person), so it's a good idea to have preferences about how slowly you want that change to happen.

Changing the substance of your body over time (assuming the pattern is preserved) doesn't influence you in any way - it doesn't have any impact on you at all - so having preferences about it is like having preferences about other things that are causally unconnected to you.
:PROPERTIES:
:Author: DuskyDay
:Score: 0
:DateUnix: 1583624824.0
:END:


*** Both sides of the debate agree that the transporter gathers pattern information about your exact body composition by breaking it down into component parts, sends that information to another place, and then puts together new component parts to create the original pattern.

The question is then framed as "Will the new person be /me/?"

Proponents of the "No" view could point out that throughout the entirety of your existence, out of all the things in the universe, "you" have shared one thing that everything that is /not/ "you" does not share, active physical brain continuity.

In principle, we could draw a bubble around your brain from birth, and move it around with you. Sometimes the bubble may expand as new neurons go online, or contract as they die off. But, we can all agree that consciousness lies inside the bubble. One could draw the path that the bubble takes from birth to death, and it would be one smooth curved line through space-time. Teleportation would destroy the brain, leaving a moment in time where the bubble could not exist, thus breaking the smooth line. Therefore, the new person would be a new birth of a new consciousness, and thus, not "you."

Proponents of the "Yes" view could point out that throughout the entirety of your existence, out of all the things in the universe, "you" have shared one thing that everything that is /not/ "you" does not share, a unique machine (the brain) that maps inputs to outputs.

In principle, "you" are the end result of every input that you have received. When you see a red object, you experience the qualia of "seeing a red object", "knowing that red objects exists", "forming a memory of a red object", "experiencing emotions associated with red", "weakly associating the current emotional state with red" and "sending signals to the mouth to say things like 'that is red'". If you had seen blue instead, then your brain would have formed slightly differently. There is nothing in the universe that would process a red object in exactly the same way that you do. Teleportation would recreate your unique brain. Therefore, the new person is not a different consciousness, which would process information differently, and is thus, "you."

Personally, I belong to the "Meh, whatever" view. Both seem like perfectly reasonable definitions of "you" in the same way that "something that causes an auditory experience" and "vibrations" are both reasonable definitions of "sound." To me, the teleporter problem is exactly equivalent to the old question about a tree falling in the middle of the woods. The answer is "depends on your definition."

As such, the only thing that bothers me about the transportation problem is how people react to it. Being semantics, the answer to the "Is the new person you?" question should not inform your willingness to get onto the transporter platform.

Take the bird's eye view. At time x, there is a person at position y. At time x+1, there is a person with an identical everything at position z. The only question to consider from this formulation is "where is the better place for there to be a person?" Everything else cancels out.
:PROPERTIES:
:Author: electrace
:Score: 8
:DateUnix: 1583541606.0
:END:


*** #+begin_quote
  it isn't the same physical atoms/molecules
#+end_quote

An issue with the cell renewal argument is that neurons get replaced extremely slowly if at all.

A [[https://www.lesswrong.com/posts/7HMSBiEiCfLKzd2gc/quantum-mechanics-and-personal-identity][LessWrong Sequence]] brought me to the same conclusion with a different argument: it doesn't make sense to attach personal identity to individual atoms because "individual atoms" don't actually exist. There's just an amplitude distribution as described by quantum mechanics.

As for continuity of consciousness, it doesn't seem important to me?

What I want is for my next mindstate to be a consequence of my current mindstate according to a particular set of rules which define me. Whether the rules are implemented by brain chemistry, by a bunch of nanobots that scan my brain and reconstruct an identical one on Mars two hours later, or by a computer that accurately simulate the effect of brain chemistry (at arbitrary speed) doesn't seem like it makes a difference.
:PROPERTIES:
:Author: Roxolan
:Score: 8
:DateUnix: 1583543313.0
:END:

**** While I find the "atoms don't exist" argument equally compelling (although probably harder to grasp for most people, even I don't really understand it, I just take it on face value from people who understand it better than I do), the fact that individual cells don't replace quickly says nothing about how fast the atoms in those cells replace. I am quite sure that even though a cell may not die/divide, the atoms that make it up are constantly cycling. It's basically the same idea scaled down a level.

And I agree with you completely about continuity not being that important /to me/. I was merely addressing the arguments that frequently come up. It sounds like you and I mostly see it the same way. I personally wouldn't consider teleportation death /even if/ no atom was ever replaced in your body and your consciousness was completely continuous throughout your life.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 3
:DateUnix: 1583556982.0
:END:


**** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1583545560.0
:END:

***** I'm afraid it's non-trivial enough that I have to ask you to elaborate.
:PROPERTIES:
:Author: Roxolan
:Score: 6
:DateUnix: 1583556450.0
:END:

****** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1583557001.0
:END:

******* The issue is that there's no /analogue/ in QM of the notion of "the same atom". There's no element of reality that keeps an individual identity and persists unchanged one second later. The issue isn't that atoms are made of parts, it's that the parts are extremely nonatomic in character.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 4
:DateUnix: 1583606128.0
:END:

******** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1583619252.0
:END:

********* #+begin_quote
  locality of continuity for quantumly entangled threads of reality
#+end_quote

As written, this isn't so much a coherent concept as it is a sequence of vaguely technical-sounding words, none of which make sense in relation to each other. Locality and continuity are entirely distinct concepts, and the usual definitions of each are such that "locality of continuity" is a nonsensical phrase.

In other words, no, we cannot appeal to "locality of continuity for quantumly entangled threads of reality" because that is not a thing, at least with the way those words are normally used. If you have an alternative concept that you're trying to put forth as a basis for valuing, please describe it directly, without obscuring it behind a veil of seemingly incoherent technical language. The latter, I've found, is occasionally even used to hide the fact that there /is/ no coherent concept under discussion, which obviously wouldn't be great for your argument.
:PROPERTIES:
:Author: Ergospheroid
:Score: 2
:DateUnix: 1583790433.0
:END:

********** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1583794673.0
:END:

*********** There isn't a locality violation - the pattern is read from the body of the person and transmitted slower than light to the destination. The entire time, the pattern moves below the speed of light (or at the speed of light, if you use radio waves) to the destination, where it's reinstantiated in new matter.

Your last conscious state before the teleportation is at time t1, your first conscious state after the teleportation is at time t2, and t2 - t1 >= d/c, where d is the distance.

I think you're imagining that the pattern doesn't exist in the intervening space - but it exists in the radio waves (otherwise, you couldn't use them to reconstruct it in the new matter on the other side).
:PROPERTIES:
:Author: DuskyDay
:Score: 1
:DateUnix: 1583878326.0
:END:


******** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1583618753.0
:END:

********* #+begin_quote
  Something "like" the same atom does exist - namely, that which hasn't teleported.
#+end_quote

Not OP, but that would render your argument empty - if you're arguing that teleportation kills you because you're not made of the same atoms, and you're saying that your concept of the same atom is "that which hasn't teleported", then your argument says: Teleportation kills you because you're no longer made of atoms which haven't teleported.

The reason why you don't need the same matter (whether or not "same matter" exists) is as follows: Let's assume that when preserving the pattern and changing the matter in your brain, you notice some difference. Then you can talk about it (because we can talk about our conscious perceptions). But that contradicts the assumption the pattern stays the same (because different words could only be generated by a different pattern). This explanation (in so many words) is [[http://consc.net/papers/qualia.html][fully covered by Chalmers]]. (The way it follows from his paper is that he shows you survive a gradual brain replacement, from which follows that you don't need the same matter.)
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1583624300.0
:END:

********** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1583705133.0
:END:

*********** #+begin_quote
  That assumes we only care about perceptible differences
#+end_quote

Perceptible differences in this context mean perceptible differences in your perspective - i.e. your consciousness continuing (rather than it being annihilated and a clone believing s/he was you).
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1583706573.0
:END:

************ [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1583709030.0
:END:

************* #+begin_quote
  I can imagine someone who agrees that the version of them that is teleported will not feel different from the inside than they do but will nonetheless be a different person.
#+end_quote

That's not possible, because to preserve your consciousness, you don't need the same matter.
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1583709976.0
:END:

************** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1583715242.0
:END:

*************** 3 months ago I had a long argument with DuskyDay about this topic. He seems to have not absorbed anything from it, especially the idea that consciousness is not a "thing" that's "in" the brain, but is instead the action that the brain performs. He still on a gut level just isn't understanding why someone would refuse to accept "another brain is performing the action of consciousness in the same way that your recently deceased brain used to perform it" as being equivalent to "your consciousness has been preserved."

Here's a link if you're interested in a different viewpoint.

[[https://www.reddit.com/r/rational/comments/e71a6s/the_whispering_earring_by_scott_alexander_there/fahr1q6/?context=10000]]
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1583718616.0
:END:

**************** [deleted]
:PROPERTIES:
:Score: 3
:DateUnix: 1583725775.0
:END:

***************** I mean, I disagree with your assessment?
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1583726067.0
:END:


*************** [[/u/hyphenomicon][u/hyphenomicon]]:

Your argument for why teleportation kills you was the proposition /P/,

*P*: You need the same matter to preserve your consciousness.

I made two arguments against /P/:

1. â€‹

#+begin_quote
  The reason why you don't need the same matter (whether or not "same matter" exists) is as follows: Let's assume that when preserving the pattern and changing the matter in your brain, you notice some difference. Then you can talk about it (because we can talk about our conscious perceptions). But that contradicts the assumption the pattern stays the same (because different words could only be generated by a different pattern).
#+end_quote

2.

#+begin_quote
  This explanation (in so many words) is [[http://consc.net/papers/qualia.html][fully covered by Chalmers]]. (The way it follows from his paper is that he shows you survive a gradual brain replacement, from which follows that you don't need the same matter.)
#+end_quote

If you want to keep claiming that /P/ is true, it's up to you to present a counterargument to both my arguments, and you haven't done that.

â€‹

[[/u/ElizabethRobinThales][u/ElizabethRobinThales]]:

You made two statements: First, that the brain doesn't contain symbols. I reacted to that here:

#+begin_quote
  There is a way of discovering a symbolic representation of whatever is being processed in every object - otherwise, the physical structure of the object couldn't transform the input into an appropriate output. So given the physical state of the object, it's guaranteed that some aspects of the object will be a symbolic representation of whatever is being processed.

  The only difference is that in a silicon computer, it's clear what constitutes the symbols (because humans engineered computers to be easily comprehensible), but in the brain it's harder (but you can already experimentally verify it directly - e.g. by translating a brain scan into a picture the person sees).
#+end_quote

You didn't respond to that.

You also said that a gradual brain replacement would destroy your consciousness. I asked:

#+begin_quote
  So, at the end of the gradual brain replacement, you will have blacked out forever, and in your place, there is going to be a fully conscious clone - acting like you, but still another person.

  The question is, in what manner would your perception cease to exist, as felt from the inside? At the beginning, you're perceiving everything normally since the gradual brain replacement hasn't begun yet. At the end, you're blacked out forever (like after a car accident). But what do you feel in between? Does your awareness of the outside world fade out gradually, or does it stop abruptly at some point?
#+end_quote

You didn't respond to that either.
:PROPERTIES:
:Author: DuskyDay
:Score: 1
:DateUnix: 1583733614.0
:END:

**************** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1583737280.0
:END:

***************** #+begin_quote
  Wrong. I can simply walk away.
#+end_quote

I'm sorry, but at this point, I have to downvote (beyond my policy of downvoting wrong comments if they have more upvotes than a neighboring correct comment), since keeping your belief despite your argument being factually incorrect is something I want to discourage.
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1583789075.0
:END:

****************** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1583795082.0
:END:

******************* #+begin_quote
  Additionally, I do not think my argument is incorrect
#+end_quote

Your argument is unsound (it has the form of ((P => S) && P) => S, where P is false). I'm afraid reiterating that you think it's not incorrect doesn't add to it.

#+begin_quote
  I'm hitting the "block user" button also
#+end_quote

I understand, and will continue correcting you wherever I judge it appropriate.
:PROPERTIES:
:Author: DuskyDay
:Score: 1
:DateUnix: 1583876775.0
:END:


******* Right, but if a pattern I care about in the amplitude distribution disappears in one place and shows up elsewhere, that's not an issue for consciousness. It's happening all the time at a subatomic level anyway.
:PROPERTIES:
:Author: Roxolan
:Score: 3
:DateUnix: 1583557411.0
:END:


*** Honestly your conclusions seem self-evident and I doubt I'm going to say anything on the ship of Theseus problem that hasn't already been said. So I think teleporters can raise far more interesting questions:

What if teleporters work like scanners and 3d printers? In that case, wouldn't it be interesting if there were innumerable copies of you living far away having experiences you'll never have because travelling at the speed of light is too hard for not light? What if you have endless backups to fixed states in your life? Can you ever really learn? Do people die over thousands and millions of years because eventually they're not willing to part with memories? Or they die because their definition of themselves becomes too fixed to change?
:PROPERTIES:
:Author: somerando11
:Score: 4
:DateUnix: 1583545487.0
:END:


*** So according to you, a clone with your same memories and brain state is not a clone, but is actually /you/?

This question is examined in some of Peter F. Hamilton's writing. We have the technology to backup and restore memories, so when someone dies in an accident, they get "re-lifed" in a new clone body with a clone brain injected with their old memories. In the books, the characters don't have a consensus about whether a re-lifed clone is actually the same person or not, but the society as a whole tends to accept it as true continuity. They call dying "body death".

I felt that re-lifing someone is more for their friends and family than for themselves. I mean, if I die in an accident, I don't care whether my memories and personality live on in another body, because I, the only me that matters, am dead and I will not experience anything my clone does.
:PROPERTIES:
:Author: Amargosamountain
:Score: 7
:DateUnix: 1583522272.0
:END:

**** If you define "you" as "you at time t", then the clone is as similar to "you" as "you at time t+1", yes.

According to my definition, "you", every second of every day, are a slightly different person than you were the second before. A clone is as close to the person who "made" the clone as that person is at a later time. (assuming crazy magic sci fi cloning with exact duplication including memories etc.)

When you say "I, the only me that matters" as something separate from your memories, to what are you referring? What is the "thing" that "you" are that is not your memories? I argue that "you" is nothing /except/ your memories (well...and the hardware on which to execute the personality built on those memories...but you get the point).
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 5
:DateUnix: 1583523102.0
:END:

***** For me it's the fact that you could theoretically make a clone of me with my exact memories, and we could coexist together and we would be distinct people. If I'm standing next to my clone and you kill me, I'm dead (and for the sake of argument let's pretend my clone and my memories haven't started diverging yet). I don't get to continue my own experience through my pre-existing clone, so why would it be different with a clone created later?

I can't conceive of /my/ consciousness ever existing in any hardware besides /my/ brain, Ship of Theseus concerns notwithstanding. That's a whole different argument :)
:PROPERTIES:
:Author: Amargosamountain
:Score: 8
:DateUnix: 1583527211.0
:END:

****** But the "you" that died is just as different from the "you pre-clone" as the clone is. So yes, that individual died. But that individual is not any more related to the "pre-clone you" than the clone itself is. From the perspective of "you pre-clone", either one of you dying is equally bad, because those two individuals are both equally related to "you pre clone", and they are not different things. [[/u/gaberockking]] point about non-binary death is a good way of thinking about it.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 3
:DateUnix: 1583527477.0
:END:

******* #+begin_quote
  But the "you" that died is just as different from the "you pre-clone" as the clone is. So yes, that individual died. But that individual is not any more related to the "pre-clone you" than the clone itself is
#+end_quote

That's not really true, since the clone of you shares none of the molecules that made up your body whereas the 'pre-clone you,' of one second ago shares 100% of your molecules. You can argue over whether that matters or not, but it's a little disingenuous to state that there's literally no difference when there demonstrably /is./
:PROPERTIES:
:Author: paradoxinclination
:Score: 5
:DateUnix: 1583534282.0
:END:


******* I think we're just disagreeing on what "death" means. I don't feel like the me of the past is dead, he just changed. Where you see distinct points (I assume) I see a series of points on a distinct line.
:PROPERTIES:
:Author: Amargosamountain
:Score: 2
:DateUnix: 1583529062.0
:END:

******** I mean...yes, that is the point of the discussion...what is death.

That's a great analogy. They are points on a line. But it isn't a straight line (once you include cloning at least). It's an every branching tree (every time a clone is created, the line branches). Every single version of you feels like they have a direct connection to the original, and from the originals perspective, there is no differnce between the different branches.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 7
:DateUnix: 1583529331.0
:END:


****** #+begin_quote
  I can't conceive of /my/ consciousness ever existing in any hardware besides /my/ brain
#+end_quote

A way to bootstrap your intuition to see that it can would be to imagine a [[http://consc.net/papers/qualia.html][gradual brain replacement]] (your brain being slowly replaced by functionally equivalent small pieces of something else (like silicon transistors)). That makes it easier to see that it's the continuity of the pattern that's responsible for the continuity of the consciousness, and the persistence of a particular hardware/matter is only necessary to the extent to which it's necessary to maintain the continuity of the pattern.
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1583791024.0
:END:


**** Death isn't binary. If you get hit on the head and lose an hour's worth of memories, have you died? Yes, but only by a few fractions of a percent. I care about myself in the immediate future more than myself in 20 years because 20 years from now I'll be fairly dead compared to current me, but given that I care for other humans, and me 20 years from now is the least dead variant of me to exist at that time, I care about them more than any other human. (For reference, I view other humans as being mostly dead versions of myself, who share some of the same brain structures, personality, and effectively equivalent memories, but lost a lot of my memories and replaced then with new ones, and they got massive brain damage they healed from.)

So in a destructive teleport, the person at the other end is still me from before the teleport. The "me" that goes through the teleporting process will have maybe a few nanoseconds of subjective experience that will be killed, but given how prone I am to forget things anyways, that's a small price to pay.

Though really, I'd much prefer to just have two instances of me. They would later diverge from each other, but from the perspective of pre-clone me, that just means there's a little less than 100% more me.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 9
:DateUnix: 1583524877.0
:END:

***** #+begin_quote
  If you get hit on the head and lose an hour's worth of memories, have you died? Yes, but only by a few percent.
#+end_quote

I expect we'll need a new category of "partial murder" at some point.

You killed some entity, and it was restored from backup, but missing some amount of time. Maybe a few minutes of realtime would be a minor infraction.

But losing a couple years of realtime (during which you met the most awesome life-partner and had many adventures together) should face punishment closer to irrevocable murder (the normal kind in the year 2020).

Motive, as usual, should also play a part. If the death was an accident that's one thing, if you did it to cover up some other crime, that's way more serious.
:PROPERTIES:
:Author: ansible
:Score: 6
:DateUnix: 1583532323.0
:END:

****** Future mystery novel where memetically induced Alzheimer's is the murder weapon.
:PROPERTIES:
:Author: hyphenomicon
:Score: 3
:DateUnix: 1583545773.0
:END:


*** #+begin_quote
  As for continuity of consciousness, there is decent evidence that consciousness is not continuous under any circumstances and that the appearance is a lie told by our brains. But even ignoring that (it certainly isn't proven), continuity is broken by comas, anesthesia, and, arguably, sleep. Meaning that, in the worst case scenario, continuity is broken multiple times every single day and under the best case scenario, almost everyone in modern society will experience it at least once or twice over the span of their existence.
#+end_quote

This is where I get stuck. I don't mind if I experience a lapse in consciousness, like you said it happens every time someone goes to sleep. When I think of continuity of consciousness, the part I'm worried about is continuing to experience things in general. If I step in the teleporter and then I'm disassembled, that's it for my subjective experience isn't it? Doesn't matter if another instance of me that wouldn't know the difference is spun up at the other end, I'm using this current brain of mine to experience things and when it goes, I stop experiencing things. If I teleport to Seattle, I don't expect the version of me that's sitting here at my laptop right now to ever experience Seattle. I'll get in the teleporter, be disassembled and never experience anything again, and then the Seattle version of me gets to continue on with an unbroken subjective experience.

People way smarter than me are comfortable calling this "not murder", and I do truly wish I understood why. I feel like I must be missing something
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 3
:DateUnix: 1583708700.0
:END:

**** The way I see it, the specifics that create your "consciousness" so to speak are your memories of previous events. As long as you have those memories you are you, to the point where creating a clone implanted with your memories would create a second you for an instant, and then it would begin having diverging experiences and become its own consciousness. Looking at it this way, I see the physical destruction of your old body as equally important to your conciousness as say, forgetting why you walked into a room. Since your conciousness is constantly being interrupted by things like zoning out, daydreaming, sleeping, or even going into a coma, all of these things have the same effect as killing a body and recreating your memories elsewhere. With this idea, the subjective you that doesn't get to visit Seattle is probably not the you that woke up this morning, since at some point you got distracted, took a nap, etc.

Obviously not scientific, but my take on how I see it.
:PROPERTIES:
:Author: lo4952
:Score: 2
:DateUnix: 1583727380.0
:END:

***** Well then let me boil it down to a single question: If you step in the teleporter, do you expect to continue your subjective experience at the destination or do you expect to stop experiencing anything at all, while another equally valid you continues on at the destination?
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 4
:DateUnix: 1583727716.0
:END:

****** I feel like I understand what you're asking, and correct me if I'm wrong, but no, I don't believe there would be a "shift" or "jump" or any kind of transition to allow you to continue your subjective experience on the other end. However without trying to be pedantic, I don't think its "another you" on the other end. Its just you, since there is only ever one you, and at this exact moment in time you are in Seattle. The "you" from before is gone, but not gone as in dead, gone as in how the you of yesterday is gone.

Additionally, I think this has a lot to do with what you consider your "continued subjective experience." In my opinion there is no way to determine whether or not it is a continued experience other than your own memories, so you could say that yes, since I remeber entering the teleporter and leaving in a new spot, that was all part of this single subjective experience I've undergone since my first memories.
:PROPERTIES:
:Author: lo4952
:Score: 2
:DateUnix: 1583728678.0
:END:

******* Okay, so I'm having a really hard time understanding how anyone would ever use the teleporter. I'd step in there, stop experiencing anything and it doesn't comfort me that I'm still out there somewhere. Especially since I would do that for something as trivial as transport

Thank you for responding by the way, it's been bothering me lately
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 5
:DateUnix: 1583728894.0
:END:

******** I think the difference in the way I see it stems from a continuous versus discrete perspective of self, or of what "you" is. In your example there are two continuous "selves" that while being the same person, are distinctly seperate, cut off by the act of teleporting.

In my opinion there is only ever one "self" and that is the "you" at this exact moment in time. Any "you" before exists only in your memory, so what does it matter if there are gaps where you sleep, teleport, etc. From this perspective there is no you to die on the front end of the teleporter, since at that exact, frozen, moment in time "you" exist in Seattle, with a fresh memory of yourself just recently being wherever you were before.

Thank you for the question, it's helped me clarify a few things to myself as well.

Edit: Something Ive considered in my goal to put it in perspective is reframing the question to look at what would happen if instead when you entered the teleporter you went to sleep and someone carried you to the new location. When you wake up you are in the new location, and while you are in the "same body," with the whole "Ship of Theseus" thing and the probability field nature of atoms themselves I think the physical aspect is much less of a question than the conscious aspect.
:PROPERTIES:
:Author: lo4952
:Score: 2
:DateUnix: 1583729961.0
:END:


*** Here's a thought experiment.

A perfect copy of you is made. An omnipotent being declares that within five minutes, one of you must die. If you volunteer to die, your copy is given a hefty sum of money. If you do not volunteer, your copy is killed and you receive a small punishment/fine.

Do you volunteer?

I expect that you'd volunteer -- based on your post it's a rational conclusion.

Personally I wouldn't volunteer. I can see that from the universe's perspective, or from another person's, it's smartest to take the money since the remainder of the outcome is indistinguishable. But from /my/ perspective, as a limited (irrational?) human observer, the-me-of-now's reality would cease to exist. Yes, someone functionally indistinguishable from me continues living, but from the perspective of old-me I'm still going to die, which (to me) is the ultimate negative utility, the worst outcome.

My overall point is that it's a matter of whose perspective you take. If you identify as you-right-now it's death. If you take an outside perspective it's not death.
:PROPERTIES:
:Author: uwu-bob
:Score: 1
:DateUnix: 1584048742.0
:END:

**** There is a difference between the two situations - namely, that in the teleporter, there is causal continuity between the computational states (your last computational state at A is the cause of the first computational state at B), while in the example with you being killed after 5 minutes there isn't (your last computational state before being killed is causally unconnected to the first computational state of your copy after the divergence).

It's possible to hold the view that causal continuity of computational states is sufficient and necessary for the continuity of consciousness, in which case the teleporter doesn't kill you but your example does.
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1584113633.0
:END:

***** You're right that despite a large overlap, the thought experiment isn't completely equivalent to the teleporter situation.

I don't personally see causal continuity as sufficient in either case, but it does make sense from a certain point of view.
:PROPERTIES:
:Author: uwu-bob
:Score: 1
:DateUnix: 1584124956.0
:END:

****** Do you believe that to preserve the continuity of consciousness, the matter would have to be exchanged gradually (while keeping the computation unperturbed), or that it needs to be kept entirely the same?
:PROPERTIES:
:Author: DuskyDay
:Score: 1
:DateUnix: 1584368336.0
:END:


** I'm looking for a Naruto fanfiction that was recommended here a while ago.

The main character is a gender-fluid SI who gets tutored by Kakashi and in turn tutors Naruto and Sasuke. They think the MC is a bit of a paranoid nutcase due to the horrifying training.

Near the end the MC is planning to prevent all death through some sort of munchkinry and runs the plan past Kakashi first. "See Kakashi, the problem is that everyone keeps dying." "Well," Kakashi thought, "that's certainly true."

Anybody remember it? Thanks very much
:PROPERTIES:
:Author: kraryal
:Score: 5
:DateUnix: 1583533772.0
:END:

*** This I think: [[https://forums.spacebattles.com/threads/kaleidoscope-naruto-si-complete.497083/]]
:PROPERTIES:
:Author: skiueli
:Score: 7
:DateUnix: 1583553132.0
:END:

**** This is it! Thank you very much.
:PROPERTIES:
:Author: kraryal
:Score: 1
:DateUnix: 1583616533.0
:END:


** [[https://docs.google.com/document/d/1BkgpQO8b_aaaG75m3cFQ-zcLd-UY6gFe5BtTR5qC3Gs/edit?usp=drivesdk]]

I have here a list of powers granted by the GLOW. That is the light lanterns of the green lantern comic books use. One thing I noticed odd was that the green light offers no extra abilities while every other light grants something useful.

I was wondering if you might be able to think up a power for the green light? I have been considering just taking emotional manipulation immunity from blue and giving it to green.

This is for an alt power Taylor that has to take on scion. She gets a ring that can access all the colors.

Thanks for any help.
:PROPERTIES:
:Author: Air_Ship_Time
:Score: 3
:DateUnix: 1583518739.0
:END:

*** The green light's special gimmick is that it is the easiest to control and has the least mental deviation for using it.
:PROPERTIES:
:Author: Aabcehmu112358
:Score: 9
:DateUnix: 1583521475.0
:END:

**** The thing about having less mental deviation is the reason I was considering giving it the blue lights immunity to other lights.
:PROPERTIES:
:Author: Air_Ship_Time
:Score: 1
:DateUnix: 1583524081.0
:END:


*** If green is immune to manipulation it defeats green's one weakness, that willpower must be maintained.
:PROPERTIES:
:Author: ketura
:Score: 7
:DateUnix: 1583528293.0
:END:

**** I don't see how one follows from the other? I thought a weakness of all colors was that you must maintain the emotion of that color to use the light.
:PROPERTIES:
:Author: Air_Ship_Time
:Score: 1
:DateUnix: 1583529504.0
:END:

***** All of them except willpower (and arguably hope) are either self-sustaining feedback loops or an amped-up response to an external stimulus. Willpower is purely internally-fueled, so removing any external manipulation puts it pretty hard into unstoppable territory.
:PROPERTIES:
:Author: ketura
:Score: 3
:DateUnix: 1583563491.0
:END:


*** Construct strength for other colors is dependent on how much the user feels that that particular construct reflects the Lantern's emotion, but green has a constant strength that is only dependent on the user's current level of willpower regardless of what they are doing. Say that you have a blue ring. Healing others is pretty easy, shielding others means your shields are super strong, etc. But if you try to make a sword to kill the person attacking you or the people around you, it's not even as strong as steel, because in a lot of cases, killing someone doesn't inspire hope. Similarly, an orange Lantern can make really, extremely strong constructs to do things that they want to do, but if said Lantern is ordered to do something they're only somewhat interested in, their constructs are weak to the point of almost failing to do the job at all.

But for green, willpower is fairly constant. Any construct you make is going to be as strong as any other given your current willpower, regardless of what those constructs are doing. Being ordered to do something you don't want to do or isn't something you're good at means your constructs are actually stronger, because you have to exert willpower to keep going even when you dislike what you're doing. Green Lanterns don't need any training to maintain their emotion even when they're doing something unaligned with it, because nothing is unaligned with willpower or focus. Similarly, you can maintain focus a lot more easily than you can being angry at something for an extended period, when said thing isn't extremely important to you, or when it isn't immediately present. Green rings excel at intermediate tasks that don't immediately cause the relevant emotion.
:PROPERTIES:
:Author: sicutumbo
:Score: 5
:DateUnix: 1583547307.0
:END:

**** Thank you, that helps me with writing a bit. Green is going to be the second light focused on after Indigo /compassion.
:PROPERTIES:
:Author: Air_Ship_Time
:Score: 3
:DateUnix: 1583549435.0
:END:


** In Buffy, it's stated that orbs of thessala are used as paperweights, implying they're not particularly rare. These items are consumed in the process of re-ensouling a vampire, albeit with the drawback that "a moment of pure happiness" reverses the process. "Moment of pure happiness" appears to only refer to having sex with someone you really like. So you don't even have to be celibate, just only have sex with your second choice or lower.

So why don't they use Angel to vamp Xander/Giles/Willow* and then quick soul them up before they do anything evil? In the short term, super-strong ally, in the long term, immortality.

*If being a vampire causes issues with magic then this should probably be put off for a while. The semi-canonical "Fray" series suggests Willow is immortal anyway.
:PROPERTIES:
:Author: chlorinecrownt
:Score: 3
:DateUnix: 1583757378.0
:END:

*** If your soul goes to a hell (or heaven) dimension it appears you may have a high rate of time dilation. You might not want to come back if its a heaven dimension, and you may suffer immensely otherwise (it also might be looked upon poorly if you die in the future). In general having a demon as a passenger (trying to grab the wheel) seems a bit risky. Also avoiding sunlight and true happiness sounds pretty annoying -- Angel's moment of true happiness is just one datapoint after all. On the other hand you can probably make a ritual that doesn't have that curse, considering spike found a way without that caveat, and the gypsies /were/ actually trying to make it a curse.
:PROPERTIES:
:Author: nohat
:Score: 3
:DateUnix: 1583879162.0
:END:


** Good Day everyone,I just watched CastleVania!netflix, and I am curious at how you could build your army of darkness to rule the world.The last season somewhat tickled my imagination about the forge masters, why would they not build confort creature ?
:PROPERTIES:
:Author: Ereawin
:Score: 1
:DateUnix: 1583694721.0
:END:
