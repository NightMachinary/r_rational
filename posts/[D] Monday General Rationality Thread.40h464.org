#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 5
:DateUnix: 1452524703.0
:DateShort: 2016-Jan-11
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** So for the last few years I've been tweaking my personal finance and budgeting system. And now it turns out I've basically re-invented double book accounting.

Obviously, I didn't do as good a job as the accountants who've been using and refining the method for the last thousand plus years.

Now I'm looking for some good software to transition all my records over to. Playing with GnuCash, but I'd ideally like something with better Android support.
:PROPERTIES:
:Author: trifith
:Score: 14
:DateUnix: 1452527427.0
:DateShort: 2016-Jan-11
:END:


** I realized that I have no good reason to be upset because of other's words, and have mostly stopped being such. How I got from point A to point B is largely a mystery, and probably isn't replicable.
:PROPERTIES:
:Author: Atilme
:Score: 4
:DateUnix: 1452575633.0
:DateShort: 2016-Jan-12
:END:


** Something I sent to a friend:

EV rising above break-even on powerball doesn't mean you should buy powerball tickets (and note, EV STILL isn't above break-even; even at 900 million. Why?

Well, odds of winning are one in 292 million.

A 1/292 million chance of winning 900 million dollars well, then our EV for a ticket of 900 / 292 = about 3 dollars. Given that a ticket costs about 2 bucks, this seems like it might be a good deal, right? But wait! If you take the payout now in lump sum rather than over time, it's only 500 million! so your EV (500 / 292) = about $1.72. Not too great for a $2 ticket.

It gets worse, though! don't forget Uncle Sam! He'll come and take about 40%, leaving you with an EV of about $1.02 for your $2 ticket.

That being said, EV isn't a great way to approach this.

Why? Well, let's say the jackpot was even HUGER. Let's say the jackpot was so huge that it was actually an EV of $3 for a $2 ticket. Should you get one?

No.

In general, any powerball jackpot is the same size. They're all the size of "If carefully managed, you never have to work again and can live in comfort for the rest of your life. Your children won't have to work, either." Like, even the "smaller" powerball jackpots are a hundred million, easily. Even taking it as a lump sum and Uncle Sam taking a fat wet bite out, you're left with 20+ million dollars. Carefully shepherded to, 20 million dollars can easily put out several hundred thousand dollars a year without eating into principal and while fighting inflation.

So given that even a small jackpot would have you and all your descendants living an upper class lifestyle forever, you really shouldn't care that the jackpot is larger now. If you weren't buying tickets before, the fact that the jackpot will end up paying out more shouldn't change anything for you. Either way, it was going to change your life in the same way.
:PROPERTIES:
:Author: blazinghand
:Score: 3
:DateUnix: 1452635044.0
:DateShort: 2016-Jan-13
:END:

*** Unless you are organizing a fund with a bunch of people and pooling your money so you will simply get a proportional outcome to the money invested.
:PROPERTIES:
:Author: IomKg
:Score: 2
:DateUnix: 1452681788.0
:DateShort: 2016-Jan-13
:END:


** What are humanity's long-term goals? What do you think is important for humanity to achieve in the next 20, 100, 1000, 10000 years?
:PROPERTIES:
:Author: scooterboo2
:Score: 2
:DateUnix: 1452528377.0
:DateShort: 2016-Jan-11
:END:

*** Now planning by backwards chaining...

FUN!

What would be the most FUN?

Why aren't we having FUN yet? Why's everyone so damn miserable much of the time? Enumerate reasons, line them up by feasibility of elimination, and solve them.

Top reasons we're not having FUN:

- Bad belief systems that teach us not to have FUN, or in fact to treat our own lives and sentiments as worthless from the get-go. These systems are often disguised under words like "normativity", "rationality", "freedom", "security", "God", and "identity".
- Artificial scarcity
- Artificial oppression, often related to above malignant belief-systems
- Natural decline of human condition with age and entropy.
:PROPERTIES:
:Score: 12
:DateUnix: 1452547689.0
:DateShort: 2016-Jan-12
:END:

**** Top reasons we're not having FUN:

- Moloch
- Too busy lifting Moloch to Heaven
:PROPERTIES:
:Author: Transfuturist
:Score: 9
:DateUnix: 1452573031.0
:DateShort: 2016-Jan-12
:END:


**** How does LW rationality teach people that their lives and sentiments are worthless?
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1452554708.0
:DateShort: 2016-Jan-12
:END:

***** I don't think [[/u/eaturbrainz]] is talking about LW rationality (which, yes, explicitly says that feelings and fun are ok).

Outside LW, "rationality" is often portrayed as the opposite of emotions, which are bad (or, if the author likes emotions, [[http://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan][rationality is bad]] instead).
:PROPERTIES:
:Author: Roxolan
:Score: 6
:DateUnix: 1452560297.0
:DateShort: 2016-Jan-12
:END:

****** He's talking about 'bad belief systems,' not the straw Vulcan.
:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1452572961.0
:DateShort: 2016-Jan-12
:END:


***** Well that opened a major can of worms.

Most usages of the word "rationality" don't refer to LW. In this case, it partially does, and partially doesn't. This isn't trolling with my Tzeentch hat on, this is just being weirded out by certain things.

[[http://nostalgebraist.tumblr.com/post/83006103140/what-is-bayesianism-we-i-just-dont-know][Some of them]] [[http://bactra.org/weblog/569.html][/are/ LW-associated.]] [[http://delong.typepad.com/sdj/2011/08/economic-downturns-the-social-darwinist-waltz-and-the-navigation-of-the-starship-asgard.html][Others are less associated, at a remove.]]

The result is just that I've become extremely suspicious of the tendency to apply "rational" or "rationality" to mean, "Use algorithm X" or "Solve well-specified problem Y", with a vast body of assumptions just /lurking behind things/ about why I should use algorithm X, or about /whether/ well-specified problem Y even /can/ be solved tractably, and how desirable it is to solve problem Y using algorithm/technique X instead of solving a similar problem, call it Z', which takes actual explicit account of the flaws in the preconceptions about Y and thus can be solved with a much more tractable, robust algorithm W, which the Xians will promptly yell at you for using because it isn't X and doesn't solve problem Y all that well.

Actually, the links about statistics are way obscure. If you really want to get what I mean, just look at the economics example, and then think of all the other times fucking economists have basically said, "Homo Economicus does X, actual human beings do different-thing Y, and we can therefore conclude that human beings are /irrational/, not because Y has /no reasons behind it/, no cognitive processes that could make sense or optimize some goal, but instead because Homo Economicus is the /normative/ theory of a /rational/ agent." (See: Robin Hanson, Tyler Cowen, Bryan Caplan, and in fact much of the rest of economics.)

Where this becomes problematic for things like the "rationality community" is that the entire edifice of the dual-process, heuristics-and-biases, and evolved-modularity approach to cognition is the work in behavioral /economics/ by Tversky and Kahneman, which founds itself on... yep, taking Homo Economicus (eg: the expected-VNM-utility maximizer with Bayesian updating of unlimited numerical accuracy and no causal reasoning) as the /normative model/ of a /rational agent/.

I mean, honestly, what the hell is the point of calling stereotypes of corporations the "normative theory" of how /human beings/ should act? Even the corporations themselves only act that way because someone /told them/ the damned theory was "normative".

In which case, sure, all the normal things like base-rate neglect seem like Bad Ideas /to me/, but what algorithm is generating them? Are they really /worse/ than /completely ignoring causal structure/ because you think a good predictive distribution is /all that matters/?

In summary, you'd think that the definition of concepts like "ought" would be an obscure matter for overly metaphysical philosophers, but actually, confusion over what "should" (ahaha) count as normativity seems to play a role in most /willfully held/ delusions, as people start asserting that by gosh, /it's a normative theory/, and that means it doesn't have to correlate with /anything/ else or match up to /anything/ or bear any resemblance to, for instance, the thing you would choose in its place given full information and full cognitive accuracy.
:PROPERTIES:
:Score: 3
:DateUnix: 1452577546.0
:DateShort: 2016-Jan-12
:END:

****** u/Transfuturist:
#+begin_quote
  [nostalgebraist]
#+end_quote

"we live in a universe in which theory X holds" does not strike me as meaningfully different from "theory X holds", and can be enumerated in pretty much the same way with a prior over the mutually exclusive theories to which theory X belongs.

Not that I have ever actually done or used such an enumeration. I'm not quite sure if we should be using systems where theories/hypotheses are the unit of currency, or systems where evidences/data are. I'm not sure if that distinction means anything. But I still think diachronous Bayes is correct.

I have never seen people pull arbitrary small probabilities out of their ass in the manner described to get '0.01'. As SSC puts it, even statistics with guessed numbers is better than guessed results, because the results can surprise. Additionally, this is why a log-odds formulation of probability is recommended, because it puts the probabilities in less alien terms. I've never actually seen a Bayesian, though.

#+begin_quote
  The result is just that I've become extremely suspicious of the tendency to apply "rational" or "rationality" to mean, "Use algorithm X" or "Solve well-specified problem Y", with a vast body of assumptions just lurking behind things about why I should use algorithm X, or about whether well-specified problem Y even can be solved tractably, and how desirable it is to solve problem Y using algorithm/technique X instead of solving a similar problem, call it Z', which takes actual explicit account of the flaws in the preconceptions about Y and thus can be solved with a much more tractable, robust algorithm W, which the Xians will promptly yell at you for using because it isn't X and doesn't solve problem Y all that well.
#+end_quote

Is this paragraph motivated by AIXI-worship vs. bounded intelligence?

#+begin_quote
  all the other times fucking economists have basically said
#+end_quote

Well, I mean, those economists are wrong. /We know/ they're wrong. The last section covered in my microeconomics class was all about how Homo economicus differs from humans. It was not presented as a "normative theory" at all, and I've never seen Homo economicus be presented as the way humans "should" be, save perhaps some very deluded ancaps.

#+begin_quote
  Are they really worse than completely ignoring causal structure because you think a good predictive distribution is all that matters?
#+end_quote

Yeah, I'm guessing AIXI.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1452581724.0
:DateShort: 2016-Jan-12
:END:

******* u/deleted:
#+begin_quote
  I have never seen people pull arbitrary small probabilities out of their ass in the manner described to get '0.01'.
#+end_quote

Funny thing: I've been /asked/ for such a thing. "What's the subjective probability you'll come work here after you graduate?" was the question.

And of course I didn't give an answer, because even back then I knew my brain didn't have a neat mechanism built-in for giving a probability mass to some arbitrary question like that.
:PROPERTIES:
:Score: 3
:DateUnix: 1452601868.0
:DateShort: 2016-Jan-12
:END:

******** I defer to your greater experience with Bayesians, then.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1452614540.0
:DateShort: 2016-Jan-12
:END:

********* Well I dunno if that'd "Bayesians" or just this one guy who was LW-associated. I really need to find the curriculum for a stats degree before I can go around saying I've got /experience/, anyway.
:PROPERTIES:
:Score: 1
:DateUnix: 1452615800.0
:DateShort: 2016-Jan-12
:END:

********** nostalgebraist does have a point with using diachronous Bayes without having a well-defined distribution... With the statistics of recent studies I've read, mostly about transgender brain anatomy, compared to (p < 0.01) and (p < 0.05), what's the recommended Bayes-ist alternative? The likelihood ratios of the update, or some such? Isn't p a likelihood bound (of the inverse)?

What on earth prior distribution would you even be using for that? Would you have a direct distribution of trans etiology theories? Or are you just pulling a number out of your ass for the odds of "gender indicator":"gonad indicator":"no indicator"? Stuff like this, you could just assign yourself a 10^{-200}:10^{-10}:remainder prior and be bigoted until the cows come home? I just don't know. It seems like an explication of consensus in probability distributions would be best, assuming you can even compute the complexity priors.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1452618699.0
:DateShort: 2016-Jan-12
:END:

*********** u/deleted:
#+begin_quote
  nostalgebraist does have a point with using diachronous Bayes without having a well-defined distribution...
#+end_quote

Yeah, that tends to be what worries me. Like, I'm mostly on-board with using Bayes methods, including diachronic ones most of the time, as long as we've actually got a well-defined distribution we can compute with (numerically or via sampling), and are pulling the prior from somewhere sensible (empirical frequencies, complexity priors, whatever). But that doesn't seem to be how a lot of "Bayesians" - in nostalgebraist's sense of talking about people who treat probability as this colloquial, model-free thing - think about it.

For instance, Jaynes explicitly said that all you /really/ need is a set of disjoint propositions that add to unity, and you've got a viable discrete distribution to which you /should/, normatively, apply Bayesian reasoning. Oh, and he advocated uniform priors for sets of propositions like those, as well as complexity priors and maximum-entropy methods for real numerical inference problems.

But if you asked me, "Hey, what's your subjective probability that $CANDIDATE will win the upcoming election, as opposed to $CANDIDATE losing", I'd have only two propositions, but I still couldn't give you a distribution that describes my actual beliefs, or even a well-behaved number. I just don't have conscious access to the mental causal models outputting my expectations on the matter, and can't draw enough samples from them to use sampling-based posterior estimation either.

From /that/ perspective, "Bayesianism" (in Jaynes' sense) seems like a normative theory of walking for six-legged creatures being applied to two-legged ones.

#+begin_quote
  With the statistics of recent studies I've read, mostly about transgender brain anatomy, compared to (p < 0.01) and (p < 0.05), what's the recommended Bayes-ist alternative?
#+end_quote

It depends what sort of parameters you're trying to infer?

#+begin_quote
  The likelihood ratios of the update, or some such?
#+end_quote

IIRC, that's called the "Bayes factor" and it is pretty common to use, yeah.

#+begin_quote
  Isn't p a likelihood bound (of the inverse)?
#+end_quote

Yeah, a p-value is supposed to numerically measure the likelihood of the null hypothesis.

#+begin_quote
  What on earth prior distribution would you even be using for that? Would you have a direct distribution of trans etiology theories? Or are you just pulling a number out of your ass for the odds of "gender indicator":"gonad indicator":"no indicator"? Stuff like this, you could just assign yourself a 10-200:10-10:remainder prior and be bigoted until the cows come home? I just don't know. It seems like an explication of consensus in probability distributions would be best, assuming you can even compute the complexity priors.
#+end_quote

From what I've read (which has only managed to be a little), you would /probably/ try to use an easy-to-shape "typical prior" like a beta distribution (it's probably something else for the discrete case), and "shape" it (tune the prior's hyperparameters by hand) to represent approximately what you think the research consensus in your field is.

The nice thing about discrete distributions is that you /can/ just book computer time for a weekend to calculate the sums behind each diachronic Bayes-update and get a numerical posterior.

But if we're talking about something like "odds of gender indicator to gonad indicator" or something, that sounds kinda like a Bernoulli/binomial sort of thing. I guess?

#+begin_quote
  It seems like an explication of consensus in probability distributions would be best, assuming you can even compute the complexity priors.
#+end_quote

Luckily, /lots/ of priors in Bayesian statistics are complexity priors, rather than /just/ the Solomonoff Measure being /the/ complexity prior. As was said on Three-Toed Sloth, even a frequentist will admit that prior distributions are a good and popular way to regularize complex models.
:PROPERTIES:
:Score: 3
:DateUnix: 1452620277.0
:DateShort: 2016-Jan-12
:END:


**** If the majority of humanity's institutions and belief systems do not feature or are opposed to the idea of maximising fun, doesn't it therefore stand to reason that FUN is not humanity's long term goal?
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 2
:DateUnix: 1452630120.0
:DateShort: 2016-Jan-12
:END:

***** I think there's a big problem with claiming there's actually a unified goal-seeking entity called "humanity", period, and then on top of that, that actually-existing institutions and belief systems have anything to do with "humanity's long-term goals" rather than to do with the material and educational conditions of the people who create and maintain them.
:PROPERTIES:
:Score: 3
:DateUnix: 1452630485.0
:DateShort: 2016-Jan-12
:END:

****** In other words, people who disagree with you were educated stupid and need to be enlightened by their own intelligence into seeing things your way?
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 0
:DateUnix: 1452631525.0
:DateShort: 2016-Jan-13
:END:

******* No... that's an extremely long distance away from what I meant.
:PROPERTIES:
:Score: 1
:DateUnix: 1452635784.0
:DateShort: 2016-Jan-13
:END:

******** u/BadGoyWithAGun:
#+begin_quote
  actually-existing institutions and belief systems have anything to do with "humanity's long-term goals" rather than to do with the material and educational conditions of the people who create and maintain them.
#+end_quote

I'm trying to find think of a more charitable interpretation of this sentence and failing. Care to weigh in?
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 1
:DateUnix: 1452636795.0
:DateShort: 2016-Jan-13
:END:

********* Institutions tend to reflect the people who create and maintain them. In order to talk about "humanity's goals", you need to build a causal structure that goes /from/ those goals, wherever in reality you found any such things, /to/ institutions. Right now we have no such structure, because there's no a priori reason for it to exist.
:PROPERTIES:
:Score: 3
:DateUnix: 1452638334.0
:DateShort: 2016-Jan-13
:END:


*** 20: Eradicate polio and take steps to cure other preventable diseases (malaria, HIV/AIDS, TB, etc.). Lift a billion or two people above the poverty line.

100: Fix a limit on the number of living humans that the Earth can support long-term. (Might be anywhere from 5 billion to 500 billion, depending on how tech advances in that time.) Make plans under the assumption that the population and economy will stay fixed in the long-term instead of steadily growing. Solve income inequality and get everyone living to first-world standards.

1000: Make AIs, contact aliens, solve ageing and enough diseases that we qualify as biologically immortal.

10000: I don't know, but I'm sure we'll discover plenty of new and exciting disasters to avert in the intervening time. Either that or our species will go extinct.
:PROPERTIES:
:Author: Chronophilia
:Score: 6
:DateUnix: 1452545164.0
:DateShort: 2016-Jan-12
:END:


*** Getting a handle on global climate change is definitely the first thing to aim for in the next 20 and 100 years. Once there are concerted efforts in that direction, we need to have some big breakthroughs in computing power in the next 10-20 years to get past the Power Wall in Moore's law. After that, goals become less concrete and I haven't given them much thought.
:PROPERTIES:
:Author: Gaboncio
:Score: 5
:DateUnix: 1452529075.0
:DateShort: 2016-Jan-11
:END:

**** Transistor density is not the lowest hanging fruit anymore. I believe there is an overhang in that particular limiting factor, and addressing other limiting factors in maximizing computational utility will result in disproportionate gains due to prior investment in transistor density.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1452554605.0
:DateShort: 2016-Jan-12
:END:

***** Anything concrete off the top of your head that's achievable in 20 years? I know (or strongly hope and suspect) that quantum computing is where we will make a lot of awesome, very productive steps, but that's only on 50+ year scales. A lot of people have been banking on Moore's law to hold for a long time as a crutch for good good coding. While on the one hand, it'll be great for our collective coding skills to have computing power stagnate for a couple of decades, I think the downsides of that scenario outweigh the benefits.
:PROPERTIES:
:Author: Gaboncio
:Score: 1
:DateUnix: 1452567136.0
:DateShort: 2016-Jan-12
:END:

****** The power consumption (I think), switching time, and read/write/cache speed of physical memory is one of the biggest bottlenecks for performance. [[http://www.agner.org/optimize/blog/read.php?i=417][See Agner's blog.]]

I don't think stagnation is a good idea, our coding style and tools will be maladapted to correct the imbalance where we should be trying to improve everything at roughly the same speed. Luckily the low-hanging fruit of transistor density is drying up, which will allow everything else to catch up.
:PROPERTIES:
:Author: Transfuturist
:Score: 4
:DateUnix: 1452572578.0
:DateShort: 2016-Jan-12
:END:


*** 20y fix diabetes, get a better grip on cancer and degenerative diseases (e.g. Alzheimer).

100y off world colonies, solve aging.

1000y solve friendly AGI.

10000y solve entropy.
:PROPERTIES:
:Author: Predictablicious
:Score: 4
:DateUnix: 1452534066.0
:DateShort: 2016-Jan-11
:END:

**** 10,000 years to beat the second law of thermodynamics might be a /wee bit/ optimistic.
:PROPERTIES:
:Author: Jace_MacLeod
:Score: 3
:DateUnix: 1452568192.0
:DateShort: 2016-Jan-12
:END:

***** If we have friendly AGI (the previous goal) 9k years is an absurd amount of time.
:PROPERTIES:
:Author: Predictablicious
:Score: 2
:DateUnix: 1452590940.0
:DateShort: 2016-Jan-12
:END:


*** Industrial automation is definitely priority #1. We like to emphasize FAI, but we /can/ get to post-scarcity without it, and from them on we're on easy mode. With an arbitrary budget, we can approach aging, cancer, and disease from a much better position.

Social problems are somewhere at the bottom of the list, around "dryer lint" and "protecting the sanctity of <blank>". As always, the little things will remain unsolvable until we acquire sufficient wealth, at which point they'll solve themselves.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 3
:DateUnix: 1452537929.0
:DateShort: 2016-Jan-11
:END:

**** Industrial automation without socially unhooking capitalism and other economic problems is likely to result in bad things. Particularly with uneven development and regulation around the globe.

Additionally, automation itself will develop unevenly, and will not result in an 'arbitrary budget', nor the sort of attention to futurist problems that you might assume.
:PROPERTIES:
:Author: Transfuturist
:Score: 5
:DateUnix: 1452555242.0
:DateShort: 2016-Jan-12
:END:

***** Point 1: When industrial automation is fully and completely solved, at least one person will have an arbitrary amount of capital to do with as he or she pleases.

Point 2: It is a fair assumption that this person or people will be more similar to Larry Page or Elon Musk than they are to any given politician.

Point 3: Google is working on AI, SpaceX is pretty self-explanatory, it's hardly unreasonable to assume that this owner of arbitrary amounts of capital is going to be futurist-friendly. We /are/ talking about a team consisting of robotics engineers and computer scientists, after all.

Conclusion: I give > 50% odds that in the future the purse strings will be held by a futurist instead of a politician.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452577982.0
:DateShort: 2016-Jan-12
:END:

****** u/Transfuturist:
#+begin_quote
  It is a fair assumption that this person or people will be more similar to Larry Page or Elon Musk than they are to any given politician.
#+end_quote

That is not a fair assumption at all. You're presuming that the single owner of all the capital (while it will in fact be more of an oligarchy, as it is in the present) will be an Enlightened Capitalist, and that all Enlightened Capitalists are futurists. I find it unlikely that even a simple majority of future capitalists will be either Enlightened or futurist. I find it unlikely that future capitalists will be much more altruistic or connected with the state of the world outside of their bubble of privilege than they are now. I find it extremely unlikely that they will mostly be like Larry Page or Elon Musk than Mitt Romney or Donald Trump.

#+begin_quote
  We are talking about a team consisting of robotics engineers and computer scientists, after all.
#+end_quote

The composition of the team has nothing to do with the owner of the team's output.
:PROPERTIES:
:Author: Transfuturist
:Score: 4
:DateUnix: 1452580180.0
:DateShort: 2016-Jan-12
:END:

******* Mitt Romney made his money in finance, Donald Trump in real estate. Elon Musk, if my memory serves, co-founded Pay-Pal, and Larry Page co-founded Google. Both branched out, Musk moreso than Page. I didn't pick those two for any particular reason, btw, they're just the first technocrats that came to mind.

So, tell me, what brought you to the conclusion that our hypothetical oligo-autocrat is going to be more like your two examples, who made their money in traditional, long-established fields, than my two examples, who made their money in bleeding edge, unproven technology? Our hypothetical oligo-autocrat whose hypothetical company is based around, I might add, /near-future robotics and software technology./
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452580824.0
:DateShort: 2016-Jan-12
:END:

******** You're privileging the hypothesis of Enlightened Capitalists when much more capitalists are like Romney and Trump. Additionally, it is the machines that automate the agricultural and manufacturing pipeline that will force-feed their owners with money, not the techno-progressives who are instead in the business of exploring the search space. What are you automating there, skunkworks? Rockets? The most Larry Page and Elon Musk will be able to do is lobby for basic income once unemployment starts rising at a dangerous rate. Just because they have money does not mean they have most of the money, or that they will be the ones to own the future post-scarcity pipeline. The ones to own the future pipeline will be the ones who already own the land, capital, and experience of managing the current pipeline. Corporations, not individuals. The oligarchy will be of shareholders.
:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1452614275.0
:DateShort: 2016-Jan-12
:END:

********* An interesting theory, but why resort to speculation when we have real-world evidence to extrapolate from? You propose that, even though there are no old-style capitalists pioneering innovative technologies /now/, one will nevertheless appear (possibly from under a bed?) just in time to swipe one of the most profitable technologies imaginable from right under its inventor's nose?

The nature of the kind of tech we're talking about /attracts/ futurists. Can you name even a single company doing something legitimately new that isn't run by futurist? Here's another example: Amazon's owner is Jeff Bezos, who is invested in, among other things, experimental tech education, nuclear fusion, 3D printing, and Stack Exchange. And if he can keep control of Amazon, how does some punk stockholder propose to take industrial robotics away from its creator?
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452616391.0
:DateShort: 2016-Jan-12
:END:

********** Automation is not a futurist concept and is not exclusive to those with progressive ideas. Its value is obvious to anyone relying on unskilled and minimum-wage workers, particularly as the minimum wage is blindly increased. The /assembly line/ was automation. Automating the old production pipeline is not actually fancy or attractive to techno-futurists. The ones with an advantage will be the existing industry that has regulations built around them, with the experience that is useful in knowing what problems need to be solved, with the land and capital pre-bought, with a massive pile of cash with no need for venture capitalists who only throw pittances at 2.0 dotcom startups. Tech startups are not going to be able to compete with existing industry, and would be vastly out of their depth even trying.

"Take industrial robotics away from its creator" lol you believe in individual creators, or even groups of creators owning their creations when they are under contract specifically so their creations are in fact the company's.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1452617060.0
:DateShort: 2016-Jan-12
:END:

*********** You're right, of course. Obviously the people who invent the technology will lose control of it to their investors, just like Jeff Bezos lost control of Amazon, died of typhoid fever, and was buried in a pauper's grave. What sort of crazy alternate universe could possibly have led to him ending up the 4th richest man in the United States? Now, if you'll excuse me, I have coal to shovel for a robber baron.

Again, /examples/. I can't think of a single incident where the scenario you describe /actually happened/ to a revolutionary technological innovation coming out of Silicon Valley. Overwhelmingly, tech startups are owned or operated by their founders. I don't doubt that it happens from time to time, probably to Facebook knockoffs or shitty iPhone games, but actual innovation tends to pay off bigtime.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452624325.0
:DateShort: 2016-Jan-12
:END:

************ I am saying /you are assuming that any substantial automation of our existing infrastructure will be produced by tech startups./ That is /incredibly naive./ And /tech startups/ are sufficiently sociopathic for me to shudder at their being handed anything resembling purse strings! You are pointing to the single most vivid examples of tech philanthropists while ignoring the thousands of people who /actually own and control the pipeline!/

You are also ignoring the difference between production and ownership. A separate producer /sells/ the equipment to the consumer, and it is the /consumer/ that attains the substantial profit gain from automation!
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1452627228.0
:DateShort: 2016-Jan-12
:END:

************* You keep saying that it's "naive", but you haven't given any reason /why/. Blanket statements are /not evidence/. Can you actually /name/ anything comparable to the kind of robotics technology required for industrial automation that is currently owned or controlled by a traditional, Trump-or-Romney-esque capitalist?

Also, do you seriously have to downvote everything I say? Goodness sake, you don't see me doing that to you. Disagreement is not sufficient justification for defection.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452627457.0
:DateShort: 2016-Jan-12
:END:

************** You are giving blanket statements as well, and pretending that your vivid examples are characteristic of the sample. Your examples are vivid /precisely because they are uncharacteristic./ I should have made it clear; I'm not arguing with you anymore.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1452627609.0
:DateShort: 2016-Jan-12
:END:

*************** So you're absolutely positive that your scenario is valid, despite the fact that you can't name /even one example of it taking place in the real world,/ and somehow I'm the naive one?
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452627903.0
:DateShort: 2016-Jan-12
:END:

**************** Of course I'm not 'absolutely positive,' do you know where you are? I haven't named any examples because I don't have the time or motivation to waste on researching this ridiculous argument (and you're talking about examples of something that we're /projecting in the future/). Goodbye.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1452628055.0
:DateShort: 2016-Jan-12
:END:

***************** I am not talking about "examples of something that we're projecting in the future," I /quite clearly/ asked for /one example/ of your model - which you are apparently confident in or you wouldn't be having this discussion - taking place in the real world /right now./ You claim to have an invisible dragon in your garage, and I refuse to believe you until you let me throw flour on it.

Also, it took me less than five minutes to look up who founded Amazon, and that's the sum total of my research so far. You hold these beliefs strongly enough that you consider me naive for disagreeing, it should /not/ be difficult to come up with /one measly example/.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452628499.0
:DateShort: 2016-Jan-12
:END:

****************** You can refuse to believe me just fine, because I don't care. I refuse to be held hostage by my argumentative urges.
:PROPERTIES:
:Author: Transfuturist
:Score: 0
:DateUnix: 1452628903.0
:DateShort: 2016-Jan-12
:END:

******************* Ah, the inevitable degeneration of an argument from "I disagree with you strongly, here are five hundred words explaining why," to "la la la la I can't hear you la la la la." How incredibly rational of you.

Interesting how you edited in "I refuse to be held hostage by my argumentative urges" /right after/ I mentioned rationality, btw. Are you trying to put on a rationality shirt to make my dismissal seem less convincing, even though you're actually saying the exact same thing? Where was that refusal when you started this thread?
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452629193.0
:DateShort: 2016-Jan-12
:END:


**** A naive definition of "Post scarcity" is that the amount of work man wants to do produces enough stuff that nobody who wants it can't have it. I suspect that pursuing this with "automated factories" is going to work about as well in the short term future as it has in the mid-term past. (Where's the 15 hour work week Keynes forcast back in 1930?) (Barring an AI foom or something) Instead, I expect the post-scarcity scene to be a gradually growing opt-in philosophical movement.

At least, the following can be said:

1) I suspect that there's enough people out there who view wealth as a relative-social-status thing (or at least are sufficiently ignorant of hedonic adaptation) that you would simply run out of matter in the universe before we got the last 20% of them happy.

1.2) I would suggest that this demand curve is very steep in the first world. I mean, how many more people retire early now, compared to in 1950, when we made much less? Almost zero, either way?

2) The "Financially Independent, Retired Early" people, despite society being basically pitted against them, are able bootstrap themselves (and their progeny) into a "post scarcity" situation, in the present day, with about 15 years of work. (This is, perhaps, not sustainable - but that's not my point.)

3) The difference between the FIRE people and most of society is mostly philosophical, rather than technological. (nevermind that philosophy is a sort of tech...)

(Edit for formatting only)
:PROPERTIES:
:Author: BoilingLeadBath
:Score: 2
:DateUnix: 1452561854.0
:DateShort: 2016-Jan-12
:END:

***** u/Transfuturist:
#+begin_quote
  Where's the 15 hour work week Keynes forcast back in 1930?
#+end_quote

There are alternate explanations for this failure. I'm not researched enough on the topic to elaborate, but I don't accept your use of it here. Although that might be the very point you're getting at.

[[http://www.thisismoney.co.uk/money/news/article-2642880/Table-700-jobs-reveals-professions-likely-replaced-robots.html][Factories are not even the tip of the iceberg.]]
:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1452574454.0
:DateShort: 2016-Jan-12
:END:


***** Any society in which production is decoupled from labour is, for all intents and purposes, post-scarcity. Because consumers and producers are separate, we can ramp up the ratio as high as we want. Ten factories per human being? A hundred? A thousand? Why not? It's not like we're running out of space in the solar system, here.

For all the fear-mongering about 1% of the population owning the robots and everyone else starving in the streets, it seems somewhat more likely that, with some effort, we'll be able to solve the /mind-bendingly difficult task/ of having enough of everything for everyone.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 2
:DateUnix: 1452579006.0
:DateShort: 2016-Jan-12
:END:

****** Not to nitpick, but wouldn't a society in which production is decoupled from labor only be post-scarcity if the rate of increase of the rate of production exceeds the rate of increase of demand. (ie, if p' = (1-a)p and d' = bd then ap > bd)

For a (pretty bad) historical example: slavery-based societies were not post-scarcity, even though the consumers were not the producers.

In any case, barring some REALLY good AI, I expect that automation will simply increase the effectiveness of what human workers do. (Thus the "Short term future" disclaimer) In this version of events, the case where production is truly decoupled doesn't actually happen.
:PROPERTIES:
:Author: BoilingLeadBath
:Score: 2
:DateUnix: 1452598310.0
:DateShort: 2016-Jan-12
:END:

******* Slaves are consumers as well as producers, which is why they aren't fully decoupled. A fully autonomous robot is a pure producer by definition, requiring no guarding, supervision, or oversight of any kind. Efficiency concerns, coupled with the fact that slaves are only minimally suited to, for example, banking or administration, prevents a slave economy from accomplishing what an automated economy can.

You are right about it being unlikely, since we'll probably hit on some form of AI before we successfully automate software engineering, which would be required for the robots to really and truly solve their own problems without human intervention. Still, I did say it was a /priority,/ not a prediction. Unlike anti-aging technology or AI, industrial automation is a gradual progression; we can reap the benefits of automated agriculture before we ever consider trying fully automated banking, and vice-versa. It'd be nice if we hit post-scarcity, but even a 1% success will be crazy profitable, and thereby encourage future innovation.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 1
:DateUnix: 1452625568.0
:DateShort: 2016-Jan-12
:END:


*** Get cryonics (at least the freezing part) perfected and widespread, solve the value alignment problem and make the solution universally spread, seed AI, in some of those orders.
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1452530750.0
:DateShort: 2016-Jan-11
:END:


** By now when I hear terms like "rationality" or "normative theory" I reach for my Bolter, but I still think global-warming denialists and Ray Kurzweil are insane.

What do? How unpack?
:PROPERTIES:
:Score: 1
:DateUnix: 1452547797.0
:DateShort: 2016-Jan-12
:END:

*** RE: Normative Theory, maybe I can help, since I don't know what normative theory is. Is it "Normative Ethics" [[https://en.wikipedia.org/wiki/Normative_ethics#Normative_ethical_theories][(wikipedia link)]]? I don't know anything about it so it's not packed in with other things for me.

RE: Rationality, if what you're saying is "I don't like rationality (as in, the rationality community on the internet), and I also don't like global-warming denialists and Ray Kurzweil", these are not mutually exclusive beliefs. I think there are lots of people who don't identify as Rationalists who are anti-Kurzweil and anti climate change denialist. Since I think you already know that, I'm guessing I missed something here. What exactly is the problem?
:PROPERTIES:
:Author: blazinghand
:Score: 3
:DateUnix: 1452558624.0
:DateShort: 2016-Jan-12
:END:

**** u/deleted:
#+begin_quote
  What exactly is the problem?
#+end_quote

I guess I had more meant, "You must X, /because logic/" as the kind of talk about "rationality" or "normative theory" I don't like hearing. Also economists.
:PROPERTIES:
:Score: 1
:DateUnix: 1452576606.0
:DateShort: 2016-Jan-12
:END:

***** Ah! Well it's okay to dislike that kind of talk! It's not necessary to be into that in order to disagree with climate change denialists. There are tons of people, most people in fact, who aren't really into that stuff, but also reasonably say "the vast majority of scientists and climatologists in particular believe in climate change. It seems reasonable to follow their lead" and that works out great. No need to get too fancy with it.
:PROPERTIES:
:Author: blazinghand
:Score: 1
:DateUnix: 1452624020.0
:DateShort: 2016-Jan-12
:END:


** Well [[http://www.eurekalert.org/pub_releases/2016-01/lu-twy011116.php][this]] is an interesting tool.

Also seems like it might work as part of the whole "smile to become happy" theme of cognitive/mood adjusting tricks.
:PROPERTIES:
:Author: Nighzmarquls
:Score: 1
:DateUnix: 1452621525.0
:DateShort: 2016-Jan-12
:END:
