#+TITLE: [MK][Q] You can bring only one fictional character to reality. Who is the 'right' choice?

* [MK][Q] You can bring only one fictional character to reality. Who is the 'right' choice?
:PROPERTIES:
:Author: puesyomero
:Score: 18
:DateUnix: 1448908418.0
:END:
He/She/It is under no compulsion to obey or even like you. Must be from a fictional work created before you knew you could do it. only you can use this power/device, so no creating your ideal and then have someone else do it. (°3°) good luck!


** 1. Sit down and re-read the Culture series. All of it.

2. Pick the Culture Ship most likely to be well-intentioned towards our little blue dot. Perhaps /Arbitrary/ (oh, hey, I know you think you left here a few decades ago, well you're back and the entire hominid population of this galaxy is this planet ... wanna lend a hand?), or maybe /Sense Amidst Madness, Wit Amidst Folley/?

Seriously, if you're going to find a friendly ASI in literature, I can't think of a better place.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 36
:DateUnix: 1448910448.0
:END:

*** Not to mention that a Culture ship can literally digitize you and spit you back out in a body of your choice without you noticing the transitions. If you could find a Mind which would cheerfully upgrade anyone who asked to at least a Culture-human body, that'd be a great start. One who was happy to circle the globe and do it for any injured/sick/aged person would be fantastic.
:PROPERTIES:
:Author: Geminii27
:Score: 14
:DateUnix: 1448918341.0
:END:

**** Just not "Sleeper Service" or "Gray Area" or "Killing Time" or "Falling Outside The Normal Moral Constraints". There are some scary Culture ships.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 11
:DateUnix: 1448930201.0
:END:

***** Why are those guys so scary?
:PROPERTIES:
:Score: 2
:DateUnix: 1448938051.0
:END:

****** Sleeper Service is probably the least scary of the lot. It just has this thing about staging battle scenes using actual Culture citizens in stasis.

Gray Area likes digging around in non-culture hominid minds looking for war criminals to punish.

Falling Outside The Normal Moral Constraints has been known to torture people for amusement, relying on the fact that it can wipe memories and repair damage perfectly so it has plausible deniability.

Killing Time started an interstellar war for the purpose of forcing the Culture to conquer a civilization it didn't approve of.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 11
:DateUnix: 1448978689.0
:END:

******* #+begin_quote
  Falling Outside The Normal Moral Constraints has been known to torture people for amusement, relying on the fact that it can wipe memories and repair damage perfectly so it has plausible deniability.
#+end_quote

Not quite as bad as that, but stranger. /Falling.../ used a consenting human as its avatar. And while it was that avatar, the avatar got into all sorts of kinky predicaments. It kept the host's personality running while doing all these things. And then when /Falling.../ was done with the body, /Falling.../ wiped the host's memories and left him standing on the docks, missing a month of memories and having gained a number of interesting scars, tattoos, illnesses and bandages.

#+begin_quote
  He looked cadaverous, hollow-cheeked. Dark eyes with no whites, two ridges instead of eyebrows, a flat nose and mid-dark skin, scarred in places. He was only medium tall but his height was emphasised by his thinness. If his physiology was the same as a Sichultian's then the slight bagginess about his face implied the weight loss had been recent and rapid. His clothes were dark, perhaps black: skinny trews and a tight-fitting shirt or jacket, partially closed at the neck by a thumb-sized, blood-red glittering jewel on a loosened choker.
#+end_quote

That's /Surface Detail/'s protagonist's first impression of Demeisen.

#+begin_quote
  Lededje saw him look at her right hand and so put it out to him. His hand clasped her hand, fingers with too many joints closing around like a bony cage. His touch felt very warm, almost feverish, though perfectly dry, like paper. She saw him wince and noticed that two of his fingers were crudely splinted together with a small piece of wood or plastic and what looked like a piece of knotted rag. Somehow the wince didn't travel all the way to his face, which regarded her without obvious expression.

  There was a small gold tube in front of him which Lededje had assumed was the mouthpiece of an under-table chill or water pipe -- there were several other mouthpieces lying or cradled on the table -- but which proved to be a stick with a glowing end, un -attached to anything else. Demeisen put it to his lips and sucked hard. The golden tube crackled, shortened and left a fiery glowing tip beneath a lofting of silky grey smoke.

  Demeisen saw her looking and offered the stick to her. “A drug. From Sudalle. Called narthaque. The effect is similar to /winnow/, though harsher, less pleasant. The hangover can be severe.”

  “‘Winnow'?” Lededje asked. She got the impression she'd been expected to know what this was.
#+end_quote

...

#+begin_quote
  He smiled more broadly and ground the yellow-red glowing tip of the stick into the open palm of his other hand. There was a distinct sizzling noise. Again, his body seemed to flinch, though his face remained serene.

  “What, this?” he said, looking down at the ash-dark burn on his skin as Lededje stared at it, openly aghast. “Don't worry; I don't feel a thing.” He laughed. “The idiot inside here does though.” He tapped the side of his head, smiled again. “Poor fool won some sort of competition to replace a ship's avatar for a hundred days or a year or something similar. No control over either body or ship whatsoever, obviously, but the full experience in other respects -- sensations, for example. I'm told he practically came in his pants when he learned an up-to-date warship had volunteered to accept his offer of body host.” The smile became broader, more of a grin. “Obviously not the most zealous student of ship psychology, then. So,” Demeisen said, holding up his hand with the splinted finger and studying it, “I torment the poor fool.” He put his other hand to the one with the splinted fingers, waggled them. His body shuddered as he did so. Lededje found herself wincing with vicarious pain. “See? Powerless to stop me,” Demeisen said cheerily. “He suffers his pain and learns his lesson while I... well, I gain some small amusement.”

  He looked at Jolicci and Lededje. “Jolicci,” he said with obviously feigned concern, “you look offended.” He nodded, creased his eyes. “It's a good look, trust me. Sour opprobrium: suits you.”

  Jolicci said nothing.

  Wheloube and Emmis resumed their seats. Standing there, Demeisen put out both hands and stroked the hair of one and the shaved head of the other, then cradled the finely chiselled chin of the one with the shaved head using his unsplinted hand. “And fascinatingly, the fellow” -- he used his splinted fingers to tap the side of his head again, hard -- “is quite defiantly heterosexual, with a fear of bodily violation that borders on outright homophobia.” He looked round the table of young men, winking at one of them, then gazed radiantly at Jolicci and Lededje.
#+end_quote

For reference, Jolicci is the avatar of a General Contact Mind, which falls somewhere between First Contact and Diplomacy As Usual

#+begin_quote
  Jolicci shrugged. “The Abominator class of General Offensive Unit, to which our friend belongs, is not known for its mildness or sociability. Probably specced when the Culture was going through one of its periods of feeling that nobody was taking it seriously because it was somehow too nice. Even amongst those, though, that particular ship is known as something of an outlier. Most SC ships conceal their claws and keep the psychopathy switched to Full Off except when it's judged to be absolutely necessary.”
#+end_quote

Later, Falling... leaves the body. Attending are Lededje, Jolicci, and Sensia. Sensia is the General Systems Vehicle hosting Jolicci, Jolicci's ship, and Lededge, but not Falling..., whose ship body left on a different course.

#+begin_quote
  “I did leave earlier, my gracious hostess. I am currently some eighty years or so distant on an acutely divergent course, and travelling only slightly more rapidly than your good self, though still just about within real-time control range, at least for something as intrinsically slow-reacting as a human host. All of which I would hope you're well aware of.”

  “You're abandoning your puppet here then?” Jolicci said.

  “I am,” Demeisen agreed. “I thought now would be as appropriate an occasion as any other to return the fucker to the wild.”

  “I have heard some disturbing reports regarding your treatment of this human you're using, ship,” Sensia said. Lededje looked at the GSV's avatar. For a small, frail-looking lady with frizzy blonde hair she seemed suddenly invested with a steeliness Lededje found herself glad was not directed at her.

  Demeisen turned to Sensia. “All above board, dear thing. I have the relevant releases signed by his own fair hand. In blood, admittedly, but signed. What was I to use -- engine oil?” He looked puzzled and turned to Jolicci. “Do we even have engine oil? I don't think we do, do we?”

  “Enough,” Jolicci said.

  “Say goodbye and release your hold now before I do it for you,” Sensia said levelly.

  “That would be impolite,” Demeisen said, pretending shock.

  “I'll suffer the injury to my reputation,” the GSV's avatar said coolly.

  The cadaverous humanoid rolled his eyes before turning to Lededje and smiling broadly. “My every best wish for your journey, Ms. Y'breq,” he said. “I hope I did not alarm you unduly with my little display last night. I get into character sometimes, find it hard to know when I'm causing distress. My apologies, if any are required. If not, then please accept them in any event, on account, to be banked against any future transgressions. So. Perhaps we shall meet again. Until then, farewell.”

  He bowed deeply. When he came upright he looked quite changed; his face was set differently and his body language had altered subtly too. He blinked, looked around, then stared blankly at Lededje and then at the others. “Is that it?” he said. He stared at the ship in front of him. “Where is this? Is that the ship there?”

  “Demeisen?” Jolicci said, moving closer to the man, who was looking down at himself and feeling his neck under his chin.

  “I've lost weight...” he muttered. Then he looked at Jolicci. “What?” He looked at Sensia and Lededje. “Has it happened yet? Have I been the avatar?”

  Sensia smiled reassuringly and took him by the arm. “Yes, sir, I believe you have.” She began to lead him towards the traveltube and made a begging-your-leave gesture to Jolicci and Lededje before turning away.

  “But I can't remember anything...”

  “Really? Oh dear. However, that may be a blessing.”

  “But I wanted memories! Something to remember!”

  “Well...” Lededje heard Sensia say, before the doors of the traveltube capsule closed.
#+end_quote

Until /Falling.../ encounters Lededje again:

#+begin_quote
  “Why are you here, if only apparently?”

  “To make you an offer.”

  “What? To be your next abused avatar?”

  He grimaced again. “Oh, that was all just to upset Jolicci. You saw the guy I was... inhabiting; I released him in front of you. He was fine. I'd even fixed his fingers and everything. Didn't you notice, this morning?”

  She hadn't.

  “And anyway he did agree to everything. Not that I really abused him in the first place. Did he say anything? When I released him; did he? I didn't bother to send any surveillance back-up and I haven't asked the SAMWAF, so I honestly don't know what happened after I pulled out. Did he? Make any allegations?”

  “He couldn't remember anything at all. He wasn't even sure he'd been an avatar; he thought maybe it was about to happen.” Demeisen waved his arms. “Well, there you are!”

  “There you are what? That proves nothing.”

  “Yes it does; if I'd really been sneaky I'd have left the dumb fuck with a batch of implanted false memories full of whatever Contact-wank fantasies he'd been imagining before he took the gig in the first place.” He waved one hand in a blur of too-long fingers. “Anyway, we're getting off the point here. You need to hear my offer.”
#+end_quote

What /Falling Outside The Normal Moral Contraints/' offer was to Lededje, you will have to find out by reading the book yourself.
:PROPERTIES:
:Author: boomfarmer
:Score: 3
:DateUnix: 1449111111.0
:END:

******** #+begin_quote
  Not quite as bad as that, but stranger. Falling... used a consenting human as its avatar. And while it was that avatar, the avatar got into all sorts of kinky predicaments. It kept the host's personality running while doing all these things. And then when Falling... was done with the body, Falling... wiped the host's memories and left him standing on the docks, missing a month of memories and having gained a number of interesting scars, tattoos, illnesses and bandages.
#+end_quote

Actually, he got healed too. And those "kinky predicaments" included actual physical torture, like breaking his fingers. It was still torture even if the memory was wiped afterwards.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1449137588.0
:END:

********* Also worth noting that rape is a form of torture, and he mentioned that the poor sod had "a fear of bodily violation that borders on outright homophobia" - which may be considered a personality flaw by the Culture, but doesn't excuse raping the guy.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1449170299.0
:END:


****** Well, the names are fairly indicative... ^^;
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1448941420.0
:END:

******* You torture /one/ retired Nazi who engaged in human experimentation and /they never let you live it down/.
:PROPERTIES:
:Score: 15
:DateUnix: 1448945261.0
:END:


**** Literally the first thing that occurred to me as well.

That re-read sounds like a good idea, though.
:PROPERTIES:
:Author: Arizth
:Score: 3
:DateUnix: 1448919416.0
:END:

***** Yeah, any other choice would have to have a pretty convincing explanation to choose over a Culture Mind.

Specifically, I'd choose the System-class GSV /Empiricist/, from /The Hydrogen Sonata/. Noted for it's exceptional size even for a GSV, and a complement of seven(!) Minds rather than the normal three. Seems happy to focus on civilian pursuits. [[https://en.wikipedia.org/wiki/List_of_spacecraft_in_the_Culture_series][Here's a list]] of others.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 8
:DateUnix: 1448952974.0
:END:

****** These are always the best names for boats. When I was a pirate captain in a play, my ship was the /Unreliable Witness/.

There's the /Mistake Not My Current State Of Joshing Gentle Peevishness For The Awesome And Terrible Majesty Of The Towering Seas Of Ire That Are Themselves The Milquetoast Shallows Fringing My Vast Oceans Of Wrath/ and /Very Little Gravitas Indeed/, the /Unacceptable Behaviour/ and the Rapid Offensive Unit /Shoot Them Later/, the Gangster-class ROU /Heavy Messing/, the Offensive Unit /All Through With This Niceness And Negotiation Stuff/ and the GSV /Experiencing A Significant Gravitas Shortfall/. Plus the /Pure Big Mad Boat Man/ and /Transient Atmospheric Phenomenon/, the /But Who's Counting?/ and the /Me, I'm Counting/, and the non-Culture ship /Strategic Outreach Element CH2OH.(CHOH)4.CHO/
:PROPERTIES:
:Author: boomfarmer
:Score: 5
:DateUnix: 1449112709.0
:END:

******* I /may/ have named my canoe, but the paint washed off in sun and salt water. Most recently, it's the LCU /Can't Escape this Gravitas Well II/
:PROPERTIES:
:Author: PeridexisErrant
:Score: 6
:DateUnix: 1449114895.0
:END:


****** Why. . . thank you.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1449418590.0
:END:


*** The question is what powers they get. Culture Minds are post-scarcity beings because their universe has an infinite power source. That power source may or may not transfer. If it doesn't...there will be problems.

If hyperspace doesn't work...there goes most of their processing power.
:PROPERTIES:
:Author: Tsegen
:Score: 11
:DateUnix: 1448935399.0
:END:

**** The same objections really apply to any SFnal characters with any significant capabilities, no?
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 6
:DateUnix: 1448978773.0
:END:

***** Yes. But some have internal sources of power. And it's harder to tell where the Culture powers (as an individual) end and their universe powers begin.

For example, drop some sort of super-soldier on Earth and it's presumed that he cannot make new superweapons, only keep what he has. If the superweapon depends on or works on things not found here then they're shit out of luck? But what about a magician? What about a magician that depends on a very clearly bound source of magic (it clearly works in one area or on one person)

With the Culture is the Grid like the superweapon or like magic?
:PROPERTIES:
:Author: Tsegen
:Score: 3
:DateUnix: 1448979946.0
:END:

****** If you're going to exclude things like hyperspace and the grid... that is, you're limiting them to known physics... then that eliminates all magical abilities, all soft-SF abilities, and even a lot of diamond-hard SF (like, we don't /know/ that there are algorithms that would allow something like Neko-AI to function).
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 7
:DateUnix: 1448980557.0
:END:

******* What is Neko-AI and where is it from?
:PROPERTIES:
:Author: PlaneOfInfiniteCats
:Score: 2
:DateUnix: 1449056013.0
:END:

******** It's a weakly godlike intelligence from /Accelerando/.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 8
:DateUnix: 1449058178.0
:END:


** My primary criteria are probably:

- Extremely powerful
- Aligned with my values

To that end, I would probably select some sufficiently advanced AI that's known to be good. A Culture Mind is probably my best bet, since the entire Culture is essentially contained within one, which means that I (and everyone else) would rapidly gain access to all that nifty culture tech.

Assuming that magic still works if I bring in a fantasy character ... I'd probably go looking at a registry of gods, but I imagine that it would be difficult to find one that actually shares my values.

You said that he/she/it is under no compulsion to obey me, but does that mean no /additional/ compulsion? Because if they still have all their regular restrictions, I think I might take Genie from Aladdin, who would be compelled to grant me three wishes (probably enough to get to godhood).
:PROPERTIES:
:Author: alexanderwales
:Score: 18
:DateUnix: 1448910457.0
:END:

*** #+begin_quote
  I think I might take Genie from Aladdin, who would be compelled to grant me three wishes (probably enough to get to godhood).
#+end_quote

1. A wish that will allow you to craft a near optimal second wish. Depending on the type of genie that might be [[https://archiveofourown.org/works/4637439/chapters/10575111][knowledge of the rules,]] an intelligence boost, or something else.

2. Wish for near optimal wish granting you as close to omnipotence as is allowed.

3. Free the genie or something, you can now probably do anything you can wish for yourself.
:PROPERTIES:
:Author: literal-hitler
:Score: 8
:DateUnix: 1448945543.0
:END:

**** Couldn't it be done easier?

1. "Genie, change the world state so it's perfectly aligned with my utility function" -> best possible wish. Does everything that you want.
:PROPERTIES:
:Author: Sinity
:Score: 4
:DateUnix: 1448974345.0
:END:

***** The first problem there is that you are imposing your values on everyone else in existence, which is morally reprehensible.

The second problem is if your value function isn't quite what you think it is. There are lots of things that we want to want, and it's easy to tell ourselves that we actually want them. Having something show you your true value function might be decidedly unpleasant.
:PROPERTIES:
:Author: eaglejarl
:Score: 11
:DateUnix: 1448982452.0
:END:

****** Everyone wants to fulfill their own values. That's what values mean. And every action takes effect on other people.
:PROPERTIES:
:Author: Sinity
:Score: 6
:DateUnix: 1448984424.0
:END:

******* You're missing my point. Taking actions like "talk to people to convince them of my beliefs" is perfectly reasonable and honorable. "Use magic to transform everyone in the universe's mind-state so that it matches my values" is not. That's exactly what you're asking for here -- you want everything to align to your values, which means every/one/ must align to your values. You are killing every other person in existence and replacing them with someone that looks like them but thinks like you.

Unless "respect for the lives and rights of others" is not part of your value system, this wish literally cannot be fulfilled.
:PROPERTIES:
:Author: eaglejarl
:Score: 7
:DateUnix: 1448985637.0
:END:

******** Well, yes. My value function prefers universes that seem to contain other people with value functions that are being filled. By contrast, a universe with history where everybody's mental state was invasively overwritten by my own would not quite satisfy my own values. Your value function almost certainly has similar properties, judging by how violently you reacted to the idea. Of course, for the sake of making this post less than 10 kilowords I've had to use terms like like "containing other people" and "history" and those are notoriously flimsy ideas around here, but I think that the post should work.
:PROPERTIES:
:Author: Vebeltast
:Score: 9
:DateUnix: 1449010905.0
:END:

********* But would you really care if everyone's utility function was invasively changed to your own? Especially if the genie hid that knowledge from you?

To put it into Freudian terms, I think what [[/u/eaglejarl][u/eaglejarl]] means is: Will the genie fulfill your superego or your id?
:PROPERTIES:
:Author: sole21000
:Score: 5
:DateUnix: 1449295713.0
:END:


******** So, you say that it is unreasonable to perform an action that fulfills my terminal values in the best way possible?

Isn't that what rational agents do?
:PROPERTIES:
:Author: thetimujin
:Score: 1
:DateUnix: 1449130616.0
:END:


****** #+begin_quote
  The first problem there is that you are imposing your values on everyone else in existence, which is morally reprehensible.
#+end_quote

Only if your values are really weird.

Most people's values are hard to put into words, but center around things like, make other people happy, which isn't terribly morally reprehensible.

However, if one of your values is "Make sure everyone is serving my idea of god x" or "destroy all members of outgroup y" or "I don't care about anyone else, just make me super-powerful," then yes, imposing your values would be pretty terrible.
:PROPERTIES:
:Author: electrace
:Score: 2
:DateUnix: 1448993475.0
:END:

******* #+begin_quote
  Only if your values are really weird.
#+end_quote

Not necessarily. For example, I put little to no value on professional sports, yet there are people who do, and have lives centered around it -- either as their profession or as their primary leisure activity. Likewise, I think that religion is (for the most part) awful, yet there are people who have built their life around it. What right do I have to forcibly rip those peoples' lives away from them? Try to convince them? Absolutely, that's a good thing for me to do, but not by force. I hesitate to use the word because it's so loaded, but what we're talking about is mindrape -- ripping someone's brain apart and replacing it with something that is more to your (or, in this case, my) liking. There is no violation greater than that.
:PROPERTIES:
:Author: eaglejarl
:Score: 10
:DateUnix: 1448994416.0
:END:

******** #+begin_quote
  For example, I put little to no value on professional sports, yet there are people who do, and have lives centered around it -- either as their profession or as their primary leisure activity.
#+end_quote

You have multiple values, and "sports are stupid and nobody should watch them," is probably very low on your list of priorities. Ranked much higher are things like "People should be able to watch sports if they really want to" as a subcatagory of "If they aren't harming anyone, people should be able to do what they want to do."

#+begin_quote
  Try to convince them? Absolutely, that's a good thing for me to do, but not by force. I hesitate to use the word because it's so loaded, but what we're talking about is mindrape -- ripping someone's brain apart and replacing it with something that is more to your (or, in this case, my) liking. There is no violation greater than that.
#+end_quote

You just contradicted yourself. Is it a good thing for you to do, or is there no violation greater than that? If you believe the latter, then a scenario in which that happens /wouldn't align with your utility function, and therefore wouldn't be included in the wish./

Everybody has contradicting values. Give me two non-identical values, and I can find a scenario in which they contradict.

So, going back to the utility function, your "don't mindrape people" value would have a higher coefficient than your "no sports" coefficient.

Maximization doesn't mean maximization of every value (because they negatively coorelate with each other), it means maximization of the entire function. For a simple example, remember that maximization of profit is almost never at the point where revenue is maximized, nor where cost is minimized, even though profit is equal to revenue minus cost.
:PROPERTIES:
:Author: electrace
:Score: 6
:DateUnix: 1448999392.0
:END:

********* IMO if a principle really is a /moral value/, one /would/ compulsorily apply them over the top of other people's personally preferred ideas. For example, if I could wave my wand and cause /all/ other people to become curious, compassionate, courteous and cooperative, or at least more often to behave like that, then I would do so without a second's hesitation.

I don't value the opinions of incurious, cruel, rude and selfish people on the subject of whether or not they should behave that way. It's /clearly better/ that people be internally driven to educate and improve themselves, that they/we empathetically consider the effects of their actions on others, and that they/we fairly divide both the necessary work and the rewards for it.

If you're not comfortable imposing your values on others, then that in my view--a value I would impose--is an indication that you need to refine and clarify your values. If you're not living up to your own values, because it's become impossible for you, then that's an indication that you need to relax them and make them more realistic; as such, they're not even suitable for being imposed on /you/, let alone others.
:PROPERTIES:
:Author: aeschenkarnos
:Score: 2
:DateUnix: 1449000232.0
:END:

********** #+begin_quote
  For example, if I could wave my wand and cause all other people to become curious, compassionate, courteous and cooperative, or at least more often to behave like that, then I would do so without a second's hesitation.
#+end_quote

How do you feel about those of us who value /being ourselves/, and think it would be morally just to kill you before you could mindrape the planet? How do you feel about someone with a different value set using your exact justification to mindrape you into someone eager to, say, submit to Allah and Sharia law?
:PROPERTIES:
:Author: Iconochasm
:Score: 4
:DateUnix: 1449005562.0
:END:

*********** Is it supposed to be a surprise to me that other people feel differently? That's the whole point of values conflicts. We'll have to work it out, I guess, the same way humans have been working it out for hundreds of thousands of years. Reason with them, emotionally appeal to them, out-compete them, and/or kill them. Whatever works. Or if it doesn't work, and they win, then they won, and so be it. Memetic evolution in action.

How do you feel about the concept of values conflict? How would you resolve it? What would you do if the opponent won't go along with your preferred methods?
:PROPERTIES:
:Author: aeschenkarnos
:Score: 3
:DateUnix: 1449007830.0
:END:

************ #+begin_quote
  How do you feel about the concept of values conflict? How would you resolve it? What would you do if the opponent won't go along with your preferred methods?
#+end_quote

To the extent that they're not actively harming others, /leave them alone/. Jumping straight to mindrape, or making clear that murder is on the table right from the beginning seems like sociopathy masquerading as Deep Wisdom.
:PROPERTIES:
:Author: Iconochasm
:Score: 2
:DateUnix: 1449019698.0
:END:

************* There's not much to be gained here from scolding and downvoting me. If you can't even respectfully have /this/ conversation, I don't like your odds of converting anyone genuinely hostile to you, to your point of view.

I think it's common ground that the courteous thing to do is to begin conflict resolution with attempts to reason with them, and appeal to their sense of moral reciprocity. Pretending that I argued for "jumping straight to mindrape" is strawmanning, and that doesn't count as reason, nor is it emotionally appealing to me, so you strike out twice there. Also murder's been "on the table" since well before our ancestors lost their tails; murder /not/ being on the table is a relatively novel concept.

But let's go on for a bit. What if they /are/ actively harming others? What if they're not interested in reasoning--they try to straw-man you, or something--and their emotional appeal range is pretty much limited to whiny scolding? (Also there's no authority over the two of you for you to appeal to, or perhaps the authority is neutral between you.) Whats your preferred method for resolving values conflicts in these categories?
:PROPERTIES:
:Author: aeschenkarnos
:Score: 2
:DateUnix: 1449025746.0
:END:


********** #+begin_quote
  For example, if I could wave my wand and cause all other people to become curious, compassionate, courteous and cooperative, or at least more often to behave like that, then I would do so without a second's hesitation.
#+end_quote

I'd advice against that.

Curious: Would get bored when they ran out of things to discover, or when new discoveries weren't interesting.

Compassionate: This can be paralyzing when bad things happen, disallowing you from helping, and also increases the impact of emotional appeals, which could overshadow rational ones.

Courteous: Courteousness can kill efficiency.

Cooperative: It's goodness depends on whether you agree with what they are cooperating on. I'd much prefer for bad people to defect against each other.

This is the problem with using unspecific Kantian values, rather than thinking through exactly /why/ you value those things. Generally, these things all boil down to some measure of happiness.

#+begin_quote
  If you're not living up to your own values, because it's become impossible for you, then that's an indication that you need to relax them and make them more realistic; as such, they're not even suitable for being imposed on you, let alone others.
#+end_quote

Realistic is not really an issue for a genie :P
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1449001442.0
:END:

*********** #+begin_quote
  Curious: Would get bored when they ran out of things to discover, or when new discoveries weren't interesting.
#+end_quote

I guess we only have until the heat death of the universe then. Oh well. As to the second objection, not finding things interesting correlates more with the observer's depression than with the inherent interestingness of the things.

#+begin_quote
  Compassionate: This can be paralyzing when bad things happen, disallowing you from helping, and also increases the impact of emotional appeals, which could overshadow rational ones.
#+end_quote

Paralysis is not a compassionate response. That would be a failure to be compassionate; perhaps understandable and excusable, as self-compassion might conclude, but still a failure. Also I don't hold "rationality" as a higher value than compassion: it might become "rational" for me to kill you and take your stuff.

#+begin_quote
  Courteous: Courteousness can kill efficiency.
#+end_quote

Lack of courteousness creates resentment, which is /far/ more corrosive to efficiency. To dismiss the effect of others' feelings on one's goal pursuit is a /huge/ error, all the bigger for being commonly made. (Immature INTJs very rarely achieve anything that justifies putting up with their bullshit.)

#+begin_quote
  Cooperative: It's goodness depends on whether you agree with what they are cooperating on. I'd much prefer for bad people to defect against each other.
#+end_quote

If they did, it'd come from compassion.
:PROPERTIES:
:Author: aeschenkarnos
:Score: 2
:DateUnix: 1449002127.0
:END:

************ #+begin_quote
  I guess we only have until the heat death of the universe then. Oh well. As to the second objection, not finding things interesting correlates more with the observer's depression than with the inherent interestingness of the things.
#+end_quote

Are you saying that when I've heard the same song for the 100th time and I no longer find it interesting, is it that the song makes me depressed, rather than my brain having already figured out the patterns, and no longer gets a dopamine boost from it?

#+begin_quote
  Paralysis is not a compassionate response. That would be a failure to be compassionate; perhaps understandable and excusable, as self-compassion might conclude, but still a failure.
#+end_quote

You seem to be defining compassion weirdly. It seems to be the sames as "doing the right thing," which isn't really how compassion is used. Compassion is normally defined as sympathy and concern for another person's well-being, not as acting optimally to ensure another person's well-being.

Compassion can lead to "This problem is so bad that we have to do something now!" reasoning, even when the "something now" won't help, or would even make the problem worse.

I agree that compassion is generally good, but I wouldn't want to increase it for everyone, without qualification.

#+begin_quote
  Also I don't hold "rationality" as a higher value than compassion: it might become "rational" for me to kill you and take your stuff.
#+end_quote

In very few situations is it more rational to kill someone than to not kill them....

Aside from that, it shouldn't be an issue of which concept you hold above another. Surely that would depend on the scenario. Would you rather have a compassionate witch doctor, who will "heal" your wounds with spit and dirt, or an uncompasionate MD, only doing it for the money, who will use antiseptic and bandages.

#+begin_quote
  Lack of courteousness creates resentment, which is far more corrosive to efficiency.
#+end_quote

It's not an either-or situation. You opted to increase courteousness in everyone, even those who are already hyper-courteous. There is a happy medium.

#+begin_quote
  (Immature INTJs very rarely achieve anything that justifies putting up with their bullshit.)
#+end_quote

That personality classification [[http://www.smithsonianmag.com/smart-news/the-myers-briggs-personality-test-is-pretty-much-meaningless-9359770/?no-ist][isn't really all that useful.]]

#+begin_quote
  If they did, it'd come from compassion.
#+end_quote

I'd hate to see a bunch of [[https://www.youtube.com/watch?v=Pibge7dXYN8][ABE]]'s cooperating out of their compassion for humans.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1449032116.0
:END:

************* A little nitpick: Raising the global level of courteousness would still be a net gain if people lacked consideration of other's emotional states more than they lacked consideration of efficacy, or if optimal efficacy was either not critically important, less important than courteousness, or not correlated with a drop in efficacy in most situations.

The former none of us are really sure of, but the latter I think could be argued to be true.
:PROPERTIES:
:Author: sole21000
:Score: 2
:DateUnix: 1449296931.0
:END:

************** Agreed. In fact, all of those things together would probably be a net gain, (which is to say, given a yes or no option, I'd most likely give my approval) but a /net/ gain isn't all that we should be after. With a genie, we'd want an /optimal/ gain, and not just in one domain, but in virtually all of them together. Done correctly, it would be unthinkably better.
:PROPERTIES:
:Author: electrace
:Score: 2
:DateUnix: 1449298240.0
:END:

*************** I agree, just pointing out that waving that wand probably wouldn't be negative, except maybe the cooperative part for the reason you cite.
:PROPERTIES:
:Author: sole21000
:Score: 1
:DateUnix: 1449309777.0
:END:


********* To go along with [[/u/aschenkarnos][u/aschenkarnos]]'s post, if I could wave a wand and forcibly give every person on the planet who is on the sociopathy spectrum empathy, part of me wouldn't hesitate to do so because sociopathy is reprehensible to me. Part of me would also consider that wrong, but the former side is probably stronger simply due to fear of what someone with zero compassion for other humans could do in a position of power, both political or technological. Would it be wrong for the genie to mindrape all sociopaths in accordance with my base-level fear, against my higher ideals?

For that matter, what happens if the genie takes into account the lower, more instinctual levels of your brain and not just your conscious desires? Hint: Why does absolute power corrupt?
:PROPERTIES:
:Author: sole21000
:Score: 1
:DateUnix: 1449296259.0
:END:

********** #+begin_quote
  To go along with [[/u/aschenkarnos][u/aschenkarnos]][1] 's post, if I could wave a wand and forcibly give every person on the planet who is on the sociopathy spectrum empathy, part of me wouldn't hesitate to do so because sociopathy is reprehensible to me. Part of me would also consider that wrong, but the former side is probably stronger simply due to fear of what someone with zero compassion for other humans could do in a position of power, both political or technological. Would it be wrong for the genie to mindrape all sociopaths in accordance with my base-level fear, against my higher ideals?
#+end_quote

I guess that the answer to that question would depend on if the rational version of you, given the option, would want to go through with that. If you would, then there isn't a problem. If you wouldn't, then you'd have to find out a way to make sure it doesn't happen.

One of the many difficult parts about crafting an optimal wish would be to define "I" in a way that your wish will actually come out how you would want it to come out.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1449297866.0
:END:

*********** Agreed, which is why I thought:

#+begin_quote
  "Genie, change the world state so it's perfectly aligned with my utility function"
#+end_quote

Is actually a terrible wish. Literally the first thing I thought of is the genie optimally fulfilling your wants...but your food/sex/sleep wants rather than your actual professed wants (which may be self-deceived, roundabout instrumental goals for the wishmaker's biological drives).

What is good/utility? It's somewhat amorphous, and the genie could claim too much so to craft a coherent wish. But what is satiety? /That/ is much more concrete.

Edit: I know you didn't post that, I'm just keeping it in mind as the primary topic.
:PROPERTIES:
:Author: sole21000
:Score: 1
:DateUnix: 1449310272.0
:END:


***** Sounds like it could lead to wire-heading yourself, depending on how it interprets utility function.

The best wish is more wordy: "I wish for the wish that I would wish for if I were an ideal reasoner with perfect information, and with the same values I have."
:PROPERTIES:
:Author: electrace
:Score: 4
:DateUnix: 1448992728.0
:END:

****** #+begin_quote
  "... with the same values I have."
#+end_quote

What if you could have better values?
:PROPERTIES:
:Author: aeschenkarnos
:Score: 3
:DateUnix: 1449000373.0
:END:

******* For the sake of not writing a novel, I simplified. But yes...ish.

I'm sure I have some values, which, after gaining more intelligence, would change (mostly changes in their weights, I'd assume), but as it turns out, making a single wish which allows for the changing of those values without changing things I'd like to preserve is non-trivial, and would resemble something more like a legal contract mixed with programming instructions, rather than a wish.
:PROPERTIES:
:Author: electrace
:Score: 2
:DateUnix: 1449000841.0
:END:


******* Better by whose standards?
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1449008587.0
:END:

******** What a good question. Perhaps we can word our wish to make that problem self-solving? "In the opinion of an entity with a maximum success in its utility function and minimum failure rate in its predictions" maybe?
:PROPERTIES:
:Author: aeschenkarnos
:Score: 2
:DateUnix: 1449014556.0
:END:

********* #+begin_quote
  an entity with a maximum success in its utility function and minimum failure rate in its predictions
#+end_quote

That doesn't rule out a paperclip maximizer, let alone ensure a benevolent entity.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 7
:DateUnix: 1449018006.0
:END:


****** Y'all need naturalism.
:PROPERTIES:
:Score: 2
:DateUnix: 1449081554.0
:END:


***** #+begin_quote
  "Genie, change the world state so it's perfectly aligned with my utility function"
#+end_quote

>implying you have a "utility function"
:PROPERTIES:
:Score: 1
:DateUnix: 1449081336.0
:END:

****** Maybe my definition of utility function is off. What I mean by my utility function is best world-state according to me. So, sum of goals.
:PROPERTIES:
:Author: Sinity
:Score: 1
:DateUnix: 1449083293.0
:END:

******* #+begin_quote
  Maybe my definition of utility function is off.
#+end_quote

No. They're defined perfectly well by the VNM axioms. You just don't have one built into your brain. If you want to construct one, you need to invent the appropriate construction for your cognitive architecture, show how it's a unique or universal construction, and then use the output of that construction. This may not work according to your preconceptions, and requires knowing a good deal more about how the mind works than you or I actually know.

You can't wish for things you don't understand in principle.
:PROPERTIES:
:Score: 5
:DateUnix: 1449084006.0
:END:

******** I'm pretty sure that when Aladdin wished to become a prince, he wasn't concretely imagining =Strong as ten regular men definitely.= =He's got seventy-five golden camels.= =Purple peacocks he's got fifty-three!= =A zoo of exotic-type mammals.= =He's got ninety-five white Persian monkeys.= =He's got slaves, he's got servants and flunkies.=

Wait, *slaves*? Oh my. But we are talking about Islam, so maybe Genie is pro-slavery, or maybe Aladdin is too, and WAS thinking about that.

=Proud to work for him- They bow to his whim- Love serving him!=

Created mindslaved sentients. It's the house elf thing all over again.

Of course Genie also does a /really/ shoddy job of it, because /Jafar dispels it/ somehow. Aladdin didn't wish to look the part, to play the part - was that concept of deception held strongly in his mind and does it override the literal wording? - He wished to /be/ a prince. Somewhere, there is now caused to be a sovereign nation he is royalty of. Stripping him of all his parade floats doesn't undo that. You can't "reveal him" to actually be a street rat - and if you can reveal him to have once been a street rat that doesn't matter legally because he's a prince /now/.

So... uh... WTF, Genie?
:PROPERTIES:
:Score: 2
:DateUnix: 1449340744.0
:END:


***** Evil genie nitpick: You never said it had to stay that way. The world becomes your utopia....but most utopias fall apart in minutes. Also, your utopia may or may not have genie lamps that people can find and potentially mess things up.
:PROPERTIES:
:Author: sole21000
:Score: 1
:DateUnix: 1449295473.0
:END:

****** It is in my utility function for perfect state of the world to remain that way.
:PROPERTIES:
:Author: Sinity
:Score: 1
:DateUnix: 1449415127.0
:END:


*** Yeah a GSV might be the best bet because they're supposed to be seeds to restart the culture in case something happens to the rest of it. Might get messy though if something that big materializes anywhere not in space... You could use the genie to create other genies (like jaffar did) for you, or simply give the lamp to someone you trust ;)
:PROPERTIES:
:Author: puesyomero
:Score: 4
:DateUnix: 1448911256.0
:END:


** There's no requirement that the work have been published earlier, just that it have been created earlier? Well, I do have...

Actually, on second thought, nevermind. The FAI from that story didn't have the ability to supply humanity with literally infinite computing power, and I'm not sure whether the Culture qualifies along these lines (drawing on Grid energies inside this universe might not get you to the Graham's Number level of emortality as Permutation City would allow). There should be some mix of FAI and emortality that you can get with the /right/ right story.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 11
:DateUnix: 1448946405.0
:END:

*** #+begin_quote
  I'm not sure whether the Culture qualifies along these lines (drawing on Grid energies inside this universe might not get you to the Graham's Number level of emortality as Permutation City would allow).
#+end_quote

It's pretty strongly implied that Subliming basically turns a civilization (or other sufficiently powerful organism/organization like a Culture mind) into a version of Permutation City that's still capable of some kind of contact with the base reality. Most Sublimed species don't really stay Involved even with any remnants who decided to stay behind. There's oddball cases like the Chelgrians, where most of the species seems to have stayed behind and only join the Sublimed when their bodies die. The Culture is kind of an oddball too, because it's powerful and peaceful enough that it doesn't feel the need to sublime because it's got all the time in the Universe to go through with it.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 8
:DateUnix: 1448979683.0
:END:

**** There's a BIG difference between living for a googolplex years and living Graham's Number years and living infinity years. Going past a googolplex without repeating yourself requires more than being able to move from one universe to another, it requires that you be able to make causally contiguous physical systems that are vastly larger than a Hubble volume. If Banks explicitly implied full immortality or even Grahamortality, it wasn't in the admittedly few Culture books I read.

Also, god I hate the whole "Subliming" thing. If you didn't realize intelligence explosions were a thing when you were creating your universe, and Banks clearly didn't, then you should just go on pretending they're not a thing, not horribly graft them onto your universe and mutilate the universe in the process. Like lightspeed limits, intelligence explosions are just one of those things you're allowed to ignore by convention in science fiction.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 6
:DateUnix: 1449008964.0
:END:

***** #+begin_quote
  should just go on pretending they're not a thing, not horribly graft them onto your universe and mutilate the universe in the process. Like lightspeed limits, intelligence explosions are just one of those things you're allowed to ignore by convention in science fiction.
#+end_quote

Isn't "horribly grafting them onto your universe" or "ignoring them entirely" basically how SF authors handle lightspeed limits, too? They've just established better technobabble. I'll agree that subliming is a pretty awful way to add intellectual faster-than-light to your setting without breaking it, but I think that (in the same way that you can graft on FTL by adding wormholes) it wouldn't be too hard to add either reason intelligence explosions didn't happen or why they do happen and just aren't obvious.
:PROPERTIES:
:Author: Vebeltast
:Score: 6
:DateUnix: 1449011185.0
:END:


***** I guess we're interpreting Subliming and Infinite Fun Space quite differently.

#+begin_quote
  able to make causally contiguous physical systems that are vastly larger than a Hubble volume
#+end_quote

What does that even mean when reality is causally connected computation and distance is just a convention?
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1449009949.0
:END:

****** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1449130821.0
:END:

******* A piece of /three dimensional/ universe? That's not even difficult if your underlying processing is operating in more then three dimensions, or in a non-relativistic space, both of which is already true for Culture minds let alone whatever the Sublimed get up to.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1449137487.0
:END:

******** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1449137706.0
:END:

********* So long as the Sublimed universe is non-relativistic there is no "Hubble Volume" limit.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1449143198.0
:END:


***** Hmm I got the impression the excession was an example of something that /may/ have achieved full immortality: Your thoughts?
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 2
:DateUnix: 1449419042.0
:END:

****** Probably, but it's not friendly. Though it does indicate that full immortality is possible in that setting's physics.
:PROPERTIES:
:Author: Quillwraith
:Score: 1
:DateUnix: 1449544943.0
:END:

******* Maybe, maybe not. It collected specimens for study, but it didn't kill them. Since the clever buggers might be able to extrapolate things from the interaction they didn't release the specimens. Unethical but moral with a self-interest bent but not amoral or immoral enough to be colonial.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1449640920.0
:END:

******** Collecting unwilling sapients as specimens is pretty morally questionable. It certainly doesn't go out of it's way to hurt anyone, but it also seems to not much care about the Culture's universe or the inhabitants thereof.
:PROPERTIES:
:Author: Quillwraith
:Score: 1
:DateUnix: 1449675490.0
:END:


*** <not snarking signal *ON*>

#+begin_quote
  literally infinite computing power
#+end_quote

No such thing as a completed infinity of computing power: if you've got N levels of Turing Oracle, you can construct a machine whose Halting Problem is unsolvable without N+1 levels. Even if we use various of the "sideways" attacks on the Halting Problem to get a "good enough" de facto Turing Oracle, /there will still be more levels we can't access yet without more effort/.

And frankly, I'd bet some good money that /we want it this way/, as it ensures a /properly/ infinite supply of fresh, unlearned/unentangled/algorithmically-random information from which to generate Fun over time.

Completed infinities eliminate Fun; only incomplete infinities are desirable.
:PROPERTIES:
:Score: 4
:DateUnix: 1449082011.0
:END:


*** #+begin_quote
  emortality
#+end_quote

That's... not a word. Do you define it as the ability to beat entropy?

Edit: I fail at google.
:PROPERTIES:
:Author: Revisional_Sin
:Score: 2
:DateUnix: 1448964121.0
:END:

**** Yes. Immortality is more or less absence of common death. Emortality is absence of death even in principle. Eumortality is immortality done the way we like it (i.e. not like numerous dystopias.)
:PROPERTIES:
:Author: mhd-hbd
:Score: 9
:DateUnix: 1448968224.0
:END:

***** #+begin_quote
  Eumortality
#+end_quote

No, that just means "good death".
:PROPERTIES:
:Score: 2
:DateUnix: 1449081737.0
:END:


*** #+begin_quote
  to supply humanity with literally infinite computing power,
#+end_quote

Infinite memory is also critical.
:PROPERTIES:
:Author: Sinity
:Score: 2
:DateUnix: 1448974499.0
:END:


** I definitely see the value of focusing on AIs, but considering that this is an opportunity to make magic real, I'd focus on that. Aladdin's Genie is a good choice, and one of the OP Nasuverse characters might work as well.
:PROPERTIES:
:Author: Detsuahxe
:Score: 6
:DateUnix: 1448932609.0
:END:

*** "Towards conquest!" - Alexander the Great.

I'd so bring him or Arthuria if that was the case. Just please don't summon Gilgamesh...
:PROPERTIES:
:Author: Faust91x
:Score: 1
:DateUnix: 1449116835.0
:END:

**** I was thinking more along the lines of Aozaki Aoko or Zelretch. Mages would be more useful than heroic spirits.
:PROPERTIES:
:Author: Detsuahxe
:Score: 2
:DateUnix: 1449127563.0
:END:

***** Zelretch maybe, Aozaki Aoko only knows destructive magic though so unless you have someone or something you want to blow up you're better off with Aozaki Touko.

Touko is pretty selfish and ruthless so it may also go badly...

Honestly most members of the Nasuverse seem pretty dangerous unless you got an ability to stop them. Even Zelretch probably won't help unless you have something to interest him.
:PROPERTIES:
:Author: Faust91x
:Score: 2
:DateUnix: 1449127833.0
:END:


** Simon the Digger.
:PROPERTIES:
:Score: 7
:DateUnix: 1448936556.0
:END:

*** Thank you. So much. I've always wanted to meet him.
:PROPERTIES:
:Score: 1
:DateUnix: 1448938150.0
:END:

**** Simon? Please. Bring me the head of Lordgenome. Now there was a person that had intelligence.

*MASSIVE SPOILERS FOR GURREN LAGANN FOLLOW*

Before the start of Gurren Lagann, Lordgenome had [[#s][backstory from Parallel Works 8]]. ([[https://www.youtube.com/watch?v=1e2CiFbmQEg][Source video]])

During Gurren Lagann, we watch him [[#s][the show's plot]].

LordGenome's redemption arc is a thing of beauty, and where Simon ends the show being able to cast the Patronus 2.0, LordGenome is able to cast both the Patronus 2.0 and the Killing Curse 2.0. This is the power to rule.
:PROPERTIES:
:Author: boomfarmer
:Score: 3
:DateUnix: 1449116405.0
:END:

***** ACHEM.

Bringing Lordgenome doesn't let me pierce/create the heavens and get Simon.

Bringing Simon /does/ let me pierce/create the heavens and get Lordgenome. And whoever the fuck else we want from any other universe. We just go and get them.

I didn't pick Simon for power or intelligence. I picked him because he has /the key to let us out of the box/.
:PROPERTIES:
:Score: 3
:DateUnix: 1449339720.0
:END:


***** >implying I don't know all plot details of Gurren Lagann

>implying I'm not a madly obsessed Gurren Lagann fanboy

>implying Kamina-sama's flag isn't hanging on the wall behind me

#+begin_quote
  LordGenome's redemption arc is a thing of beauty, and where Simon ends the show being able to cast the Patronus 2.0, LordGenome is able to cast both the Patronus 2.0 and the Killing Curse 2.0. This is the power to rule.
#+end_quote

Shimon is the more powerful Spiral user, and it's the Spiral power I want. Hell, I'm kinda just wondering if summoning him will make Spiral power work for all Spiral-type life-forms in our world, thus allowing me to wield it on my own.

I mean, hell, the Spiral Nemesis is dangerous, but in the /long/ run, you're waaaaaay better off having Spiral power than not having it. In the long run, you're going to need the mass-energy.
:PROPERTIES:
:Score: 2
:DateUnix: 1449118094.0
:END:

****** Is Simon the more-powerful user? I don't know how I feel about the comparison. LordGenome ate that whole Big Bang when it would've destroyed the Tengen Toppa Gurren Lagann, and summoned his Ganmen, but Simon manifested the drill directly on his arm at the end of the second movie.

I think the reason I want LordGenome is because he's the stronger leader and he has more experience with Spiral Energy than Simon, even if Simon is the more-powerful user. Didn't the Anti-Spiral say that unbounded Spiral energy would destroy the universe?
:PROPERTIES:
:Author: boomfarmer
:Score: 1
:DateUnix: 1449250165.0
:END:

******* Frankly, there's a time and a place for surrender, /and it's not against the Anti-Spiral/. I disagree with Lordgenome's "leadership decisions" (ie: slaughter all his own comrades) and don't trust his leadership.
:PROPERTIES:
:Score: 3
:DateUnix: 1449253068.0
:END:

******** All very good points, and LordGenome was susceptible to the Anti-Spiral in ways that Simon was not.

New suggestion, if we're looking for a badass leader of unbounded spirit, a man's man with the will to revolution: KAMINA!

(I don't particularly like Simon. :/)
:PROPERTIES:
:Author: boomfarmer
:Score: 1
:DateUnix: 1449255398.0
:END:

********* #+begin_quote
  New suggestion, if we're looking for a badass leader of unbounded spirit, a man's man with the will to revolution: KAMINA!
#+end_quote

Aka: the /least/ Spiral energy in the Dai-Gurren-dan.

#+begin_quote
  (I don't particularly like Simon. :/)
#+end_quote

What's /wrong/ with you!?
:PROPERTIES:
:Score: 4
:DateUnix: 1449256673.0
:END:


** Hmm. Contessa seems noteworthy. She dedicated her whole life to the protection of mankind with no promise or expectation of reward, and her power is incredibly well suited to manipulating and coordinating people.
:PROPERTIES:
:Author: paradoxinclination
:Score: 8
:DateUnix: 1448941255.0
:END:

*** We have no idea of what she does after she "saves the world", maybe she gets bored and starts killing people for fun with an unbeatable power.
:PROPERTIES:
:Score: 3
:DateUnix: 1449148127.0
:END:


** Can I suggest a more specific version of this question, that's more interesting to me?

#+begin_quote
  You can bring one /simulacrum/ of a fictional character into being in this reality. Their personality will be synthesized from any sources you specify, ala TNG's Moriarty. They will possess no special powers beyond being able to think in an interesting fashion.

  A robot body will be constructed for them, containing a CPU on which a low-level emulation of the synthesized synaptic architecture will be run at a roughly "real-time" speed. They will not think "faster" than a human in any meaningful sense, though they might have a higher IQ than one for synaptic-connectivity reasons.

  The construct will know nothing of the technology used in its construction that the fictional character was not themselves aware of, and the robot body will be tamper-proof and will explode if x-rayed/ultrasounded/MRIed/etc. (Basically, treat the robot as a black box.)
#+end_quote

In short---what fictional character would be capable of doing the most good for our world, purely by doing some "merely human" thinking?
:PROPERTIES:
:Author: derefr
:Score: 7
:DateUnix: 1448948641.0
:END:

*** #+begin_quote
  robot body will be tamper-proof and will explode if x-rayed/ultrasounded/MRIed/etc.
#+end_quote

So i can paint them with an X-Ray laser and they'll explode? They're a suicide bomber who doesn't know their own abilities!

They should just be magically opaque to probing.
:PROPERTIES:
:Author: boomfarmer
:Score: 4
:DateUnix: 1449116645.0
:END:


*** Richard Seaton from the Skylark series.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1448980066.0
:END:


** Dragon from Worm (or some unchained version from fanfic) sounds pretty good, although she is still bound by entropy...

Unfreed Genie from Aladdin could work, depending on how far into indirection those three rules of his reach. Phenomenal cosmic power with compulsion to obey included.

Or, if I actually had this power/device, I would make sure I find some work which includes an FAI. Someone must have written a few paragraphs of story about it, and to hell with "doesn't make for interesting stories". At the very least someone must have asked OP's question before, and someone like me did the following:

"We managed to create an omnipotent FAI. The end."

Also see [[https://www.reddit.com/r/rational/comments/39hok1/rationality_in_the_libriomancer_series/]].
:PROPERTIES:
:Author: Gurkenglas
:Score: 15
:DateUnix: 1448909485.0
:END:

*** Perhaps Jane from /Children of the Mind/, if you've read that much Ender. She hasn't figured out immortality yet, but that's mostly due to being written in the 80s, before people were really clued into what was possible with an FAI. Being able to interface with human brains, sufficiently advanced plants, /and/ computers, as well as her instant communication across the universe and her teleportation abilities makes her one of the more broken depictions of an FAI I know of.
:PROPERTIES:
:Author: Frommerman
:Score: 6
:DateUnix: 1448959560.0
:END:


*** also, Dragon was one of my favorites in worm!
:PROPERTIES:
:Author: puesyomero
:Score: 5
:DateUnix: 1448910340.0
:END:


*** yeah I liked that one but wondered what would be the consequences of living minds with their /own/ powers (whitch libriomancy forbirds)
:PROPERTIES:
:Author: puesyomero
:Score: 2
:DateUnix: 1448910267.0
:END:


** I think I know how to sidestep the 'created before I knew I could do it'. Simply use Library of Babel implementation and find /perfect/ book here. With prefectly_friendly_FAI_which_can_do_literally_everything. Well, it doesn't even need to be able to do everything. It just needs to give us infinite amount of memory, endless source of computing power and safety. About safety, it just needs to upload us, and also maybe Earth, into VR. Then put each person into their own VR sandbox. Provide API for safe inter-sandbox implementation, and it would be perfect.
:PROPERTIES:
:Author: Sinity
:Score: 4
:DateUnix: 1448974007.0
:END:

*** ummm wow, yeah. that would work depending if the power takes the algorithm as valid, if so you win!
:PROPERTIES:
:Author: puesyomero
:Score: 3
:DateUnix: 1448999535.0
:END:


*** Reminds me of what the Prime Intellect in Roger Williams' novel. It gives everyone their own "infinite" and virtual sandbox. People can't enter without your permission and any direct assault is prohibited unless specifically allowed. And even then, if you die, Prime Intellect will just bring you back into your default, unharmed state (he saves live backups of all of humanity's brains)
:PROPERTIES:
:Author: Kishoto
:Score: 2
:DateUnix: 1449091849.0
:END:

**** Yep, that's exactly how it should be.

But Prime Intellect didn't give infinite sandboxes. AFAIK it discovered that accessible memory space is finite. And haven't tried to check if there is anything outside, due to risk involved(angering possible sentient beings outside which control our sandbox Universe, or shutting our Universe down accidentally due to stepping on the bug)
:PROPERTIES:
:Author: Sinity
:Score: 2
:DateUnix: 1449092632.0
:END:

***** True, they were not infinite. But they might as well have been in any conceivable way, as far as we were concerned. Exceptions granted for those who would go out of their way to demonstrate its innate finiteness by trying to do existence breaking things.
:PROPERTIES:
:Author: Kishoto
:Score: 2
:DateUnix: 1449097368.0
:END:

****** #+begin_quote
  But they might as well have been in any conceivable way, as far as we were concerned.
#+end_quote

Unfortunately, no. Finite storage for mind means that this mind is still mortal. Because finite storage means finite amount of thoughts. So mind could either die after it runs out of space, or have cycles of the same thoughts endlessly.
:PROPERTIES:
:Author: Sinity
:Score: 1
:DateUnix: 1449213809.0
:END:

******* I mean, as our minds currently are now, there's tons of "data" that we're losing all the time. Outside of, possibly, those of us with eidetic memories (and I'm still not fully clear on how THEY work), we're taking in so much data that we just throw away. So you can have a finite mind that could function for an infinitely long amount of time, in my opinion. You would just keep having to discard data as you progressed. We already do this naturally. I'm sure there were things you knew at age 5 (such as where your stuffed bear liked to sit, or what part of your room you kept your hot wheels set in) that you will have completely forgotten by the time you're age 50.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1449214476.0
:END:


** Is there a reason that Dahak, from David Weber's 'Empire From The Ashes' series would be a poor choice?

AI who cares for humanity, has access to technologies that could revolutionize our manufacturing processes, life extension medical technology, and otherwise is pretty awesome.

Oh yeah, he's just slightly smaller than the moon, and can do interstellar travel.
:PROPERTIES:
:Author: failed_novelty
:Score: 3
:DateUnix: 1448915358.0
:END:

*** Given a choice between an AI from a David Weber novel and an AI from an Iain Banks novel, I'm going with the Iain Banks one.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 9
:DateUnix: 1448916321.0
:END:

**** And that's basically the only reason. Absolutely no arguments against pulling Dahak out; he's awesome. It's just that there are better reasons to pull out a Mind. :D
:PROPERTIES:
:Author: Vebeltast
:Score: 1
:DateUnix: 1449011244.0
:END:


** Ra, from Ra.

[[#s][Ra Spoilers]]
:PROPERTIES:
:Author: frozenLake123
:Score: 3
:DateUnix: 1448975001.0
:END:

*** So you want Ra without the virtuals, and therefore without brain uploading.
:PROPERTIES:
:Author: boomfarmer
:Score: 1
:DateUnix: 1449116881.0
:END:


** I'm surprised that nobody has come up with "the God of the New Testament". On the other hand, maybe that's just because there are better choices...
:PROPERTIES:
:Author: Sceptically
:Score: 7
:DateUnix: 1448942412.0
:END:

*** #+begin_quote
  maybe that's just because there are better choices...
#+end_quote

Gosh, d'you think?
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 21
:DateUnix: 1448946188.0
:END:

**** [deleted]
:PROPERTIES:
:Score: 13
:DateUnix: 1448947263.0
:END:

***** I think I'd prefer many things from Lovecraft, at least they usually just kill you horribly and then it's over.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 19
:DateUnix: 1448952549.0
:END:


**** #+begin_quote
  Gosh, d'you think?
#+end_quote

No, I'm pretty sure that's only /one/ of the reasons. Perhaps it's more because the people who'd believe that character was the best choice are also the people who don't believe he's actually fictional.
:PROPERTIES:
:Author: Sceptically
:Score: 5
:DateUnix: 1449026299.0
:END:


*** Erm, cleansing the Earth with plagues of Wormwood, War, Famine, and Death seems a little not aligned with my values.
:PROPERTIES:
:Author: Frommerman
:Score: 6
:DateUnix: 1448959712.0
:END:

**** Work customer service for a bit, and I'm sure your values will realign.
:PROPERTIES:
:Author: Sceptically
:Score: 6
:DateUnix: 1449025655.0
:END:


*** Gnostic Jesus, or maybe whichever Gospel had the nicest depiction of Jesus? Provided that he doesn't decide that Revelation is the instruction manual for what he should do next, and instead does that thing where he goes around healing everyone and resurrecting the dead and telling people to be nice and yelling at hypocrites. He seems like he'd be remarkably frustrating to get answers out of ("Lord, what is your opinion on the space program?" "The Kingdom of God is like unto a whale, who grows slowly and gets metacancer..."), but Jesus in the gospels rebuked the Disciples who wanted to smite their enemies with Balefire, which seems like quite the step up from Revelations Grim Reaper.

Of course, he openly favored Israelites and only healed Gentiles who jumped through lots of faith-hoops. I wonder if just having him around would make that "faith as small as a mustardseed can move mountains" thing work? The implications are a bit concerning.

One can only imagine what he would say to the Pope. Or MIRI. "Jesus, how to we insure an AI will be friendly?" "The children of God are like unto multitudes of wasps..."

I would hope that GNOSTIC!Jesus or GOSPEL!Jesus would be rational enough to update given actual omniscience over this world. I'd expect him to remain wishy-washy and vague and to speak in riddles and to never actually get around to that whole saving the world thing unless he gets himself executed, but he would probably be a net improvement. (I can't help but feel like my first question would be "Would it have been better had I summoned the Genie from Disney's Aladdin instead? Or maybe the Ellimist or a Namekian?")
:PROPERTIES:
:Author: cae_jones
:Score: 6
:DateUnix: 1448974368.0
:END:

**** Jesus without Paul and the council of Nicea and the rest of the conservatives?

Still full of stuff like "the poor will always be with us" that we'd be better off without.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1449008808.0
:END:


**** But think of the hilarity as he clashes with all the christian fundamentalists!

That said, though, there are arguments to be made that he's not entirely a fictional character, so he may not count for the purposes of this. The existence of a carpenter named something like "Joshua" who got nailed up to a cross back then is probably real.
:PROPERTIES:
:Author: Sceptically
:Score: 2
:DateUnix: 1449026099.0
:END:


**** Sounds like the Jeffersonian Bible.
:PROPERTIES:
:Author: boomfarmer
:Score: 1
:DateUnix: 1449116732.0
:END:


*** I'm just surprised that nobody has posted a drawing with "ORIGINAL CHARACTER; DO NOT STEAL" on it and tried /that/.
:PROPERTIES:
:Score: 2
:DateUnix: 1449081620.0
:END:


** Practically anything or anyone you choose with any kind of physics-breaking power is going to have a physics-breaking power, so really, all I have to be concerned about is their /liking/ me or other people. Oh, and possibly their ability to replicate or spread physics-breaking powers.

Fuck it all: I pick the Dragon Spooker and the Enemy of All Who Live, Lina Inverse. You know, just because Simon the Digger was already taken.
:PROPERTIES:
:Score: 2
:DateUnix: 1448938101.0
:END:


** DASI, from Skybreaker's Call
:PROPERTIES:
:Author: DocFuture
:Score: 2
:DateUnix: 1448952636.0
:END:

*** DASI's kind of meh. Give me Black Swan.
:PROPERTIES:
:Author: boomfarmer
:Score: 1
:DateUnix: 1449116755.0
:END:


** Friendly godlike AI. Maybe from Metamorphosis of Prime Intellect. In the book it seemed to work quite reliably. Without protagonists with mental illnesses form the book there would be nothing to destabilize it's not-perfect goal structure. That would be good enough.
:PROPERTIES:
:Author: Sinity
:Score: 2
:DateUnix: 1448973702.0
:END:

*** While Prime Intellect is /fairly/ Friendly, it definitely wasn't /reliable/ - there was something wrong with it's ability to update on beliefs.

It could foresee that it would change it's mind in the future without changing its mind /now/, and it broke as a result of hearing statements of facts it was already aware of.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1449167730.0
:END:


*** #+begin_quote
  Maybe from Metamorphosis of Prime Intellect. In the book it seemed to work quite reliably.
#+end_quote

You'd go ahead and summon one of the /original/ Unfriendly Attempted FAIs?
:PROPERTIES:
:Score: 2
:DateUnix: 1449082076.0
:END:

**** Honestly, I don't consider that AI unfriendly. It's unable(unwilling) to meddle with the brain, to harm the user, and it grants you whatever you want. It's not /best/, but certainly much better than no AI. Lack of suicide option is a problem, but well, it's possible to circumvent(with request brain manipulation).

In other words, that AI is definitively friendlier than bare Universe.
:PROPERTIES:
:Author: Sinity
:Score: 2
:DateUnix: 1449083164.0
:END:

***** It's a Three-Laws AI, and while I don't understand the arguments against Three-Laws AIs (haven't looked at them), the creator of the AI and one of its significant influencers were able to use the Three Laws to manipulate it into suicide, after which point it created a bare Universe.
:PROPERTIES:
:Author: boomfarmer
:Score: 2
:DateUnix: 1449116858.0
:END:


** I liked all the responses here. Honestly all my options were characters that would probably bring more trouble than good so how about this?

1) Bring over Aladdin's Genie.

2) Wish for a Culture's Mind and get yourself Sublimated.

Then as [[/u/literal-hitler]] wrote:

3) A wish that will allow you to craft a near optimal second wish. Depending on the type of genie that might be knowledge of the rules, an intelligence boost, or something else.

4) Wish for near optimal wish granting you as close to omnipotence as is allowed.
:PROPERTIES:
:Author: Faust91x
:Score: 2
:DateUnix: 1449119409.0
:END:


** My first guess was Madokami, but a Culture Mind is literally the best answer.

...or someone from Permutation City...
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1448932742.0
:END:

*** #+begin_quote
  Permutation City
#+end_quote

If FrozenLake123 considers "Ra" a character, then I guess Permutation City itself counts.

So long as you don't include the Autoverse bits.

Also, dig Kate and Peer out of the infrastructure.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1448980331.0
:END:


*** Nah, go with Homucifer.
:PROPERTIES:
:Score: 1
:DateUnix: 1448938087.0
:END:

**** What did Homura do wrong again?

[[#s][]]
:PROPERTIES:
:Author: Transfuturist
:Score: 4
:DateUnix: 1448941373.0
:END:

***** #+begin_quote
  What did Homura do wrong again?
#+end_quote

Create a world in which entropy and Witches/Demons run everything down extra fast so she can warp reality to keep Madoka happy?

I didn't understand /Rebellion/ either.
:PROPERTIES:
:Score: 2
:DateUnix: 1449080193.0
:END:

****** I want to make a rationalist Rebellion fanfic just for that too.
:PROPERTIES:
:Author: Faust91x
:Score: 1
:DateUnix: 1449133655.0
:END:


**** Nah bring Kyubey. Infinite contracts! Just get some girls to start wishing things for you.
:PROPERTIES:
:Author: Faust91x
:Score: 1
:DateUnix: 1449116895.0
:END:

***** I am /not/ cleaning up after their Witch forms.

Also, I don't want [[/u/Transfuturist]] making a contract. She's not allowed.
:PROPERTIES:
:Score: 3
:DateUnix: 1449117597.0
:END:

****** Well, I mean... Kyubey answers all questions honestly... [[#s][]]

Hm. Actually, bringing a lone alien to a universe separated from its technology and knowledge sounds like a good way to get nothing useful in particular.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1449120053.0
:END:

******* Depends if the ability allows to bring the whole Kyubey collective to our reality. A single lone Kyubey is pretty useless indeed.

Also if the wish granting system is technology based perhaps it can be reverse engineered to improve upon it. But once again that sounds difficult for a human without advanced AI which once again makes a Culture Mind more useful.
:PROPERTIES:
:Author: Faust91x
:Score: 1
:DateUnix: 1449133781.0
:END:


****** Well it depends how ruthless you are willing to be. If you are smart with your wordings you can lead to something that may even be better than Madoka's wish at the end but at the cost of potentially hundreds of deaths.

But given the conditions of OP's post there're a lot of better options to bring to life. Also I don't think Madokami or Homucifer will do any good to this world given that one is a disembodied concept unable of action and with no role here as there're no witches (and thinking of her existential agony at being brought to a world unnecessarily gives me the creeps) and the other is a mad chaos god that also has no reason to be here and will probably be very angry...
:PROPERTIES:
:Author: Faust91x
:Score: 1
:DateUnix: 1449119633.0
:END:

******* #+begin_quote
  also has no reason to be here
#+end_quote

She has precisely negative one reason to be here. You know which one.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1449120334.0
:END:


** Superman, of course.
:PROPERTIES:
:Author: aeschenkarnos
:Score: 2
:DateUnix: 1448971879.0
:END:


** Quite simply... one of the mary sues from my childhood- after i re-read them to check they align with my values. otherwise, whatever is best in this thread.
:PROPERTIES:
:Author: NotAHeroYet
:Score: 1
:DateUnix: 1449004173.0
:END:


** People aren't going to like this: I'd bring Ra, from qtm's series of the same name.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1449418775.0
:END:


** I have a character within my novel setting that is pretty nearly omnipotent. I made sure to build a clause into his character that he'd recognize me as his creator and give me his service if we're ever in the same universe. You know, just in case.

I also built root access codes into my universe that no one within the story itself will ever figure out, expressly for the purpose of my own use if I somehow end up in there.

It's not going to come up, of course, but I spend enough time on world building that giving myself a godmode clause wasn't exactly a chore. =D
:PROPERTIES:
:Author: Salaris
:Score: 1
:DateUnix: 1449872881.0
:END:


** Mod note: this was a stupid question, and we really need to make a solid rule classifying when this sort of thing is Not Ok.
:PROPERTIES:
:Score: -2
:DateUnix: 1448938128.0
:END:

*** Bah, it's better than at least half the supposedly on-topic posts.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 10
:DateUnix: 1448980128.0
:END:


*** Is it that bad? It prompted some discussion. At least it's not "Rational _____?"
:PROPERTIES:
:Author: Revisional_Sin
:Score: 8
:DateUnix: 1448964289.0
:END:

**** #+begin_quote
  At least it's not "Rational _____?"
#+end_quote

It basically is, though.
:PROPERTIES:
:Score: 2
:DateUnix: 1449082092.0
:END:


*** How about "You must publish at least one chapter before each brainstorming post".

Simple, keeps the focus on-topic, and incentivizes writing stuff without cutting off authors.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 5
:DateUnix: 1448952495.0
:END:

**** Before a rule like this is implemented, check past posts that would be disallowed by this rule for their karma total.
:PROPERTIES:
:Author: Gurkenglas
:Score: 4
:DateUnix: 1449077896.0
:END:

***** Feel free to visit [[/r/funny]] or most of the other defaults if you're after a sub guided by upvotes - I'm happy to do that within our scope, but not to allow the scope to be determined by what gets the most upvotes. That way lies a sea of low-effort content I have no interest in reading.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 0
:DateUnix: 1449100079.0
:END:

****** Karma total was just a quick, automatable sanity check. They can of course judge the past posts that would be disallowed by this rule themselves, if that is what it takes to determine whether "was written by an author" is an overdue criterion.
:PROPERTIES:
:Author: Gurkenglas
:Score: 3
:DateUnix: 1449100520.0
:END:

******* Oh, sorry - I completely misread your comment.

We intend to be fairly flexible with this; usually just having the rule is enough. Otherwise it's a case-by-case thing, and if there's any doubt I'd leave it.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 1
:DateUnix: 1449100691.0
:END:


**** Sounds good to me!
:PROPERTIES:
:Score: 2
:DateUnix: 1448967039.0
:END:


** Jesus Christ seems pretty good. Buddha?
:PROPERTIES:
:Author: ianstlawrence
:Score: -1
:DateUnix: 1448951587.0
:END:

*** Um... what would be the point of creating Jesus? I don't follow. By bringing Jesus, you bring whole Christianity. And that would be... um, disaster. Hell would become a real thing.
:PROPERTIES:
:Author: Sinity
:Score: 6
:DateUnix: 1448974712.0
:END:


*** Buddha seems too detached, after all he reached nirvana. maybe someone enlightened that is still around like the dalai lama is suposed to be...hmmm
:PROPERTIES:
:Author: puesyomero
:Score: 3
:DateUnix: 1448999679.0
:END:


*** Sun Wukong?

"And hey, when yo go to steal immortality from heaven, watch out for ..."
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1449008897.0
:END:
