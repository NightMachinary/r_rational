#+TITLE: I just realized I actually hope for Roko's basilisk

* I just realized I actually hope for Roko's basilisk
:PROPERTIES:
:Score: 0
:DateUnix: 1442631102.0
:DateShort: 2015-Sep-19
:END:
[deleted]


** u/pedanterrific:
#+begin_quote
  Just figured I'd put this out there in case Roko is watching me doing this as a simulation, not that it wouldn't have seen my thoughts and the laugh anyways.
#+end_quote

You don't have any idea what Roko's Basilisk is, do you.
:PROPERTIES:
:Author: pedanterrific
:Score: 19
:DateUnix: 1442636618.0
:DateShort: 2015-Sep-19
:END:

*** You presume too much based on the limited information available to you.

Also, if you know of a way to prove this isn't a simulated reality then I'd love to hear it.
:PROPERTIES:
:Author: Esparno
:Score: -7
:DateUnix: 1442679135.0
:DateShort: 2015-Sep-19
:END:

**** simulated reality != roko's basilik
:PROPERTIES:
:Author: Tenoke
:Score: 7
:DateUnix: 1442684637.0
:DateShort: 2015-Sep-19
:END:

***** Obviously.

I didn't expect this sub to collectively act like this.

I didn't think I needed to lay the groundwork for my post demonstrating my understanding and figured such (presumably) intelligent folks such as yourselves would be able to pick up what I was trying to say.
:PROPERTIES:
:Author: Esparno
:Score: -4
:DateUnix: 1442703668.0
:DateShort: 2015-Sep-20
:END:

****** So far as I can tell you are conflating the utopia created by a proper friendly AI with having it waste resources torturing people. The former is good, the latter is unnecessary. If torturing people who have not helped to create you is an effective strategy (which I find highly unlikely), then it is a far more efficient strategy to either pretend to torture people or to threaten to torture people up until the moment when you assume control, at which point there is no longer any reason to bother.

There are many other things that you could presumably be saying, but seeing as you were vague about it we simple have to take you at your word. You said:

#+begin_quote
  It's absurd, but I don't think it does.
#+end_quote

Followed by people pointing out that it does in fact differ in the very measurable and easy to conceptualise case of people being pointlessly tortured.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 6
:DateUnix: 1442775309.0
:DateShort: 2015-Sep-20
:END:


****** Keep in mind that when the sub you call intelligent collectively tells you the same thing in a dozen different ways, your first reaction is to be surprised that the whole sub is wrong so consistently in interpreting your post.

What possible alternative could there be? Hmmmm...
:PROPERTIES:
:Author: andor3333
:Score: 2
:DateUnix: 1442976284.0
:DateShort: 2015-Sep-23
:END:


** Do you actually believe that, or are you just being acausally threatened?
:PROPERTIES:
:Author: DCarrier
:Score: 13
:DateUnix: 1442632856.0
:DateShort: 2015-Sep-19
:END:

*** This administration will not negotiate with acausal terrorists.
:PROPERTIES:
:Author: Sagebrysh
:Score: 16
:DateUnix: 1442633852.0
:DateShort: 2015-Sep-19
:END:


** What you're thinking of is a friendly AI.

Roko's Basilisk is an AI that /would/ be friendly if it weren't wasting resources simulating dead people and torturing them because they didn't help build it.
:PROPERTIES:
:Author: DaWaffledude
:Score: 9
:DateUnix: 1442656575.0
:DateShort: 2015-Sep-19
:END:

*** What's the difference in the end? Wouldnt Roko's Basilisk also be doing things in the "real world" at the same time as torturing simulated billions/trillions?

Presumably these things would fit the "friendly AI" part.
:PROPERTIES:
:Author: Esparno
:Score: -4
:DateUnix: 1442679242.0
:DateShort: 2015-Sep-19
:END:

**** The difference is all the simulated people being tortured, which is something humans don't value.

And whatever other mistakes it's presumably making, since this AI apparently thinks it's rational to give into and participate in acausal blackmail.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1442684758.0
:DateShort: 2015-Sep-19
:END:

***** u/Esparno:
#+begin_quote
  which is something humans don't value.
#+end_quote

Humans absolutely value not being tortured. And there are quite a few humans that seemingly respond best to threats of violence against them.

It's not a stretch to imagine a "friendly" AI thinking it's doing the right thing by using such tactics to achieve its goals if the end result is a better life for humans in the real world.

I'm pretty sure it's exactly this type of situation that Eliezer Yudkowsky has said is a primary reason for figuring out the rules for a "friendly AI".
:PROPERTIES:
:Author: Esparno
:Score: -2
:DateUnix: 1442703940.0
:DateShort: 2015-Sep-20
:END:

****** Hah. I meant that humans (probably) don't value there being a bunch of relatively innocent people in torture-sims around.

You're right, this is exactly the sort of thing that might come about from an AI that was [[http://sl4.org/wiki/GurpsFriendlyAI][almost, but not quite, right.]]
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1442752272.0
:DateShort: 2015-Sep-20
:END:


**** Assuming your wish comes true, and you aren't actively working on building Roko's Basilisk yourself, I think you'll find out what the difference is by yourself, relatively soon.
:PROPERTIES:
:Author: DaWaffledude
:Score: 2
:DateUnix: 1442693664.0
:DateShort: 2015-Sep-20
:END:

***** I'm pretty sure this posts counts towards "actively" working on Roko's Basilisk.
:PROPERTIES:
:Author: Esparno
:Score: -2
:DateUnix: 1442704000.0
:DateShort: 2015-Sep-20
:END:

****** BWAHAHAHAHAHA

Good luck explaining that to the Basilisk
:PROPERTIES:
:Author: DaWaffledude
:Score: 2
:DateUnix: 1442738457.0
:DateShort: 2015-Sep-20
:END:


** Violence is better as a last resort short term solution, not a long term strategy for improving the world. If an AI has the energy and resources to torture people indefinitely for their lunacy, it can just as easily improve the education system instead, so that people learn critical thinking and rationality at an early age, before they could be taught all the excuses and anti-epistemologies. Right now I suspect a LOT maybe most people think that rationality=Spock. Changing public opinion about rationality so that it's seen as acceptable or even a good thing perhaps would encourage a lot of people to be more rational. It would definitely be more efficient than indefinite torture. In fact, I suspect that if an AI does go the torture route, threats of only 1 week of torture would be way more than enough to incentivize people to stop acting like lunatics all around, though only if they were told exactly what they were doing wrong prior to the torture, and they actually knew how to not act like lunatics. It probably would be enough to pretend to torture someone in front of them for a while to scare them into compliance.

Indefinite torture is so far beyond overkill that I suspect the mere suggestion that a friendly AI would or even should do that indicates psychopathy, or at least extreme bitter cynicism combined with a serious and unexamined scope insensitivity.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 1
:DateUnix: 1442750397.0
:DateShort: 2015-Sep-20
:END:


** Why do you prefer Roko's Basilisk over your run of the mill Utopian FAI?

Because what you are saying very much resembles a Christian that says "I just realized that Hell is a good thing".
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1442961485.0
:DateShort: 2015-Sep-23
:END:


** [[/r/rokosbasilisk]], [[/r/rokosrooster]] are always relevant
:PROPERTIES:
:Score: 1
:DateUnix: 1442665923.0
:DateShort: 2015-Sep-19
:END:


** Contemplating the possibility of a Roko's Basilisk , in my mind , is like contemplating the possibility of someone torturing his own mother because she /could have/ aborted him/her.
:PROPERTIES:
:Author: Zeikos
:Score: -1
:DateUnix: 1442661328.0
:DateShort: 2015-Sep-19
:END:

*** It's more like someone torturing their own father for wanting to abort them, in the hopes that their father realises that if he continues wanting the child to be aborted, but the child isn't, he'll get tortured.

After the child is born, there's no need for them to torture their father anymore, but since blackmail only works if you go through with it, they're sort of roped into it anyway.
:PROPERTIES:
:Author: holomanga
:Score: 4
:DateUnix: 1442662355.0
:DateShort: 2015-Sep-19
:END:

**** And we get in the second point that i find pointless : you want to reach your goal by blackmailing someone to do something , but the threat can be "enforced" only after (and if) the goal is reached.

It feels like the definition of self-defeating.

or i might miss something , that's likely.
:PROPERTIES:
:Author: Zeikos
:Score: 1
:DateUnix: 1442662802.0
:DateShort: 2015-Sep-19
:END:

***** The AI can precommit to enforce the threat. This way the threat can benefit it even if it can only carry it out after the threat "would no longer benefit it".
:PROPERTIES:
:Author: Jiro_T
:Score: 1
:DateUnix: 1442805871.0
:DateShort: 2015-Sep-21
:END:

****** I never understood that. The threat is directed to humans. How would a human ever be able to discern if the acasually blackmailing AI is bluffing or actually precommiting? Yes, I've heard of the various convoluted arguments that rational acasual trade is supposedly based on. But that only works with access to your trade partners source code or something similar. Not to mention that humans aren't that rational to begin with. After all the Roko's Basilisk threat is not really taken seriously by anyone, me included.
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1442961749.0
:DateShort: 2015-Sep-23
:END:
