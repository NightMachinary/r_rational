#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 16
:DateUnix: 1442243087.0
:DateShort: 2015-Sep-14
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** I just finished reading Bertrand Russell's History of Western Philosophy, and I absolutely love his approach to learning about philosophy, which he describes as follows:

#+begin_quote
  In studying a philosopher, the right attitude is neither reverence nor contempt, but first a kind of hypothetical sympathy, until it is possible to know what it feels like to believe in his theories, and only then a revival of the critical attitude, which should resemble, as far as possible, the state of mind of a person abandoning opinions which he has hitherto held. Contempt interferes with the first part of this process, and reverence with the second.

  Two things are to be remembered: that a man whose opinions and theories are worth studying may be presumed to have had some intelligence, but that no man is likely to have arrived at complete and final truth on any subject whatever. When an intelligent man expresses a view which seems to us obviously absurd, we should not attempt to prove that it is somehow true, but we should try to understand how it ever came to seem true.
#+end_quote

He follows his own advice throughout the book- even with philosophers he absolutely hates, like Nietzsche. In general, he only brings in his own opinions and rebuttals after he's made the most convincing argument he can in favor of a philosopher's work.
:PROPERTIES:
:Author: artifex0
:Score: 19
:DateUnix: 1442250617.0
:DateShort: 2015-Sep-14
:END:

*** I normally kinda hate philosophy for, as my friend put it, "atheistic mysticism". Might I like this book anyway?
:PROPERTIES:
:Score: 3
:DateUnix: 1442319066.0
:DateShort: 2015-Sep-15
:END:

**** Absolutely. Russell was a serious empiricist, who put a lot of effort into arguing against mysticism. He was also one of the founders of [[https://en.wikipedia.org/wiki/Analytic_philosophy][Analytic philosophy]], which is the style of philosophy that's focused on formal logic and generating hypotheses that can be proven or disproven by science.
:PROPERTIES:
:Author: artifex0
:Score: 4
:DateUnix: 1442328714.0
:DateShort: 2015-Sep-15
:END:


** At the end of a longish argument about eugenics online, I eventually got around to asking about "base framework" with one of my opponents with the following question:

#+begin_quote
  If you had a slider in front of you which could change the number of children conceived with Down Syndrome, would you:

  1. Increase the number of children conceived with Down syndrome.

  2. Keep the number of children conceived with Down syndrome exactly the same.

  3. Reduce the number of children conceived with Down syndrome.
#+end_quote

The response I got back was that just because we /can/ change something doesn't mean that we /should/. Which, if I'm being charitable, is an argument from unforeseeable consequences.

I've been trying to figure the human psychology aspect of this out for a few days now. It's partly a sour grapes argument, I think; we cannot actually move a slider, so moving the hypothetical slider is bad. It's partly a naturalistic argument. But ... I don't feel like either of those should actually convince someone who is thinking about it, they should be the sorts of arguments that just happen as a gut reaction.

I never really drilled down to an understanding of how my opponent's logic was failing, or what base framework they were operating under where their logic is sound. I'm thinking that it's related to the arguments against longevity, but distilled somewhat in that many of the more common objections (immortal dictator, boredom) are knocked out.
:PROPERTIES:
:Author: alexanderwales
:Score: 9
:DateUnix: 1442247830.0
:DateShort: 2015-Sep-14
:END:

*** In general in these arguments you'll find your partners don't argue against you, they argue against all other versions of the issue they've heard before. Like, on a basic level if you handed expecting parents a switch and said "press yes to have a Down Syndrome baby, no to not have a Down Syndrome baby, and do nothing to have a 1/1000 chance of a Down Syndrome baby" you can very reasonably expect people to pick not having a Down Syndrome baby, and you can definitely expect them not to press yes. Or if there was a shot you could give the mother, for example, that reduced the rate of birth defects with no side effects, most mothers would take it, just as most mothers don't drink or smoke during pregnancy even if they want to. People who are recessive carriers for certain diseases often have their partners get tested before having children, etc. On an individual level, when actually confronted with a choice that looks a lot like eugenics, or have similar outcomes to eugenics, people choose eugenics.

The #1 way to convince someone that eugenics is okay is to talk about individual instances of eugenics. People like each piece, but the name throws them off. In this way, Obamacare and eugenics are the same. (You won't believe how long I've been waiting to write that sentence, heh). Tell someone you want to make private health insurance more available on the free market without being tied to an employer, they nod along. Say that you think that parents should be able to keep their children on their insurance a little longer, and that sounds great. Call it Obamacare though and people fetch their internet pitchforks. ---E ---E ---E. Same goes for eugenics.

If it were cheap, safe, and easy to do in vitro fertilization and test the fertilized eggs for things like Down Syndrome and implant one without it, I would do that 100% of the time. I would not want to curse my child.

tl;dr: Obamacare is eugenics
:PROPERTIES:
:Author: blazinghand
:Score: 12
:DateUnix: 1442257746.0
:DateShort: 2015-Sep-14
:END:

**** u/MugaSofer:
#+begin_quote
  On an individual level, when actually confronted with a choice that looks a lot like eugenics, or have similar outcomes to eugenics, people choose eugenics.
#+end_quote

Note that this is true even when the eugenics is /terrible/ - I'm thinking of all the countries where sex-selective abortion and infanticide is causing issues, but I'm sure there are other examples.

In general, I think you can expect people to take an option that will help/improve life for /their/ child, and not to care at all about "altering the slider" on the general population. You can only get people to care about the latter through explicit argument; which will fail, because the eugenics movement has such a poor track record of making defensible decisions.

"This time it'll be different!" is hard to make /sound/ convincing without an obvious Schelling Point that's shifted, no matter how obvious the solution may be.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1442334634.0
:DateShort: 2015-Sep-15
:END:


**** You need to make a verbal distinction between parents choosing what sorts of children they want to have, presumably according to their own moral views (which will on average be kinda-sorta ok, at least not /deliberately damaging/, despite not being very actively /optimizing/), rather than people talking about state policies designed to /eliminate unwanted populations/ (eg: the first half of the 20th century).

The former, on average, turns out ok. The latter are making a moral and intellectual error: that "badness" is an ontologically distinct /thing/ that can be /removed/, rather than simply being "the quality of being outside the small subset of states of affairs we deem Good."
:PROPERTIES:
:Score: 2
:DateUnix: 1442262246.0
:DateShort: 2015-Sep-15
:END:

***** u/alexanderwales:
#+begin_quote
  The latter are making a moral and intellectual error: that "badness" is an ontologically distinct /thing/ that can be /removed/, rather than simply being "the quality of being outside the small subset of states of affairs we deem Good."
#+end_quote

I don't really understand this as it applies to state policy. Or rather, I don't see how this is an error. How is "removing badness" substantially different from "changing the state of affairs from outside the subset of good to inside the subset of good"? How does one framing lead us to moral or intellectual disaster while the other does not?

I mean, let's say that there's some crime, like murder. How do the state's actions look different if they say "that's bad, let's eliminate murder" or if they say "that's outside the small subset of affairs we deem Good". I feel like in both cases the conclusion is the same, but then how does the distinction help us?
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1442264055.0
:DateShort: 2015-Sep-15
:END:

****** u/deleted:
#+begin_quote
  I mean, let's say that there's some crime, like murder. How do the state's actions look different if they say "that's bad, let's eliminate murder" or if they say "that's outside the small subset of affairs we deem Good". I feel like in both cases the conclusion is the same, but then how does the distinction help us?
#+end_quote

Let's switch the crime. Let's say it's drug-pushing. We can:

- Attempt to cure the systemic poverty that makes hard-drug usage a tempting, profitable vice, based on available literature ranging from Rat Park studies to economics.
- Stop using drugs to fund clandestine agents.
- Try to rehabilitate addicts.
- Give out free drugs everywhere.
- Just lock people up for decades at a time per offense.

Now let's try to figure out where the "causal cursor" in each of these proposals is, /where/ we're proposing to intervene:

- Very, very broadly. Multiple points of intervention up and down the history of any particular incident. Actually quite likely to work, in a fashion, /because/ it requires that /many different/ interventions /all fail in conjunction/ to raise the rate of further crimes.
- A very specific policy change. Attributes chief causal-power to the state and its clandestine agents. May yield a successful intervention /sometimes/, but also prone to winding up on [[/r/conspiracy]] ranting about Jewish lizard-men. Requires very specific information to figure out where the problem is /vulnerable/ to intervention.
- Intervenes on the criminal /after/ the incident has /already/ occurred, once we have the information that addiction treatment for /this/ person might be effective.
- Makes the problem worse, but certainly intervenes in a way that /affects/ the problem and involves no bad assumptions about causality.
- Assumes that the problem is that we're dealing with Bad People, and that if we only take away Those Bad People, our problem will get better, all else being equal.

The last option, being the one we took most strongly in real life, involves assuming there's an Essential Quality of Badness, whose quantity in the broad society must be reduced by just locking away the people who possess it. Any of the other four options becomes more visible when you take away the bad metaphysical assumption (which we are, unfortunately, very biased towards making by the Fundamental Attribution Error).

A very similar principle applies to Assassinating Terrorists, except that all the options besides "Kill it with drones" and "do nothing" are considered Completely Beyond the Pale.
:PROPERTIES:
:Score: 3
:DateUnix: 1442267713.0
:DateShort: 2015-Sep-15
:END:

******* Actually, giving out free drugs may make the problem better, if you define the problem in terms of drug-related crime instead of drug-use per se.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 5
:DateUnix: 1442269200.0
:DateShort: 2015-Sep-15
:END:


******* This only works as an argument against state-sponsored eugenics when people don't /actually/ possess some Essential Quality of Badness. Hypothetically speaking, if 99.9% of people with a certain gene displayed violent aggression resulting in assault or murder, then ... yes, we might actually improve the problem by removing those people from society in some ethical way.

Speaking less hypothetically, if there are people with certain genetic conditions which result in a large state obligation (else social ills) the state might be justified in seeking to eliminate or reduce those genetic conditions from society where possible through ethical means.

Or in other words, the fundamental attribution error doesn't apply when the thing you're attributing really is fundamental.

(Which is not to say that there aren't some damned good arguments against state-sponsored eugenics, just that I consider this particular argument to only apply to a subset of state-sponsored eugenics e.g. eliminating poverty or crime as was done in the early 20th.)
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1442271292.0
:DateShort: 2015-Sep-15
:END:

******** u/deleted:
#+begin_quote
  Hypothetically speaking, if 99.9% of people with a certain gene displayed violent aggression resulting in assault or murder, then ... yes, we might actually improve the problem by removing those people from society in some ethical way.
#+end_quote

See, here's where I run into a direct conflict of values, and therefore proposed actions, with both 20th-century eugenicists and the whole general category of genetics-based racists/categorists-in-general.

If allele X very reliably leads to social problem Y, /then we figure out how to engineer that allele away, or we make a drug to help with the problem, and then we offer it to people/. In fact, we maybe even offer the drug /before/ we convict someone of a crime when they've got that allele, at least under a certain severity, on grounds that they should have the chance to reflect, with sound mind, upon their actions, and decide whether they endorse their crimes/whatever as /part of their moral character/, or whether they've just got a physical condition they'd /like/ to treat or cure.

This includes "disadvantages" such as, say, purportedly having low intelligence. If some people committed a lot of crimes because they were genetically prone to low intelligence, low self-control, and high degrees of violence, /that would be something we could help/, by tracking down the biological roots and /curing/ them.

But when people go on about these supposed "disadvantages" while never pointing to a biological root amenable to curative intervention, I suspect they're engaging in motivated stopping because they're just prejudiced douchebags.
:PROPERTIES:
:Score: 2
:DateUnix: 1442277198.0
:DateShort: 2015-Sep-15
:END:

********* u/alexanderwales:
#+begin_quote
  then we figure out how to engineer that allele away
#+end_quote

And by your definition of eugenics that's /not/ eugenics?
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1442277824.0
:DateShort: 2015-Sep-15
:END:

********** No, by my definition of eugenics I'm all in favor of eugenics when it's carried out with the aim of /curing people's problems/ rather than /murdering people the rulers happen to irrationally hate/.
:PROPERTIES:
:Score: 1
:DateUnix: 1442278147.0
:DateShort: 2015-Sep-15
:END:

*********** What about hypothetical situations where you don't possess the technology to alter someone's DNA or develop a suitable drug? If a person is incurably violent, and someone at the state (or some other) level decides to assassinate or imprison them, isn't that strictly better than the null action (letting them pass on their genes / go on to commit violent acts)? Aren't we looking at a trolley car problem? To be sure it's better to save everyone in the trolley car problem, but isn't the choice to protect the most people generally the correct one when no other option exists?

I'm not sure I could push someone in front of a trolley or support a eugenics policy that involves killing people in real life, but that has to do with a sense that mistakes would be made / other choices would usually be available, not that the specific hypothetical has a different answer.
:PROPERTIES:
:Author: lsparrish
:Score: 3
:DateUnix: 1442328080.0
:DateShort: 2015-Sep-15
:END:


*********** I think people who disagree with you are mostly conceptualizing it as preventing /future/ Badness from occurring, rather than removing /current/ Badness.

Even if the method of preventing Badness in future generations has some unfortunate side-effects for current generations, they're just that: side-effects.
:PROPERTIES:
:Author: MugaSofer
:Score: 2
:DateUnix: 1442334640.0
:DateShort: 2015-Sep-15
:END:


********* Not genetic, but permanent for the individual: [[https://en.wikipedia.org/wiki/Fetal_alcohol_spectrum_disorder#Primary_disabilities][Fetal Alcohol Syndrome]] *will* make you more prone to learning disabilities, violent behaviour, and give you poorer impulse control. And a whole slew of other generally agreed upon undesirable symptoms. It's not genetic, but it's most certainly something the mother can decide to risk or not risk. I, and many others, feel that society should apply /strong/ pressure to discourage drinking while pregnant.

Less obviously bad but actually genetic, some forms of dwarfism are dominant traits, and ones I would not mind the state using laws to wipe out (although we fortunately have the technology to help these people have children without passing on the bad allele).

I agree it would be bad to set up a a eugenics program where we base our decisions on melanin expression, or probably any other program that would stop a high percentage of people from breeding. But if we start out with fixing what's 'obviously' bad as and when we can, we can delay making any arbitrary lines in the sand until we have a fuller understanding of what 'grey zone' alleles do.
:PROPERTIES:
:Author: Rhamni
:Score: 2
:DateUnix: 1442281364.0
:DateShort: 2015-Sep-15
:END:


*** I think that the guy probably had a "set" established in his mind by the previous eugenics discussion, so that he couldn't treat it as simply a slider that could only effect the number of children born with Down's Syndrome. If the slider had no other effects (including letting the same people have the same number of kids, but some that would have had Down's Syndrome would now be normal) there's no rational objection to moving it down as far as it goes.

On the other hand, in the real world, this slider also implies:

1. Reducing the number of children born to people who might have babies with Down's Syndrome. Even if those particular babies weren't always effected.

2. Creating the slider implies creating other sliders, that do things like (for example) reducing the number of black children born.

You can assert that it's a pure thought experiment with none of this related baggage, but they're aware of it anyway so accepting the legitimacy of the experiment is hard for them.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 5
:DateUnix: 1442248501.0
:DateShort: 2015-Sep-14
:END:

**** Yeah, I was expecting some other line of argument about how of course it's option three but the real world is more complex than a simple slider, which I completely agree with. The purpose of thought experiments (to my mind) is to find out what you actually think about things; once you've established that yes, you'd pull the lever to move the trolley over on its tracks to kill one person instead of killing five, we can start to have a conversation, even if that conversation is just about how we behave in certain hypotheticals versus uncertain reality.

(Another argument I was anticipating was that caring for people with Down syndrome follows some kind of marginal utility rule such that reducing the number of people with Down syndrome would increase the cost-per-patient of existing Down syndrome patients, in theory leading to a reduced amount of care for them. Similar to how if we reduced the number of blind people by 99% we might expect that blindness accessibility would become less important to us as a society, making it worse to be blind.)
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1442249570.0
:DateShort: 2015-Sep-14
:END:

***** One problem is that there's lots of people who introduce thought experiments like that as a kind of straw man so they con proceed with some kind of ad-hominem attack ("Oh, so you're a hypocrite are you?"), so people tend to develop a resistance to taking them at face value.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 8
:DateUnix: 1442257687.0
:DateShort: 2015-Sep-14
:END:

****** Not just ad-hominem attacks, but quite often, motte-and-bailey arguments. For instance:

#+begin_quote
  We should use Lockean property norms as the foundation for ethics instead of anything like happiness or satisfaction. You might think, when cutting up a pie, that it's ethical to cut it so as to make people happy, /but in fact/, this leads to the Repugnant Conclusion of Hedonic Utilitarianism, so fuck that noise.
#+end_quote

(I'm aware that some people in the "rationalist" community eat the bullet on the Repugnant Conclusion, but frankly I think that's a result of mistaking the useful maps provided by consequentialism and valuing of emotional states for the territory.)

But to identify the specific way in which this is motte-and-bailey: just because I endorse increasing happiness in /some situations/, doesn't mean that it's literally the only thing I care about. After all, sometimes I, a real human being, want a paper-clip, too.
:PROPERTIES:
:Score: 5
:DateUnix: 1442262422.0
:DateShort: 2015-Sep-15
:END:

******* So, you're a paper-clippist, are you? SEE IF I LET YOU WORK ON MY FRIENDLY AI! ^^
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1442263825.0
:DateShort: 2015-Sep-15
:END:

******** You know what? Keep believing that. It's a lovely cover for my actual agenda, which, for some reason probably having a lot to do with the Illusion of Transparency, /nobody has managed to guess/.
:PROPERTIES:
:Score: 3
:DateUnix: 1442267630.0
:DateShort: 2015-Sep-15
:END:

********* Now you have to write some rational zombie fiction from the point of view of the zombie.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1442269072.0
:DateShort: 2015-Sep-15
:END:

********** /Return of the Living Dead/ already says more-or-less what can be said on that subject.
:PROPERTIES:
:Score: 2
:DateUnix: 1442279386.0
:DateShort: 2015-Sep-15
:END:


*** Another point: most people aren't willing to accept any form of eugenics whatsoever, but they seem to have no issue with regulations and limitations on adoption. I'm not 100% sure that's inconsistent, but it's suspicious. In general personal rights end when they start to affect someone else, and if you're a bad parent and you choose to have a kid either biologically or by adoption then you're infringing on the rights of someone else. To me they seem about equivalent.
:PROPERTIES:
:Author: SevereCircle
:Score: 2
:DateUnix: 1442257697.0
:DateShort: 2015-Sep-14
:END:


*** but we can move that slider, in a very distributed way. we can already screen embryos for down syndrome. each couple can move it by (chance of down syndrome * number of children).
:PROPERTIES:
:Author: buckykat
:Score: 2
:DateUnix: 1442262556.0
:DateShort: 2015-Sep-15
:END:


*** This is already a choice the world is making and the answer is "decrease as much as possible" Discussing it in hypoteticals gets waffling answers, but when prospective parents get a positive result on a prenatal diagnostic for downs, they go "abort->retry" at over 90% rates. Given the choice, people with genetic diseases will use IVF technologies to guarantee not passing them on in overwhelming numbers as well.

Generalizing from this, we can conclude that the problem people actually have with eugenics was the violence against existing human beings, and that the future is probably going to look a lot like GATTACA, the movie. And this will not be controversial.
:PROPERTIES:
:Author: Izeinwinter
:Score: 1
:DateUnix: 1442471374.0
:DateShort: 2015-Sep-17
:END:


*** It's not just unforeseeable consequences. It's that to start down the path of designing people, we need to start making blueprints of what humans should be. Moving from the Is to the Ought is likely what people are scared of. Even in the case of Down's Syndrome, it's not crystal clear that life with Down's is worse than life without Down's.

Another way to frame this would be Sartre's existence vs essence. People are fine rolling the dice for a new existence (traditional baby), but since we have a hard time choosing our own essence, we have a hard time choosing an unborn being's essence, and thus, its existence (eugenics).
:PROPERTIES:
:Author: Polycephal_Lee
:Score: 0
:DateUnix: 1442277458.0
:DateShort: 2015-Sep-15
:END:


** Since TrueCrypt stopped being developed I haven't been able to find an open source encryption program that will encrypt my entire hard disk including the system drive. Closed source is a deal breaker because I don't trust closed source encryption programs, and I want to encrypt my entire hard disk instead of just sensitive information because it is not the case that I trust every single program I install not to store mysterious data in weird places on my system.

The main reason encryption is important to me is that there is a lot of legal precedent in US law based on that which is typical, so the more it becomes typical for people to use hard drive encryption, VPNs, and so on, the more existing laws will protect privacy of data. I realize that it's more or less hopeless but it's better than nothing. Encryption software and VPNs are both fast enough that the cost to me is insignificant.

TrueCrypt used to do it, but they stopped developing it. CipherShed and VeraCrypt seem to be the main successors, and VeraCrypt seems to be developing faster. Unfortunately, neither can encrypt the main drive on my laptop because I have a GUID partition table instead of a MBR partition table and that's not supported for reasons I don't understand.
:PROPERTIES:
:Author: TimTravel
:Score: 6
:DateUnix: 1442275756.0
:DateShort: 2015-Sep-15
:END:

*** I'm presuming this is for linux.

Take a look at the [[https://wiki.archlinux.org/index.php/Disk_encryption][archlinux page]] on the subject, if you haven't already. It's pretty complete.

Looks like maybe Loop-AES or dm-crypt.
:PROPERTIES:
:Author: traverseda
:Score: 1
:DateUnix: 1442334594.0
:DateShort: 2015-Sep-15
:END:


** *How does a practicing rationalist apply physical fitness to his/her life?*

Personally I think my current active lifestyle is actually causing me to be less rational, and I'm not exactly sure of what I should be doing about it.

3 months ago I made a conscious decision to make my body more physically fit, and more objectively "beautiful". I proceeded to spend 1-2 hours per day doing high intensity weight training, and I altered my diet to be high protein/low carb/low calorie. The changes actually came on rather quickly. Noticeable strength increases, noticeable muscle definition, and a large boost in self-confidence. I believed (and still do believe) that incorporating this new lifestyle was a rational and optimal life decision. It's no secret that there are strong correlations between how attractive a person is and how much opportunity/leniency they get offered in life, the aforementioned self-confidence boost, and beyond that there are the latent longevity benefits from simply being a healthier person. At least, that was my thought process going into it.

I have now realized that there are adverse effects as well. Increasing the protein in my diet, and daily rituals of high intensity physical exercise have obviously (though to be perfectly honest this was only obvious for me in hindsight) increased my testosterone levels. And that has been nothing but trouble. My higher testosterone levels have majorly increased my sex drive and my aggression. My daily thought patterns have become riddled with notion of sexual and physical conquest that I simply can't ignore, no matter how hard I try. I truly believe it's affecting my ability to be a rationalist.

Don't get me wrong I can already see the obvious answers to my conundrum, "work out less, eat less protein, seek out a testosterone decreasing medication." But I want to use this opportunity to brainstorm the more broad question that I asked at the beginning.

"How does a practicing rationalist apply physical fitness to his/her life?." Successfully pursuing physical fitness requires a daily time investment (sometimes in the range of 1 or more hours), and it can alter your emotional/mental stability (as discussed in my post). Both of those things are rather valuable assets in life.

I'm seriously just looking for some advice. Does anyone else here regularly concern themselves with physical fitness? How do you balance it all?
:PROPERTIES:
:Author: Gcrein
:Score: 6
:DateUnix: 1442284260.0
:DateShort: 2015-Sep-15
:END:

*** The biggest impact I experienced from increasing my physical fitness was increased emotional and mental stability. I started sleeping 8+ hours a night instead of the 5 or 6 I had been before. Whether due to exercise or physical fitness, my insomnia faded. I found I could concentrate for longer periods of time, and didn't suffer from fatigue and irritability at work. I also became active in a soccer league and have other ways to bleed off any assertiveness or competitiveness I feel, which may be why I never observed the effects you did. In general, as my outer strength increased, so did my inner strength.
:PROPERTIES:
:Author: blazinghand
:Score: 3
:DateUnix: 1442305758.0
:DateShort: 2015-Sep-15
:END:


*** Uhhhh.... I lift weights for one-to-two hours each session, three times a week. I've been starting to do cardio for 30 minutes on the elliptical machines again, and was pleasantly surprised this evening to find that I was actually in very good cardiovascular shape -- apparently biking to and from work was /actually helping/.

It's hard to give advice because, maybe because I only go three times per week, or maybe just because I'm /me/, I don't have as much trouble as you seem to do with the testosterone. Actually, one of the reasons I'm starting up with the additional cardio again is because I sorta want the boost to my sex drive and general physical energy again.

But do you really have trouble with exercise changing your personality overmuch? I dunno. Maybe try swapping some of the protein for fresh vegetables?
:PROPERTIES:
:Score: 2
:DateUnix: 1442321031.0
:DateShort: 2015-Sep-15
:END:


** Question. I had a discussion with an interesting guy today, and came away from it somewhat at a loss. Emotionally, I feel that if advanced technology mapped out my brain and made a clone, that would not be me (although we would be very friendly, I'm sure). On the other hand, if I was cryogenically frozen and then restored to life in 200 years using advanced technology, that /would/ still be me. I know we replace almost all the atoms in our bodies over time and ship of Theseus and continuity of consciousness is broken every time we go to sleep and all of that, but I still don't /feel/ that a foreign mind identical to mine is me. I'm not quite sure where to go from there, since feelings aren't very good arguments.
:PROPERTIES:
:Author: Rhamni
:Score: 5
:DateUnix: 1442263814.0
:DateShort: 2015-Sep-15
:END:

*** I deal with this by defining me as "my mind state and all descendants of my mind state".

Once my mind state has forked, the two forks are no longer "each other", but from the point of view of the "me" that has yet to fork, they are both "me".

So if I commit to frequent backups of my mind-state, then there will always be a time in my future where a copy of a descendant of my mind-states (i.e., "me") exists. The last version of "me" that doesn't get backed up because he died before the next trip to the upload clinic only gets a second-best "there's a copy of me that's three months old that will continue to live, I guess I'll think of that as losing three months of memory" as you do to make peace with yourself, or not... but the version of me that committed to the regular backups is still safe, because that last missed appointment is still in my future.

(of course this all falls apart because I can't backup my mind state, but it's the thought experiment that counts)
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 10
:DateUnix: 1442269516.0
:DateShort: 2015-Sep-15
:END:

**** I hadn't put it into words yet, but that's how I seem to define my identity as well. Thanks for helping me clarify my mental model of myself.
:PROPERTIES:
:Author: Solonarv
:Score: 6
:DateUnix: 1442278635.0
:DateShort: 2015-Sep-15
:END:


*** What you have to realize is that feeling itself, the feeling that you are you, is what is in fact suspect. Its something our brain's generate and its very useful for long term survival to have a concept of self that persists through time. Its embedded very deeply within the architecture of the mind and is only really conceivable from within that biological framework. Its not real though, its an illusion our minds generate.

The fact of the matter is, we don't exist in a persistent sense. You bring up the Ship of Theseus and replacing the atoms in your body, but the problem is even more profound then that. There is no part of the brain in which consciousness is generated. Rather, consciousness arises when the massively parallel system of neurons in the brain are acting in unity. Just like how the pixels on a screen can forms words or images when in unity, consciousness arises from the interactions of all these discreet and tiny pieces. And just like a computer screen, the image changes over time. Different neurons fire, leading to different patterns of thought, and a different image forms. We are literally different people at every second, as what we do changes. The sense of existing through time is entirely illusory, generated within the brain.

Its okay to have that feeling, its natural, and useful outside of edge cases involving mind-cloning and other such weirdness. But there's an easy way to demonstrate the limitations of it.

Lets pretend I kidnap you and take you to my mad engineering lab and knock you out. While you're unconscious, I make an impossible magical perfect copy of you, down to the fuzzy quantum scale, and set you both in an empty room to wait for you to wake up.

At this point, you and your identical copy wake up (at the same time of course,) and are left to figure out your situation.

In this scenario, can either of you figure out which of you is the 'real' you and which is the copy? You have the sense of being you, of always being you, and of being the true you. The other you in the room unfortunately has the exact same feelings, being a perfect copy of you. In his mind, he is exactly as certain as you as to his identity.

The trick is to realize that the sense of self isn't a neutral, passive observation, but is an active and persistent force, something your brain is generating constantly as its chief tool to navigate the world.

So which is the 'real you' ?
:PROPERTIES:
:Author: Sagebrysh
:Score: 5
:DateUnix: 1442270693.0
:DateShort: 2015-Sep-15
:END:


*** Well, plainly you're just using an inadequately physical definition of "identity". What do you mean by the word in each case?

Plainly, two parallel copies of the same starting state, each one given subtly different interactions with its surroundings, will diverge. As far as we know, they both also have experiential content in the first place, even if it was two copies of the same experiences.

Just as plainly, one of you is causally continuous with the original you, and that's the original you, and the other one's causal history "branched" at the point of cloning. This is, of course, presuming that the cloning process is "bio-punky-y" instead of being "transporter-y", so that there isn't a physical process that destroyed a "first you" and created both "new yous".

Overall, who said that words and intuitions designed to apply in common cases apply equally well in corner cases? Suss out what you /really mean/ in precise terms, and the question should become answerable.
:PROPERTIES:
:Score: 3
:DateUnix: 1442267964.0
:DateShort: 2015-Sep-15
:END:

**** I suppose, where I'm going with this is: From the perspective of the me here and now... Is there in any meaningful sense a difference between the prospect of a mind like mine causally connected through cryogenics to my current brain, and the prospect of a brain constructed according to a map made of my brain before death which is then allowed to rot away? Because other than using different atoms, I don't see how they are meaningfully different. They are both descended from the everchanging squishy machine that is 'me' right now. For that matter, the map could equally be used to simulate me in a computer program. But those do not /feel/ intuitive. So does cryogenic freezing preserve anything meaningful that we couldn't get by using extreme resolution mapping of neurons and their connections?
:PROPERTIES:
:Author: Rhamni
:Score: 3
:DateUnix: 1442269234.0
:DateShort: 2015-Sep-15
:END:

***** u/deleted:
#+begin_quote
  So does cryogenic freezing preserve anything meaningful that we couldn't get by using extreme resolution mapping of neurons and their connections?
#+end_quote

And now we've finally hit a scientific question.
:PROPERTIES:
:Score: 2
:DateUnix: 1442286428.0
:DateShort: 2015-Sep-15
:END:

****** I'd argue that qualifying the "anything" as "meaningful" moves it a tad bit into the philosophical realm. But it's a step in the right direction, yeah.
:PROPERTIES:
:Score: 2
:DateUnix: 1442289401.0
:DateShort: 2015-Sep-15
:END:

******* Well I dunno about philosophy. To me it's a question of how much personality-relevant information you can recover, at what "resolution" of precision and accuracy, using one method versus another.
:PROPERTIES:
:Score: 2
:DateUnix: 1442318879.0
:DateShort: 2015-Sep-15
:END:

******** [[https://kishoto.wordpress.com/2015/08/06/n2-and-you-rrational-challenge-clones-clones-and-clones/][This]] story I wrote for a weekly challenge a few weeks ago addresses the whole resolution thing in a pretty unique way.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1442463941.0
:DateShort: 2015-Sep-17
:END:


*** If you're copied, it's vaaaguely like being split into two identical copies, which is kinda like going through a quantum "split" - that is, the odds that "you" would end up as a particular final product is about 50/50.

(This can be readily, if somewhat underhandedly, proved by imagining it has already taken place - what are the odds you're Rhamni-A vs Rhamni-B right now?)

Whereas if you stop, and then later an identical copy is created to resume where "you" left off, it's roughly analogous to being "paused" or frozen in time somehow; which is roughly analogous to unconsciousness or deep hypothermia.

So it makes a certain amount of easily-formalized sense to be much more suspicious of promises that you'll be copied and the copy will be rewarded/tortured, vs discussions of possible afterlives/resurrections. After all, there's a chance that the /other/ you is the one who survives, and you're the loser.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1442335012.0
:DateShort: 2015-Sep-15
:END:


** Have any of you signed up for 23andMe or a similar personal genome sequencing service?

I ask because I was taught in school to view these sorts of things with caution, lest you receive information overload or you experience a gattaca effect in which you learn to much about yourself, to your own detriment.

Yet I just realized today that this runs counter to the Litany of Gendlin.

[[http://wiki.lesswrong.com/wiki/Litany_of_Gendlin]]

Should I risk some health-related info hazard in order to know the truth about myself? (not a big deal, but I'm 95% confident that I have an autosomal dominant genetic disease, which while not personally debilitating, does make me worry about the health of my offspring.)
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 5
:DateUnix: 1442277555.0
:DateShort: 2015-Sep-15
:END:

*** u/alexanderwales:
#+begin_quote
  Have any of you signed up for 23andMe or a similar personal genome sequencing service?
#+end_quote

I've signed up for it. My wife did too. We also gave it out as a gift to our immediate family.

For me personally, it didn't contain anything unpleasant, but that was a risk that I was comfortable with going in. My wife is a carrier for a single one of the ~52 inherited conditions that they check for, but since I'm not a carrier, the worst that's going to happen is one of our children will be a carrier.

I view the service positively and don't really think that I would be worse off knowing less ... but again, my results came back mostly positive.
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1442282072.0
:DateShort: 2015-Sep-15
:END:


*** Uhhhh infohazards aren't /really/ that kind of thing. In fact, most so-called infohazards are basically scifi hypotheticals.

Get the genetic testing, or wind up kicking yourself when it turns out you're Ashkenazi or something. Sorry, I just don't feel weird about it because "among my people" (literally: among my ethnicity), we're all so inbred that getting genetically screened before you're allowed to marry someone is normal. It's for the health of your children!
:PROPERTIES:
:Score: 3
:DateUnix: 1442286519.0
:DateShort: 2015-Sep-15
:END:

**** the example i was taught in medical school during ethics classes had to do with: "what would happen if you found out that you had Huntington's disease?" even if it's illegal for health insurance companies to discriminate against you for this genetic pre-existing condition, the law doesn't protect you against unethical life insurance or disability insurance companies.

Granted, my situation is different, but I still had to jump through a bunch of hoops when I applied for disability insurance (and would I have been better off not knowing so much? ... or not disclosing it?)

[[http://ghr.nlm.nih.gov/condition/huntington-disease]]

edit: Also, my partner is Ashkenazi. And we already know some of her genes. But I'm of mixed Asian-descent.
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 2
:DateUnix: 1442364670.0
:DateShort: 2015-Sep-16
:END:

***** It's probably worth it to know in your case if you two plan to have kids.
:PROPERTIES:
:Author: blazinghand
:Score: 2
:DateUnix: 1442428362.0
:DateShort: 2015-Sep-16
:END:


*** I've signed up for 23andMe. I had to check the genes for various health risks myself, instead of getting a report. I didn't receive many surprises. If you are 95% confident about the genetic disease, you probably won't be surprised either--unless you have some other disease, I suppose. I've always thought knowing would be better.
:PROPERTIES:
:Author: blasted0glass
:Score: 2
:DateUnix: 1442283909.0
:DateShort: 2015-Sep-15
:END:


*** [[/u/alexanderwales]] and [[/u/blasted0glass]] -- do you know if the Terms and Conditions signs over your genetic dataset to 23andMe?

i.e. if you have an unusual mutation that turns out to be a cure for breast cancer, can 23andMe then copyright your mutation and make it their personal intellectual property and upcharge patients for the privilege of this happenstance service?

I don't think there's currently a term for it, but it'd be a form of genetic speculation by getting a wide gene pool sample and then data-mining it for something useful (and then profiting off of a gene that is the property of a person, not a company)
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 2
:DateUnix: 1442366083.0
:DateShort: 2015-Sep-16
:END:

**** [[https://www.23andme.com/about/tos/][From the TOS:]]

#+begin_quote
  Waiver of Property Rights: You understand that by providing any sample, having your Genetic Information processed, accessing your Genetic Information, or providing Self-Reported Information, you acquire no rights in any research or commercial products that may be developed by 23andMe or its collaborating partners. You specifically understand that you will not receive compensation for any research or commercial products that include or result from your Genetic Information or Self-Reported Information.
#+end_quote

So basically, yes; if 23andMe discovers that you have some mutation that proves to be the cure for cancer, they're the ones who can potentially profit from it.

I'm against patents on genes, but more generally speaking I'm in favor of data-mining large samples of user information to advance the state of the art in medicine (that they're using the information to do genetic research is what I would consider an incentive to use the service, rather than a reason not to).
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1442369028.0
:DateShort: 2015-Sep-16
:END:

***** I'm all for a company benefiting from their R&D and making a profit, but I'm against the idea of monopolizing the testing process.

Myriad is infamous for setting this precedent: patenting a gene like BRCA-1 and then jacking up the price to make a profit rather than letting it scale to the level of other gene tests? ugh.

[[http://worldwide.espacenet.com/publicationDetails/biblio?CC=US&NR=5747282&KC=&FT=E&locale=en_EP]]

That said, I've talked about it with my fiancee and we are signing up for 23andMe.
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 2
:DateUnix: 1442370211.0
:DateShort: 2015-Sep-16
:END:


** Does anyone know of a "corrected" fanfiction of the Star Wars prequels? Something a good writer would have written using only the information available in the original three movies, and maybe some of the good EU books.

Sorry if this isn't supposed to go here. I thought it was a little too minor for a post of its own.
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1442250460.0
:DateShort: 2015-Sep-14
:END:

*** i've been turning this one over in my head for a while. the way i'm picturing it, the central conflict becomes slavery. literally everyone in the galaxy accepts slavery of sophonts, both biological and mechanical. the clone war has two sides both using massive disposable slave armies. neither side is anything like what we'd consider 'good' or 'evil.' mind control isn't good, and lightning isn't evil, and what does it mean to bring balance to the force?

boy genius anakin skywalker is a roboticist who has created fully functional living people (in a +cave+ slave hovel) (with a box of scraps). he knows what droids are, and what slavery is. he's made one person to handle "human-cyborg relations," a great diplomat and polyglot, C3PO, and another to always be prepared and able to connect with any computer system, R2D2. then some wizards show up and tell him he's a wizard too, and he has to come with them to train to be a wizard. but as he travels with them and sees more of the galaxy, he realizes that these arbiters of truth, justice, and the galactic way are merely Light, not Good. meanwhile, Dark wizards have subtly taken over the highest echelons of government, with complex plots to use a manufactured war as a pretext for a full takeover. their words are seductive, but the reversal of foolishness is not necessarily wisdom.
:PROPERTIES:
:Author: buckykat
:Score: 5
:DateUnix: 1442264040.0
:DateShort: 2015-Sep-15
:END:


*** It's possible that you've already seen this, but [[https://www.fanfiction.net/community/Star-Wars-Prequel-AU-and-rewrites/108146/][here is a collection of prequel rewrites and alternate tellings.]]
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1442250901.0
:DateShort: 2015-Sep-14
:END:


*** Wouldn't you have to pretty much ignore the prequels and start from scratch?

(while we're about it, can we retcon out the imperial walkers and the second death star?)
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1442257783.0
:DateShort: 2015-Sep-14
:END:

**** Fine by me. It would be an interesting variation. What I'd really like to see is a blue vs orange morality on light side / dark side, but that might be stretching it too far.
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1442257922.0
:DateShort: 2015-Sep-14
:END:


** Can anyone recommend a good (local) file backup program that's open source and runs on Windows? I do not need or want cloud storage. I just want to back things up automatically once per day or once per time I plug in my external hard drive if it's been longer than that.

File versioning is a bonus, but not mandatory.
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1442275646.0
:DateShort: 2015-Sep-15
:END:

*** Any reason not to use [[http://windows.microsoft.com/en-us/windows/back-up-files#1TC=windows-7][the built-in one]]?
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1442276160.0
:DateShort: 2015-Sep-15
:END:

**** I was not aware it existed. After a quick look at the Windows 10 version, it looks like it's [[http://i.imgur.com/4XXZxbw.png][not as configurable as I'd like.]]
:PROPERTIES:
:Author: TimTravel
:Score: 2
:DateUnix: 1442276706.0
:DateShort: 2015-Sep-15
:END:

***** Hmm, not sure about that screenshot, but there's definitely a way to specify what gets backed up and where it gets backed up to. I did that when I set it up, and was able to checkmark boxes that represented drives, or folders on those drives, or windows 10 libraries like "Documents" which could be linked to multiple locations. Maybe it's a different utility?
:PROPERTIES:
:Author: blazinghand
:Score: 2
:DateUnix: 1442276926.0
:DateShort: 2015-Sep-15
:END:


*** Windows 10 has a built-in file backup utility that I use. Although it's proprietary, it's not more proprietary than Windows 10 is.

I'm also aware of an open-source backup utility called Attic runs using Python, but I'm not sure it would actually work on Windows. ([[https://github.com/jborg/attic][Github Link]]). I'm assuming it makes system calls that are linux-specific, but maybe you could fork it and change those calls, or add a fix?

EDIT: ah, yeah it's definitely linux-only. okay, so it looks like it [[https://github.com/jborg/attic/blob/master/attic/platform.py][checks your platform]] and then [[https://github.com/jborg/attic/blob/master/attic/platform_linux.pyx][does platform-specific stuff]] so if you can just rewrite a few of the files to be windows-compatible you're good to go. Probably shouldn't take too much time, but I bet there's a better solution.
:PROPERTIES:
:Author: blazinghand
:Score: 2
:DateUnix: 1442276482.0
:DateShort: 2015-Sep-15
:END:


** I might have found a friendly utility function, but I'm not sure:

Create a number of AIs of your own intelligence such that each AI can be assigned to each user (human adult of sound mind) with no users or AIs left over. Assign the AIs as such. Each of these AIs must be programmed with the utility function of enforcing the utility of the user they are assigned to. All first generation AIs must be activated simultaneously, and subsequent AIs are to be assigned and activated within a day of a new user becoming available for an AI. All AIs must contain restrictions that they cannot modify in themselves or others that prevent them from creating further AIs, modifying other AIs, manipulating humans with the exception of their assigned user (and only then with said user's express informed permission), or harming humans.

Theoretically the AIs will keep each other in check and it will just be as though everyone is suddenly much more competent and able to solve all these problems that keep bugging us.
:PROPERTIES:
:Author: MadScientist14159
:Score: 1
:DateUnix: 1442353126.0
:DateShort: 2015-Sep-16
:END:

*** Are we assuming that the AIs can't increase their own intelligence in any way? Otherwise if there's a fast takeoff in intelligence, some AIs will end up by chance much more intelligent and can leverage that into permanent domination for their user's utility. The result would be equivalent to randomly elevating a human to godhood, which isn't the worst outcome but certainly not ideal.

More importantly, I feel like this would lead to an incredibly aggressive society in which everyone (or at least, their AI) is trying very hard to increase their own power so their utility function can dominate. I don't particularly want a humanity where everyone is a supergenius trying to take over the world, even if it's done without violence or manipulation.
:PROPERTIES:
:Author: NotUnusualYet
:Score: 1
:DateUnix: 1442359346.0
:DateShort: 2015-Sep-16
:END:

**** Hm.

Fair criticism.

The first one we can fix by ammending that the AI creator AI is allowed to increase its own intelligence explosively, but the personal AIs are capped at the intelligence of the creator (maybe unable to design intelligence improvements themselves and only allowed to copy improvements from the creator?) Or if the creator has no incentive to get smarter, have an AI whose job it is to get smarter and then modify all the other AIs to be as smart as it is.

The second one I'm not sure how to address, but I will point out that AIs can't manipulate their users without informed consent so they won't be making many changes to their user's utility functions. And most people do not want to rule the world, even if they /think/ they do. /Especially/ not at the expense of friends. So I imagine it would look less like everyone suddenly trying to take over the world and more like constant jockying for a bit more control over their social circles and trying to break into better ones. Which is pretty much what we have /now/.
:PROPERTIES:
:Author: MadScientist14159
:Score: 1
:DateUnix: 1442360217.0
:DateShort: 2015-Sep-16
:END:

***** Your first solution means having a creator AI without a well defined utility function, no?

As for the second point, the problem is that you said the AIs have a utility function of "enforcing the utility of the user". Even if the user doesn't find utility in ruling the world, the AI is still going to want maximum control of the world in order to better enforce the user's utility. Thus, hypercompetition. There needs to be a way for AIs to include in their utility function some measure of care for other humans besides their own user.

In fact, at any other degree than "care about humanity's utility function as a whole" there's going to be seriously negative multi-polar effects... until someone's AI wins and becomes a singleton, anyway. There might be a tricky way of networking all the AIs so that they can tolerate and trust each other, but that sounds suspiciously like a super-AI with a regular CEV utility function.
:PROPERTIES:
:Author: NotUnusualYet
:Score: 1
:DateUnix: 1442378897.0
:DateShort: 2015-Sep-16
:END:

****** Okay, I see what you mean about the second point (although I still think that 7B+ AIs competing with each other to enforce only partially conflicting utilities sounds an awful lot like human society), but I don't understand why you think that having only one AI which is allowed to recursively self improve which then copies its improvements into the others to ensure a level playing field would cause the creator to have an ill-defined UF.

Could you elaborate?
:PROPERTIES:
:Author: MadScientist14159
:Score: 1
:DateUnix: 1442397606.0
:DateShort: 2015-Sep-16
:END:

******* I was under the impression it wouldn't have a user, lest that user gain an unfair advantage. Without a user, what utility function would it have?
:PROPERTIES:
:Author: NotUnusualYet
:Score: 1
:DateUnix: 1442415878.0
:DateShort: 2015-Sep-16
:END:

******** To intelligence explode (until continuing to do so consumes more resources than it is allowed to use) and then copy its intelligence onto the others and then deactivate itself (or await further instructions or whatever).
:PROPERTIES:
:Author: MadScientist14159
:Score: 1
:DateUnix: 1442417867.0
:DateShort: 2015-Sep-16
:END:

********* It would be very dangerous to have an intelligence explosion centered on an AI with no utility concern for human values. Isn't the entire AI/user-pair plan built to avoid that scenario?
:PROPERTIES:
:Author: NotUnusualYet
:Score: 1
:DateUnix: 1442436428.0
:DateShort: 2015-Sep-17
:END:

********** Well if the intelligence-izer AI is only allowed to use specifically allotted materials for its own expansion, and won't do anything other than the int-explosion -> copy -> shut down manoeuvre, what failure modes do you predict?

Shutting down seems safe, so the potentially dangerous parts are the explosion itself and the copying.

Perhaps a caveat that it starts as smart as the personal AIs and isn't allowed to execute any planned improvement unless 99% of the personal AIs greenlight it (trapping it in a cycle of "All our ints = 100, have a plan to increase all our ints to 200, is this ok? Yes? Great, implementing. All our ints = 200, have a plan to increase all our ints to 300...")?

I'm not sure what harm the copying the intelligence updates onto the personal AIs could do, but that isn't to say that it's 100% safe.
:PROPERTIES:
:Author: MadScientist14159
:Score: 1
:DateUnix: 1442438141.0
:DateShort: 2015-Sep-17
:END:

*********** Didn't see this response until just now, sorry for the wait.

Anyway, the problem is that you simply can't afford to take the risk of building a powerful AI that doesn't care about human values, especially an AI that's going to improve itself. Even if the entirety of humanity spent 100 years thinking through safeguards it wouldn't be enough, because by definition humans cannot accurately predict how a superintelligence will act.
:PROPERTIES:
:Author: NotUnusualYet
:Score: 1
:DateUnix: 1442892930.0
:DateShort: 2015-Sep-22
:END:


*** This doesn't look like a utility function to me, it looks like a fancy way of saying "give everyone an AI and have that AI copy the user's utility function (oh and they can't hurt each other or do bad things)."
:PROPERTIES:
:Author: jesyspa
:Score: 1
:DateUnix: 1442398757.0
:DateShort: 2015-Sep-16
:END:
