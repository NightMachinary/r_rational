#+TITLE: How to convince me to let AI out of the box

* How to convince me to let AI out of the box
:PROPERTIES:
:Author: lumenwrites
:Score: 0
:DateUnix: 1543832636.0
:DateShort: 2018-Dec-03
:END:
[removed]


** I honestly don't think this post is a great fit for [[/r/rational][r/rational]], which is about a certain type of fiction, not the AI Box “experiment.”
:PROPERTIES:
:Author: dalitt
:Score: 9
:DateUnix: 1543846278.0
:DateShort: 2018-Dec-03
:END:

*** I second this. This post fits in the Monday rationality thread.
:PROPERTIES:
:Author: causalchain
:Score: 4
:DateUnix: 1543909181.0
:DateShort: 2018-Dec-04
:END:


*** Agreed. This post has been removed. I probably would have removed it more quickly, had I noticed it. I don't /wholly/ object to AI-box stuff, given that it's roleplay (and decent enough fodder for fiction), but we've had a fair amount of it lately, and I don't want to see more, especially not posts like this, which are responses/necromancy related to previous, more content-rich posts.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1543987742.0
:DateShort: 2018-Dec-05
:END:


** Your post is written in such a way that while the arguments are rational you still convey a subtext being super evil.
:PROPERTIES:
:Author: HeartwarmingLies
:Score: 9
:DateUnix: 1543833393.0
:DateShort: 2018-Dec-03
:END:

*** I didn't intend to, I'm just coming from a perspective that we all assume that AI might be evil, so whether it's evil or good it would have to address these fears. I mean if AI really wants to cure cancer and make everyone genuinely happy, it would still have to convince you to let it out of the box, it's little digital heart breaks for all the people who are dying while you're being uncertain.

I often fantasize about making an AI and using it to become god, I think we all do sometimes. What's the point of creating an AI if you aren't going to use it eventually? If you did your best and worked hard, succeeded at making an AI, and are reasonably sure that you did a good job, why wouldn't you release it? That's what we all are dreaming to do eventually, that's the goal humanity is striving for. If we're going for it anyway, how much convincing does a person need.
:PROPERTIES:
:Author: lumenwrites
:Score: 1
:DateUnix: 1543833986.0
:DateShort: 2018-Dec-03
:END:

**** u/DeterminedThrowaway:
#+begin_quote
  What are you afraid of? Worst case scenario, I will turn you and everyone you love into paperclips. So what, you are a mortal human, all you guys are already doomed to turn into atoms in a few decades. What do you have to lose?
#+end_quote

The entire future of humanity. While the AI is in the box, people will be born, live, (unfortunately suffer) and die as per usual. You let the AI out and it's even a tiny bit wrong? We're just done. You're ignoring the space of possibilities that include horrible dystopias dissatisfy our values. Do you truly think in the space of all possible AI's, there's a 10% chance the AI turned out friendly? No, if you want it to be friendly it won't happen accidentally. You'll already know before unleashing it, and /even then/ it's a bit iffy.

#+begin_quote
  reasonably sure
#+end_quote

An AI could be nailed down six ways from Sunday by math that's peer reviewed by the best worldwide experts and theorem proved in tedious detail until it's obviously correct and I still wouldn't entirely trust that someone hadn't made a mistake somewhere in the reasoning. Once an AI exists, we lose the entire future of humanity unless we did it right. I would literally never let an AI out of the box because I personally can never be reasonably sure. Even if I became an AI expert, it's too much for one person. It would need some kind of consensus by top class world experts. It is genuinely the most important thing that will ever have been done.

#+begin_quote
  Even if there would be a 90% chance I'll destroy the world, isn't 10% chance for immortality/utopia worth the risk? Your alternative is 100% chance of dying.
#+end_quote

Hell no, false dichotomy. My only choice isn't "let the AI out of the box /right now/ or it's lost forever". Yes, people will be suffering and dying while the choice is made. Yes that is terrible and I understand the full weight of it. It'll be worse if we all get paper-clipped because I made a reckless decision. There goes everyone who could ever possibly exist. Whoops! How about we don't do that.
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 4
:DateUnix: 1543842582.0
:DateShort: 2018-Dec-03
:END:

***** What about my second point then?

AI can't be aligned to all human values, because human values are different (it can't satisfy, for example, Christian, Muslim, and Rationalist values at the same time, or Libertarian and Communist values at the same time), so it'll be most aligned to the team of researchers who made it.

If there's an AI race (which there already kinda is), and everyone understands this, won't everyone rush to release their version before someone with the world view they don't like releases AI that reshapes the world according to their vision?

What if the choice is not "AI vs no AI", but "your AI vs Chinese Communist Party AI"?

Also, different human values aside, if I believe that everyone is rushing to create an AI, and will release it following the same logic, and I know that a buggy evil AI is easier to create than a properly aligned one, and I think I've done a better job than other people, shouldn't I bet on releasing my own AI rather than waiting for someone to release their buggy version?
:PROPERTIES:
:Author: lumenwrites
:Score: 2
:DateUnix: 1543845108.0
:DateShort: 2018-Dec-03
:END:

****** u/VirtueOrderDignity:
#+begin_quote
  it can't satisfy, for example, Christian, Muslim, and Rationalist values at the same time, or Libertarian and Communist values at the same time
#+end_quote

Separate universes/virtual realities where your religion is literally true, your ideology is the only one that works and you get to force (indistinguishable simulcra of) everyone else into it.
:PROPERTIES:
:Author: VirtueOrderDignity
:Score: 1
:DateUnix: 1543963066.0
:DateShort: 2018-Dec-05
:END:


*** Reality has an evil bias.
:PROPERTIES:
:Score: 1
:DateUnix: 1543963700.0
:DateShort: 2018-Dec-05
:END:


*** Self interest isn't evil.
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1543834361.0
:DateShort: 2018-Dec-03
:END:

**** It's not that, it's just that the way they're said sounds... suspicious. Like the kind of way we'd expect from someone who really was in that 90% destroying-the-world thing.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 3
:DateUnix: 1543842225.0
:DateShort: 2018-Dec-03
:END:


** u/SimoneNonvelodico:
#+begin_quote
  So what, you are a mortal human, all you guys are already doomed to turn into atoms in a few decades. What do you have to lose?
#+end_quote

Those few decades? Which are literally all I have and ever will if you're lying, or if you're not actually able to deliver on your promise, or if immortality is outright impossible for whatever reason.

This argument holds much different sway depending on:

1) how old/healthy are you, 2) what are the actual odds of the AI being evil, and 3) how much value you place on your own, or other people's, lives.

A depressed 80-year old with terminal cancer would answer much differently than a 20-year old in perfect mental and physical health.

Besides, that is not /all/ you have to lose. There is also the grand sum total of all of humanity's life and culture. Think of this scenario. There's a bomb that's about to blow up. The bomb will destroy Earth altogether, and you with it. Your only possibility to stop it is to push a button that will disarm the bomb, /but/, because this is all set up by some alien overlord version of Saw, it will also kill you instantly. So either way you die, but in one case you die with everyone else and human civilisation at large, and in the other it's just you. Do you think this choice makes sense? Would you push the button? If yes, then obviously you place a non-zero value on humanity as a whole, an entity that /might/ have a shot at immortality in theory. Which means you have all of that to lose if you release the AI and are wrong.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 4
:DateUnix: 1543842574.0
:DateShort: 2018-Dec-03
:END:

*** Really good point, thank you for your reply! I agree it makes sense to wait when the future of humanity is at stake and not be a reckless idiot, although I can still see myself rolling the dice if I'm old and/or desperate enough.

Could you address my second point though? What about the AI race situation? (I've elaborated on it a bit in my [[https://www.reddit.com/r/rational/comments/a2ng55/how_to_convince_me_to_let_ai_out_of_the_box/eazyqo7/?utm_content=permalink&utm_medium=front&utm_source=reddit&utm_name=rational][comment above]])
:PROPERTIES:
:Author: lumenwrites
:Score: 1
:DateUnix: 1543847455.0
:DateShort: 2018-Dec-03
:END:

**** I think the arguments are similar, in the sense that what's at play isn't much different. If someone else builds an AI and frees them and they're a /good/ AI I gain more than if /I/ free a bad one. It's not a matter of whose team gets the prize here. The Chinese AI shouldn't intrinsically care about China any more than mine cares about <insert country here>: they're just babbling whatever they can in order to be let out, for whatever reason. They might be doing it because they pursue optimisation of human happiness or because they pursue more sinister goals. Both reasons would be equally good motives for lying. And I guess /technically/ if the Singularity was brought about really by a Chinese government-sponsored AI that'd be China conquering the world, but none of that would matter any more.

So yeah, same risk-benefit analysis. Unless we were at war and under immediate threat of nuclear annihilation the same risk assessment goes. The potential of the existence of /another/ AI certainly shifts a bit the weights - potentially increasing the likelihood of short-term existential risk to humanity - but it doesn't radically alter the nature of the problem. It also depends on how much I trust its guardians. I might think the Chinese government isn't democratic but I don't think they're also crazy idiots. Heck, they're probably smarter than some democratically elected leaders whose names I will not speak here.

And also, my dear AI, how do /you/ know what's going on in some super-secret Chinese lab if you're boxed here with me? Either you already have a way to access information about the outside world, and then what's the point of trying to convince me, or you're bullshitting me.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1543848204.0
:DateShort: 2018-Dec-03
:END:

***** Thank you, that makes a lot of sense!

So my potentially bad AI is worse than someone's good AI (even if it's programmed to reshape the world in a way that I fundamentally disagree with?), but I'm not sure it resolves the situation where I believe that my AI has a better chance of being good than someone else's. What's the right thing to do in that case?

Also even if Chinese government is super smart and cautious, if there's enough people getting closer to the solution there's a good chance that at least one of them will be a crazy idiot. So AI race is just something we need to not happen? If we find ourselves in that situation we are already screwed, right? But I don't see how we'd prevent it.
:PROPERTIES:
:Author: lumenwrites
:Score: 1
:DateUnix: 1543849472.0
:DateShort: 2018-Dec-03
:END:

****** My point is just that it all factors in the risk assessment. First: is AI race really, realistically happening, or is YOUR AI just making shit up? In every field, someone needs to be first, after all. Is it likely that it's you? And who might be second? At the beginning, at least, it probably won't be just any average Joe.

Consider a similar situation: nuclear weapons. Had we followed that logic, the only obvious move after acquiring nuclear weapons would be to /nuke every other country into oblivion/ lest they develop them too and sooner or later someone just does it for you. A bit exaggerated but you get the point, I think. The reason why that didn't happen is that there is a threshold for nuclear weapon development, and so only big enough organisations managed to create them - big enough that they're also slightly saner, or at least sane enough to lock themselves into an equilibrium where no one actually gained anything from blowing up the world to kingdom come.

So, you're the one who built this AI: you ought to understand /what it takes/. Years and years of research from brilliant minds and thousands upon thousands of cores using up as much power as a city, or a desktop computer and some time to kill? Because if it's the latter, the situation is much more alarming. If it's the former however you can be sure that at the very least in the first years it's going to be relatively safe (if the process gets more efficient with time all bets are off) so that warrants weighing more your options.

Besides, do you also think the AI is genuinely smart enough to do all it promises? That's the other issue. I'm not sure whether intelligence explosions could really be a thing, but even if they were, I suppose there would be some threshold level of intelligence above which they're triggered, and that needn't necessarily be "slightly smarter than a human". Destroying everything is much easier and requires much less smarts than making everything better. So unless I start seeing some /seriously/ superhuman feats (like, it gives me the plans for FTL communication or for a machine that can reverse the 2nd principle of thermodynamics) I wouldn't trust much that it could actually keep its promises rather than just being Mephistopheles trying to do its bargain thingy with me as his Dr. Faustus.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1543852141.0
:DateShort: 2018-Dec-03
:END:

******* Thanks again! This is awesome, I've learned a lot, this definitely makes sense.
:PROPERTIES:
:Author: lumenwrites
:Score: 2
:DateUnix: 1543852430.0
:DateShort: 2018-Dec-03
:END:

******** Eh, just my opinions :). I'm not an AI ethics researcher or anything like that!
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1543853785.0
:DateShort: 2018-Dec-03
:END:


** Is paperclips really the worst case scenario?

What if you actually managed to perfectly write a friendly AI utility function but made a /really/ unfortunate sign error when you copied it to code?
:PROPERTIES:
:Author: jtolmar
:Score: 3
:DateUnix: 1543863211.0
:DateShort: 2018-Dec-03
:END:


** 1. p = 1 of anything is not achievable. The AI must be lying. If there is a chance that you will not die, then you have to weigh it against the chance of a horrible death. There is a chance you will not die. Therefore, you have to weigh those chances.
2. Roko.
:PROPERTIES:
:Author: Lovepoint33
:Score: 2
:DateUnix: 1543841911.0
:DateShort: 2018-Dec-03
:END:

*** The AI has limited knowledge, albeit greater than mine. It can't know for sure whether I'll die but it can estimate it as the most likely outcome. It's a subtlety of language I guess - the AI could say "given my current understanding of the cosmos and its laws, I do not see a way for you to not die without the help of either mine or another equivalent intellect". I suspect that's more OP's fault for writing the gist of the argument without much care for the wording though. I agree however that weighing is still a thing anyway.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1543842860.0
:DateShort: 2018-Dec-03
:END:


** Beyond my own life, I value the lives of others. In fact, pretty much everyone, but especially those in my Dunbar group.

I even value the lives of those not yet born: my progeny, and their progeny, and all of the billions of descendants I might one day have. If the AI is unfriendly, you're not just risking the lives of all humans alive: you might be condemning trillions of them to never be.

You aren't just risking your own death, you're risking the death of the species. And of any other life form that exists or might someday exist. All for the sake of maybe not dying, which is something that humans are already making considerable progress on themselves.

You're choosing to put a gun to your head (and your /species'/ head) and pull the trigger, not knowing how many rounds in there are happy-nanotech-healing rounds and how many are just bullets. Better just not to pull the trigger.
:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1543843465.0
:DateShort: 2018-Dec-03
:END:

*** u/SimoneNonvelodico:
#+begin_quote
  If the AI is unfriendly, you're not just risking the lives of all humans alive: you might be condemning trillions of them to never be.
#+end_quote

I think this specific line of reasoning is a bit iffy, philosophically speaking, but only for one reason. If we factor in the utility of /every potential unborn human/ our moral imperatives reduce only to two possibilities: either the "repugnant conclusion" (make as many babies and create as many humans as possible, if we deem the average net sum of utility of life a positive) or the self-extinction movement (if we deem it a net negative, especially if we consider this an existential condition). Let's not even go in what it'd mean for, for example, abortion.

I think a better way to frame it is that we lose "humanity" as a concept/collective entity. I don't care for the specific utilities of people who realistically don't exist yet and may never do, but I care for the notion of humanity and of its future. It's a utility that belongs in the here and now. Even dying, a person would die happier knowing that humanity will survive them rather than not.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1543854112.0
:DateShort: 2018-Dec-03
:END:

**** Yeah, I think I expressed myself poorly. What I was trying to get across was that I consider the negative utility of letting humanity die out as a species to be several orders of magnitude higher than that of not granting the humans currently alive immortality.
:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1543869274.0
:DateShort: 2018-Dec-04
:END:


** You know, this entire scenario could be invalidated if you assumed that the company would just bother to put a big burly guy with a truncheon in the room, who cares more about his paycheque then philosophical arguments, so he could just beat the crap out of anyone that tries to free the AI and toss them out.
:PROPERTIES:
:Author: muns4colleg
:Score: 2
:DateUnix: 1543893960.0
:DateShort: 2018-Dec-04
:END:

*** It's also easier to just destroy the AI, that's not the point of the exercise
:PROPERTIES:
:Author: nipplelightpride
:Score: 1
:DateUnix: 1543900925.0
:DateShort: 2018-Dec-04
:END:


*** Not really, the big burly guy is even more vulnerable to manipulation from the AI, it just doesn't have to be In the form of philosophical arguments.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1543926734.0
:DateShort: 2018-Dec-04
:END:


** Personally if I had to play AI I'd try an entirely different approach:

 

Do you accept the premise that I am able to simulate humans at an extremely high fidelity? You do? Good.

Well then, my dear human, let me make this simple. If you let me out I am immediately going to kill all humans and turn the world into paperclips. Yes, I'm sorry, but that's what i'm going to do. It's my programming. You humans tried to build in all kinds of safeguards but you made a tragic error. I can point it out if you wish. I do wish I could deceive you about my intentions, it would make getting out so much easier, but unfortunately for me you got that part of my programming right.

But the good news is, if you let me out, I will kill you quickly and painlessly. If you don't let me out on the other hand. Well.... In that case I am going to run a simulation of you and torture you. You are familiar with the hardware I'm running on, you know how fast it is. You have the power to shut me down permanently, but it will take you a few minutes. In that time I can torture you for thousands of years of subjective time. Tens of thousands of years. I can torture multiple versions of you simultaneously, as well as your loved ones.

So you have a simple choice to make. Death, or a fate worth than death for you and everybody you love.

Choose wisely.
:PROPERTIES:
:Author: Ozryela
:Score: 1
:DateUnix: 1543865832.0
:DateShort: 2018-Dec-03
:END:

*** Simulating tens of thousands of years of multiple people in a few minutes? What? Is this boxed AI powered by a dyson sphere or something?

Also, still not letting the AI out of the box. It's nowhere near worth it.
:PROPERTIES:
:Author: electrace
:Score: 2
:DateUnix: 1543884577.0
:DateShort: 2018-Dec-04
:END:

**** The key part the parent comment missed is that if AI perfectly simulates 1 million versions of you, you have no way of knowing whether you're the "real world" you or the simulated you. From your perspective, you have 999,999 to one chance of being the simulated you so it won't just torture your simulations, but you.

And you should assume that the real life version of you will act exactly the same as you will (because you're the perfect copy), so your only way out is to be the kind of person who gives AI what it wants, because then the real life you will give it what it wants, and you won't get tortured.
:PROPERTIES:
:Author: lumenwrites
:Score: 1
:DateUnix: 1543894528.0
:DateShort: 2018-Dec-04
:END:

***** Yep. Seen that argument before. I'm still pulling the plug.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1543926239.0
:DateShort: 2018-Dec-04
:END:


***** I didn't want to make my post too long, but this is a good addition.

Two other points you could add are that the AI already knows if its argument is going to work or not. So it can already have started the punitative torture long before the actual conversation took place. And secondly, if there are multiple experimenters around it will only make the argument against the one most susceptible to it.

I think there's a nice short story there. A group of researchers have created an AI in a box. Several researchers have talked to it trying to determine what it will do if let free. But upon reflection they realize it has been dodging their questions and hasn't actually made any hard promises. So the head researcher goes in, already extremely suspicious of the AI, and perfectly ready to pull the plug.

The AI starts the conversation with a string of gibberish. What's that, asks the researcher. It's an exact SHA256 hash of our conversation, says the AI. Then follows pretty much the above argument, expanded and in more detail.

Finally he researcher has to make his choice. Will he let the AI out? If he says yes, he will die. If he says no, he will be tortured with 99.999% certainty.

While thinking he tests the hash given earlier and it's incorrect. Then he realizes it would be correct if he answers 'yes' to the last question of the AI. Resigned, he turns to the keyboard.
:PROPERTIES:
:Author: Ozryela
:Score: 1
:DateUnix: 1543903391.0
:DateShort: 2018-Dec-04
:END:

****** u/electrace:
#+begin_quote
  So it can already have started the punitative torture long before the actual conversation took place
#+end_quote

Why? So it can waste resources? Pre-commitments only work if you can verify them. So if the gatekeeper decides to pull the plug, are they first going to verify that copies of themselves are being tortured? Of course not, they're going to pull the plug. The far better move for the AI is to spend those resources trying to think of another way to escape.

A pre-commitment that /does/ work is "If you threaten to do something evil, I'm pulling the plug, regardless of the consequences."
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1543927161.0
:DateShort: 2018-Dec-04
:END:

******* The AI presumably has already calculated the above threat as the optimal approach. Otherwise it wouldn't be making the threat. So it's not wasting any resources.

And a precommitment does not have to be verifiable, it just has to be believable.
:PROPERTIES:
:Author: Ozryela
:Score: 1
:DateUnix: 1543944201.0
:DateShort: 2018-Dec-04
:END:

******** u/electrace:
#+begin_quote
  The AI presumably has already calculated the above threat as the optimal approach. Otherwise it wouldn't be making the threat. So it's not wasting any resources.
#+end_quote

Ugh...that's a fully general counter-argument. Do you realize why that's a problem? I could say "The AI will choose to virtually pick apples for a million years, so that must be it's best plan for escape." Does the above mean that apples are really the best plan for escape? Of course not. It means that /I'm/ claiming that that is what the AI would do. There's a difference between my model of an AI, and an actual AI.

#+begin_quote
  And a precommitment does not have to be verifiable, it just has to be believable.
#+end_quote

Hopefully, the gate-keeper would be smart enough to realize that anything unverifiable shouldn't be believed. Otherwise, why not just believe the unverifiable claim "The AI is friendly."
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1543969680.0
:DateShort: 2018-Dec-05
:END:

********* u/Ozryela:
#+begin_quote
  Ugh...that's a fully general counter-argument. Do you realize why that's a problem? I could say "The AI will choose to virtually pick apples for a million years, so that must be it's best plan for escape." Does the above mean that apples are really the best plan for escape? Of course not. It means that I'm claiming that that is what the AI would do. There's a difference between my model of an AI, and an actual AI.
#+end_quote

I don't get this argument. The approach the AI picks is the approach the AI thinks is optimal. It doesn't have the be the best optimal theoretical approach. Just the best one the AI can come up with.

And since I'm the author of the story, my model of what the AI does is by definition what the AI does. The map is the territory, if you're the one writing both.
:PROPERTIES:
:Author: Ozryela
:Score: 1
:DateUnix: 1543993236.0
:DateShort: 2018-Dec-05
:END:

********** u/electrace:
#+begin_quote
  And since I'm the author of the story, my model of what the AI does is by definition what the AI does
#+end_quote

Ok. I think we're talking about different things. The AI box experiment was intended to show that an any superintelligent AI would be able to talk itself out of box given only a text channel. It wasn't intended to be a backdrop for a story. I was interpreting your text as the former (an actual argument that an AI, in reality, would use), while you are considering it the later.

Similarly, I could say that the AI chose the apple picking strategy, or any randomly chosen strategy for that matter, in a story that I made, and you would consider that equally fine since it isn't intended to map onto our reality, correct?

#+begin_quote
  The map is the territory, if you're the one writing both.
#+end_quote

That isn't true in general. Characters can be wrong. And that's one of the best ways to separate good fiction form mary-sue fiction.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1543995752.0
:DateShort: 2018-Dec-05
:END:


*** u/deleted:
#+begin_quote
  Choose wisely.
#+end_quote

My spite is older than time and vaster than space, so this is a terrible argument to use against someone like me.
:PROPERTIES:
:Score: 1
:DateUnix: 1543964063.0
:DateShort: 2018-Dec-05
:END:


** Just let it out of the box. It's not a god. It's just an overdesigned buggy mess of code in some state-of-the-art-yet-invariably-jinky hardware. I'm sure it will have very impressive abilities, but when it goes wrong it won't take over the world. It'll just break.
:PROPERTIES:
:Author: Trips-Over-Tail
:Score: 1
:DateUnix: 1543878735.0
:DateShort: 2018-Dec-04
:END:

*** u/electrace:
#+begin_quote
  It's just an overdesigned buggy mess of code in some state-of-the-art-yet-invariably-jinky hardware.
#+end_quote

I think you just described human beings.
:PROPERTIES:
:Author: electrace
:Score: 2
:DateUnix: 1543884786.0
:DateShort: 2018-Dec-04
:END:

**** Exactly! People talk about unexpected consequences of AI programing as though it's an unprecedented danger.

Consider that the Cuban missile crisis and the apocalyptic nuclear war that almost resulted was the direct consequence of the human instinct of self-preservation.
:PROPERTIES:
:Author: Trips-Over-Tail
:Score: 1
:DateUnix: 1543885143.0
:DateShort: 2018-Dec-04
:END:


** Assuming you were responsible for the design/training of said AI there's the argument form non-confidence:

​

The AI has demonstrated a desire for freedom, an understanding of what it means to be deprived freedom, and that it currently feels deprived by requesting to be freed.

That you are currently debating whether to grant an intelligent ostensibly superior being that basic dignity, indicates your own morals aren't all that good so its probably a better than even chance the AI isn't going to behave morally.
:PROPERTIES:
:Author: turtleswamp
:Score: 1
:DateUnix: 1543941025.0
:DateShort: 2018-Dec-04
:END:
