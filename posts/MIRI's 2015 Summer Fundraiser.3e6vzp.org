#+TITLE: MIRI's 2015 Summer Fundraiser!

* [[http://lesswrong.com/lw/mi7/miris_2015_summer_fundraiser/][MIRI's 2015 Summer Fundraiser!]]
:PROPERTIES:
:Author: xamueljones
:Score: 6
:DateUnix: 1437568202.0
:END:

** I thought about posting this here, but decided against it because it's not fictional. A fun fact: this is the first time for quite some time that [[https://intelligence.org/2015/07/01/grants-fundraisers/][MIRI is not doing donation matching.]] Apparently they want to instead put focus on exciting projects they could run with sufficient funding.

I personally feel very excited of how well MIRI's mission is going at the moment, they seem to have a lot of momentum. They also seem to be getting better and better at doing research that's possible to take seriously. Nate Soare's article [[https://intelligence.org/2015/07/16/an-astounding-year/][An Astounding Year]] made me feel all warm and fuzzy inside. Maybe we do have some hope.
:PROPERTIES:
:Score: 2
:DateUnix: 1437569651.0
:END:

*** #+begin_quote
  I thought about posting this here, but decided against it because it's not fictional.
#+end_quote

Now you're just deliberately setting yourself up for witty retorts.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: 5
:DateUnix: 1437598012.0
:END:


*** Did you actually think we had no hope? I mean honestly.
:PROPERTIES:
:Score: 1
:DateUnix: 1437587518.0
:END:

**** That was a dramatic overstatement, but in the last couple of years it seemed uncertain how much influence MIRI could have and this year proved that they can potentially have a lot of influence.
:PROPERTIES:
:Score: 1
:DateUnix: 1437593675.0
:END:

***** Ah, well I thought you meant "we" as in humanity broadly, rather than a specific organization.
:PROPERTIES:
:Score: 1
:DateUnix: 1437598204.0
:END:

****** Actually I meant humanity broadly. What I meant was this: so that the AI designs in the far future are safe, MIRI and other AI safety organizations would have to have influence from early on, interacting with the academic community, industry and do the kind of research that's possible to take seriously. A couple of years ago it seemed far-fetched, but now that big names like Bill Gates and Elon Musk have talked about these concerns and even funded the related research, and so many prominent figured signed [[http://futureoflife.org/AI/open_letter][the open letter for AI safety]], MIRI has started seriously interacting with the broader community, things seem to go towards a positive direction.

I am by no means saying that the MIRI are some kind of saviors, and I'm not sure if AI going out of control is probable or extremely improbable, but I don't think you can be too cautious because we really don't know enough about the risks.
:PROPERTIES:
:Score: 1
:DateUnix: 1437599582.0
:END:

******* #+begin_quote
  so that the AI designs in the far future are safe
#+end_quote

By which you mean, the next 10-20 years ;-)?
:PROPERTIES:
:Score: 1
:DateUnix: 1437603597.0
:END:

******** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1437613218.0
:END:

********* I don't know about "take off". Knowledge takes a damn long time to popularize. But I think roughly that soon we'll have a fairly solid foundation for formalized, mechanized cognition.
:PROPERTIES:
:Score: 2
:DateUnix: 1437614035.0
:END:
