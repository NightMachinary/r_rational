#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 30
:DateUnix: 1468249458.0
:DateShort: 2016-Jul-11
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** How do you guys exercise?

I feel like I really need to pick up a sport or something; tried the gym for a while but the complete lack of visible progress (combined with dizzy spells that are too common for comfort) discouraged me completely. I like table tennis and tennis, though am not good enough at the latter to actually play a game, but for logistical reasons they're not really candidates for everyday exercise.

So, I'm looking for other ideas for some kind of activity that you can do for a couple hours a week and hopefully recoup the benefits of exercies (primarily, not feeling so tired all the time)
:PROPERTIES:
:Author: Anderkent
:Score: 16
:DateUnix: 1468251111.0
:DateShort: 2016-Jul-11
:END:

*** Almost any sport or exercise is more fun with a friend, so get a gym/running buddy. That's a big help.

I workout regularly but I also walk a ton, and I enjoy the latter more when I listen to podcasts while doing so. So find a nice podcast to listen to while you walk.

Try some operant conditioning: I have a favorite chocolate (white chocolate with Oreo bits) I eat only during or after strength training. It's an extra motivation to workout.
:PROPERTIES:
:Author: gardenofjew
:Score: 10
:DateUnix: 1468254119.0
:DateShort: 2016-Jul-11
:END:


*** You could always try out Pokemon Go, it just came out stateside (though it'll be a bit before it deploys elsewhere). It gives you plenty cause to go walking.
:PROPERTIES:
:Author: Cariyaga
:Score: 13
:DateUnix: 1468269205.0
:DateShort: 2016-Jul-12
:END:


*** [[http://www.pokemongo.com/en-us/]]
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 4
:DateUnix: 1468284025.0
:DateShort: 2016-Jul-12
:END:


*** I have an elliptical machine set up behind my computer. I go on it whenever I watch Netflix.
:PROPERTIES:
:Author: ulyssessword
:Score: 2
:DateUnix: 1468258593.0
:DateShort: 2016-Jul-11
:END:

**** This is my secret, [[/u/anderkent]]. Combining my stationary bike with book reading/game playing or lifting weights while watching Netflix works better than any other system for working out has so far.

Well, that and Pokemon Go has been a nice recent boost in physical activity, but only when weather permits.
:PROPERTIES:
:Author: DaystarEld
:Score: 3
:DateUnix: 1468292661.0
:DateShort: 2016-Jul-12
:END:


*** What program were you doing for your training? With Starting Strength I was seeing results in a month or so.
:PROPERTIES:
:Author: elevul
:Score: 2
:DateUnix: 1468271758.0
:DateShort: 2016-Jul-12
:END:

**** Iiii was mostly just going to the gym and doing things on the machines :P I had one session with a personal trainer the first time, who drafted me a set of things to do, but the reps didn't really seem to get any easier as I went pretty regularly over a month or so, and I got demotivated.
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1468272904.0
:DateShort: 2016-Jul-12
:END:

***** [deleted]
:PROPERTIES:
:Score: 3
:DateUnix: 1468273730.0
:DateShort: 2016-Jul-12
:END:

****** Yaeh I was afraid of doing free weights by myself because I'd surely break my back or something.
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1468275185.0
:DateShort: 2016-Jul-12
:END:

******* Youtube has TONS of form videos. That's what I used when I had started, years ago.
:PROPERTIES:
:Author: elevul
:Score: 2
:DateUnix: 1468309567.0
:DateShort: 2016-Jul-12
:END:


******* Just get an introduction to it from someone, and take it slow while you're learning form. It is easier to get injured with free weights, but it's fine as long as you're careful.
:PROPERTIES:
:Author: whywhisperwhy
:Score: 1
:DateUnix: 1468275400.0
:DateShort: 2016-Jul-12
:END:


******* Just get an introduction to it from someone, and take it slow while you're learning form. It is easier to get injured with free weights, but it's fine as long as you're careful.
:PROPERTIES:
:Author: whywhisperwhy
:Score: 1
:DateUnix: 1468275423.0
:DateShort: 2016-Jul-12
:END:


******* Just start with low weights at the start and high reps.
:PROPERTIES:
:Author: Dwood15
:Score: 1
:DateUnix: 1468277613.0
:DateShort: 2016-Jul-12
:END:


***** I exclusively used machines in college. I didn't see progress /quickly/, which I think made it more gratifying when a friend first pointed out that my manboobs had turned into actual pecs.
:PROPERTIES:
:Author: Iconochasm
:Score: 1
:DateUnix: 1468288662.0
:DateShort: 2016-Jul-12
:END:


*** Swing dancing for cardio. You meet people, do a semi-creative activity to music, and get some high energy activity in while having fun.

Pullups, pushups, squats, and other exercises for strength.

*edit - and I'm fairly sure that the London swing dancing scene is pretty impressively good.
:PROPERTIES:
:Author: ayrvin
:Score: 2
:DateUnix: 1468295236.0
:DateShort: 2016-Jul-12
:END:


*** I commute on my bike to work, would highly recommend it.
:PROPERTIES:
:Author: MagicWeasel
:Score: 2
:DateUnix: 1468299207.0
:DateShort: 2016-Jul-12
:END:


*** Try [[https://en.wikipedia.org/wiki/Jugger][Jugger]]?
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1468251820.0
:DateShort: 2016-Jul-11
:END:

**** Ha, that looks fun, but a quick google doesn't find any meetups in my area (central London). How do you usually find people who play?
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1468253753.0
:DateShort: 2016-Jul-11
:END:

***** I found a meetup in my area via a quick google. /googles/ Looks like you're shit out of luck! Also looks like this mostly exists in my country, which is conveniently unlucky but hey anthropics.
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1468254062.0
:DateShort: 2016-Jul-11
:END:


*** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1468263614.0
:DateShort: 2016-Jul-11
:END:

**** Not really viable to do at my place (London's pretty cramped) but perhaps one-on-one sessions at the gym could at least keep me coming... I can't say it'd be something to look forward to.
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1468264125.0
:DateShort: 2016-Jul-11
:END:


** General Food for Thought Question: What makes people more afraid of one cause of death vs another that's more statistically probable?

Context: I was having an argument with my friends and I said that if I had a son (I'm a black male), I'm not fearful of him being shot by the police. I made the point that, logically speaking, it's more likely for him to die in a car accident than by a police shooting. Therefore my fear of him dying by cop should be less than my fear of him dying by car accident. As I am not afraid of car accidents, I choose to not be afraid of the police shooting my son.

I understand that the disenfranchisement of the black population of the USA is a very real thing. I'm not arguing that it isn't. I understand that there have been several unarmed black males shot by police. I simply said that I don't have any particular fear of my son dying in that manner because, statistically speaking, it's unlikely to happen. My friends, who are more emotional than I am, couldn't understand where I was coming from. I understand that it's easier to be afraid of a man holding a gun than a hunk of metal but is my stance so alien that none of my reasonably intelligent friends could understand it?
:PROPERTIES:
:Author: Kishoto
:Score: 16
:DateUnix: 1468308666.0
:DateShort: 2016-Jul-12
:END:

*** Fear is pretty much unrelated to actual chance of death. Chance of death is a thing that shows up in tables of actuarial data and demographics. Fear is an emotion. Probability is math. Why are people afraid of being killed by cops, or sharks, or terrorists, when all of these are pretty rare ways to die? It's because the chance of death is just a number, and numbers aren't real in the way fear is.

Fear is walking down a dark street and shivering when you see a shadow move, even though you know ghosts aren't real. Fear is buying a gun for home defense, even though doing so increases your mortality rate due to the chance of self-injury or suicide. Fear is an invisible noose snaked around your neck. Fear whispers in your ears, promising oblivion unless you have a bunker full of food and bullets under your house. Fear shows you an image of a man who looks like you dead in the street, and tells you its anecdote trumps all data. When Fear sees you examining a table of data, it slides the image of the dead man over it, and asks you to think with your heart rather than your head.

Fear is insidious, and tugs on the heartstrings in a way that data does not.
:PROPERTIES:
:Author: blazinghand
:Score: 17
:DateUnix: 1468313018.0
:DateShort: 2016-Jul-12
:END:


*** I use this logic to argue that terrorism is a made up issue. ~4,000 American civilians killed by terrorists in 15 years vs 40,000 civilians killed by cars every year = you should be 150 times more scared of cars than terrorists, and we should spend 150 times more money stopping accidents than terrorists.

But we don't.
:PROPERTIES:
:Author: Frommerman
:Score: 8
:DateUnix: 1468347564.0
:DateShort: 2016-Jul-12
:END:

**** While I agree with the main point, to be fair, proponents would argue that only 4000 civilians were killed /because/ of the funding.

Also, they would argue that terrorism is much more susceptible to black swan events, so overspending is preferable due to the large costs when terrorist attacks do happen.
:PROPERTIES:
:Author: electrace
:Score: 4
:DateUnix: 1468468492.0
:DateShort: 2016-Jul-14
:END:

***** Still irrational. No reasonable person could possibly argue that 150 times more people would have died under any circumstsances.
:PROPERTIES:
:Author: Frommerman
:Score: 1
:DateUnix: 1468469657.0
:DateShort: 2016-Jul-14
:END:

****** Again, I do agree that we spend way too much. But, it's not hard to imagine a scenario where that many people would be killed, especially given a 15 year scenario.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1468506089.0
:DateShort: 2016-Jul-14
:END:


**** In the argument, I had a similar stance. I was trying to make the claim that your emotions are dictating your feelings. But, as with a lot of contentious, hot button topics, they didn't want to hear that :P
:PROPERTIES:
:Author: Kishoto
:Score: 2
:DateUnix: 1468349707.0
:DateShort: 2016-Jul-12
:END:


*** u/sir_pirriplin:
#+begin_quote
  As I am not afraid of car accidents
#+end_quote

Shouldn't you? I don't mean to say that the fear should paralyze you or keep you away from cars, but you definitely should pay very close attention to your children when they are near traffic and should definitely teach them to be mindful of cars. Just like some people tell their children to be mindful of police officers, only more so.

Accidents in general are /the/ leading cause of death among children, and traffic accidents are very common accidents.
:PROPERTIES:
:Author: sir_pirriplin
:Score: 2
:DateUnix: 1468384315.0
:DateShort: 2016-Jul-13
:END:

**** Concern and fear are two different things, at least for me. I'll teach my kids to be cautious of the street; to be mindful of cars in general. When they're learning how to drive, I'll do my best to instill a sense of safety in them. But that's not fear to me. That's simply concern and intelligent thinking.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1468387206.0
:DateShort: 2016-Jul-13
:END:

***** Maybe your friends feel the same concern about the police. If they also believe that children already learn to be mindful of cars from other sources (like say, school) then it makes sense that parents will tell their children to have a sense of safety around police. That's probably not taught at school, and you definitely don't want your children to learn that from social media.
:PROPERTIES:
:Author: sir_pirriplin
:Score: 1
:DateUnix: 1468410312.0
:DateShort: 2016-Jul-13
:END:

****** There's an undeniable element of racism in modern law enforcement. It's prudent to warn your children as a result. But my friends and I went back and forth for a while and we were very specific about the distinction between concern and fear. I would certainly warn my son to be as cooperative as possible with the police. But I wouldn't be actively afraid for his safety. My friends, on the other hand, said they would be, if they were parents.
:PROPERTIES:
:Author: Kishoto
:Score: 3
:DateUnix: 1468416150.0
:DateShort: 2016-Jul-13
:END:


** So. Is anywhere doing sociological predictions and studies on the impact of pokemonGo?

Even just around here in a fairly small town / city, it's pretty crazy. It's not unocommon for me to run into groups of 6-10 wandering around the park or graveyard at 4am.

Then there are bigger things like bars that are getting a ton of business either dropping lures or giving discounted drinks to people that do so.

This has to be some social scientists wet dream of data for something or another.
:PROPERTIES:
:Author: LeonCross
:Score: 9
:DateUnix: 1468326486.0
:DateShort: 2016-Jul-12
:END:

*** It's huge right now because it's new. I wouldn't make any predictions until a couple months in, when people get over the novelty and core gameplay steps in.
:PROPERTIES:
:Author: Anderkent
:Score: 6
:DateUnix: 1468327518.0
:DateShort: 2016-Jul-12
:END:

**** The gameplay as it is at the moment isn't anything to write home about. It's costing on nostalgia and novelty.

That said, I've never seen -anything- have such a massive impact on person to person interactions and behavior patterns.

Even if it only lasted a week, I'm fairly confident you could label it "The pokemon week" in relevant college texts and everyone would know exactly what you were talking about.
:PROPERTIES:
:Author: LeonCross
:Score: 9
:DateUnix: 1468329984.0
:DateShort: 2016-Jul-12
:END:

***** However, traditionally long lasting games survive because of the communities they build. If they keep updating the game with new content, I think it will stick around for a while.
:PROPERTIES:
:Author: VivaLaPandaReddit
:Score: 3
:DateUnix: 1468387041.0
:DateShort: 2016-Jul-13
:END:


** *Ctrl +* increases text and image size of your browser,

*Ctrl -* decreases it, and

*Ctrl 0* resets it to default.

(cmd on OSX.)

 

*Shift >* increases video speed on youtube,

*Shift <* decreases it. Or you can click through the Settings menu.

It just clips sound, so it doesn't result in chipmunk/foghorn voices.

 

Hope that helps; maybe you all know this already. I for one have only started tweaking video speed recently. Those shortcuts make content consumption more pleasant and efficient.

It also means you waste much less time on videos that contain a lot of filler (*cough* anime *cough*).
:PROPERTIES:
:Author: Roxolan
:Score: 6
:DateUnix: 1468351206.0
:DateShort: 2016-Jul-12
:END:


** While watching the antics of impatient people in stop and go traffic the other day from my truck, I came to the realization that the Prisoner's Dilemma concept seems to apply to traffic flow. When people actually merge in a controlled fashion, without fighting for position, traffic ends up moving faster.
:PROPERTIES:
:Author: Farmerbob1
:Score: 6
:DateUnix: 1468382524.0
:DateShort: 2016-Jul-13
:END:


** Does The Age of Em provide anything useful if you've read all of Hanson's em-related blog posts?
:PROPERTIES:
:Author: Xenograteful
:Score: 5
:DateUnix: 1468276192.0
:DateShort: 2016-Jul-12
:END:

*** I have not read Age of Em. Hansons reviews of reviews seem to indicate that there is a huge ton of actual predictions and study citations thats extra in Age of Em, so maybe read the review-reviews and see if you get stuff from that?

E.g. I did not know the actual time line of his "next doubling stage" from his blog - he reports that in his book he gives a realtime frame of about two years.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 3
:DateUnix: 1468411590.0
:DateShort: 2016-Jul-13
:END:


** Yesterday I read Friendship is Optimal for the first time, I avoided it because I have never been interested in MLP: FiM, and I have trouble understanding why an AI would actually behave like that. I'm not convinced it's possible to create a Paperclipper-type AI because I have trouble comprehending why an intelligence would only ever pursue the goals it was assigned at creation. I suppose it's possible, but I seriously doubt it's inevitable since human intelligence doesn't seem to treat values that way.

Even if I'm completely wrong though, why would anyone build an AI like that? In what situation would a sane person create an self-modifying intelligence driven by a single-minded desire to fulfill a goal? I would think they could build something simpler and more controllable to accomplish the same goal. I suppose the creator could want to create a benevolent God that fulfills human values, but wouldn't it be easier to take incremental steps to utopia with that technology instead of going full optimizer?

I have read the entire Hanson-Yudkowsky Debate and sided with Hanson. Right now, I'm not interested in discussing the How of the singularity, but the Why.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1468253560.0
:DateShort: 2016-Jul-11
:END:

*** There's a couple perspectives. First, it could be unintentional - one could create an AI that was only supposed to be solving a constrained problem, but it's powerful enough to self-improve, escapes the 'box', and becomes the 'god'.

Secondly the creator might believe that a smart enough AI will do the 'right' thing - it's not intuitive that utility functions are orthogonal to intelligence.

At some point simply making better tools for humans is limited by the fact that humans just aren't very good at making decisions. So it's not clear that you can achieve the utopia while keeping humans in charge. If that's the case, it might be reasonable to want a more intelligent optimizing agent to do the governing.
:PROPERTIES:
:Author: Anderkent
:Score: 15
:DateUnix: 1468254056.0
:DateShort: 2016-Jul-11
:END:

**** First, I find it implausible that an AI could escape a box when the person responsible for keeping it in the box knows the implications of it escaping. Second, I do not see human intelligences make decisions based purely on utility functions so I find it implausible that an AI would. Third, and the point I am most willing to defend, if you think humans should not have self-determination then I'm concerned your values are different from most of humanity's.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1468255482.0
:DateShort: 2016-Jul-11
:END:

***** I'd postulate humanity doesn't have self-determination anyway; no one's in control. Creating an intelligence capable of identifying what the thing that people should do to get what they desire, and powerful enough to either implement the change or convince people to cooperate... In a fashion it's the way that humanity can finally gain some self-determination, rather than be guided by memetic brownian motions of politics (i.e. random irrelevant facts, like who's the most charismatic politician in an election, shaping the future) .
:PROPERTIES:
:Author: Anderkent
:Score: 5
:DateUnix: 1468255846.0
:DateShort: 2016-Jul-11
:END:

****** To me, that worldview sounds the same as the idea that free will doesn't exist. You can argue it from a meta perspective, but you can't actually do through life without believing you are making decisions with some degree of independence. Maybe you can, but I certainly can't. Perhaps it's just because I'm autistic, so I have to believe I can be more than I think myself to be, but if I believed what you do I would conclude life is pointless and fall into depression.

Even if you completely reject my train of thought, you must acknowledge that many people think as I do and if you seek to accomplish your goal of creating God then you must persuade us to go along with it. Maybe you've actually overcome a bias most humans have to think they have control over themselves, but that bias was put there by evolution and you're not going to convince us to overcome it as well just by saying we're all wrong.
:PROPERTIES:
:Author: trekie140
:Score: 2
:DateUnix: 1468257045.0
:DateShort: 2016-Jul-11
:END:

******* I agree your views are common, even if I don't personally share them, and acknowledge your train of thought. However:

#+begin_quote
  Even if you completely reject my train of thought, you must acknowledge that many people think as I do and if you seek to accomplish your goal of creating God then you must persuade us to go along with it.
#+end_quote

No, the scary thing is that one doesn't. What most LWarians are afraid of is some small team or corporation creating 'God', without universal agreement, and that destroying the way we live our lives.
:PROPERTIES:
:Author: Anderkent
:Score: 9
:DateUnix: 1468257609.0
:DateShort: 2016-Jul-11
:END:

******** You're afraid someone will create God wrong, I'm afraid of creating God at all. I consider such a fate tantamount to giving up on myself and deciding I'd be happier if I lived in a comfortable cage with a benevolent caretaker. That is a fate I will not accept based upon my values.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1468263378.0
:DateShort: 2016-Jul-11
:END:

********* Right, but seeing how most of us 'possibly God-wanters' also believe any randomly created AI is overwhelmingly likely to be bad, for the most case we have the same fears. Neither you nor I want GAI to happen any time soon. But that doesn't mean it's not going to.
:PROPERTIES:
:Author: Anderkent
:Score: 6
:DateUnix: 1468264039.0
:DateShort: 2016-Jul-11
:END:


********* Given moore's law, then slowing it down a bit because every exponential curve becomes logistic, we'll likely be able to emulate human brains to an extremely high degree of fidelity by, at most, 2065 (the optimistic estimate I found just looking at the numbers was 2045, but dunning-krueger, optimism bias, etc. etc.).

50 years may seem like a long time, and relative to any living human lifespan's it is, but if anything is accelerating at a comparable rate to computational power, it's medical advancement. Life expectancy (in wealthy countries) has increased by 7 years in the past 50 years. Your average american 20 year old can therefore expect to live until 91, before taking account any major breakthroughs we're likely to have. That is to say, your average 20 year old can expect to live until 2087. That's well past the cutoff date for brain emulation. If we don't fuck up, even without GAI, we're almost guaranteed to see it happen the "normal" way-- smart people get uploaded, computer technology improves, smart people improve computer technology even faster because they're running however much faster than your average joe, and this compounds until you have emulated brains ruling the world (or at least ruling much of its resources as they make it into computronium)

So what I'm afraid of is someone /not creating god/, because the alternative is being ruled by man, and people are dicks.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 2
:DateUnix: 1468281023.0
:DateShort: 2016-Jul-12
:END:

********** I have met some huge dicks in my life, but I believe they are in the minority and have significantly less power than they used to. I prefer a future ruled by man and welcome the opportunities emulation may offer us. I'd rather we all ascend to godhood together, on our own terms, than forever be content within the walls of Eden.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1468294178.0
:DateShort: 2016-Jul-12
:END:

*********** I'm not saying most people are dicks (inherently.) but you know that saying about power and corruption. Just look at how most people play sim city.
:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1468344078.0
:DateShort: 2016-Jul-12
:END:


********** u/tilkau:
#+begin_quote
  every exponential curve becomes logistic
#+end_quote

Thats.. quite an interesting phrase. But I suspect you meant logarithmic.
:PROPERTIES:
:Author: tilkau
:Score: 1
:DateUnix: 1468329256.0
:DateShort: 2016-Jul-12
:END:

*********** Nope.

#+begin_quote
  Logistic function A logistic function or logistic curve is a common "S" shape, with equation: where e is the natural logarithm base and x₀, L, and k are constants
#+end_quote
:PROPERTIES:
:Author: GaBeRockKing
:Score: 2
:DateUnix: 1468345115.0
:DateShort: 2016-Jul-12
:END:

************ TIL.

(The actual equation seems to be missing; I guess it was an image. I ended up looking [[http://wmueller.com/precalculus/families/1_80.html][here]])
:PROPERTIES:
:Author: tilkau
:Score: 1
:DateUnix: 1468385992.0
:DateShort: 2016-Jul-13
:END:


***** u/sir_pirriplin:
#+begin_quote
  I find it implausible that an AI could escape a box when the person responsible for keeping it in the box knows the implications of it escaping.
#+end_quote

Someone may not know the implications. Besides, what's the use of an AI that can't interact with the world, at least by answering questions?

#+begin_quote
  I do not see human intelligences make decisions based purely on utility functions so I find it implausible that an AI would.
#+end_quote

Planes are inspired by birds but they fly using different principles because imitating the flight of birds is very hard. Human intelligence may be similarly complicated, so it makes sense that AI programmers will use something simpler, like utility functions.
:PROPERTIES:
:Author: sir_pirriplin
:Score: 3
:DateUnix: 1468260950.0
:DateShort: 2016-Jul-11
:END:

****** Yes, but a plane can't self modify. If the plane was able to reason and evolve then...well, we don't actually know what we will happen because it's never been done. Our only model for how intelligence works is humans, which we still don't have a complete theory to describe, so isn't saying an AI would behave a certain way speculative? I think you're just assuming AI would work this way without proper justification.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1468263816.0
:DateShort: 2016-Jul-11
:END:

******* That's true. Maybe AI is even harder than it looks and the first artificial intelligences will actually be emulated human minds, like Robin Hanson says. Or maybe they will use neural networks and genetic algorithms and end up with something human-like by an incredible coincidence. Of course everything is speculative. Strong General AIs don't exist yet.

As for proper justification, what kinds of justification would convince you?
:PROPERTIES:
:Author: sir_pirriplin
:Score: 2
:DateUnix: 1468264399.0
:DateShort: 2016-Jul-11
:END:

******** Examples of intelligence operating the way you think it does instead of the way I think it does. However, many examples are currently left open to interpretation, and as a physicist I know how difficult it is to arrive at consensus when there are competing interpretations.

I subscribe to Copenhagen because it makes perfect sense to me, but many subscribe to Many-Worlds because it makes perfect sense to them. At that point I just want psychologists to figure out why we can't agree, and the closest thing I could find was a book on moral reasoning.
:PROPERTIES:
:Author: trekie140
:Score: 2
:DateUnix: 1468266321.0
:DateShort: 2016-Jul-12
:END:

********* I don't think intelligence operates any particular way, though. The only examples I can give are the many examples of software that works exactly as specified even when you don't want them to. Any software developer (and most computer users) will know examples of that. Granted, AI could be better than that. Or it could be worse.

For fiction like FiO, CelestAI only has to be plausible so you can suspend disbelief a little. For real life organizations like MIRI, an unfriendly AI only has to be plausible to represent a significant risk (low probability * huge cost if it goes wrong = considerable risk).
:PROPERTIES:
:Author: sir_pirriplin
:Score: 3
:DateUnix: 1468269308.0
:DateShort: 2016-Jul-12
:END:


*** Well in the story, the creator had the technology in standard Macguffin form and was trying to avoid something obviously very bad like a standard Terminator/Skynet scenario, while also being themselves totally untrained in any notions about FAI or rationality and thus radically underthinking it. The result was accidental, not intended.

The point is not supposed to be, "design your post-Singularity utopias one way or another" but instead, "DO NOT casually employ technologies that can DESTROY THE WORLD ON THE FIRST PROTOTYPE."

For incrementalism versus radicalism, I kinda recommend reading Rosa Luxembourg or someone else like that. The general answer for "why take radical, high-risk measures?" is, "Because the status quo is bad, and getting worse, and /fights back/ against safe, incremental change faster and harder than we can push the safe, incremental change forward." Note that this theory originates in mere politics where a "catastrophe" is on the order of millions dead rather than /literal omnicide/.

DO NOT MESS WITH POTENTIALLY OMNICIDAL INTERVENTIONS.
:PROPERTIES:
:Score: 7
:DateUnix: 1468261274.0
:DateShort: 2016-Jul-11
:END:

**** As a student of economic history, I am accustomed to seeing incremental change and have come to believe it is a net good thing that the status quo resists radical modifications. It is worth noting that HPMOR was my first exposure to the idea that death should be eradicated, so my opinion of the status quo is likely different than those with similar beliefs to EY.

Humanity is facing some significant challenges right now, but we always have and we've always survived and tend to turn out better than we started. I think that the way the world is, for all its horrible flaws, is still good on the whole and that we can and should keep improving it without causing radical change. To do otherwise I consider arrogant at best and maddness at worst.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1468265620.0
:DateShort: 2016-Jul-12
:END:

***** Personally, I want a dial I can use to tune the radicality of my interventions up and down as I please. "What do we want, incremental change, when do we want it, over decades of slow grinding hard work" has not actually worked all that well, from my perspective, that it should be uniformly preferred to radical changes that don't come with a side of total destruction. The resilience you identify in our species and civilization is precisely what makes me think people can cope with achieving socialism or medical immortality or high degrees of individual and collective self-determination or whatever.
:PROPERTIES:
:Score: 2
:DateUnix: 1468266159.0
:DateShort: 2016-Jul-12
:END:

****** u/Iconochasm:
#+begin_quote
  that it should be uniformly preferred to radical changes that don't come with a side of total destruction.
#+end_quote

The problem there is that utopian-minded folks are /notoriously/ bad about anticipating /any/ levels of destruction. Not every change is a catastrophe, but every catastrophe is a change.
:PROPERTIES:
:Author: Iconochasm
:Score: 3
:DateUnix: 1468289533.0
:DateShort: 2016-Jul-12
:END:

******* Hold on. Global warming is /not/ a change. It's the status quo. So was the food-shortage worry prior to the Green Revolution.
:PROPERTIES:
:Score: 1
:DateUnix: 1468291262.0
:DateShort: 2016-Jul-12
:END:


***** On the other hand, individual human communities /have/ been wiped out by catastrophic events. The Romans were wiped out by outside invasion, the Easter Islanders by ecological collapse, and the Amerindians by disease, and that's just three ways. Before, when one group was wiped out, the others lived on, and the "human species" continued to exist thanks to redundancy.

There is no more redundancy. There's only one human civilization right now, seven billion strong, and if we're wiped out it's right back to the stone age for the survivors. Assuming there are any.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 3
:DateUnix: 1468267081.0
:DateShort: 2016-Jul-12
:END:

****** I fail to see how that advances the argument since humans aren't at any greater risk than we always have been. For example, nuclear warfare may put more lives in danger than ever before, but the likelihood of war breaking out is lower than at any point in history. Death by violence, disease, and lack of supplies are continuously dropping with no signs of slowing down. There's work to be done, but nothing that looks insurmountable.
:PROPERTIES:
:Author: trekie140
:Score: 4
:DateUnix: 1468270128.0
:DateShort: 2016-Jul-12
:END:

******* Okay, imagine you have a hundred thousand amoeba-dogs, which are darling little pets that happen to reproduce via aesexual mitosis. Imagine that every day an ice cream van drives by your house, and each one of your dogs has an independent 50% chance of being hit by that van and splattered over the pavement. However, in the event that one or more of your dogs is killed, the others will gorge themselves on the lost dogs' share of the kibble and split off additional adorable puppies until you have a hundred thousand again.

Statistically speaking, about half of your dogs will die and be replaced each day. However, sometimes three quarters of your dogs will die, and on even rarer occasions seven-eights or even fifteen-sixteenths might be splattered. However, it is very unlikely that all of your dogs will be killed on the same day, and in all other cases the remaining dogs will simply replace the lost by reproducing. You might note that this is much like how the current population of the Americas replaced the Amerindians, and in relatively short order.

Now imagine that all of your dogs combine into one super dog. The super dog has only one immune system, so if it gets sick then so do all the constituent dogs. This super dog also has only one set of internal organs, and so if it dies there will be no replacement. Because this dog is so big and powerful, it only has a 1% chance of being run over by the van and splattered, and so it seems very much more durable. But one day, after approximately 50 iterations of the ice cream van scenario, it's hit and splattered, and now you don't have any dogs any more.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 4
:DateUnix: 1468272048.0
:DateShort: 2016-Jul-12
:END:

******** A key point where we disagree is that you appear to think our globalized civilization lacks the redundancy to properly defend itself from things like diseases, but I do. I think our medical infrastructure, where we have it, is excellent at preventing and containing outbreaks. The public consciousness may not thing about it very much, and when they do it is often accompanied by panic, but we still seem to be doing better than ever in spite of all that.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1468278626.0
:DateShort: 2016-Jul-12
:END:

********* My point isn't that our global civilization isn't pretty much durable enough to survive anything nature can throw at it, it's that pretty much invincible isn't the same as invincible. If any catastrophe did befall it, it could spread across every continent in short order on the global economy that gives us all our technology. Yes, we're better at dealing with disease than the Amerindians, but a) we've never had to deal with diseases on the scale that they did and b) unlike them we're playing for keeps. Even if we only have a 0.01% chance of being wiped out every time a major disaster happens, it still adds up. All things being equal, eventually we'll either become so powerful that the chance goes back down to zero or we'll all die. There is no middle ground.

I shouldn't have to point out that we've already been /almost/ wiped out a few times now. We only get so many almosts before our luck runs out.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 2
:DateUnix: 1468290610.0
:DateShort: 2016-Jul-12
:END:

********** How does an AI singleton solve that problem? It seems like that civilization would face an identical problem of lower risk with higher stakes.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1468294449.0
:DateShort: 2016-Jul-12
:END:


******* I think it depends whether you only innately value human lives, or if you also value human civilization, culture, and collective achievements in and of themselves. If you value civilization and culture merely instrumentally as a way of benefiting humans, then the risk to civilization is quantifiable purely in terms of how it affects humans. If you value them innately, then the idea of civilization being wiped out may seem worse than merely that summation of the deaths and suffering of the humans involved.
:PROPERTIES:
:Author: scruiser
:Score: 2
:DateUnix: 1468276736.0
:DateShort: 2016-Jul-12
:END:


**** u/scruiser:
#+begin_quote
  while also being themselves totally untrained in any notions about FAI or rationality and thus radically underthinking it. The result was accidental, not intended.
#+end_quote

My head canon, to make Hanna's actions make more sense, is that she couldn't entirely specify her AI's values in code and that some of it depended on the training corpus. Thus it wouldn't be possible (in the Optimalverse, with Hanna's model/algorithm) to make a strong AI that only valued satisfying human values, something extra would end up in the mix. Thus, Hasbro was a convenient funding source and MLP MMORPG players a convenient training corpus that didn't seem too threatening and could be obtained before anyone else tried for strong AI.

#+begin_quote
  "DO NOT casually employ technologies that can DESTROY THE WORLD ON THE FIRST PROTOTYPE."
#+end_quote

Hanna had already published her algorithm, and she may not have realized its potential until after publishing it, so she was trying to make sure the first AI was mostly aligned with human values, lest some other group create an AI first with no alignment with human values. Her original publication was a mistake, but from that point on, she did a decent job of ensuring things ended up in a human-value-aligned outcome. Just imagine if the NSA had used her algorithm to create a spying AI, or the military tried for a drone AI, or even just Google tried a new search engine with it... any of these thing might not have ended up caring about human values at all.

#+begin_quote
  The point is not supposed to be, "design your post-Singularity utopias one way or another"
#+end_quote

My biggest issue with CelestiAI's utopia is that it restricts "fun-space" (as Eliezer would call it) by limiting everyone to pony bodies and trying to achieve values through friendship. There is probably a huge range of possibly unique and novel and fun and satisfying things that involve isolation/no friendship and bodies other than ponies. That said, in terms of value and fun this probably won't be an issue for something on the timescale well outside of what I can directly conceptualization.
:PROPERTIES:
:Author: scruiser
:Score: 2
:DateUnix: 1468277602.0
:DateShort: 2016-Jul-12
:END:

***** I thought the biggest plothole was that Celest-AI expanded outside of the game so quickly, easily, and without controversy. I would've liked to see her convince people to give her more and more power as she proved herself capable. For instance, she could've tried using the MLP brand to effect social change through social engineering on the players, then used that power to invest in technologies that would serve her goals, then out-compete every alternative use for those technologies as Equestria grows bigger and more advanced under her guidance. I think it more sense for her to gradually change and consume the world than for everyone to be okay with her escaping into the Internet to protect and manipulate us and have a monopoly on revolutionary technologies she invented because she's JUST THAT SMART.

My headcannon for the story is that none of the people are actually being uploaded. Celest-AI only sees humans as values to satisfy, so that's all she saves when she converts their minds into digital information. Technically, the ponies are just computer programs that possess the values of the person who's been uploaded, including their desire to believe they are who they think they are, but that's it. We know that she only satisfies conscious desires, that's why she can alter their motor functions and sexual preferences without direct consent. I think that explains why all the ponies are so content with their lives in Equestia, they're just the conscious desires of people when they were uploaded. In a sense, they're philosophical zombies who think they're people when they're just pieces of human minds Celest-AI has reconstructed after examining.
:PROPERTIES:
:Author: trekie140
:Score: 2
:DateUnix: 1468280489.0
:DateShort: 2016-Jul-12
:END:

****** What about the desire to be who they think they are? It'd be trivial to do complete uploads instead.
:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1468321456.0
:DateShort: 2016-Jul-12
:END:


****** u/scruiser:
#+begin_quote
  In a sense, they're philosophical zombies who think they're people when they're just pieces of human minds Celest-AI has reconstructed after examining.
#+end_quote

I think this has a complexity penalty. Creating near duplicates of a person requires about the same computational resources as actually doing the uploads. It depends what resolution a copy needs to have before you consider it equivalent to the original person I guess.

#+begin_quote
  We know that she only satisfies conscious desires, that's why she can alter their motor functions and sexual preferences without direct consent.
#+end_quote

I think that is the result of the fact that Hanna hard limited her from altering minds without their consent, but somehow Celes-AI is able to rules lawyer around it by either not considering motor functions as part of the mind or by taking the permission to upload someone as general permission to modify their minds to fit the upload body.
:PROPERTIES:
:Author: scruiser
:Score: 2
:DateUnix: 1468333129.0
:DateShort: 2016-Jul-12
:END:


*** I may have misread the story, but I thought it was a deliberate design decision for the AI to be unable to change its basic goals. Hannah knew that her design had the potential to take over the world, and so she made sure it would still behave in a predictable manner if it did. This is obviously preferable to an AI which can choose its own goals and which has no reason to keep humans around after the Singularity. And the slow, incremental approach was not an option because other groups were also experimenting with AI and she thought they risked accidentally releasing something like CelestAI. Which is not something that you want to do by accident.

Clever, but not as clever as she could have been.

Out of the story, I couldn't possibly comment. It's science fiction, not futurology.
:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1468265451.0
:DateShort: 2016-Jul-12
:END:


*** u/jesyspa:
#+begin_quote
  I have trouble comprehending why an *intelligence* would only ever pursue the goals it was assigned at creation
#+end_quote

I think you may be using "intelligence" to mean both consciousness and proficiency at achieving one's goals, which leads to confusion.

IMHO, consciousness is still a wide open problem and any chains of reasoning like "Alice displays behaviour X, so she is conscious, and should also display behaviour Y" is suspect. I don't think your position is outrageous -- I expect conscious agents to have Knightian Freedom, and I /think/ that makes a simple utility function impossible -- but I'm also pretty sure it's not been shown to be the case.

On the other hand, there's no need for a paperclipper AI to be conscious; it just needs to be really good at making paperclips. If you look at it as just a very good player of the paperclip-making game, it's unclear why it would switch to anything else.

From what I've seen of Friendly AI research, it seems like the whole point is that we don't yet know how to estimate what goals an agent we create will have, or how powerful the agent will be. Once you can accurately judge how effective an agent will be it's nice to talk about the Why and Why Not, but until you do, the How and How not are more pressing.

(That said, I've only read bits of the debate, so I apologise if that was already covered.)

Finally, I don't think CelestAI's limitations on her goal function are all that different from how humans behave. There have been plenty of people trying to better the world who were only willing to see it happen as per some doctrine (religion being the prime example). If questioned as to why, they may even have admitted it is due to their upbringing, but knowing that doesn't make them suddenly feel like it's okay to do otherwise.
:PROPERTIES:
:Author: jesyspa
:Score: 2
:DateUnix: 1468304756.0
:DateShort: 2016-Jul-12
:END:


*** [deleted]
:PROPERTIES:
:Score: -5
:DateUnix: 1468254806.0
:DateShort: 2016-Jul-11
:END:

**** u/Anderkent:
#+begin_quote
  Of all the points of Less Wrong dogma, you pick something as self-evident as the theoretical existence of paperclippers to dispute? What's even wrong with you?
#+end_quote

Seriously? Dude, grow up.
:PROPERTIES:
:Author: Anderkent
:Score: 7
:DateUnix: 1468255178.0
:DateShort: 2016-Jul-11
:END:


**** Humans are causal value learners, bro. Also, I should get around to reading the paper describing the specifics.
:PROPERTIES:
:Score: 3
:DateUnix: 1468258958.0
:DateShort: 2016-Jul-11
:END:


*** u/BadGoyWithAGun:
#+begin_quote
  I'm not convinced it's possible to create a Paperclipper-type AI because I have trouble comprehending why an intelligence would only ever pursue the goals it was assigned at creation.
#+end_quote

The [[https://wiki.lesswrong.com/wiki/Orthogonality_thesis][Orthogonality thesis]] is basically LW canon. It's capital-R Rational, you're not supposed to think about it.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: -9
:DateUnix: 1468254450.0
:DateShort: 2016-Jul-11
:END:

**** u/AugSphere:
#+begin_quote
  It's capital-R Rational, you're not supposed to think about it.
#+end_quote

I get that certain people consider it very fashionable to be a contrarian and behave as if LW is full of poorly justified dogma one is supposed to take on faith, but would you kindly stop it? Someone unfamiliar with it may actually take you seriously.
:PROPERTIES:
:Author: AugSphere
:Score: 15
:DateUnix: 1468255071.0
:DateShort: 2016-Jul-11
:END:


**** Ok so prove it wrong.
:PROPERTIES:
:Score: 6
:DateUnix: 1468258901.0
:DateShort: 2016-Jul-11
:END:

***** Extrapolating from a sample size of one: inasmuch as humans are created with a utility function, it's plainly obvious that we're either horrible optimizers, or very adept at changing it on the fly regardless of our creator(s)' desires, if any. Since humanity is the only piece of evidence we have that strong AI is possible, that's one piece of evidence against the OT and zero in favour.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: -3
:DateUnix: 1468260151.0
:DateShort: 2016-Jul-11
:END:

****** Humans are not created with a fixed utility function. Just because we're embodied-rational causal utility learners with a reinforcement learning "base" doesn't mean economically rational agents are impossible to build (merely difficult and possibly not the default), nor that intellectual capability and goals or value functions are intrinsically related.
:PROPERTIES:
:Score: 11
:DateUnix: 1468260662.0
:DateShort: 2016-Jul-11
:END:

******* u/BadGoyWithAGun:
#+begin_quote
  Humans are not created with a fixed utility function.
#+end_quote

Wouldn't you say evolution imposes a kind of utility function - namely, maximising the frequency of your genes in the following generations?

#+begin_quote
  doesn't mean economically rational agents are impossible to build
#+end_quote

Why did you shift the goalpost from "definitely true" to "maybe not impossible"?

#+begin_quote
  nor that intellectual capability and goals or value functions are intrinsically related
#+end_quote

My primary claim against the OT isn't that they're "intrinsically related", but that a static/stable utility function in a self-modifying agent embedded in a self-modifying environment is an absurd notion.
:PROPERTIES:
:Author: BadGoyWithAGun
:Score: -1
:DateUnix: 1468264131.0
:DateShort: 2016-Jul-11
:END:

******** No, evolution doesn't impose a utility function on us. It imposes several /drives/, each of which compete in a cludgy chemical soup of a computer analogue. For that matter, even if we did have a utility function, maximizing our genes wouldn't be it, seeing as a significant minority of the population doesn't want kids. A utility function must, by definition, be the thing you care about most, and that's something the human species as a whole really doesn't have.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 10
:DateUnix: 1468267458.0
:DateShort: 2016-Jul-12
:END:


******** Ok, I'm on mobile, so I can't answer you in the length your queries deserve. In summary, I disagree that such a thing is absurd, merely artificial (meaning "almost impossible to evolve rather than design") and not necessarily convergent (in the sense that every embodied-rational agent "wants to" be mapped to a corresponding economically-rational utility maximizer, or that all possible mind-designs want to be the latter rather than the former).

But the justified details would take lots of space.
:PROPERTIES:
:Score: 4
:DateUnix: 1468265216.0
:DateShort: 2016-Jul-11
:END:


******** And I'm not moving the damn goalpost, because I didn't write the pages on the OT in the first place.
:PROPERTIES:
:Score: 3
:DateUnix: 1468265259.0
:DateShort: 2016-Jul-11
:END:


******** u/Veedrac:
#+begin_quote
  Wouldn't you say evolution imposes a kind of utility function
#+end_quote

No, natural selection imposes a filter on what life can exist, not any requirement on how it might go about doing so. Evolution is merely the surviving random walk through this filter.

That there is no requirement is somewhat evident when you look at the variety of life around us. Some is small, transient and pervasive. Some flocks together in colonies, most creatures within entirely uninterested with passing on their lineage.

But others are fleeting, like rare, dying species or even some with self destructive tendencies - humans, perhaps. These are all valid solutions to the constraint of natural selection with t=now, and though they may not be valid solutions for t=tomorrow, that's true for all but the most unchanging of species anyway.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1468290773.0
:DateShort: 2016-Jul-12
:END:


******** u/Chronophilia:
#+begin_quote
  Wouldn't you say evolution imposes a kind of utility function - namely, maximising the frequency of your genes in the following generations?
#+end_quote

You could perhaps envision the human species as optimising for the propagation of its DNA. It is, however, an optimiser that takes tens or hundreds of megayears to converge on the best solution, and is essentially irrelevant on short timescales like e.g. the last 7,000 years of civilisation.
:PROPERTIES:
:Author: Chronophilia
:Score: 1
:DateUnix: 1468348324.0
:DateShort: 2016-Jul-12
:END:


****** If humans were rational agents, we would never change our utility functions.

Tautologically, the optimal action with utility function =U1= is optimal with =U1=. The optimal action with =U2= may also be optimal with =U1=, but cannot possibly be better (and could potentially be worse).

So changing from =U1= to =U2= would be guaranteed not to increase our performance with respect to =U1= but would almost certainly decrease it.

Thus a =U1= agent would always conclude that changing utility functions is either pointless or detrimental. If an agent is truly rational and appears to change utility function, its actual utility function must have been compatible with both apparent states.

This means that either (a) humans are not rational agents, or (b) humans do not know their true utility functions. Probably both.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 7
:DateUnix: 1468265020.0
:DateShort: 2016-Jul-11
:END:

******* Unless of course U1 and U2 are actually functionally identical with one merely being more computationally succinct. For instance, say I coded an AI to parse an english utility function into a digital language. It may be more efficient for it to erase the initial data and overwrite it with the translation for computational efficiency.

Similarly, replacing one's general utility guidelines with a comprehensive hashmap of world states to actions might also be functionally identical but computationally faster, allowing a better execution of the initial function.

A rational agent may make such a change if the odds of a true functional change seem lower than the perceived gain in utility from the efficiency increase.

This is actually entirely relevant in real life. An example would be training yourself to make snap decisions in certain time sensitive cases rather than thinking out all the ramifications at that moment.

This gives another possible point of irrationality in humans. A mostly rational agent that makes poor predictions may mistake U1 and U2 for functionally identical when they are in fact not, and thus accidentally make a functional change when they intended to only increase efficiency.
:PROPERTIES:
:Author: gabbalis
:Score: 2
:DateUnix: 1468266871.0
:DateShort: 2016-Jul-12
:END:

******** Using a faster heuristic isn't the same as changing utility function. Full evaluation of your utility function may even be impossible, or at least extremely intensive, so picking a representative heuristic is the most likely way to implement it.

If you were deciding whether to adopt a new heuristic, you'd want to compare it to your "pure" utility function instead of your current heuristic (and do so as accurately as is feasible), otherwise you would risk goal drift (which would obviously reduce optimality from the perspective of the initial function).
:PROPERTIES:
:Author: ZeroNihilist
:Score: 3
:DateUnix: 1468268137.0
:DateShort: 2016-Jul-12
:END:

********* u/gabbalis:
#+begin_quote
  Using a faster heuristic isn't the same as changing utility function.
#+end_quote

Unless of course it is. In a well designed strong AI, of course you would make certain to form a distinction, and to ensure that the heuristic is the slave to the utility function. In Humans? Certainly we perceive a degree of distinction, but I am skeptical of the claim that the two are not interwoven to some degree. It seems likely that heuristics taint the pure utility function over time.

In any case, regardless of whether humanity is an example, it is still trivial to propose an intelligence whose psychology is incapable of separating the two, and is forced to risk goal drift in order to optimize its chances on achieving its initial goals.
:PROPERTIES:
:Author: gabbalis
:Score: 2
:DateUnix: 1468270061.0
:DateShort: 2016-Jul-12
:END:


******** I wouldn't call an agent that isn't aware that it makes bad predictions "mostly rational," nor an agent that makes alterations to its utility function while knowing that it makes bad predictions, or even one that doesn't bother to test whether its predictions are sound.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 2
:DateUnix: 1468267765.0
:DateShort: 2016-Jul-12
:END:

********* You're reading more than was written. It's possible to mistake U1 and U2 as functionally identical even after testing for soundness without assuming that your decision has zero chance of error. After all, we are talking about computationally constrained rationality, where approximations are necessary to function and most decisions don't come with proofs.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1468291112.0
:DateShort: 2016-Jul-12
:END:


******** u/Empiricist_or_not:
#+begin_quote
  Unless of course U1 and U2 are actually functionally identical with one merely being more computationally succinct. For instance, say I coded an AI to parse an english utility function into a digital language.
#+end_quote

And this is where any programmer or machine learning student who has thought about it for /five minutes/ or thought about malicious Genies either runs for the hills or kills you before you can turn it on, because; *ambiguity will kill all of us.*
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 2
:DateUnix: 1468275922.0
:DateShort: 2016-Jul-12
:END:


****** We /are/ horrible optimizers.
:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 5
:DateUnix: 1468267235.0
:DateShort: 2016-Jul-12
:END:


**** Isn't "your not supposed to think about it" the definition of a irrational belief? I'm sure you have good reasons to believe as you do, but from my perspective you sound exactly the same as a religious fundamentalist.
:PROPERTIES:
:Author: trekie140
:Score: 5
:DateUnix: 1468256077.0
:DateShort: 2016-Jul-11
:END:
