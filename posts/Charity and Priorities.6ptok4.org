#+TITLE: Charity and Priorities

* Charity and Priorities
:PROPERTIES:
:Author: Tsegen
:Score: 5
:DateUnix: 1501130357.0
:DateShort: 2017-Jul-27
:END:
[removed]


** I've never found a charity evaluator more rigorous than GiveWell, so I'd probably start with their top pick, Against Malaria Foundation. My priority is alleviating suffering, so this suits me. AMF takes a very cost-effective approach to alleviating suffering.
:PROPERTIES:
:Author: CeruleanTresses
:Score: 14
:DateUnix: 1501143064.0
:DateShort: 2017-Jul-27
:END:

*** One of the things that should be considered though is diminishing returns on investment. I can't recall the exact amount but I'm pretty sure Givewell mentions how much more AMF should revive before we reprioritize to other stuff like deworming initiatives or Give Directly, and I'm pretty sure [citation needed] that the amount is waaaay lower than a hundred billion.
:PROPERTIES:
:Author: gregx1000
:Score: 8
:DateUnix: 1501153670.0
:DateShort: 2017-Jul-27
:END:

**** Yes, that's correct.
:PROPERTIES:
:Author: CeruleanTresses
:Score: 3
:DateUnix: 1501161642.0
:DateShort: 2017-Jul-27
:END:


** As mentioned elsewhere, GiveWell is among the most rigorous metacharities out there. In all honesty, I trust in the competency of people who's life work is evaluating charities over my own in regards to maximizing utility. So if I really had a hundred billion I'd shoot them an email and ask for advice, or maybe just give it to them straight up to redistribute at their discretion.
:PROPERTIES:
:Author: gregx1000
:Score: 8
:DateUnix: 1501153849.0
:DateShort: 2017-Jul-27
:END:

*** Step 1: Donate $100M to GiveWell asking them to develop in 10 years time a plan for effectively donating $100B.

Step 2: Wait 10 years.

Step 3: Follow plan.

Step 4: Profit???
:PROPERTIES:
:Author: Daneels_Soul
:Score: 5
:DateUnix: 1501172070.0
:DateShort: 2017-Jul-27
:END:

**** And then if you invested the $99.9B in the meantime, you have even more to distribute at the end of the ten years.
:PROPERTIES:
:Author: CeruleanTresses
:Score: 3
:DateUnix: 1501179549.0
:DateShort: 2017-Jul-27
:END:


** Someone reported this. It /is/ pretty borderline according to our rules that stuff should be a fictional story, but honestly, I'm inclined to allow it.
:PROPERTIES:
:Score: 8
:DateUnix: 1501171117.0
:DateShort: 2017-Jul-27
:END:

*** It bothers me that [[/r/rational][r/rational]] is restricted to fiction. I'd like it if we could also allow STEM news, like linking and discussing papers.
:PROPERTIES:
:Author: narfanator
:Score: 2
:DateUnix: 1501177948.0
:DateShort: 2017-Jul-27
:END:

**** That's... a step too far. [[/r/science]] discusses papers, and unfortunately that gets really horrible really quickly. [[http://explosm.net/comics/3557/][Too many people don't love science, they love looking at its butt when it walks by.]]

And I don't mean "too many" as in, "the users here are bad and like that". I mean that if we opened up the sub that way, we would basically be flashing science's butt in front of a mass user base, who would then flood in to look at it.

Actually discussing papers from a level of even amateur fluency, by the way, tends to suffer the opposite problem: it's really hard to get two people in a room who understand the same paper unless you've got a community of experts. It turns into one person lecturing everyone else.

(No mod hat in this comment, personal opinions, no community decision has been made.)
:PROPERTIES:
:Score: 12
:DateUnix: 1501178434.0
:DateShort: 2017-Jul-27
:END:


** If I had 100 billion dollars I'd just buy a bunch of US Congresscritters and have them give out federal grants and loans to charities.
:PROPERTIES:
:Score: 4
:DateUnix: 1501176855.0
:DateShort: 2017-Jul-27
:END:


** If I had $100 billion to distribute I would do a pretty ridiculous amount of research and consulting with people, but I would be surprised if I didn't end up giving at least a billion each to both [[https://intelligence.org/][MIRI]] and [[http://www.sens.org/][SENS]] who are working on what I percieve to be very important problems from promising angles and are heavily underfunded considering that.
:PROPERTIES:
:Author: HeckDang
:Score: 6
:DateUnix: 1501135076.0
:DateShort: 2017-Jul-27
:END:

*** Seconding SENS, I'm on the fence about the effectiveness of MIRI
:PROPERTIES:
:Author: gods_fear_me
:Score: 3
:DateUnix: 1501145859.0
:DateShort: 2017-Jul-27
:END:


*** "Poor, misguided random internet person- your donation to MIRI/LessWrong will not help save the world. Even if you grant all their (rather silly) assumptions MIRI is a horribly unproductive research institute- in more than a decade, it has published fewer peer reviewed papers than the average physics graduate student does while in grad school. The majority of money you donate to MIRI will go into the generation of blog posts and fan fiction. If you are fine with that, then go ahead and spend your money, but don't buy into the idea that this money will save the world."

-[[https://danluu.com/su3su2u1/hpmor/][su3su2u1]]
:PROPERTIES:
:Author: abcd_z
:Score: 10
:DateUnix: 1501139089.0
:DateShort: 2017-Jul-27
:END:

**** Publication number might not be the best metric, though I have no idea what the best metric is :)
:PROPERTIES:
:Author: himself_v
:Score: 6
:DateUnix: 1501148563.0
:DateShort: 2017-Jul-27
:END:


**** I've read a lot of su3su2su1's criticisms, and there's plenty of merit to them. Even so I think MIRI is underfunded relative to where they currently stand. Note that I didn't say I wanted to give the entire $100 billion to them.
:PROPERTIES:
:Author: HeckDang
:Score: 8
:DateUnix: 1501139247.0
:DateShort: 2017-Jul-27
:END:

***** I'd donate quite a lot of money to EY to have him write more fiction like HONOR. Not just for my own enjoyment, I think that is the best way to spread his ideas and advance his goal of raising the sanity waterline.
:PROPERTIES:
:Author: Frommerman
:Score: 1
:DateUnix: 1501186165.0
:DateShort: 2017-Jul-28
:END:


**** To put this with the most possible /niceness/ towards people who are literally on this subreddit and will probably read this comment...

Has anyone tried paying the MIRI folks to go train at rather more productive institutions? I know most of them have PhDs by now, but come on, guys. The academic process and scientific method exist for very real reasons as applied to actually-existing humans, and reciting the formulas for Solomonoff Induction like Psalms is not going to make you anything other than an actually-existing human.

If you truly believe in dedicating yourself to solving the problems you care about, go subject yourself to being trained and/or beaten-up until you can demonstrate to a broad community that you're producing useful, understandable research.

If you truly believe that advocacy is the most effective way forward for the things you care about, become an excellent advocate for your cause, like McKaskill or even freaking Bostrom.

If you just wanna do you, keep at exactly what you're doing, since you're already you.
:PROPERTIES:
:Score: 3
:DateUnix: 1501171426.0
:DateShort: 2017-Jul-27
:END:

***** Okay, so quick question: have you read MIRI's latest papers? (Real question, not some kind of rhetorical technique [though I think it says disheartening things about the current state of discourse that such a disclaimer is necessary].)
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 2
:DateUnix: 1501180237.0
:DateShort: 2017-Jul-27
:END:

****** How latest is latest? I've read the one about logical inductors, I've read papers that came earlier than that, and I might have checked more recently than late 2016 or early 2017.

I like their work on the Loebian obstacle, but IMHO they need to find a way to generalize it. Several people have asked them, why work with first-order logical systems? They've said: because those make it easy to express [[http://www.retrologic.com/jargon/Q/quine.html][quining]], and therefore to express reasoning about systems that can quine themselves and how to reason reflectively (ie: under quining).

In the same way that most paradox theorems reduce to a fixed-point theorem or a theorem about needing some number of Halting Oracles, I'd like to see the various good results MIRI has produced on /individual/ forms of Loebian reasonig generalized into a single "Quining Problem" to which all other quined-reasoning problems can be computationally reduced. It seems like Logical Inductors might be heading in this direction, but even then I'd like to see a more computational phrasing -- just because I think an infinite loop is a "deeper" phenomenon than unprovability of reflective theorems.

I've also seen much of their work on decision theory, including the recent one on Functional Decision Theory. After a long period of thinking about it, including attending in-person sessions about it, I don't like it. I have a fairly specific set of reasons /why/ I don't like it, and here, in short, I'll just say that I disagree with some of the premises, and discussions have tended to get deep into the weeds and wind back a whole load of premises I don't share with people who like the decision-theory research. If you want to hash that out on a PM, we can, but I'm going to stop here. Elaborating reasons why I think their research is unhelpful on a reddit comment, without the professional philosophy and cogsci chops to explain why I think a whole subfield is /just wrong/, is an unkind waste of everyone's time.
:PROPERTIES:
:Score: 2
:DateUnix: 1501183992.0
:DateShort: 2017-Jul-28
:END:


***** I remain somewhat unconvinced that academia is doing anything the UBI wouldn't do better.
:PROPERTIES:
:Author: traverseda
:Score: 1
:DateUnix: 1501201005.0
:DateShort: 2017-Jul-28
:END:

****** Well, go find me a UBI experiment that produces DeepMind-level AI research, or NASA-level climate research. Those are part of the same system as academia.

I'll wait.
:PROPERTIES:
:Score: 1
:DateUnix: 1501206190.0
:DateShort: 2017-Jul-28
:END:

******* :/

You're asking me to compare a very small sample size to the /best/ of a very large sample size. It's reasonable to get a bit defensive, but at least check to see if what you're saying passes the smell test. You're still a rationalist. That's not an argument that's a soldier.
:PROPERTIES:
:Author: traverseda
:Score: 2
:DateUnix: 1501207642.0
:DateShort: 2017-Jul-28
:END:

******** Well, here's the perspective I act on, hence the one I actually believe: 90% of what academia produces is shit. The difference is that you can't tell which 90% /ahead of time/.

I've published a shitty paper. I'm also reading a thesis that makes me stare out at the mainstream of a whole field yelling, "WHEN ARE YOU IDIOTS GOING TO DROP YOUR STUPID FAD AND CATCH UP!?" At the same time, I can see where the thesis author is cutting a corner or two and making some simplifications in order to finally fucking graduate -- but their results still stand in stark contrast to the field's mainstream paradigm.
:PROPERTIES:
:Score: 1
:DateUnix: 1501210068.0
:DateShort: 2017-Jul-28
:END:

********* That's a pretty reasonable perspective. But I'd also say it's true for pretty much any group. Academia also has a bunch of other perverse incentives causing trouble, and a bunch of people going into it for entirely the wrong reasons.

I mean, it's probably the best thing out there, but I stand by my opinion that academic culture causes a bunch of problems, and is probably something you want to mostly avoid if you're doing serious research and you /can/ mostly avoid it.

Is it an AI paper? Give me a reference.
:PROPERTIES:
:Author: traverseda
:Score: 2
:DateUnix: 1501213965.0
:DateShort: 2017-Jul-28
:END:

********** Nah, I'm not in AI. It was a shitty six-page conference paper in a shitty, irrelevant field that my adviser had for some reason spent has late career working on. Thank God I didn't continue to a PhD with him.
:PROPERTIES:
:Score: 2
:DateUnix: 1501240240.0
:DateShort: 2017-Jul-28
:END:


**** I remain forever puzzled as to how a statement filled with nothing but ad hominem and snark can accumulate so many upvotes on a subreddit ostensibly dedicated to rationality.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 2
:DateUnix: 1501179862.0
:DateShort: 2017-Jul-27
:END:

***** Because humans?
:PROPERTIES:
:Author: Amonwilde
:Score: 2
:DateUnix: 1501182687.0
:DateShort: 2017-Jul-27
:END:


** Along with givewell as others have mentioned, if you care about animal suffering, some of that money could be sent to Animal Charity Evalutators: [[https://animalcharityevaluators.org/]]

I feel like $100b would probably be more than enough to research a way to get lab-grown meat at a scale large enough to be price competitive, which would essentially end factory farming and the suffering of billions of animals each year. It'd be a one time investment that solves a heckload of suffering (and no doubt a lot of nutrition issues in low-income countries like kawashikor/marasmus).

That said, suggestions like MIRI would be good too for the moonshot angle. I'd probably put $200m in ACE, give MIRI an endowment fund to last for 20 years (so maybe another $200m if we give them ~$10m a year), and put the remaining $600m wherever GiveWell recommends. Maybe reduce GiveWell's portion and distribute it in 5x $200m packages, to animal suffering, givewell, and 3x X-Risk charities (maybe include one to vaccinate pertussis or something out of existence?).
:PROPERTIES:
:Author: MagicWeasel
:Score: 2
:DateUnix: 1501198550.0
:DateShort: 2017-Jul-28
:END:


** I start funding a broad variety of socialist parties with different platforms, strategies, and tactics to see what can do well. The revolution will not be achieved from waving protest signs at our overlords.
:PROPERTIES:
:Score: 2
:DateUnix: 1501171192.0
:DateShort: 2017-Jul-27
:END:
