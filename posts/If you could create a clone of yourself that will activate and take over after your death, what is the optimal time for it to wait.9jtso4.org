#+TITLE: If you could create a clone of yourself that will activate and take over after your death, what is the optimal time for it to wait?

* If you could create a clone of yourself that will activate and take over after your death, what is the optimal time for it to wait?
:PROPERTIES:
:Author: dinoseen
:Score: 12
:DateUnix: 1538195132.0
:END:
In this example, let's assume that the clone is basically in a coma until activated, and they (somehow) receive your memories. In this scenario, for whatever reason you don't want a clone of yourself running around and interfering with your life.

To prevent the clone from waking up, you have to visit it and perform some kind of action, basically signifying, "Hey, I'm still alive", with this being your only way of preventing an early wake up. This is presumably when the memory transfer happens. The idea being that if you don't check in you're obviously dead and it's time for the clone to take over.

For the sake of more info, let's assume you face mortal danger roughly every couple months at minimum (say, you're a super hero or something).

Obviously, there's the chance that you could be incapacitated, kidnapped, or otherwise unable to make your check in. How do you balance this factor against how long the clone should wait?

I've given specific info for this example, but speculation based on different assumptions would also be welcome.


** Wake up the clone immediately and start getting twice the superheroing done.
:PROPERTIES:
:Author: MrCogmor
:Score: 32
:DateUnix: 1538196106.0
:END:

*** Definitely a valid answer, depending on how trusting of yourself you are. Although, it wouldn't be "yourself" in the sense of what they are going to do, would it? We trust ourselves at least in part because we have control of ourselves, but you don't have control over a clone. They just naturally act the way you would.

Problems arise when you would act in a way that is incompatible with having a clone, thus creating a conflict. Example: you have a wife, and you only ever want a monogamous relationship. The clone also remembers having that, and wants the same things you do.

Even if the wife would be okay with polygamy, there's still a conflict between you and your clone, since you both want her for yourself. Either one or both of you must go without. This can obviously be extrapolated into much more lethal situations as well.
:PROPERTIES:
:Author: dinoseen
:Score: 9
:DateUnix: 1538203300.0
:END:

**** So change your views on polygamy until you are fine with sharing with a copy of yourself and don't miss on a times two (can you make more clones? Potentially times many more) efficiency multiplier on everything you do. Defecting on a literal copy of yourself for any reason seems like a really dumb idea to me. At worst, you can always flip a coin and let random chance decide which one of you copies is going to do a thing.
:PROPERTIES:
:Author: melmonella
:Score: 12
:DateUnix: 1538226912.0
:END:

***** #+begin_quote
  Defecting on a literal copy of yourself for any reason seems like a really dumb idea to me.
#+end_quote

(That'd make for an interesting murder mystery - on the one hand, if duplicates of you are dying you're a suspect. On the other hand, the culprit could be...another one of your clones.)

What if /you/ have to pay the price - the clone stays, but you go? (Or more generally, the clone decides how things are resolved.)

â€‹
:PROPERTIES:
:Author: GeneralExtension
:Score: 6
:DateUnix: 1538375337.0
:END:

****** I don't understand the question. There is no meaningful difference between you and your clone, and both of you know that. Whomever decides will arrive at the same decision, what with having the same information at their disposal and the same patterns of thinking.

Unless there is a practical difference between you and the clone a-la Simulacra in MoL, in which case again, both of you know the clone is more expendable and both would be okay with it.
:PROPERTIES:
:Author: melmonella
:Score: 2
:DateUnix: 1538394128.0
:END:

******* #+begin_quote
  There is no meaningful difference between you and your clone, and both of you know that
#+end_quote

This is a big assumption that needs to be shown.

#+begin_quote
  Unless there is a practical difference between you and the clone a-la Simulacra in MoL, in which case again, both of you know the clone is more expendable and both would be okay with it.
#+end_quote

Being 'more expendable' doesn't necessarily mean you are okay with it. In fact an 'expendable' clone has a lot of incentive to find other ways to survive, which puts it at odds with the original.
:PROPERTIES:
:Author: Anderkent
:Score: 3
:DateUnix: 1538419361.0
:END:

******** #+begin_quote
  This is a big assumption that needs to be shown.
#+end_quote

What is there to show? Same memories, same personality, same ideals, same goals, same everything except position in space. You'd probably make an interference pattern if someone threw you through a double slit experiment for people. What's the meaningful difference here?

#+begin_quote
  has a lot of incentive to find other ways to survive
#+end_quote

I genuinely don't understand this. Clone knows the same things you know. If a third party knocked you and the clone out at the moment of cloning, and then put you in a room together without either of you knowing (both have only memories up til the moment of cloning, and wake up in the same room but on opposite sides) which one is the "original", there would be no way for either of you to tell which one was the "original" without analysing the body composition. Ergo, both have the same goals of insuring their mind state keeps existing into the future, and their goals get accomplished. That can be accomplished by sending the clone into danger while keeping the squishy original holed up in a bunker, because original is necessary to make clones in the future. What kind of insanity would your clone have to be under to decide "no, you know what, I am not going to do this thing I have just decided I should do, I am instead going to risk my survival and indeed the survival of any possible copy of myself by attacking the original and attempting to chance into some kind of permanent life for myself by killing myself."

As for emotional impact of death, I frankly consider myself to be emotionally stable enough to not freak out in this situation, and be capable of braving mortal danger when I have physical proof that some copy of me will stay undamaged. If hundreds of thousands of soldiers can charge enemy lines /without/ any kind of assurance besides a glass of vodka beforehand and following orders from a general they never even seen, I damn well should be capable of doing the same /with/ rock solid assurance in order to achieve my own personal objectives.
:PROPERTIES:
:Author: melmonella
:Score: 2
:DateUnix: 1538453190.0
:END:

********* #+begin_quote
  What kind of insanity would your clone have to be under to decide "no, you know what, I am not going to do this thing I have just decided I should do, I am instead going to risk my survival and indeed the survival of any possible copy of myself by attacking the original and attempting to chance into some kind of permanent life for myself by killing myself."
#+end_quote

Because some goals don't transfer to clones. Things like "I want ice cream" translate into "the clone wants ice cream", not "the clone wants me to have ice cream".

There's also the very simple thing that by dividing your assets between two copies of yourself, both copies are now less well off than they were before the split.

#+begin_quote
  What kind of insanity would your clone have to be under to decide "no, you know what, I am not going to do this thing I have just decided I should do, I am instead going to risk my survival and indeed the survival of any possible copy of myself by attacking the original and attempting to chance into some kind of permanent life for myself by killing myself."
#+end_quote

What is the 'I have just decided I should do' here? Clearly the original decided that the clone should do X. There is an asymmetry here, and if there is an asymmetry then the 'clone and original are exactly the same person' argument doesn't apply.

If there's two tasks X and Y, and both need to be done, but X is nicer to do and Y is painful and hard, that's another asymmetry. You can't assume that the clone will gladly do things that the original wouldn't want to.

#+begin_quote
  Ergo, both have the same goals of insuring their mind state keeps existing into the future, and their goals get accomplished.
#+end_quote

But their mind states diverge almost immediately, and so they would both prefer the mind state of this exact instance to be propagated, rather than the close-but-not-the-same state of the copy.

#+begin_quote
  If hundreds of thousands of soldiers can charge enemy lines without any kind of assurance besides a glass of vodka beforehand and following orders from a general they never even seen
#+end_quote

Unrelated, but that's not how armies work. Soldiers don't fight for the great cause; they fight for the close friends right next to them.
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1538478586.0
:END:


******* If you're the one to die, you'll still experience death and the human fear of it, providing incentive to not die. We are far from rational beings, despite what some of us desire.
:PROPERTIES:
:Author: dinoseen
:Score: 1
:DateUnix: 1538440655.0
:END:

******** If hundreds of thousands of soldiers can charge enemy lines /without/ any kind of assurance their mind will survive the battle besides a glass of vodka beforehand while following orders from a general they never even seen, I damn well should be capable of doing the same with rock solid assurance in order to achieve my own personal objectives. If I don't think I could do that, I'd try some mental exercises, hire a shrink to talk to me, or something similar. Losing out on a 2 times multiplier out of /fear/ of all things would be simply disgraceful IMO, and here the potential gains could be much larger. Imagine if someone said "we will pay you a million dollars per day for the rest of your life, but you'd have to pet this trapdoor spider each day to receive the check. It's not poisonous and doesn't bite people." Even if you are a hardcore arachnophobe, surely you'd at least try to find some ways to do so.

Don't let animal fear pull you to the ground. Pet the damn spider.
:PROPERTIES:
:Author: melmonella
:Score: 1
:DateUnix: 1538453713.0
:END:


**** The other side of this arguement is that if you do not trust yourself to the point that you would not want to live in a world with this clone, why would you release this clone with other people?
:PROPERTIES:
:Author: Rouninscholar
:Score: 1
:DateUnix: 1538408044.0
:END:

***** Because myself or a clone are both going to act differently than usual when both exist, but when only one does there is no problem.

I trust it to make the same decisions I would make and not be evil (if it's actually a proper clone), but I know that I would see a clone as a potential threat, and thus I might decide it needs to be dealt with. From the clone's perspective, I'm the clone, and it works the same way.
:PROPERTIES:
:Author: dinoseen
:Score: 1
:DateUnix: 1538440530.0
:END:


*** Or just, like, have myself^{clone} kill myself^{original,} which I imagine would be incredibly cathartic.
:PROPERTIES:
:Author: callmesalticidae
:Score: 3
:DateUnix: 1538200333.0
:END:

**** If we're saying the clone inherits the same body and mind, I don't think it's useful to differentiate between them as original vs clone: Both are clones or both are originals. The specific previous states of the body they come in shouldn't change their identity
:PROPERTIES:
:Author: causalchain
:Score: 1
:DateUnix: 1538293005.0
:END:


** The distrust of what is essentially a copy of yourself is really bizarre to me. That said, I'm willing to take the scenario at face value.

My answer is that the ideal set-up allows for early check-ins. If your "adventures" mostly stay in the same city and threats are likewise local: I'd use a four week wake-up timer that I reset every week. If your threats are likely to use planes, I'd double the length; and double again for fantastic-four style interdimensional/interplanetary shenanigans.
:PROPERTIES:
:Author: vaegrim
:Score: 19
:DateUnix: 1538198157.0
:END:

*** #+begin_quote
  The distrust of what is essentially a copy of yourself is really bizarre to me.
#+end_quote

This really depends who you are. Let's take Person A as an example.

A is a really selfish fellow. That isn't to say that he's rude or mean - he can compromise on the small things that he doesn't care about. But if it's something he /really/ wants? The rest of the world can go burn itself.

Now lets bring in his clone, Person B. He has the exact same selfishness as Person A. Unlike A, he doesn't have a wife, a child, a family and friends. He doesn't have a job, a passport - all those are in the possession of Person A.

But he has memories of having them.

So how can B "regain" his life? How can he get back all that was stolen from him?

Well, the answer is simple - remove A from the picture and replace him.

--------------

Mother of Learning uses this in a similar fashion - selfish people are incapable of using simulacrums, not because they can't cast the spell, but because the simulacrum of selfish people tend to work against the caster, for their own goals. Remember, they have the same personality as the person who created them, so someone hateful will have a clone that is also hateful.
:PROPERTIES:
:Author: xland44
:Score: 13
:DateUnix: 1538199793.0
:END:

**** That mode of thinking is so alien to me as to be incomprehensible. Person A and Person B are the same person; they both have the same wife, child, family, friends, job, and passport. This single person just happens to occupy two places at the same time and can't remember half of his life as of the instant of divergence.

Killing your clone would be like building an elaborate death trap over your bed on the off-chance that, next morning you forget you did so the night before and so were *a di^{ffe^{rent}} p^{ers^{on}} no^{w}*!
:PROPERTIES:
:Author: vaegrim
:Score: 10
:DateUnix: 1538200118.0
:END:

***** Yeah. I'd /love/ to have a double. I could take breaks while ensuring that there was somebody still doing work (we'd switch off, obviously; I wouldn't want to be taken advantage of, so my duplicate wouldn't want to, either).
:PROPERTIES:
:Author: callmesalticidae
:Score: 12
:DateUnix: 1538200542.0
:END:

****** Would take some time to figure out how to deal with needing twice as much money for food while only having one ID to get a job with, but yeah, should still be an awesome benefit.
:PROPERTIES:
:Author: melmonella
:Score: 5
:DateUnix: 1538227196.0
:END:

******* I'd probably look into freelancing.
:PROPERTIES:
:Author: callmesalticidae
:Score: 4
:DateUnix: 1538324825.0
:END:


***** In this example, if they're clones of someone like you, then obviously the clones would get along fine.

But our lives are obviously ruled by much more than just our own perspective. Philosophically, sure, you're the same person (at least at the start), but socially? I would wager in a society where clones aren't widespread (like ours, and most superhero fiction), people would not think of you and your clone as the same person in all ways.

Would your wife and kids really be okay with having two dads? Since you selected/raised them, quite possibly, but it's no guarantee.

Perhaps a way to try understanding this way of thinking is that there is only so much "social space" in relationships. Most people just aren't going to be able to think of both of you as one person. There's only enough space in their conception of "you" for one individual, not two individuals who are the same. They will treat you and the clone differently, and thus socially you will basically be different people. Obviously not everyone will act in this way, but I would say definitely enough people to make an impact would.
:PROPERTIES:
:Author: dinoseen
:Score: 6
:DateUnix: 1538202788.0
:END:


***** #+begin_quote
  Person A and Person B are the same person; they both have the same wife, child, family, friends, job, and passport. This single person just happens to occupy two places at the same time and can't remember half of his life as of the instant of divergence.
#+end_quote

What are your criteria for being 'the same person', then? It can't be memory, or the clone would diverge into its own person after some time. I doubt it's DNA, or truly identical twins would count as one person with a very odd case of amnesia.

So what are your criteria?
:PROPERTIES:
:Author: CCC_037
:Score: 5
:DateUnix: 1538205632.0
:END:

****** I am everyone who both believes they are me and agrees with this sentence.
:PROPERTIES:
:Author: vaegrim
:Score: 8
:DateUnix: 1538207104.0
:END:

******* So, in the case where your clone runs into a rogue philosophy professor and is persuaded that the above sentence is false, then you are no longer the same person as your clone because your clone no longer agrees?
:PROPERTIES:
:Author: CCC_037
:Score: 4
:DateUnix: 1538208098.0
:END:

******** That's correct. If the clone either believed it was someone else, or no longer accepted the contagious nature of my identity I'd consider it a fundamentally different person.
:PROPERTIES:
:Author: vaegrim
:Score: 10
:DateUnix: 1538210180.0
:END:

********* Fair enough. What would you think about Crazy Dave, who believes he is you (but shares none of your memory, DNA, or anything else)?
:PROPERTIES:
:Author: CCC_037
:Score: 7
:DateUnix: 1538210390.0
:END:

********** Again, correct. If Crazy Dave happened to know, understand and accept my self-definition then Dave and "Crazy Dave" are just Daves separated by space and experience.

In the same way that if Dave's mind gets uploaded into a time-traveling robot and goes to the past during a time when Dave had amnesia, both amnesia-Dave and robot-Dave are still ultimately Dave.

If my name wasn't Dave, I'm not sure I'd believe he was me if he insisted on calling himself "Crazy Dave" though.
:PROPERTIES:
:Author: vaegrim
:Score: 3
:DateUnix: 1538241423.0
:END:

*********** Huh. Well, your definition is strange to me, but it certainly seems self-consistent, at the very least.
:PROPERTIES:
:Author: CCC_037
:Score: 3
:DateUnix: 1538242522.0
:END:


********* So you will "die" if you change your model of identity?

Also what counts as someone believing they are you?

Is them knowing your name enough?

Or some kind of reference to yourself?

How detailed?
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1538341449.0
:END:

********** If in the future I changed the definition for my identity, the person I am now would consider that future person someone else. You could call this death insofar as after that change, there may not exist anyone alive Now-Me would consider to be Me. Whether that changed future person agrees is entirely about whatever criteria that future person used.

As to what "counts as" sufficient to share my identity, they have to believe they share my individual personhood. It's not like a true-name magic word; if they actually agree we're the same person then we are.
:PROPERTIES:
:Author: vaegrim
:Score: 1
:DateUnix: 1538354409.0
:END:


**** I mean... why would person A bother making a clone of himself to be woken up when he dies? If he's selfish, then why would he care about making a clone to wake up once he's no longer capable of experiencing the world himself?
:PROPERTIES:
:Author: The_Magus_199
:Score: 3
:DateUnix: 1538231325.0
:END:

***** My explanation was more in general for why having a clone isn't always good, but to answer your question:

It could be that he /didn't/ have the clone be made voluntarily. Perhaps made as a scheme by an enemy, and it's all he can do to delay the clone from being released. Also, if the clone's location is also hidden, he can't destroy it.

If you've read Worm, it could be a clone that has been altered to hate his progenitor and everything they stand for. Said clone also has the original's memories, which means he can easily come up with ways to ruin their lives.
:PROPERTIES:
:Author: xland44
:Score: 3
:DateUnix: 1538234943.0
:END:


*** It is not necessarily distrust. If forking is possible, I would expect there to very shortly to be very severe penalties for doing it. Up to and including "We are going to shoot you on sight until we are sure there is at most one copy of you, and we will not be making /that/ much of an effort to make sure we do not get the count wrong".

Because unrestricted forking has predictable horrible outcomes on a society wide level.
:PROPERTIES:
:Author: Izeinwinter
:Score: 3
:DateUnix: 1538407263.0
:END:

**** That makes sense, I hadn't considered the angle of legal restrictions.
:PROPERTIES:
:Author: vaegrim
:Score: 1
:DateUnix: 1538413907.0
:END:


** There are so many different arguments that are more or less equally valid that I think the best answer is whatever makes the story most interesting.

Personally, I'd check up on the clone as often as possible and have something like a week before it wakes up. Wherever it's located I'd have a computer or something on which I could archive what was happening in my life. Something I could email with updates even if I wasn't able to visit. Ideally I'd have somebody I trust doing the same thing whether I'm alive or not.

If the clone waking up when I'm still alive is that much of a problem I'd have to establish some network of allies who could rescue me from situations who would be on a similar but smaller deadman switch. If the clone wakes, not only have I died but the other options have failed.

They can be on a short timer, because the consequences of having my friends go out looking for me aren't as disastrous as if my clone wakes, right? Five days should be enough time for them to resolve most non-James Bond situations.
:PROPERTIES:
:Author: Sparkwitch
:Score: 15
:DateUnix: 1538196151.0
:END:


** It depends on what you expect the clone to do. If you want it to take on next monthly mortal danger lest you fall, then the answer is rather trivial. Less than a month, and while you are alive you check up on it daily.

In fact, it is better to have a cascade of clones, set to irregular intervals after the first one, preferably violating the 'no doubles' rule. For your nemeses it looks like this. 1 - you are killed. 2 - ~2 weeks later a clone arrives and tries to finish them off and resume the normal routine. If it fails, 17 weeks later five clones arrive, and take on the danger with overwhelming force. You are dead, let /those five shmucks/ deal with the inconvenience of copies of themselves 'interfering with their life'.
:PROPERTIES:
:Author: Xtraordinaire
:Score: 7
:DateUnix: 1538217549.0
:END:


** At the very least you want a Google news alert for your obituary. The law has a system to decalre the missing "legally dead" but that also goes wrong sometimes. There's no real way to guarantee that you're dead without your body, and if you're limiting commands to your personal check-in and a timer, which is essentially how the law does it, then you are going to run the risk.

I just want to have it barge in halfway through my funeral. "I object!"
:PROPERTIES:
:Author: Trips-Over-Tail
:Score: 5
:DateUnix: 1538231597.0
:END:


** Have a wearable or implantable device which continuously monitors your vitals. Device & device communications should be secure. Device continuously sends out a signal to let it be known that you are alive. In the event that /all/ your vitals flatline, the device sends out a signal to state that you are dead. In the event that the device does not send a signal for x days (depending on whether you are Superman, fighting across worlds, or Batman, fighting within the city) the clone activates and investigates your disappearance. Need to consider the event where your device sends out signals indicating incapacitation but not death, for y days without improvement. Solution is contingent on the device bring tougher than your body.
:PROPERTIES:
:Author: wndering_wnderer
:Score: 3
:DateUnix: 1538229762.0
:END:

*** I think you'd pretty much need something like this anyway for the clone to "receive your memories." So, when the clone stops receiving memories for, say, a week, wake it up.
:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1538230084.0
:END:


** Depends on the situation. Are you a villain who is a big enough threat that the entire world will unite against you? Wait a few generations. Are you a hero who needs to be constantly saving the world? Wait a day at most, you can't afford to be gone too long even if it means forking yourself.

The best solution is probably to get someone you trust to activate the clone when /they/ think you're dead. After all, they can react to a fluid situation better than an algorithm.
:PROPERTIES:
:Author: EthanCC
:Score: 3
:DateUnix: 1538235133.0
:END:


** Why would you not want to make your clone wake?
:PROPERTIES:
:Author: RMcD94
:Score: 3
:DateUnix: 1538195967.0
:END:

*** You could argue that it would interfere with your life. While I understand many would not feel this way, the point of this post is more to ask how people would act if they /did/.
:PROPERTIES:
:Author: dinoseen
:Score: 4
:DateUnix: 1538202092.0
:END:

**** The motivation for keeping it asleep impacts what you'd be willing to do. Like wear a suicide vest
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1538210829.0
:END:

***** I'm slightly lost, what do you mean with the suicide vest? The clone would be wearing it?
:PROPERTIES:
:Author: dinoseen
:Score: 2
:DateUnix: 1538214108.0
:END:

****** If you want to avoid the clone waking up when you are alive you wear a suicide vest that you trigger when you can't get back to the clone. You will die and the clone will be alive
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1538214234.0
:END:

******* Right. Personally, I'd rather deal with the clone situation than die. The clone is just there to ensure my interests will be attended to if I DO die. My friends and family won't be too badly off, the things I care about enough to act on them will still be acted upon, etc.

I suppose, however, that if I can create clones and transfer memories I can probably mind control them into thinking of me as senior in the chain of command or whatever. But, I'd rather not subject myself to a sub-me existence, so I'd only have this active in circumstances I would judge it necessary, i.e. no clone slaves, only for resolving false alarm wake ups.
:PROPERTIES:
:Author: dinoseen
:Score: 3
:DateUnix: 1538215117.0
:END:

******** My solution then is that the clone should awaken whenever your family and friends vote for him to awaken.

This way they will wait for you to return until it is unbearable
:PROPERTIES:
:Author: RMcD94
:Score: 3
:DateUnix: 1538221768.0
:END:


*** #+begin_quote
  Why would you not want to make your clone wake?
#+end_quote

If someone wants to kill you, you /don't/ do it so that you can't /both/ die. In which case, the 'clone' has the disadvantage of going up against an enemy who pulled it off, but holds the element of surprise - they think you're dead.
:PROPERTIES:
:Author: GeneralExtension
:Score: 1
:DateUnix: 1538375766.0
:END:

**** This scenario seems worthless compared to the benefit two people could achieve.
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1538377333.0
:END:

***** Two identical people?
:PROPERTIES:
:Author: GeneralExtension
:Score: 1
:DateUnix: 1538441566.0
:END:

****** Yes
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1538445240.0
:END:

******* I would agree, if you could function as one mind in two bodies if you woke the clone up before dying. Otherwise, there's the opportunity to do things with the guarantee you have one more life. This /isn't/ true if both of you are in the same vehicle - one accident and you're both toast.

If your clone is aging, by all means wake it up, if not - you don't have to sign up for cryonics.
:PROPERTIES:
:Author: GeneralExtension
:Score: 2
:DateUnix: 1538514184.0
:END:


** I'm not sure what /is/ the optimal time, but I can say that 10 years didn't work out so well [[https://en.wikipedia.org/wiki/Vision_of_the_Future][for Grand Admiral Thrawn]]
:PROPERTIES:
:Author: thrawnca
:Score: 1
:DateUnix: 1538339467.0
:END:
