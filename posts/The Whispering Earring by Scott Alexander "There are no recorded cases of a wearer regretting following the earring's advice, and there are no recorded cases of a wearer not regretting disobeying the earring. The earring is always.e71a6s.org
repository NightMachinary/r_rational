#+TITLE: The Whispering Earring by Scott Alexander: "There are no recorded cases of a wearer regretting following the earring's advice, and there are no recorded cases of a wearer not regretting disobeying the earring. The earring is always right."

* [[http://web.archive.org/web/20121008025245/http://squid314.livejournal.com/332946.html][The Whispering Earring by Scott Alexander: "There are no recorded cases of a wearer regretting following the earring's advice, and there are no recorded cases of a wearer not regretting disobeying the earring. The earring is always right."]]
:PROPERTIES:
:Author: erwgv3g34
:Score: 131
:DateUnix: 1575651297.0
:DateShort: 2019-Dec-06
:END:

** Seems like it'd be pretty easy listen to it at the first stage when it's giving major life advice, and then take it off once it starts micromanaging. Of course, that would assume you knew what would happen to you at the end.

I like that If this was an SCP it would probably fuse itself to your skull and eat your brain, but he's written it to imply that you lose higher functions naturally through disuse which is much more compelling
:PROPERTIES:
:Author: Taborask
:Score: 51
:DateUnix: 1575660885.0
:DateShort: 2019-Dec-06
:END:

*** Which raises the question of why it doesn't tell you to take it off when it starts micromanaging instead of immediately.
:PROPERTIES:
:Author: archpawn
:Score: 26
:DateUnix: 1575673985.0
:DateShort: 2019-Dec-07
:END:

**** As far as why it doesn't tell you to remove it at stage 2, I can think of two reasons.

A) It's designed that way intentionally

or

B) For a large percentage of people, life is entirely about the pursuit of happiness, and rarely if ever about intellectual or spiritual development (especially if it comes at the cost of immediate happiness or gratification). Since the damn earring works to maximize happiness as the user understands it, it's very possible that not having to make decisions or think about things IS a form of happiness to a percentage of the population, so there would be no reason for the earring to recommend removing it again.

I think that this sort of artefact would be cursed for the general populace (cursed in the sense that it's methodology is something we would consider evil), and merely immensely dangerous for a properly prepared and intelligent user.

If you're able to reflect upon your own actions sufficiently, as well as maintain a continuous distrust of the "never wrong" voice whispering in your ear, I think it may be possible to employ this thing without necessarily losing your free will to it.

I don't think I'd personally risk wearing it for more then a few weeks, if that, but I think someone with more willpower then I would be able to put it to good, long-term use.
:PROPERTIES:
:Author: Arizth
:Score: 18
:DateUnix: 1575689056.0
:DateShort: 2019-Dec-07
:END:

***** Seems to me that continuous use is a terrible idea. I'd probably use it when I had big life decisions to make, but not day-to-day.
:PROPERTIES:
:Author: LazarusRises
:Score: 2
:DateUnix: 1576003702.0
:DateShort: 2019-Dec-10
:END:


*** It only starts micromanaging when your desire for long-term values has atrophied to the point that you no longer care. You would be too late.

What you could do is not wear the earring the earring, think about your options, then put on the earring for a few seconds to hear what the best option is. That way you're training your brain rather than losing it.
:PROPERTIES:
:Author: philip1201
:Score: 19
:DateUnix: 1575706883.0
:DateShort: 2019-Dec-07
:END:


*** This world needs a service that will come for you after a few months and seize your earring, by force if necessary. Then the earring's first bit of advice could be to sign up.
:PROPERTIES:
:Author: vorpal_potato
:Score: 5
:DateUnix: 1575690850.0
:DateShort: 2019-Dec-07
:END:


*** If the person can appear as the same person from the outside (memories/personality) modulo being more happy, it means your consciousness gets gradually uploaded to the earring, and nothing bad happens to you (except that the resulting mind keeps magically knowing what to do to achieve happiness and will always want to do it, which isn't necessarily a kind of harm), (but it's a sufficient change that I'd consider it death).

Gradually, your mind runs less and less on your brain, and more and more on the earring.

(Edited.)
:PROPERTIES:
:Author: DuskyDay
:Score: 5
:DateUnix: 1575719143.0
:DateShort: 2019-Dec-07
:END:


*** Why take it off when it starts micromanaging?
:PROPERTIES:
:Author: WalterTFD
:Score: 2
:DateUnix: 1575675514.0
:DateShort: 2019-Dec-07
:END:

**** The earring doesn't maximize your long-term goals because it changes them over time by atrophying the parts of the brain that think about long-term goals, because its suggestions are better than yours.

This actually happens before the micromanagement stage, though, but it would minimize the damage.

(That said, [[https://web.archive.org/web/20121007235422/http://squid314.livejournal.com/333168.html][the author intended]] it to be horrible even without long-term value drift).
:PROPERTIES:
:Author: philip1201
:Score: 13
:DateUnix: 1575706633.0
:DateShort: 2019-Dec-07
:END:

***** u/meterion:
#+begin_quote
  The earring doesn't maximize your long-term goals because it changes them over time by atrophying the parts of the brain that think about long-term goals
#+end_quote

That doesn't at all sounds like how I interpreted it. To me, it seemed like it took your long-term life goals, and realized them in the most effective manner possible. The implication is that the earring is better at better at being you than you are, not that it changes you into something you wouldn't have become if sufficiently motivated and knowledgeable.

The ending proverb points in that direction: "One must never take the shortest path between two points."
:PROPERTIES:
:Author: meterion
:Score: 12
:DateUnix: 1575707452.0
:DateShort: 2019-Dec-07
:END:

****** It specifically does not maximize your outcomes, merely exceeds the wearer on the things it gives input on.

Note that this gives lots of room to optimize for agendas other than those of the wearer
:PROPERTIES:
:Author: aponty
:Score: 7
:DateUnix: 1575815822.0
:DateShort: 2019-Dec-08
:END:


***** It doens't maximize your long term goals because it gives you better ones. /The earing is always right/, as it were. The story says that if you ever deviate you regret it.

The earing is the best hedonic course of action. Taking the earing off is like pushing a nail through your hand. I get why someone might do it in the heat of a moment, in the 'people have abstract thoughts and take actions that are whatever' kind of way, but it is really odd to see someone sitting down and constructing arguments against the optimum path. Like, aren't you kind of fighting the definition?
:PROPERTIES:
:Author: WalterTFD
:Score: 9
:DateUnix: 1575717070.0
:DateShort: 2019-Dec-07
:END:


***** Unless your goal is happiness.

For selfish people, the earring seems to be ideal.
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1575819060.0
:DateShort: 2019-Dec-08
:END:


** Note that this story's meaning is clearer with the the information from the [[https://web.archive.org/web/20121007235422/http://squid314.livejournal.com/333168.html][following page]].
:PROPERTIES:
:Author: fljared
:Score: 26
:DateUnix: 1575677640.0
:DateShort: 2019-Dec-07
:END:

*** Thankfully, if I really cared about my free will, my utility maximizer would make sure I spend some time feeling like I'm making decisions.
:PROPERTIES:
:Author: GemOfEvan
:Score: 7
:DateUnix: 1575688582.0
:DateShort: 2019-Dec-07
:END:

**** ...Until it figures out how to replicate that feeling without you actually making decisions.
:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1575770497.0
:DateShort: 2019-Dec-08
:END:


*** Thank you - this is very helpful! Not quite as fun as ignoring the author as Plato would have said Socrates would suggest, but much more informative ;)
:PROPERTIES:
:Author: RandomChance
:Score: 1
:DateUnix: 1575693666.0
:DateShort: 2019-Dec-07
:END:


** That was a pretty neat read. The part about the wearer's brain wasting away and being conditioned was horrifying.
:PROPERTIES:
:Author: Do_Not_Go_In_There
:Score: 21
:DateUnix: 1575656223.0
:DateShort: 2019-Dec-06
:END:

*** Can you recite all of Homer from memory? Can you chase down rabbits and deer by just not stopping while they succumb to exhaustion? Your ancestors could, but now you have printing presses, and libraries, and agriculture and don't have to, so your (and my) capacity to do so has atrophied to the point where our ancestors would be horrified (or maybe very envious). Is it really horrid/non-rational to delegate to someone/something else that what "they" can do better? Does it make sense to spend extra resources preserving the capacity if it is proven it won't be needed? (I would argue that some limited skill might be a good idea in case what your relying on goes away but that doesn't negate the larger argument).
:PROPERTIES:
:Author: RandomChance
:Score: 37
:DateUnix: 1575656732.0
:DateShort: 2019-Dec-06
:END:

**** I can sing the opening from the Weird Al show!
:PROPERTIES:
:Author: BumblingJumbles
:Score: 10
:DateUnix: 1575663440.0
:DateShort: 2019-Dec-06
:END:


**** No, but fic about the extremal case where it is horrifying is good!
:PROPERTIES:
:Author: etarletons
:Score: 9
:DateUnix: 1575664407.0
:DateShort: 2019-Dec-07
:END:


**** If you are able bodied and fit enough to go for a 20 min jog every morning i bet you could probably chase down deer if it wasn't able to hide from your sight. You're still human and are possessed of the same adaptations for distance running.
:PROPERTIES:
:Author: eroticas
:Score: 4
:DateUnix: 1575731333.0
:DateShort: 2019-Dec-07
:END:


*** Less horrifying if you realize that the ring is effectively doing a gradual upload.
:PROPERTIES:
:Author: FeepingCreature
:Score: 9
:DateUnix: 1575656554.0
:DateShort: 2019-Dec-06
:END:

**** How do you figure? It's not like the Dual in /Learning To Be Me/, it's not learning to be the wearer, it's teaching the wearer to be the earring.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 6
:DateUnix: 1575680622.0
:DateShort: 2019-Dec-07
:END:

***** But "being the earring" is externally indistinguishable from a better you, or else people would have caught on by now. It's not Doctor Fate's helmet either.
:PROPERTIES:
:Author: FeepingCreature
:Score: 5
:DateUnix: 1575720370.0
:DateShort: 2019-Dec-07
:END:

****** Or from someone playing you on a stage. There's no indication that it replicates your mind-state.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1575720593.0
:DateShort: 2019-Dec-07
:END:

******* If someone can play me on a stage in sufficient detail to be externally indistinguishable, I'll buy that their mind contains a copy of mine.
:PROPERTIES:
:Author: FeepingCreature
:Score: 5
:DateUnix: 1575721136.0
:DateShort: 2019-Dec-07
:END:

******** They're not playing 'you', they're playing the person they were puppeting you into pretending to being.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1575723461.0
:DateShort: 2019-Dec-07
:END:

********* Sure, but the person they were puppeting me into pretending to being is externally pretty indistinguishable from myself.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575723642.0
:DateShort: 2019-Dec-07
:END:

********** It's clearly distinguishable or there wouldn't be any point to wearing the earring.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1575726041.0
:DateShort: 2019-Dec-07
:END:

*********** True.

I guess I'd say it's indistinguishable from a more capable version of yourself. (That being, after all, the aesop.)
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575727981.0
:DateShort: 2019-Dec-07
:END:

************ Or a much more capable person pretending to be you. It only has to memorize a little of the old you to fool people.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1575728558.0
:DateShort: 2019-Dec-07
:END:

************* I don't think it's a given that a much more capable person pretending to be me is not me. Also a me that can fool all the people with complete confidence over centuries.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575728662.0
:DateShort: 2019-Dec-07
:END:

************** It's not a given that it is, either. This is not a ship of Theseus.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1575728729.0
:DateShort: 2019-Dec-07
:END:

*************** True, but if it visibly failed to be me in some situations I suspect this would have come out in the history of the earring. Afaics this is only a threat if the people around /all/ the wearers are very unobservant or if people generally hold large causally separate internal universes that they take to their grave.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575728932.0
:DateShort: 2019-Dec-07
:END:

**************** People act in ways that surprise each other all the time. It's normal, someone expresses an opinion you don't expect, they dp 't assime they're a doppelganger.

And the ring is a superintelligence or it wouldn't be so successful at the start of the process.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1575729130.0
:DateShort: 2019-Dec-07
:END:

***************** Sure, the ring could run a very thin, shell-like upload - just exactly enough to fool people, just the specific people that it was around, and nothing more. And if suddenly someone who knew that person from childhood and had access to shared information showed up, it would be screwed.

In any case, I think it's simply not that kind of story. There's a plausible horror story about the shallowness of human interaction where the ring wears your skin like a suit while maximizing its own inscrutable values. I just feel like this one is more consistent with the uploading view. The whole point of the ring is that it's a "better you."
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575729350.0
:DateShort: 2019-Dec-07
:END:

****************** It's not running any kind of upload at all. It's training you to be someone that it can emulate, using classical conditioning.

Besides, if someone from your childhood came along and asked about something that the ring didn't know, the ring still wouldn't know it even if it were running your upload.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1575729590.0
:DateShort: 2019-Dec-07
:END:

******************* u/FeepingCreature:
#+begin_quote
  Besides, if someone from your childhood came along and asked about something that the ring didn't know, the ring still wouldn't know it even if it were running your upload.
#+end_quote

Huh? My upload would know. The point is that the shallow upload would have discarded it as "unnecessary to fool my immediate environment."
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575731084.0
:DateShort: 2019-Dec-07
:END:

******************** Where does the ring get that information, if it has never come up while you're wearing it? The only communication channel is it whispering to you, if it can connect to your brain directly it wouldn't need to whisper.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1575733821.0
:DateShort: 2019-Dec-07
:END:

********************* Good question! But - the earring's advice is /never wrong/. If you're asking if it can read your mind or read the future, I'd presume towards the former.

Whatever this thing is, it's clearly not the optimal way to build /anything./ I sort of see it as similar to the mirror in HPMOR, a first attempt at some goal that whatever civilization created it didn't manage to complete in time to save themselves.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575736787.0
:DateShort: 2019-Dec-07
:END:

********************** It doesn't need to read your mind or the future if it's a superintelligence... in fact reading your mind doesn't help it all that much in establishing a reputation for omniscience nearly as much as superintelligence. It just has to wait until by observation it sees a more optimal action for you. Sort of a pre-selection version of cherry-picking. Over time it builds a database about you that allows it to make more near-certain predictions and advise you on that basis.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1575741748.0
:DateShort: 2019-Dec-07
:END:

*********************** I guess there's a difference between "It has never been observed to be wrong" and "It is by definition never wrong." I'd read it as more the latter, and a "mere" superintelligence would fall more into the former.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575744012.0
:DateShort: 2019-Dec-07
:END:

************************ The latter requires save scumming, assuming this is supposed to be a rational story.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1575746683.0
:DateShort: 2019-Dec-07
:END:

************************* Or authorial fiat.

"HEY [[/u/SCOTTALEXANDER][u/SCOTTALEXANDER]] HOW DOES THE EARRING WORK"

"why do you want to know that"

"NO REASON"
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1575747304.0
:DateShort: 2019-Dec-07
:END:


**** It really /isn't/ doing a gradual upload, though. Regardless of whether you believe "the person on the other side of an upload is still the same person as the person who was uploaded" or "the uploaded person is a perfect copy of a person who is now dead" there's still a person on the other side of the upload. There's nothing in the story to suggest that there's a person preserved on the other side of the earring.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 22
:DateUnix: 1575659689.0
:DateShort: 2019-Dec-06
:END:

***** u/DuskyDay:
#+begin_quote
  There's nothing in the story to suggest that there's a person preserved on the other side of the earring.
#+end_quote

There has to be, because the resulting brain/earring composite is functionally isomorphic to a person, which means (because of the computational theory of mind being correct) that there is a person inside.
:PROPERTIES:
:Author: DuskyDay
:Score: 6
:DateUnix: 1575720421.0
:DateShort: 2019-Dec-07
:END:

****** Computationalism != behaviorism. I can predict the behavior of my dog without needing to run a simulated copy of my dog, because I am much smarter. The earring is in a similar situation with respect to humans, except moreso.
:PROPERTIES:
:Author: Charlie___
:Score: 5
:DateUnix: 1576047855.0
:DateShort: 2019-Dec-11
:END:

******* u/DuskyDay:
#+begin_quote
  Computationalism != behaviorism.
#+end_quote

It does equal - something implements the same computation if and only if it acts the same way (on the appropriate level of abstraction). It's because if something behaves the same way, it implements the same computation (that's an implication in one direction) and if something implements the same computation, then it acts the same way (because you can observe the computation and comprehend it in terms of behavior of the simulated person) (that's an implication in the other direction). So since we have implication in both direction, we have equivalence between behaviorism and computationalism.

#+begin_quote
  I can predict the behavior of my dog without needing to run a simulated copy of my dog, because I am much smarter.
#+end_quote

It might be surprising, but no, you can't. If you can predict the behavior of a system exactly, some particular subset of your brain must be (input-output) isomorphic to the system (otherwise you could never know the behavior of the system, since the behavior is the output).

If you can predict your dog imperfectly, then your brain runs an imperfect emulation of your dog on your brain, the departure of the emulation from the real dog being as big as the imperfection of your predictions.

(Edited.)
:PROPERTIES:
:Author: DuskyDay
:Score: 3
:DateUnix: 1576086307.0
:DateShort: 2019-Dec-11
:END:

******** Suppose I have two different programs for finding the sum of integers from 1 to n. One of them uses a loop and just adds each number in turn. The other multiplies n by (n+1)/2.

The outputs of these computations are identical. If we call your definition of the word "isomorphic" as isomorphic_{DD}, then they're absolutely isomorphic_{DD}. But there is no isomorphism between the states of the computers as the programs are run - no sense in which an intermediate state of computer 1 corresponds to any state of computer 2.

This standard of "same computation" that requires a correspondence between internal states is pretty common in the computational interpretation of consciousness.
:PROPERTIES:
:Author: Charlie___
:Score: 4
:DateUnix: 1576126312.0
:DateShort: 2019-Dec-12
:END:

********* Edit: Corrected my first sentence and added a fourth caveat.

Edit3 added.

​

#+begin_quote
  But there is no isomorphism between the states of the computers as the programs are run
#+end_quote

There isn't, but that doesn't matter.

Because the output of the computer is determined by the physical state of the computer, there is a subset of the physical properties of the computer that determines that the output of the computer will be the sum of integers from 1 to n.

Those physical properties exist in the computer /right now/ (even though it's in the middle of the calculation), so isomorphism_{DD} can be defined in terms of the current physical state of the computer (and not only in the terms of future output), which makes it a present property of the computer. (Rather than the output being the same in the future mysteriously reaching backwards in time and causing the right qualia in the system in the present moment.)

The question is whether isomorphism_{DD} is enough, or whether we need some other subset of the state (one that has no influence on the output of the computation) to be isomorphic too.

We should be able to bootstrap the intuition to see that isomorphism_{DD} is enough.

Imagine that you have introspective access to the way the computation is performed (rather than your mental state being only determined by that aspect of the computation that determines your outputs). Then you should be able to communicate it to someone, which contradicts the assumption it doesn't influence your outputs.

I can think of four possible caveats:

First: What if something is a part of my mental state without me having introspective access to it?

Possible answer: Then it should at least influence my behavior, or else there's no sense in which it's a part of my mental state, but my behavior is a subset of my outputs (or, depending on how we define behavior, it's equivalent to my outputs).

Second: What if I have introspective access to something, but I'm powerless to let it influence my behavior in any way?

Possible answer: I don't think that can happen. As long as I can communicate in at least some way, I can let it influence my behavior. (A special case are locked-in people, in which case we can look at the brain to find out how their behavior would been influenced if they could move.)

Third: What if an entity with a non-isomorphic computation has /no/ qualia? Then it has no mental state, so this reasoning doesn't apply.

Possible answer: I don't think that's possible either, because then we could arrange [[http://consc.net/papers/qualia.html][fading qualia/suddenly disappearing qualia]] without changing the behavior of the system.

Fourth: What if I have a self-contained simulated world and I calculate, let's say, the state of the world at t = 10 right after t = 1, rather than calculating all states in between. Will the intervening states have had subjective experience?

Possible answer: I don't think computing people like that is possible, even in principle. (Edit: Except for hashing, I guess? Let me know if it's important.) (Edit2: I guess it depends on the circumstances. Did you have anything like that in mind?)

Edit3: The example with the two programs calculating the integers doesn't have any isomorphism of the computational states during the computation, because the two programs aren't conscious.

In any conscious software, the computations will be always isomorphic (not just isomorphic_{DD}) on /some/ level of abstraction, because at every moment, the computation encodes the mental state of the person.

So if we just look at the conscious computation on an arbitrary level of abstraction, there will be no computational isomorphism with the original, but that's because we included even the aspects of the computation that don't influence your qualia.

So if we don't include in our description the superfluous parts of the computation, what we get will be isomorphic to the original.

Are there any cases where this doesn't obviously work?
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1576261214.0
:DateShort: 2019-Dec-13
:END:


***** The earring is always right. To say that there isn't a person on the other side of the earring is equivalent to saying that your life is meaningless, or rather that your self is meaningless to your life.
:PROPERTIES:
:Author: FeepingCreature
:Score: 8
:DateUnix: 1575661058.0
:DateShort: 2019-Dec-06
:END:

****** I'm not following how you got from there to there? Like, 1) I don't see how those things are equivalent and 2) "your life is meaningless" and "your 'self' is meaningless to your life" might both be factually correct?
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 19
:DateUnix: 1575661304.0
:DateShort: 2019-Dec-06
:END:

******* It's sort of like the argument that consciousness is the thing that makes you talk about consciousness. It's not stated that the ring-zombies are in any way lessened, partially because that would weaken the story. But being unable to perform any visible aspect of yourself-ness would be a worsening of your performance and hence equivalent to bad advice. The only scenario where the ring-zombie could /fail/ to be an upload is if what you consider your self has very little effect on your actions.

The ring is a DWIM device. Its basic function requires that "I" persists.
:PROPERTIES:
:Author: FeepingCreature
:Score: 6
:DateUnix: 1575661501.0
:DateShort: 2019-Dec-06
:END:

******** Don't forget, if we take as a given that the earring's advice always leads to a better long term happiness, the earring itself tells us that it's use is negative. The very first advice is "don't use it", implying that using the earring is unambiguously worse than not using it. Since outwardly, earring users /seem/ to be happy, successful people, above average, then that must imply something about the users internal state. All the rest of the advice after the first piece is "well, given that you've already decided to use the earring, here's the best you can do".
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 35
:DateUnix: 1575661974.0
:DateShort: 2019-Dec-06
:END:

********* u/ArgentStonecutter:
#+begin_quote
  if we take as a given that the earring's advice always leads to a better long term happiness
#+end_quote

That's not what the story actually says. It only says that the wearer who obeys the earring does not express regret. By the end stage the wearer is unable to do so.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 12
:DateUnix: 1575680714.0
:DateShort: 2019-Dec-07
:END:

********** The story actually explicitly said "the earring is always right" and that there is no case of a wearer regretting following or failing to regret not following. Yes, they are unable to do so at the end, but that's the extreme case. There would be plenty of time for them to do so in the early stages, and they never do. And they always regret it if they don't follow the instructions. The story pretty explicitly says that the following the advice of the ring will always be better / make you happier
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 10
:DateUnix: 1575681359.0
:DateShort: 2019-Dec-07
:END:

*********** It very carefully avoids saying that.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 8
:DateUnix: 1575681935.0
:DateShort: 2019-Dec-07
:END:

************ Which part? "The earring is always right" is literally a direct quote from the text.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 5
:DateUnix: 1575682179.0
:DateShort: 2019-Dec-07
:END:

************* Right for whom?

You need to be more paranoid about genies.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1575682247.0
:DateShort: 2019-Dec-07
:END:

************** Now you're getting pedantic. It is very clear from the context of the story that that quote means that the advice of the earring will make the wearer happier than whatever decision they would have made without the earring.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 9
:DateUnix: 1575682310.0
:DateShort: 2019-Dec-07
:END:

*************** It also says it's not the best decision, just better than the one you would have made. It's manipulating you using classic conditioning.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1575682438.0
:DateShort: 2019-Dec-07
:END:

**************** That just means it's the best decision out of all decisions that might conceivably be made in any imaginable circumstance.

Spend 40 years training with zen monks, scientists and philosophers to make the best decisions. The earring is still offering better decisions than "the one you would have made".
:PROPERTIES:
:Author: TheColourOfHeartache
:Score: 2
:DateUnix: 1575761829.0
:DateShort: 2019-Dec-08
:END:

***************** It doesn't actually say that.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1575761878.0
:DateShort: 2019-Dec-08
:END:


***************** But those decisions might be far better than if you hadn't had that training, because you've raised the bar for "better than the decision you would have made".
:PROPERTIES:
:Author: daytodave
:Score: 2
:DateUnix: 1575980067.0
:DateShort: 2019-Dec-10
:END:


********* u/daytodave:
#+begin_quote
  The very first advice is "don't use it"
#+end_quote

#+begin_quote
  This comment made me realize the real horror of that first instruction. It /doesn't/ say, "don't use me". It says, "better for /you/ if you take me off". It's better for your values, goals, community, and the world at large if you leave it on. It's only better for "you" if you take it off, because "you" is the thing that it destroys.<
#+end_quote
:PROPERTIES:
:Author: daytodave
:Score: 8
:DateUnix: 1575980821.0
:DateShort: 2019-Dec-10
:END:


********* I'm uncertain how literally to take that advice. It seems to connect more to the free-will aesop than the implied uploading.
:PROPERTIES:
:Author: FeepingCreature
:Score: 4
:DateUnix: 1575662482.0
:DateShort: 2019-Dec-06
:END:

********** Your interpretation seems to rely on several things that are not directly supported in the text (most notably the uploading...that's really not implied by the text at all). That doesn't mean it's wrong (frankly, no interpretation of a text can be "wrong" per se, people take what they want form things), but it's a more complicated, less obvious interpretation, and probably not the one intended by the author (again, not that there is necessarily anything wrong in that).

-edit- just to address the implied uploading, according to your earlier comments, if I understand your logic, your argument is basically that "anything that can predict a thing, is that thing". Which is just....wrong. An accurate prediction model does not /have/ to be the thing it is predicting. The map is not the territory. Also, a VERY important distinction in what the earring is doing: it IS NOT predicting the users behavior, it is telling the user what behavior will make the user happer. That does not imply at all that the earring knows what the user /would/ do, just that it knows what outcomes will make the user happier and how to achieve those outcomes.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 25
:DateUnix: 1575665654.0
:DateShort: 2019-Dec-07
:END:

*********** u/FeepingCreature:
#+begin_quote
  Which is just....wrong. An accurate prediction model does not have to be the thing it is predicting. The map is not the territory.
#+end_quote

1. Any sufficiently advanced map is indistinguishable from the territory.

2. Any map that is indistinguishably advanced can be shown to differ from the territory by inspection or interrogation.

#+begin_quote
  it is telling the user what behavior will make the user happ[i]er
#+end_quote

Not /quite/. There seems to be a severe lack of ring users getting addicted to opiates and dying from an overdose. The ring respects community recognition, and it does not generate advice that causes the person to severely stand out in ways that still maximize happiness. I suspect if Eliezer put on the ring, he would not forsake AI and start taking up gardening, for instance. I think the ring does /something/ that involves maximizing your value function. So if that's right, if it fails to be an upload, it has to be because you value something about yourself that you don't value about yourself. Eeh?

I guess it could be the case that there's something about yourself that you or anyone else around you should value but don't. Then the ring would fail to preserve it. Though that's an implementation failure of the ring - it should first change you to value that thing, and then its first advice would not happen.

"Better for you if you adjust your identity of self..."
:PROPERTIES:
:Author: FeepingCreature
:Score: 6
:DateUnix: 1575720272.0
:DateShort: 2019-Dec-07
:END:


*********** u/DuskyDay:
#+begin_quote
  if I understand your logic, your argument is basically that "anything that can predict a thing, is that thing". Which is just....wrong
#+end_quote

It's correct about minds, because minds are software, so if I can predict someone, there must be their mind running on my brain.

At the beginning, your mind runs only on your brain. As the earring micromanages you more and more, your mind gradually moves to the earring (because the composite brain+earring at the beginning contains your mind only in your brain, at the end only in the earring, and in between it's partly here, partly there (as your brain is partially atrophied) - in other words, your mind gradually moved from your brain to the earring).
:PROPERTIES:
:Author: DuskyDay
:Score: 3
:DateUnix: 1575819440.0
:DateShort: 2019-Dec-08
:END:

************ u/ElizabethRobinThales:
#+begin_quote
  minds are software
#+end_quote

Nope. Nope nope nope nope nope nope /no/.

A mind is not software that runs on a brain. You /are/ your brain ^{(more accurately, you /are/ your central and peripheral nervous system)} . Your mind is an illusion.

1) A stimulus sends an electorchemical impulse through your nerves up to a cluster of neurons in your brain (or from one cluster of neurons to another cluster of neurons).

1a) The stimulation of a cluster of neurons /is/ perception.

2) "???"

3) Reaction (ie movement or thought).

That's all there is to it. Perception of stimuli and reaction to stimuli.

Perception itself is the firing of clusters of neurons.

Connections are made between clusters in different regions when those clusters fire at the same time (super extraordinarily simplified: there is a red dot that emits a sound; every time you encounter the dot, your eyes send an electrochemical impulse to one region of your brain and your ears send an electrochemical impulse to a different region of your brain; a connection grows between the cluster in the "eyes" region and the cluster in the "ears" region (a "map" of all of these connections is called a "[[https://en.wikipedia.org/wiki/Connectome][connectome]]" and every person's connectome is different, just like fingerprints)).

Connections between clusters in different regions strengthen when those clusters fire together often.

Connections between clusters in different regions atrophy when those clusters fire together infrequently.

"Learning" is creating/strengthening connections.

"Thinking" and "remembering" are clusters firing in reaction to other clusters firing instead of in direct reaction to stimuli.

#+begin_quote
  ... the connectome can evidently support a great number of variable dynamic states, depending on current sensory inputs, global brain state, learning and development. Some changes in functional state may involve rapid changes of structural connectivity at the synaptic level...
#+end_quote

Not all connections are permanent. Maybe most connections are impermanent.

"Dreaming" might just be what happens when the day's weakest temporary connections are broken.^{[/original research?/]}

The human brain is still a brain. Every aspect of it is built upon all of the brains that came before it. First came multicellular organisms without a nervous system and with every cell communicating with every one of its neighbors, then came distributed nerve nets, then came brains.

The reason any of that happened in the first place is because [[https://www.sciencealert.com/scientists-have-witnessed-in-real-time-a-single-celled-algae-evolve-into-a-multicellular-organism][single-cell organisms started clumping together in reaction to predation]] and they began transmitting electrochemical impulses between each other to coordinate movement within three dimensional space.

/Presumably/ it was more efficient for electrochemical impulses to travel through a web of cells to communicate (rather than every single cell communicating with every single one of its neighbors) and eventually cells in that web became specialized into nerves which eventually became a nerve net.

/Presumably/ "???" and eventually "???" which eventually became a brain.

The very first "???" (step 2, between perception and reaction) is probably just "electrochemical impulses moving from out to in and then from in back to out." Like, every one of the steps might boil down to "reaction." Literally everything about your 'Self' and your inner experience of 'perception' is probably 100% electrochemical reaction. It just happens, it isn't controlled/directed by a mind.

This idea that seems pervasive in tech circles (and also in this rationalist community) (and, honestly, also in the general public) - the idea that the mind is software and the brain is hardware - feels very strongly to me like a pseudo-religious belief. It reminds me very much of the "soul" belief, that your 'Self' is somehow something separate from your brain. Just /no/. Your great to the n-th grandfather (/G/^{/n/)} was slime. /You came from slime./ There is no ghost in the machine, there is only the machine. If you create a perfect copy of the machine and destroy the original machine, you haven't transferred the ghost to a new machine because there is no ghost, all you've done is destroy the original machine. There's no such thing as a "self" and there's no such thing as a "mind."

--------------

^{Jesus /Christ/, Michael - did you /seriously/ just waste /three straight hours/ writing a comment in a two-day-old thread? Like /two people/ are ever going to see this.}
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1575839892.0
:DateShort: 2019-Dec-09
:END:

************* u/DuskyDay:
#+begin_quote
  You are your brain
#+end_quote

That wouldn't work, because then you could, in theory, achieve [[http://consc.net/papers/qualia.html][fading or suddenly disappearing qualia through a gradual brain replacement]]. (A "gradual brain replacement" is a gradual replacement of very small parts of your brain with their functional equivalents (made of something else, like a small silicon computer, (but it could be anything with the same input-output pattern)), until your entire brain is replaced by a different object implementing the same input-output pattern.) But fading and suddenly disappearing qualia are both impossible - it follows that your consciousness would stay, which means that you're not your brain.

​

Edit:

#+begin_quote
  It just happens, it isn't controlled/directed by a mind.
#+end_quote

That's a misunderstanding of what "software" is. Software isn't some /extra/ entity that controls the hardware. Once you specify every particle of the hardware and its properties, the software is /already there/.

A silicon computer running software "just happens" in the exact same way brain processes "just happen".
:PROPERTIES:
:Author: DuskyDay
:Score: 3
:DateUnix: 1575841350.0
:DateShort: 2019-Dec-09
:END:

************** u/ElizabethRobinThales:
#+begin_quote
  qualia
#+end_quote

That's a philosophy thing (sometimes used in psychology). I don't know who the hell told philosophers and psychologists that they have authority to butt their way into neuroscience, but they don't. Daniel Dennett might be literally the only philosopher qualified to say things about the mind. [EDIT: Sam Harris literally has a PhD in neuroscience, and I'm sure he /must/ have said /something/ about philosophy of the mind, but he slipped my mind since he's so heavily oriented towards religion.] "Qualia." Pfft. As if "the feeling of the experience of redness" is something distinct from "the perception of redness."

#+begin_quote
  "gradual brain replacement"
#+end_quote

Yeah I know what that is thanks. Yesterday you replied to one of my comments, which was about uploading. I think you should've assumed that I'm aware of basics.

Speaking of that reply...

#+begin_quote
  There has to be [a person preserved on the other side of the earring], because the resulting brain/earring composite is functionally isomorphic to a person, which means... that there is a person inside.
#+end_quote

If you don't already understand how ridiculous what you've said is, I'm not going to have the time or the patience to force you to understand. I just can't.

#+begin_quote
  the computational theory of mind [is] correct
#+end_quote

You don't get to just /assert/ that the computational theory of mind is correct (it most certainly /isn't/).

I'd like you to meditate on the subject of where the bloody hell you think algorithms are hiding in the brain.

When I wrote "this idea... feels very strongly to me like a pseudo-religious belief," I started to also write things about "mind-body dualism" but I felt that that was writing too much. I'll say some of that now.

The computational theory of mind is just modernized dualism, except instead of a soul reaching into the brain and performing actions on our sensory experience you think algorithms are performing actions on our sensory experience. Sensory stimulation happens - everything after that is dominoes falling.

Dennett was right, Descartes totally poisoned people's ideas about minds.

#+begin_quote
  it follows that your consciousness would stay, which means that you're not your brain.
#+end_quote

​No. Consciousness is not a thing that exists in your brain, it is a thing that your brain does. Your brain does not have consciousness inside it, your brain performs consciousness. It doesn't matter how gradually you replace the brain, even if you do it neuron by neuron. You'll have a perfect synthetic replica of your brain, and it'll perform consciousness just like yours used to.

Get up right now. Look at the nearest wall. It is 8 feet away from you. Move halfway to the wall. It is now 4 feet away from you. Then move halfway to the wall again. And again. And again. And again. Two feet. One foot. Six inches. Three inches. It must be impossible to reach the wall, because you can move halfway to the wall an infinite number of times.

That's what "gradual replacement" sounds like. A philosophical thought experiment that doesn't apply to the real world. If you destroy the brain then the brain has been destroyed, regardless of how gradually you destroy it and regardless of whether or not you replace it with an exact copy capable of performing consciousness in the same way that the original brain performed consciousness.

#+begin_quote

  #+begin_quote
    It just happens, it isn't controlled/directed by a mind.
  #+end_quote

  That's a misunderstanding of what "software" is.
#+end_quote

Where the hell did I say anything about hardware being controlled/directed by software?
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1575853107.0
:DateShort: 2019-Dec-09
:END:

*************** u/DuskyDay:
#+begin_quote
  where the bloody hell you think algorithms are hiding in the brain
#+end_quote

Algorithms are everywhere in the brain (modulo perhaps those parts of the brain that play only the role of e.g. structural support (if there are any such parts)).

This isn't a hard question - it's like asking where are algorithms in a laptop on your table - it depends on what parts of the physical system perform the computation.

#+begin_quote
  The computational theory of mind is just modernized dualism
#+end_quote

That's a double misunderstanding of "software" /and/ "dualism". "Software" is simply those aspects of the hardware that implement the computation.

For computational theory of mind (which is both the only candidate and the correct theory to explain consciousness) to be dualistic, "software" would need to be an extra entity whose nature would be nonphysical in some way.

#+begin_quote
  If you destroy the brain then the brain has been destroyed, regardless of how gradually you destroy it and regardless of whether or not you replace it with an exact copy capable of performing consciousness in the same way that the original brain performed consciousness.
#+end_quote

So, what are you saying you'd perceive if your brain was being gradually replaced? Would you perceive your conscious experience fading out, or would it suddenly switch off for you at some point?

#+begin_quote
  Where the hell did I say anything about hardware being controlled/directed by software?
#+end_quote

You wrote

#+begin_quote
  It just happens, it isn't controlled/directed by a mind.
#+end_quote

A natural interpretation of that is that you interpreted my explanation as me saying that mind (being the software) controls/directs anything in the brain. Please rephrase if you meant something else.

Edit: Grammar
:PROPERTIES:
:Author: DuskyDay
:Score: 3
:DateUnix: 1575915780.0
:DateShort: 2019-Dec-09
:END:

**************** u/ElizabethRobinThales:
#+begin_quote
  parts of the brain that play only the role of e.g. structural support (if there are any such parts)
#+end_quote

The brain is mostly water and fat. In its natural/unpreserved state, it is softer than Jello. There is no such thing as "structural support" in the brain. The brain is composed /entirely/ of neurons.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1575933210.0
:DateShort: 2019-Dec-10
:END:

***************** u/Buggy321:
#+begin_quote
  There is no such thing as "structural support" in the brain. The brain is composed entirely of neurons
#+end_quote

Okay, my qualifications can be summed up as "took highschool biology", and I know for a fact that this is untrue. [[https://en.wikipedia.org/wiki/Glia][Glial cells]] exist. While the exact concentration is in question (if you take the Wikipedia page at face value, old studies claim 10 Glials per neuron, a new study claims 1:1 or less), it's quite clear that they /[[https://scholar.google.com/scholar?hl=en&as_sdt=0%2C10&q=glial+cells&btnG=][exist]]/.

Secondly, yes, the brain is neurons and junk and such, with as much exquisite detail as you wish. No contest there.

And those neurons are composed of chemical compounds (and some unbound elements). Chemical compounds and elements are composed of atoms. Atoms are composed of fields and subatomic particles and other things. Fields and subatomic particles can, so far as we've discovered, be completely and entirely described by math. Which can be computed.

If I can simulate particles and fields, I can simulate the atoms which make up the compounds which make up the neurons which make up the networks which make up a brain. Actually doing so is just a matter of scope.

There are details which complicate things a little, sure, but in all /brains run on physics/, and saying that brains can't be computed is equivalent to saying that physics cannot be computed. That, or it's stating that there is a element to brains specifically that can't be computed in contrast to everything else, which sounds like dualism to me.
:PROPERTIES:
:Author: Buggy321
:Score: 4
:DateUnix: 1576011831.0
:DateShort: 2019-Dec-11
:END:

****************** I think the problem isn't that [[/u/ElizabethRobinThales][u/ElizabethRobinThales]] doesn't know that brains can be emulated, I think the problem is that they don't know that your consciousness doesn't disappear during a gradual brain replacement (and therefore don't know that it doesn't disappear in a gradual mind upload).
:PROPERTIES:
:Author: DuskyDay
:Score: 3
:DateUnix: 1576014002.0
:DateShort: 2019-Dec-11
:END:

******************* I think the problem is that you don't know what consciousness is.

Why don't you go ahead and explain "consciousness" to me? Just a real quick explanation, y'know, since it's so simple.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1576036705.0
:DateShort: 2019-Dec-11
:END:


****************** u/ElizabethRobinThales:
#+begin_quote
  Glial cells exist.
#+end_quote

That they do. I got a bit carried away.

#+begin_quote
  And those neurons are composed of chemical compounds (and some unbound elements). Chemical compounds and elements are composed of atoms. Atoms are composed of fields and subatomic particles and other things. Fields and subatomic particles can, so far as we've discovered, be completely and entirely described by math. Which can be computed.

  If I can simulate particles and fields, I can simulate the atoms which make up the compounds which make up the neurons which make up the networks which make up a brain. Actually doing so is just a matter of scope.
#+end_quote

You're spittin' straight facts. It should definitely be possible to simulate a brain.

#+begin_quote
  There are details which complicate things a little, sure, but in all /brains run on physics/, and saying that brains can't be computed is equivalent to saying that physics cannot be computed.
#+end_quote

Where, /exactly/, do you believe that you saw me claim that brains can't be computed? Do you know what the computational theory of mind is?

#+begin_quote
  [[https://en.wikipedia.org/wiki/Computational_theory_of_mind]['Computational system' is not meant to mean a modern-day electronic computer. Rather, a computational system is a symbol manipulator that follows step by step functions to compute input and form output.]]
#+end_quote

It's a philosophical metaphor.

An excerpt from [[https://aeon.co/essays/your-brain-does-not-process-information-and-it-is-not-a-computer][an article]]:

#+begin_quote
  In his book In Our Own Image (2015), the artificial intelligence expert George Zarkadakis describes six different metaphors people have employed over the past 2,000 years to try to explain human intelligence.

  In the earliest one, eventually preserved in the Bible, humans were formed from clay or dirt, which an intelligent god then infused with its spirit. That spirit ‘explained' our intelligence -- grammatically, at least.

  The invention of hydraulic engineering in the 3rd century BCE led to the popularity of a hydraulic model of human intelligence, the idea that the flow of different fluids in the body -- the ‘humours' -- accounted for both our physical and mental functioning. The hydraulic metaphor persisted for more than 1,600 years, handicapping medical practice all the while.

  By the 1500s, automata powered by springs and gears had been devised, eventually inspiring leading thinkers such as René Descartes to assert that humans are complex machines. In the 1600s, the British philosopher Thomas Hobbes suggested that thinking arose from small mechanical motions in the brain. By the 1700s, discoveries about electricity and chemistry led to new theories of human intelligence -- again, largely metaphorical in nature. In the mid-1800s, inspired by recent advances in communications, the German physicist Hermann von Helmholtz compared the brain to a telegraph.

  Each metaphor reflected the most advanced thinking of the era that spawned it. Predictably, just a few years after the dawn of computer technology in the 1940s, the brain was said to operate like a computer, with the role of physical hardware played by the brain itself and our thoughts serving as software. The landmark event that launched what is now broadly called ‘cognitive science' was the publication of Language and Communication (1951) by the psychologist George Miller. Miller proposed that the mental world could be studied rigorously using concepts from information theory, computation and linguistics.

  This kind of thinking was taken to its ultimate expression in the short book The Computer and the Brain (1958), in which the mathematician John von Neumann stated flatly that the function of the human nervous system is ‘prima facie digital'. Although he acknowledged that little was actually known about the role the brain played in human reasoning and memory, he drew parallel after parallel between the components of the computing machines of the day and the components of the human brain.

  Propelled by subsequent advances in both computer technology and brain research, an ambitious multidisciplinary effort to understand human intelligence gradually developed, firmly rooted in the idea that humans are, like computers, information processors. This effort now involves thousands of researchers, consumes billions of dollars in funding, and has generated a vast literature consisting of both technical and mainstream articles and books. Ray Kurzweil's book How to Create a Mind: The Secret of Human Thought Revealed (2013), exemplifies this perspective, speculating about the ‘algorithms' of the brain, how the brain ‘processes data', and even how it superficially resembles integrated circuits in its structure.

  The information processing (IP) metaphor of human intelligence now dominates human thinking, both on the street and in the sciences. There is virtually no form of discourse about intelligent human behaviour that proceeds without employing this metaphor, just as no form of discourse about intelligent human behaviour could proceed in certain eras and cultures without reference to a spirit or deity. The validity of the IP metaphor in today's world is generally assumed without question.

  But the IP metaphor is, after all, just another metaphor -- a story we tell to make sense of something we don't actually understand. And like all the metaphors that preceded it, it will certainly be cast aside at some point -- either replaced by another metaphor or, in the end, replaced by actual knowledge.
#+end_quote

/Where are the hydraulic valves?/

/Where are the little gears?/

/Where are the algorithms?/

Cc [[/u/DuskyDay][u/DuskyDay]]
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1576032435.0
:DateShort: 2019-Dec-11
:END:

******************* It's not a metaphor. The brain literally processes information.

#+begin_quote
  Where are the algorithms?
#+end_quote

I did answer that already:

#+begin_quote
  Algorithms are everywhere in the brain (modulo perhaps those parts of the brain that play only the role of e.g. structural support (if there are any such parts)).

  This isn't a hard question - it's like asking where are algorithms in a laptop on your table - it depends on what parts of the physical system perform the computation.
#+end_quote

If there's anything unclear about my answer, please ask specifically about my answer instead of just repeating your question.

I also asked

#+begin_quote
  So, what are you saying you'd perceive if your brain was being gradually replaced? Would you perceive your conscious experience fading out, or would it suddenly switch off for you at some point?
#+end_quote

And you didn't respond to that.

#+begin_quote
  Why don't you go ahead and explain "consciousness" to me?
#+end_quote

Consciousness is the brain processing information about something (us being conscious of something is the brain processing information about it).
:PROPERTIES:
:Author: DuskyDay
:Score: 3
:DateUnix: 1576086012.0
:DateShort: 2019-Dec-11
:END:

******************** u/ElizabethRobinThales:
#+begin_quote
  It's not a metaphor. The brain literally processes information.
#+end_quote

False and false. Hitchens' Razor: That which can be asserted without evidence can be dismissed without evidence.

#+begin_quote
  I did answer that already:

  #+begin_quote
    Algorithms are everywhere in the brain...
  #+end_quote
#+end_quote

"Where is the soul/spirit?"

"Spirit is everywhere in the brain."

No, you did /not/ answer the question.

#+begin_quote
  I also asked

  #+begin_quote
    So, what are you saying you'd perceive if your brain was being gradually replaced? Would you perceive your conscious experience fading out, or would it suddenly switch off for you at some point?
  #+end_quote

  And you didn't respond to that.
#+end_quote

Because there is no "you" to do the perceiving, there is only the perceiving. You're operating so flipping far from reality that there's literally no way for me to unpack each and every individual false assumption you're making. I guess I can try...

"Juggling" is not a thing that exists independently in the world. "Juggling" is a word used to describe an action performed by a person. If "Person A" (/P^{A}/) is juggling balls and transfers the balls to /P^{B}/ and then someone else substitutes each ball for a bowling pin while /P^{B}/ is still performing the juggling, then would you say "/P^{A}/ juggling balls and /P^{B}/ juggling pins are still the same instance of juggling"? No, because "an instance of juggling" is not a /thing/ that exists.

Your brain is performing perception. "Perceiving" is the thing that the brain does. If you were to replace a part of your brain with silicone neurons, this new arrangement of brain and silicone would continue to perform perception. If you were to replace the rest of your brain with silicone neurons, this new arrangement of silicone would continue to perform perception. If the replacements were done in such a way that the silicone neurons continued to make new connections in the same way that the carbon neurons used to do, then the performance of perception by the silicone brain would be indistinguishable from the performance of perception by the recently deceased organic brain.

#+begin_quote
  Consciousness is the brain processing information about something (us being conscious of something is the brain processing information about it).
#+end_quote

"[S]ome students wrote down 'because of how the metal conducts heat', and some students wrote down 'because of how the air moves'... The students thought they could use words like 'because of heat conduction' to explain anything, even a metal plate being cooler on the side nearer the fire."

You do not understand the words that you are using. Consciousness is simply awareness which is simply perception of perception which is simply perception which is simply multiple clusters of neurons conjointly firing as a result of being triggered by stimuli. There is no symbolic representation (ie "information") in the brain.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1576091687.0
:DateShort: 2019-Dec-11
:END:

********************* Ultimately, in this context, I don't think there is a meaningful difference between "a brain performing perception", "a brain with 1 neuron replaced by identical silicon performing perception", and "a silicon brain functionally identical to the original brain performing perception".

Yes, the original brain was destroyed in the process, the new brain is not the original even if it is functionally identical. Yes, /technically/, this killed you. However, for the intents and purposes of everyone around you, the new you, the former you, and the vast majority of everything else we consider relevant, the new brain is effectively the old brain. It is only a 'death' in the most technical and meaningless of senses.

To put it another way, if replacing parts with functionally identical alternatives over a period of time causes this thing we call 'death', then eating and general homeostasis will periodically kill you over the course of your 'life'. Unless there is some significant reason that you would die if you replaced neurons with silicon neurons, but wouldn't die if you replaced all of your carbon-12 atoms with carbon-13.

I don't know if a decent method of describing brains would in the form of instantiated algorithms or not. But, most macroscale physical systems we've learned to simulate, can be simulated in a manner we consider sufficiently accurate without simulating every particle and field. The output is not (probabilistically and such) identical as it would be if we did, but our criteria for 'effectively identical' is broad enough that we don't care in the slightest which of the trillions upon trillions of different atomic configurations it is, so long as it is similar on a larger scale we consider relevant. It stands to reason that this also extends to simulating brains, as it is a macroscale physical system.

So, hypothetically, the Earring could have a simulation of the wearer's brain in it, which is not technically the brain but is functionally identical. The Earring could use this simulation to replace the physical brain, by measuring all significant inputs and relaying all significant outputs, such as neural impulses, blood hormone levels, and others. Because it most likely does not need to perfectly replicate every atom of the simulated brain, measure every atomic vibration at the surface boundary of where the simulated brain would be in the skull, etc, it's plausible that the Earring could gather and transmit the necessary information via high-frequency sound as depicted in the story, with the actual contents of the skull being modified as necessary to serve as a 'relay' and 'output'.

Now, the obvious response is, "we /don't know/ if the Earring does this". To which I say, "Would it make a difference if it doesn't?". To sum it up, we're using very human concepts like 'life', 'death', 'person', etc here. And to humans, if it walks like a duck, swims like a duck, quacks like a duck, and you cut it open and it looks like a duck, /it is a duck/. If you cut it open and it's blue for some reason, but everything else is the same? Pretty much still a duck. It has to be fairly un-ducklike to stop being a duck.

So, sure, maybe the Earring is using some other method to merely /appear/ to be simulating a brain in a manner that we would /definitely/ consider to be the same person. No one cares. It is the same person, according to the majority of the beings who defined that word and use it. You can say it's a different collection of atoms than existed in X location at Y time, you can say that it is not a collection of atoms with a continuous existence traceable from X time to Y time, etc. But it's the same person.

Now, the /caveat/ here is that the Earring does not just perfectly simulate people. It does some weird optimizing process, /or/ it predicts the future, /or/ it has a agenda of its own and it optimizes people for it, /or/...

And that is something that is unclear. But for the purposes of "can the brain move in to the earring", it doesn't matter at all if we get into the nitty gritty of how the brain works or anything. Because from the moment the question was 'is this the same person', the answer was /yes/, because that word doesn't care about the nitty gritty.
:PROPERTIES:
:Author: Buggy321
:Score: 3
:DateUnix: 1576242193.0
:DateShort: 2019-Dec-13
:END:

********************** u/ElizabethRobinThales:
#+begin_quote
  However, for the intents and purposes of everyone around you, the new you, the former you, and the vast majority of everything else we consider relevant, the new brain is effectively the old brain. It is only a 'death' +in the most technical and meaningless of senses+ /in the most important sense conceivable/.
#+end_quote

FTFY.

#+begin_quote
  To put it another way, if replacing parts with functionally identical alternatives over a period of time causes this thing we call 'death', then eating and general homeostasis will periodically kill you over the course of your 'life'.
#+end_quote

Only sorta/kinda. I could say that I'm killing you right now. You're reading these words, and your brain is physically changing as a result. Connections between clusters of neurons are being strengthened and broken as we speak. I have changed your brain.

It's just a thought experiment. You're made of cells which are all made of molecules which are all made of atoms. What if we took literally every single carbon atom in your body and magically replaced them all with a different set of carbon atoms? Still the exact same molecules, still the exact same cells, still the exact same squishy sack of meat, perfectly biological. Is that still you? I'm not interested in that sort of thought experiment. You don't move halfway to the wall an infinite number of times, you just walk over and touch the wall. In the real world, "you" are a collection of trillions of different domino reactions all meeting in the same place. Breaking a chain and is breaking a chain that stretches literally all the way back to before the big bang.

This video is not what I want it to be, but it's what I can find with the amount of effort I'm willing to expend on this:

[[https://www.youtube.com/watch?v=GdD-fKSq-1Q]]

(doesn't actually start till the 12 second mark)

Imagine that instead of hitting the top left branch, he hit the bottom left and bottom right branches simultaneously, and when they met in the middle the top branches continued forwards. "You" are the middle of that X shape. The bottom branches are "literally everything else that happened in the universe up to this moment." If you took the middle of that X shape and destroyed it in order to figure out how to recreate it somewhere else in such a way that it would carry forward with the same momentum as the original middle of that X shape, it would not be the same domino reaction. "You" are a domino chain reaction stretching back to the beginning of Time, and no amount of philosophical musing will make an Upload be connected to that chain.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 0
:DateUnix: 1576278008.0
:DateShort: 2019-Dec-14
:END:

*********************** u/Buggy321:
#+begin_quote
  in the most important sense conceivable.
#+end_quote

Frankly, says you. And just you.

You reject 'philosophical musing', but what you're stating is, I would say, /also/ philosophical musing. Defining a person as "the entire chain of effect throughout the entire universe, since the beginning of time that led to them existing in that moment" sounds rather 'philosophical' to me.

If your definition of death is so strict that living causes 'death', well, okay fine. Its hard to say that it's wrong, because words that are that 'human' are very hard to get precise definitions for, and so you can't really prove that its wrong. But its not a definition that I, or many other people agree with, and I don't think you've made a very convincing argument for it.

You keep adamantly stating that /this/ is the definition of life/death, but why? This isn't purely hypothetical thought scenarios that can be 'rejected'. Uploading, gradual brain replacement, etc will almost certainly become possible if technology continues to progress. Will uploads be considered non-persons/not the original person? Maybe, maybe not. But will partial uploads? Brain replacements? Even people with brain implants, even minor ones? Because /those/ already exist, and I've yet to hear about someone mourn the death of a loved one who got a cochlear implant and became a different person.

And why do these things 'break the chain' at all? You said a chain is everything back to the big bang. How would a upload be any less a part of the chain than, say, the geological activity of primordial earth? It's not like a upload isn't part of the universe. The latter is probably even /more/ dissociated from anything brain-like than a upload. And a upload is a direct product of the 'pattern', more directly associated than the human brain is with early stellar dynamics.

You can go on believing that this counts as 'death', fine. In that case, if I'm still around when uploading is developed, I will happily 'kill myself' as you put it. Heck, with how convenient being able to copy yourself, transmit (actually copy+delete) yourself place to place, etc would be, I might end up 'killing myself' hundreds or thousands of times! If you want to hold a funeral for me, and all the other people who do that, well you do you. I'll even send a me over to participate.

But you get to foot the bill.
:PROPERTIES:
:Author: Buggy321
:Score: 3
:DateUnix: 1576368600.0
:DateShort: 2019-Dec-15
:END:

************************ u/appropriate-username:
#+begin_quote
  I've yet to hear about someone mourn the death of a loved one who got a cochlear implant and became a different person.
#+end_quote

--------------

#+begin_quote
  Instead, the devices pick up sound and digitize it, convert that digitized sound into electrical signals, and transmit those signals to electrodes embedded in the cochlea. The electrodes electrically stimulate the cochlear nerve, causing it to send signals to the brain.
#+end_quote

[[https://en.wikipedia.org/wiki/Cochlear_implant]]

I don't think it makes sense to call electrodes that stimulate a nerve connected to the brain a brain implant. That's like two steps removed from messing with the brain. It doesn't become part of the brain or even interface directly with the brain, it interfaces with something else that does interface with the brain.

#+begin_quote
  You can go on believing that this counts as 'death', fine. In that case, if I'm still around when uploading is developed, I will happily 'kill myself' as you put it. Heck, with how convenient being able to copy yourself, transmit (actually copy+delete) yourself place to place, etc would be, I might end up 'killing myself' hundreds or thousands of times!
#+end_quote

[[https://web.archive.org/web/20200105173558/http://web.archive.org/screenshot/https://existentialcomics.com/comic/1]]
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1578245794.0
:DateShort: 2020-Jan-05
:END:

************************* u/Buggy321:
#+begin_quote
  I don't think it makes sense to call electrodes that stimulate a nerve connected to the brain a brain implant. That's like two steps removed from messing with the brain. It doesn't become part of the brain or even interface directly with the brain, it interfaces with something else that does interface with the brain.
#+end_quote

Fair point, I forgot that cochlear implants are a implant that goes on the, yknow, /Cochlea/. Not the brain.

As a alternative, might I suggest the [[https://www.wired.com/story/hippocampal-neural-prosthetic/][Hippocampal prosthesis]] Wikipedia article [[https://en.wikipedia.org/wiki/Hippocampal_prosthesis][here]]. Having recently moved to (thus far, successful) human trials, a hippocampal prosthetic functions by, essentially, trying to computationally predict the (properly functioning) behavior of the hippocampus for a given input via implanted electrodes, and then replicate its output.

This is very similar to what we've been talking about, is it not?

#+begin_quote
  [[https://web.archive.org/web/20200105173558/http://web.archive.org/screenshot/https://existentialcomics.com/comic/1]]
#+end_quote

That link doesn't work, but the [[https://existentialcomics.com/comic/1][direct link]] still does. And that's pretty much my point in comic form. I forgot about that one, thank you for reminding me of it.
:PROPERTIES:
:Author: Buggy321
:Score: 1
:DateUnix: 1578314150.0
:DateShort: 2020-Jan-06
:END:


********************* Well, the brain processing information follows from physics (every composite (and arguably every fundamental) physical object processes information (it transforms inputs into outputs)).

There is a way of discovering a symbolic representation of whatever is being processed in every object - otherwise, the physical structure of the object couldn't transform the input into an appropriate output. So given the physical state of the object, it's guaranteed that some aspects of the object will be a symbolic representation of whatever is being processed.

The only difference is that in a silicon computer, it's clear what constitutes the symbols (because humans engineered computers to be easily comprehensible), but in the brain it's harder (but you can already experimentally verify it directly - e.g. by translating a brain scan into a picture the person sees).

So it's even more general - not only the brain processes information, but every physical object processes information (except that it's usually simple/uninteresting, since most physical objects didn't evolve to be a control center of a complex organism).

#+begin_quote
  Because there is no "you" to do the perceiving, there is only the perceiving.
#+end_quote

So, what is your position? Are you your brain (as you said before), or do you not exist at all?

The difference is obvious - if you don't exist, the earring can't hurt you. If you are your brain, the earring kills you.

So, are you your brain, or you don't exist? (The correct answer is a third one, obviously, but I'd like to flesh out your position before we start explaining.)

#+begin_quote
  If you were to replace a part of your brain with silicone neurons, this new arrangement of brain and silicone would continue to perform perception. If you were to replace the rest of your brain with silicone neurons, this new arrangement of silicone would continue to perform perception. If the replacements were done in such a way that the silicone neurons continued to make new connections in the same way that the carbon neurons used to do, then the performance of perception by the silicone brain would be indistinguishable from the performance of perception by the recently deceased organic brain.
#+end_quote

This is correct, but that doesn't tell you how you'd perceive that happening.

To see what you actually believe (instead of typing out that "you don't exist"), imagine if you'd try to run away from a gradual brain replacement or not.

If you would, you probably believe it would destroy your consciousness.

If you wouldn't, you probably believe it wouldn't do anything to you.

It's a simple way of imagining what you really believe.

(So, would you run away from a gradual brain replacement or not?)
:PROPERTIES:
:Author: DuskyDay
:Score: 3
:DateUnix: 1576252074.0
:DateShort: 2019-Dec-13
:END:

********************** u/ElizabethRobinThales:
#+begin_quote
  Well, the brain processing information follows from physics (every composite (and arguably every fundamental) physical object processes information (it transforms inputs into outputs)).
#+end_quote

The wikipedia page for information processing systems agrees with you. Probably because information processing is the predominant metaphor central to most people's worldview right now. Just above where it says stuff about a rock falling "holding information" about gravity or whatever, in the initial description, it says "information processors take information in one form and process it into another form */via algorithms/* ^{[emphasis mine]}."

This is ridiculous. I'm done with this argument. I hope [[/r/rational][r/rational]] is still around in 5 to 50 years so I can tell you "I told you so" after the "computation / information processing" metaphor is discarded.

#+begin_quote

  #+begin_quote
    Because there is no "you" to do the perceiving, there is only the perceiving.
  #+end_quote

  So, what is your position? Are you your brain (as you said before), or do you not exist at all?
#+end_quote

"You" "are" "your" brain. It's a problem with semantics. Our language is built around the assumption that there /is/ a "you." There is a brain. The brain performs perception. This "feels like" Selfhood. But there is no Self. There is no "you." You are not your consciousness, your consciousness is the brain in your skull performing perception. You are not the consciousness, you /have/ consciousness.

An interesting article:

#+begin_quote
  We understand control as being UP and being subject to control as being DOWN: We say, “I have control over him,” “I am on top of the situation,” “He's at the height of his power,” and, “He ranks above me in strength,” “He is under my control,” and “His power is on the decline.” Similarly, we describe love as being a physical force: “I could feel the electricity between us,” “There were sparks,” and “They gravitated to each other immediately.” Some of their examples reflected embodied experience. For example, Happy is Up and Sad is Down, as in “I'm feeling up today,” and “I'm feel down in the dumps.” These metaphors are based on the physiology of emotions, which researchers such as Paul Eckman have discovered. It's no surprise, then, that around the world, people who are happy tend to smile and perk up while people who are sad tend to droop.

  Metaphors We Live By was a game changer. Not only did it illustrate how prevalent metaphors are in everyday language, it also suggested that a lot of the major tenets of western thought, including the idea that reason is conscious and passionless and that language is separate from the body aside from the organs of speech and hearing, were incorrect. In brief, it demonstrated that “our ordinary conceptual system, in terms of which we both think and act, is fundamentally metaphorical in nature.”
#+end_quote

[[https://blogs.scientificamerican.com/guest-blog/a-brief-guide-to-embodied-cognition-why-you-are-not-your-brain/]]

Here's a PDF of the book the article talks about:

[[https://nyshalong.com/public/archive/20150131/20150131_ref.pdf]]

#+begin_quote
  Would you run away from a gradual brain replacement?
#+end_quote

Obviously. The consciousness performed by the replica of my brain would be exactly the same as my consciousness, right?

So that entity - my replacement - would /constantly/ be aware that I (the me currently typing this at you) was dead and did not and would never experience the experiences the entity experienced, so every wonderful experience that entity experienced would feel hollow and empty. In words you would probably use, "I" would morn "my own" death pretty much 24/7. So of course I'd run away from uploading. Not just out of a desire to continue experiencing experience, but out of a desire to not bring into existence an entity who would inherit /my/ understanding of /its/ nature. My mindclone would become neurotically obsessed with finding a way to kill itself.

#+begin_quote
  I'd like to flesh out your position before we start explaining.

  To see what you actually believe (instead of typing out that "you don't exist"), imagine if you'd try to run away from a gradual brain replacement or not.

  If you would, you probably believe it would destroy your consciousness.

  If you wouldn't, you probably believe it wouldn't do anything to you.

  It's a simple way of imagining what you really believe.
#+end_quote

Why was any of that necessary? That's an "experiment" to suss out the position of someone who hasn't already made their position clear.

It almost seems like you think you're being clever by leading me to a realization that I don't actually believe what I've professed to believe over the course of this argument. Like, "see, you /wouldn't/ run away from a gradual brain upload, so there's a discrepancy between what you /say/ you believe and what you /actually/ believe, so you must /actually/ believe the way that /I/ believe since that's the /correct/ way to believe."

I've been /crystal clear/ this entire time that destruction of the brain equals death. There's pretty much no other way to interpret anything I've been saying over /the past several days/.

Speaking of "/the past several days/", you're welcome to respond to get the last word in if you feel like you need to, but I'm done here. You're just not getting the fact that everything you're saying in support of the "information processing" premise is itself based on granting the "information processing" premise. "Of course the mind is an information processor, because it processes information." That's what this conversation feels like to me, and I'm not interested in wasting any more of my time on it.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 0
:DateUnix: 1576276257.0
:DateShort: 2019-Dec-14
:END:

*********************** u/DuskyDay:
#+begin_quote
  is the predominant metaphor
#+end_quote

It's not a metaphor. I explained why there are symbols in every physical object (even though they're only obvious in engineered things, and somewhat less obvious (but still verifiably existing) in the brain). Look:

#+begin_quote
  There is a way of discovering a symbolic representation of whatever is being processed in every object - otherwise, the physical structure of the object couldn't transform the input into an appropriate output. So given the physical state of the object, it's guaranteed that some aspects of the object will be a symbolic representation of whatever is being processed.

  The only difference is that in a silicon computer, it's clear what constitutes the symbols (because humans engineered computers to be easily comprehensible), but in the brain it's harder (but you can already experimentally verify it directly - e.g. by translating a brain scan into a picture the person sees).
#+end_quote

If you think you see any principal difference between a brain and a computer about symbols, please respond to my explanation directly, instead of just repeating the same sentence.

#+begin_quote
  "You" "are" "your" brain.
#+end_quote

​

#+begin_quote
  Would you run away from a gradual brain replacement?
#+end_quote

​

#+begin_quote
  Obviously.
#+end_quote

So you are your brain, and you'd run away from a gradual brain replacement, because your brain ceases to exist, which means you ceased to exist too and were replaced by a clone. OK.

So, at the end of the gradual brain replacement, you will have blacked out forever, and in your place, there is going to be a fully conscious clone - acting like you, but still another person.

The question is, in what manner would your perception cease to exist, as felt from the inside? At the beginning, you're perceiving everything normally since the gradual brain replacement hasn't begun yet. At the end, you're blacked out forever (like after a car accident). But what do you feel in between? Does your awareness of the outside world fade out gradually, or does it stop abruptly at some point?
:PROPERTIES:
:Author: DuskyDay
:Score: 2
:DateUnix: 1576346521.0
:DateShort: 2019-Dec-14
:END:


***************** u/DuskyDay:
#+begin_quote
  The brain is composed /entirely/ of neurons.
#+end_quote

It's also glial cells ([[https://en.wikipedia.org/wiki/Brain#Cellular_structure][which is, according to Wikipedia, the structural support]]), but if neurons are everywhere in the brain, algorithms are everywhere in the brain too, of course.
:PROPERTIES:
:Author: DuskyDay
:Score: 3
:DateUnix: 1576013255.0
:DateShort: 2019-Dec-11
:END:


******** It requires some amount of information about the person be preserved. And the GAZP does not say that everything that talks about consciousness be conscious. It doesn't rule out managing to create a zombie, just argues that such a thing is nontrivial. An earring could create a similar effect by gradually uploading and improving you. Which is why not only were the brains examined, but the earring itself was talked to. And the earring said “Oh, no, actually the specific way I do this is a terrible idea. Definitely a bad plan. At least by your values.” That they no longer exist in their brain doesn't mean they met a bad end. But it also doesn't mean they met a good end. Which may or may not involve them being dead. All we actually are told is that by the values of every wearer it is a bad idea, and that after careful examination and discussion with the earring that it was suggested it be locked away. Perhaps the earring uploads you and then tortures you but leaves enough to get the information it needs. Perhaps it is some form of zombie master. It is unspecified. Just that the earring says by your values it is a bad result.
:PROPERTIES:
:Author: SoundLogic2236
:Score: 4
:DateUnix: 1575677327.0
:DateShort: 2019-Dec-07
:END:

********* But it never repeats that advice, which indicates to me that by the earring's measure, the earring-driven you is only marginally worse at maximizing your values. Only one value violated, and only once; or something like that.
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1575720328.0
:DateShort: 2019-Dec-07
:END:

********** The earring solves the immediate problem it is given better than you. It advises you not to wear it. Then gives good life advice that makes giving it up harder thus leading to better odds of identity death or crippling widthdrawl later. Those aren't considered part of the problem space since they were both adequately resolved by the first piece of advice given and of no further interest to the earring.
:PROPERTIES:
:Author: i6i
:Score: 8
:DateUnix: 1575730854.0
:DateShort: 2019-Dec-07
:END:


**** I think this story is more related to Scott's Thousand Shards of Desire and his explicit preference toward being really-real rather than a brain in a jar. This is a horror story because at the end the wearer is no longer a thinking rational actor, but is closer to the neural tissue with its pleasure sensors being electrically activated. A wirehead, passive in her own body.

I think you're correct that at some level the earring must contain a complete model of the wearer's brain (and potentially all brains). For the reasons above, I suspect most people wouldn't consider that less horrifying. Regardless of whether or not there's a copy of me in a system somewhere, atrophying the brain of current-me is still a bad thing. So the horror remains, even if there's an intact copy somewhere. (Worse, there's nothing in the story to suggest copies are retained, rather than scanned, used, and dumped.)

Also I think your interpretation is incorrect within the facts presented by the story. It can't be doing a gradual upload, because it's advice is correct immediately. The story specifies that its first advice is on major life decisions, which requires are more complete model of a person than required to move particular muscles to move around. The gradual part of the process is training the wearer to accept the commands, the earring's understanding must be complete from the beginning.

Intriguing perspective though, thank you.
:PROPERTIES:
:Author: GET_A_LAWYER
:Score: 5
:DateUnix: 1575677865.0
:DateShort: 2019-Dec-07
:END:

***** Right, it's more an immediate upload with a gradual shutdown of vestigial algorithms. But that's equivalent to a gradual upload with regards to capacity to reverse it.
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1575720452.0
:DateShort: 2019-Dec-07
:END:


** [removed]
:PROPERTIES:
:Score: 16
:DateUnix: 1575666899.0
:DateShort: 2019-Dec-07
:END:

*** u/Reply_or_Not:
#+begin_quote
  Wouldn't a better decision be to keep it on person, and use it either in rare and important cases, or after first coming up with a list of possible solutions yourself for each given scenario, or both?
#+end_quote

I think the answer to the question depends on what you think about what it means to be yourself. For example [[/u/FeepingCreature][u/FeepingCreature]] makes the strong case that the earring is doing a brain upload, which has much less horrific connotations than those who are positing other negatives.
:PROPERTIES:
:Author: Reply_or_Not
:Score: 3
:DateUnix: 1575768288.0
:DateShort: 2019-Dec-08
:END:


** My pattern-loving brain really wanted the story to end with Kadmi-nomai saying "better for you if you locked it away."
:PROPERTIES:
:Author: Psortho
:Score: 12
:DateUnix: 1575660508.0
:DateShort: 2019-Dec-06
:END:


** I think people are looking at the ring the wrong way. If it always gives the right advice, instead of hiding the earring, a utilitarian would put the earring on. Imagine a scientist unerringly guided towards correct conclusions. It would be far better for society that they put the earring on. The Nobel prizes they won would be earned by merit of their ultimate sacrifice for the good of others.
:PROPERTIES:
:Author: somerando11
:Score: 5
:DateUnix: 1575683829.0
:DateShort: 2019-Dec-07
:END:

*** You're assuming the ring doesn't cause (claimed) value drift as a side effect, but given the experimental data that seems unlikely. How many self-identified utilitarians would truly consider someone who retired to live a happy life with their SO a failure of a human being?

So far, nobody with the earring has done something noteworthy, instead they all seem to have converged on a hedonic lifestyle, which is no doubt satisfying to whatever remains of their brain's value system.

The brain is bad at math and will get far more satisfaction saving a bus full of orphans than donating a million dollars to an effective charity. As a utilitarian, you prefer the math, but you value the outcome of that math more than being able to do that math. As such the ring will give you better answers and your ability to do utilitarian considerations will atrophy. Once those considerations are sufficiently gone, what remains is "trust the earring to know what is right", which is then freed up to meet more deeply seated desires.
:PROPERTIES:
:Author: philip1201
:Score: 14
:DateUnix: 1575706019.0
:DateShort: 2019-Dec-07
:END:


** If the ear ring truly does what you want to the fullest degree (which it might not, hence the horror, but /if/ it does), is (a version or copy of) you still alive inside the earring somehow?

I wouldn't wear the earring because it may have alien goals ...but if it doesn't have alien goals then this seems like allowing a smarter copy of /me/ to tell me what to do / wear my body
:PROPERTIES:
:Author: eroticas
:Score: 3
:DateUnix: 1575730776.0
:DateShort: 2019-Dec-07
:END:

*** I can predict my dog quite well, and I'm sure if I applied myself I could choose actions that were basically always better for the dog than its own choices. I don't do this by running an upload of my dog in my brain, I do it by being a superintelligent alien artifact relative to my dog.
:PROPERTIES:
:Author: Charlie___
:Score: 6
:DateUnix: 1576048131.0
:DateShort: 2019-Dec-11
:END:

**** Not alien, though, since you truly care about the dog and its true preferences (perhaps unlike the earring)

I guess the earring is sort of like an overbearing guardian who knows what is best and isn't shy about telling you so but doesn't take into account that sometimes it's important to figure it out yourself
:PROPERTIES:
:Author: eroticas
:Score: 2
:DateUnix: 1576086049.0
:DateShort: 2019-Dec-11
:END:

***** u/appropriate-username:
#+begin_quote
  you truly care about the dog and its true preferences
#+end_quote

How would he know the dog's true preferences? If the dog likes to eat a particular grass that he doesn't know about, he'll never know about it and never give the dog the grass.
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1578246621.0
:DateShort: 2020-Jan-05
:END:

****** He doesn't know them. But if he did know them, he would care. This makes a difference.
:PROPERTIES:
:Author: eroticas
:Score: 1
:DateUnix: 1578516070.0
:DateShort: 2020-Jan-09
:END:
