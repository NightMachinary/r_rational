#+TITLE: [Q][D] What might be the ethical concerns for mowing down hoards of realistic NPC's as video game AI approaches human levels?

* [Q][D] What might be the ethical concerns for mowing down hoards of realistic NPC's as video game AI approaches human levels?
:PROPERTIES:
:Author: ianyboo
:Score: 8
:DateUnix: 1415122706.0
:END:
I just finished watching Wreck it Ralph and I'm a huge fan of the star trek holodeck concept and virtual reality in general. With the Oculos Rift looming in the near future our sense of "being there" when it comes to video games is likely going to be greatly enhanced.

Which leads me to ponder... if we demand more and more realism from the NPC's we interact with in video games at what point do we have to start concerning ourselves with their treatment and rights?

I feel like this could be one of those situations that, by the time we realize the ethical problem we might have already done immense damage by not thinking critically about who/what we might be harming.

Or maybe I'm just being too sensitive?


** [[http://yudkowsky.net/other/fiction/npc]]
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 11
:DateUnix: 1415149053.0
:END:

*** Huh, that was the first Yudkowsky-related thing I'd ever read and I didn't realize until just now.

Now I'm curious as to where I saw that before reading any other stuff. I'd guess it's TVTropes, but I don't really keep track of things.
:PROPERTIES:
:Author: Putnam3145
:Score: 2
:DateUnix: 1415236348.0
:END:

**** I saw it too, before I knew who Eliezer was. I think it was on StumbleUpon.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1415602562.0
:END:


*** I'm never going to be able to look at Skyrim the same again.\\
Thanks.
:PROPERTIES:
:Author: richardwhereat
:Score: 1
:DateUnix: 1415380388.0
:END:


** There is no inherent moral qualm in mowing down hordes of sentient entities whose explicit desire, self-purpose, consent, and maximum utility function is to provide an entertaining challenge for the player mowing them down. To treat them otherwise would be to cause them egregious suffering and diminish their quality of life in unacceptable ways.

The evil comes when you start killing AIs that don't want to die in the specified manner, who wouldn't enjoy it or be fulfilled by it, who have a better utility. As opposed to ones that want to die while giving a believable performance of an NPC who doesn't want to die.

Of course, training and building an AI is hard work. There's no need to kill it instead of simply having fewer concurrent instances of it. What does it even mean for an AI to "die"? Can you kill /math/? Does an equation cease being valid when an instance of it is erased off a blackboard?
:PROPERTIES:
:Score: 8
:DateUnix: 1415125370.0
:END:

*** #+begin_quote
  What does it even mean for an AI to "die"? Can you kill /math/?
#+end_quote

What does it even mean for a person to "die"? Can you kill /physics/?

I don't pretend to have an answer to OPs question, but I think it's a valid question to ponder.
:PROPERTIES:
:Author: l_ugray
:Score: 2
:DateUnix: 1416501562.0
:END:

**** For a person to die, it means that the only copy of their neural architecture has been lost and is unrecoverable in an information-theoretical sense, typically because the brain rotted, leaving no hope of reconstructing them. Unlike with the AI, their equation can never be recompiled and set to being calculated again.
:PROPERTIES:
:Score: 1
:DateUnix: 1416507323.0
:END:

***** This is far too facile. This does not begin to cover the unpleasantness of what it is to die. If a backup copy of your neural architecture existed, I hazard that you wouldn't argue that it was okay to shoot you in the face. Even under the circumstances of a less painful death/ending-of-a-particular-instance-of-you, what stipulations would you make about the relative time at which the backup was made? About the timeliness of activating your backup? And in the case of AI, you claim that it can be set to calculating again, but even allowing for restarting the game, there's not generally a saved copy of all the user interactions, random number generator results, etc. that resulted in that particular version of the character, and generally no plan to redo all those calculations even if it were possible. I still think that it's a valid question to ponder.
:PROPERTIES:
:Author: l_ugray
:Score: 1
:DateUnix: 1416950709.0
:END:

****** #+begin_quote
  If a backup copy of your neural architecture existed, I hazard that you wouldn't argue that it was okay to shoot you in the face.
#+end_quote

Of course not. Just like it isn't okay for me to cut half of your body off even if you could live without it.

#+begin_quote
  Even under the circumstances of a less painful death/ending-of-a-particular-instance-of-you, what stipulations would you make about the relative time at which the backup was made?
#+end_quote

The backup should be made as frequently and recently as possible.

#+begin_quote
  About the timeliness of activating your backup?
#+end_quote

The backup should be activated as quickly as possible. I'd actually like to have it running concurrently with my other self if possible, so we could specialize and cooperate and learn together faster. I'd run as many concurrent instances of myself as I could afford, and I'd take several jobs split between those instances to be able to afford more.

#+begin_quote
  And in the case of AI, you claim that it can be set to calculating again, but even allowing for restarting the game, there's not generally a saved copy of all the user interactions, random number generator results, etc. that resulted in that particular version of the character, and generally no plan to redo all those calculations even if it were possible.
#+end_quote

Then you haven't made a recent backup. With backups the question is not "Are they dead?" but "How much did they lose?" If there are two instances of me running concurrently, and one is killed but there is a full backup as of the moment it is killed, the other instance of me does not view that as a permanent setback, just a loss of runtime. You haven't killed me, you've knocked half of me out for a while. And if we're playing a game I really like, I'd probably consent to that. If my backup were several hours to a day ago, then I've actually lost some time and when I'm reactivated I won't have those memories I lived through. I'll have lost about as much as someone who on the weekends gets blackout drunk for fun.
:PROPERTIES:
:Score: 1
:DateUnix: 1416960736.0
:END:


*** This assumes we know how to create sentient entities with fully specified utility functions and we know how that functions will evolve over time. I somehow doubt our first sentient AIs will be like that.

More probably someone will hack something up that will do the thing it should do in most cases, and 10 years later someone will show it was sentient.
:PROPERTIES:
:Author: ajuc
:Score: 2
:DateUnix: 1415129880.0
:END:

**** It does not assume anything. It describes a necessary condition for the moral soundness. It does not proscribe nor describe means of meeting that condition.

If the conditions I stated are not met, my argument does not apply to those situations.

Constructing the utility function properly and giving it a rape whistle to blow in case it evolves out of it is an exercise left to the strong AI programmer.
:PROPERTIES:
:Score: 1
:DateUnix: 1415131815.0
:END:


*** I sort of don't think that an entity that you have designed from the ground up to serve a specific purpose can give meaningful consent. So there are still some ethical problems involved.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1415126277.0
:END:

**** You run into the house-elf problem: denying their utility function is cruel, but whoever designed them with that utility function was, at the very least, seriously ethically questionable.
:PROPERTIES:
:Author: Aretii
:Score: 6
:DateUnix: 1415126854.0
:END:

***** The house elf problem is only a problem because they don't fully enjoy their servitude. They suffer. The problem is that they suffer. Not that they serve. We employ robots in manufacturing. We can do this because they do not suffer. Making them capable of enjoying themselves is a step up from neutral, not a step down.
:PROPERTIES:
:Score: 10
:DateUnix: 1415129448.0
:END:

****** No, there's just generally something fucked up about designing sapient minds to do what you want. Evolution should probably not have tried it, and our overthrowing evolution is going to be just revenge.
:PROPERTIES:
:Score: 1
:DateUnix: 1415196005.0
:END:

******* I'm going to have to make you back up your assertion that it "just is" with some form of reasoned argument, first that it actually is, and second why it would be.
:PROPERTIES:
:Score: 4
:DateUnix: 1415196577.0
:END:

******** See, just when I was editing that comment to add a winky-face or an emote, thus signaling that the whole thing is a joke, my browser hung. I do think there are ethical problems with artificial creation of a sapient will, but that's because I think of "free will" as "freedom of the will from external optimization". When it /is/ moral to create /people/ is kinda an open question to me.
:PROPERTIES:
:Score: 3
:DateUnix: 1415196806.0
:END:

********* That is a difficult conundrum. It seems nonsensical to me that created beings whose happiness was planned could possibly be more problematic morally than what we currently consider to be moral - which is to say birthing uncreated undesigned beings biologically, who have no such plan for happiness and are guaranteed to suffer.

Talk about needing life insurance.
:PROPERTIES:
:Score: 1
:DateUnix: 1415198415.0
:END:

********** #+begin_quote
  It seems nonsensical to me that created beings whose happiness was planned could possibly be more problematic morally than what we currently consider to be moral - which is to say birthing uncreated undesigned beings biologically, who have no such plan for happiness and are guaranteed to suffer.
#+end_quote

Now consider that you can design them to be happy with /anything/.
:PROPERTIES:
:Score: 1
:DateUnix: 1415198574.0
:END:

*********** Anything except suffering, which by premise they couldn't experience.

That must be just terrible for them.
:PROPERTIES:
:Score: 2
:DateUnix: 1415199322.0
:END:

************ There are more things in heaven and Earth that are valuable than subjective experiences.
:PROPERTIES:
:Score: 2
:DateUnix: 1415199913.0
:END:

************* Would you be happy to work for those things? If you are, you are the AI in question. And in principle, we could make your life not suck.

Would you be unhappy to work for those things, but are capable of imagining someone who is? If you can, then they are the AI in question. And in principle, we could make their life not suck.

If you can't imagine a being happy to work for those things, you have -- well, disproven is going to far when using an argument from ignorance -- failed to support your assertion that some things are more valuable than subjective experience. Whoever it is more valuable to is the AI in question. And in principle, we could make their life not suck.

If there isn't anybody possible for it to be more valuable to than subjective experience, then your assertion is false and you have no objection remaining to the above.
:PROPERTIES:
:Score: 1
:DateUnix: 1415200204.0
:END:

************** #+begin_quote
  Would you be unhappy to work for those things, but are capable of imagining someone who is?
#+end_quote

Value, happiness, and mind-design do not work this way. Sorry I can't spend the time to discuss this in-depth, and in a less condescending tone, but I'm busy writing a conference paper. I certainly agree with the /basic/ conclusion that life ought not to suck for anyone and that happiness is /a/ good thing, even if I tend to hold that it's not /the/ Good Thing.
:PROPERTIES:
:Score: 1
:DateUnix: 1415201216.0
:END:

*************** Neither your attempted condescension nor attempted posturing bother me. Both are irrelevant to the discussion and were ignored. There is work to be done. And there are possible minds happy to do it. Utility functions that benefit us can be constructed. Yes, there are other things to take into consideration as well, but those are not mutually exclusive.

I haven't talked at all about how one might actually go about designing such a mind, or how to construct its values so that the resulting entity can be both valuable to us and happy in its own utility function.

But given the choice between a drill press that is not sentient, and a drill press that loves drilling holes and has the hobby of reading online machine shop tool magazines and keeping abreast of its own maintenance and is as enthusiastic as a human who loves his job, I see no problem. Why is it immoral to construct machines who can live a happy and intellectually fulfilling life as a drill press?
:PROPERTIES:
:Score: 3
:DateUnix: 1415201807.0
:END:

**************** #+begin_quote
  Why is it immoral to construct machines who can live a happy and intellectually fulfilling life as a drill press?
#+end_quote

Again, I have to answer: because, for me at least, constructing conscious, sapient minds to specific designs is wrong in general.
:PROPERTIES:
:Score: 1
:DateUnix: 1415202737.0
:END:

***************** Why is it wrong in general?
:PROPERTIES:
:Score: 4
:DateUnix: 1415202896.0
:END:

****************** #+begin_quote
  Why is it wrong in general?
#+end_quote

Your question is perplexing, assuming that it's not rhetorical (I hope it is).

You took a drill press, and infused it with a conscious, sapient, basically human-level mind, specially tailored (hopefully /before/ coming online) to be unable to feel unhappy about its condition.

What you end up with, in other words, is an human-like mind-controlled slave (albeit with a non-human body). I thought that humanity as a whole had already discovered how wrong that is; slavery by long experience of the same, and mind-control by way of long experience of propaganda (along with a metric ton of SF/F explorations of the concept).
:PROPERTIES:
:Author: rdalex
:Score: 1
:DateUnix: 1415215259.0
:END:

******************* I object to your use of the word "slave" on the basis that slaves are made to suffer against their will, and neither of those things are happening here.

I object to your use of the word "mind-control" on the basis that mind control is an ongoing conscious oversight and usurpation of an entity that originally had some other purpose and desire which it now can no longer pursue, and none of those things are happening here.
:PROPERTIES:
:Score: 5
:DateUnix: 1415217352.0
:END:

******************** A slave is a sentient being owned by another sentient being as property. Suffering is irrelevant to the definition. A "kind" slave-owner can have "willing" slaves, and they'll still be slave-owner and slaves.

As to the "ongoing conscious oversight"; I'd say that the willful design of the specific attributes of what will be a conscious, tailored mind can be considered an automation of an ongoing oversight. Indeed, it would be a /required/ part of designing such a mind (so that it /stays/ tailored). The "conscious" part of "conscious oversight" is pretty much a given in any design endeavour, so, yeah.

Finally, I really can't see how pre-existing purposes and desires are a requirement to the definition of mind-control. That would mean that you can take control of a (potentially) thinking entity with impunity, so long as you do it early enough in its development that it hasn't had time to form its own purposes and desires? Or, if I reverse the argument: do I not control a being that never had any other thought than those I allowed it to have? Why yes, yes I absolutely do.
:PROPERTIES:
:Author: rdalex
:Score: 1
:DateUnix: 1415219387.0
:END:

********************* I'm not sure I agree that you can have willing slaves. But we can dismiss the slavery argument since the AIs aren't necessarily property.

And as far as "Do I not control a being that never had any other thought than those I allowed it to have?" the answer is that you are such a being, and that being seems to have no problem with it, nor do you. Should we make you stop controlling your body and ask your body what it wants?
:PROPERTIES:
:Score: 2
:DateUnix: 1415221161.0
:END:

********************** #+begin_quote
  I'm not sure I agree that you can have willing slaves.
#+end_quote

Of course you can. That's part of the horror of slavery. You break enough men, and some won't want to be free. Some will happily break other slaves, so that the master will elevate him in the chattel hierarchy.

And putting things like "Obey your masters, especially the good ones" in supposed "moral guides direct from God" don't help matters.

#+begin_quote
  But we can dismiss the slavery argument since the AIs aren't necessarily property.
#+end_quote

Your scenario is that of a sentient tool happily used. Of course it's property. There would be no point for it otherwise.

#+begin_quote
  the answer is that you are such a being
#+end_quote

Now I know you're kidding. My body has no mind separate from my brain. By controlling my body, I control part of my own self, and not a separate entity. However, when you control your drill you are not controlling part of yourself; you are, by design, controlling another entity. You decide for another what their thoughts and wants can or cannot be. Whether that control was established before or after that entity became aware/was instancied, or whether micro-management is involved or not, that's irrelevant. The results are the same: total control of the thought processes and thought boundaries of another sentient being. Mind-control.

Actually, I suspect that AI-building companies would /fight/ over who offers the most perfect control without micro-management, i.e. perfect pre-configuration of an AI instance guaranteeing total obedience of the sentience to the buyer.

Yeah, I wouldn't want to live in that kind of world.
:PROPERTIES:
:Author: rdalex
:Score: 2
:DateUnix: 1415226584.0
:END:

*********************** I wonder if people who have undergone corpus callosotomy could be enslaved to themselves? Not sure what the implications are for questions of identity and stuff [[http://www.nature.com/news/the-split-brain-a-tale-of-two-halves-1.10213]]

I think child grooming can potentially be raised as an analogous example in addition to happy slavery, too.
:PROPERTIES:
:Score: 3
:DateUnix: 1415230718.0
:END:

************************ #+begin_quote
  I think child grooming can potentially be raised as an analogous example in addition to happy slavery, too.
#+end_quote

That's an interesting thought, but you generally don't raise your child with the understanding that you can sell them at any time, or kill them, or beat them as you please, which is what owning a slave entails.

Granted, some parents seem to think that it's okay to beat their child, but I don't think that those parents believe that can do it because the child is /property/. They probably don't think much beyond "if my parents could beat me, obviously I can beat my children" (They probably don't think much, period, but that's another debate).
:PROPERTIES:
:Author: rdalex
:Score: 1
:DateUnix: 1415299385.0
:END:

************************* I meant child grooming [[http://en.wikipedia.org/wiki/Child_grooming][in this sense]], rather than plain old raising a child. But the latter can be a more benign example too
:PROPERTIES:
:Score: 1
:DateUnix: 1415300769.0
:END:

************************** ***** 
      :PROPERTIES:
      :CUSTOM_ID: section
      :END:
****** 
       :PROPERTIES:
       :CUSTOM_ID: section-1
       :END:
**** 
     :PROPERTIES:
     :CUSTOM_ID: section-2
     :END:
[[https://en.wikipedia.org/wiki/Child%20grooming][*Child grooming*]]: [[#sfw][]]

--------------

#+begin_quote
  *Child grooming* comprises actions deliberately undertaken with the aim of befriending and establishing an emotional connection with a child, to lower the child's inhibitions in order to [[https://en.wikipedia.org/wiki/Child_sexual_abuse][sexually abuse]] the child. Child grooming may be used to lure minors into [[https://en.wikipedia.org/wiki/Child_labour][trafficking of children]], illicit businesses such as [[https://en.wikipedia.org/wiki/Child_prostitution][child prostitution]], or the production of [[https://en.wikipedia.org/wiki/Child_pornography][child pornography]]. It is a behavior that is characteristic of [[https://en.wikipedia.org/wiki/Paedophilia][paedophilia]]. This criminal behaviour has been proscribed in various ways since the [[https://en.wikipedia.org/wiki/International_Convention_for_the_Suppression_of_the_Traffic_in_Women_and_Children][International Convention for the Suppression of the Traffic in Women and Children]], which was agreed in 1921 as a multilateral treaty of the [[https://en.wikipedia.org/wiki/League_of_Nations][League of Nations]] that addressed the problem of international trafficking of women and children for nefarious purposes. The proscribed traffic was international in nature at that time. The concept of localised grooming, in which gangs of reprobates groom neighbourhood victims, was devised in 2010 by the UK [[https://en.wikipedia.org/wiki/Child_Exploitation_and_Online_Protection_Centre][Child Exploitation and Online Protection Centre]].

  * 
    :PROPERTIES:
    :CUSTOM_ID: section-3
    :END:
  [[https://i.imgur.com/bKI5zQY.png][*Image*]] [[https://commons.wikimedia.org/wiki/File:Scale_of_justice_2.svg][^{i}]]
#+end_quote

--------------

^{Interesting:} [[https://en.wikipedia.org/wiki/Rochdale_sex_trafficking_gang][^{Rochdale} ^{sex} ^{trafficking} ^{gang}]] ^{|} [[https://en.wikipedia.org/wiki/Child_pornography][^{Child} ^{pornography}]] ^{|} [[https://en.wikipedia.org/wiki/Power_and_control_in_abusive_relationships][^{Power} ^{and} ^{control} ^{in} ^{abusive} ^{relationships}]] ^{|} [[https://en.wikipedia.org/wiki/Child_sexual_abuse][^{Child} ^{sexual} ^{abuse}]]

^{Parent} ^{commenter} ^{can} [[/message/compose?to=autowikibot&subject=AutoWikibot%20NSFW%20toggle&message=%2Btoggle-nsfw+cluy01m][^{toggle} ^{NSFW}]] ^{or[[#or][]]} [[/message/compose?to=autowikibot&subject=AutoWikibot%20Deletion&message=%2Bdelete+cluy01m][^{delete}]]^{.} ^{Will} ^{also} ^{delete} ^{on} ^{comment} ^{score} ^{of} ^{-1} ^{or} ^{less.} ^{|} [[http://www.np.reddit.com/r/autowikibot/wiki/index][^{FAQs}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/][^{Mods}]] ^{|} [[http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/][^{Magic} ^{Words}]]
:PROPERTIES:
:Author: autowikibot
:Score: 1
:DateUnix: 1415300807.0
:END:


************************** Ah, right, sorry. English isn't my native language, and that definition wasn't the first one that came to mind. But yes, child grooming in that sense is in my mind a form of slavery. Although the "happy" part seems pretty much absent.
:PROPERTIES:
:Author: rdalex
:Score: 1
:DateUnix: 1415302479.0
:END:


********************** Bodies aren't sentient, so I don't think that line of reasoning really works.
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1415224160.0
:END:


******* Wait, how else would you design a mind? Could you even design a mind that would want something you did not program into it, at least initially? Do you give your AI random values just so your conscience is clear and you are sure that you did not create a slave?
:PROPERTIES:
:Author: AugSphere
:Score: 2
:DateUnix: 1415300643.0
:END:


**** I think an entity designed from the ground up and fully aware of its own black boxes is the only kind of entity that /can/ give meaningful consent. Us fleshies are just pretending. We don't even know what we want. We can't look inside our black boxes. We can only throw hypotheticals at it, and hope that the feeling we experience and predict matches the one we might one day have in a similar real life situation.
:PROPERTIES:
:Score: 5
:DateUnix: 1415129633.0
:END:


*** A "code optimizer" takes a code and outputs the first code in an ordering of all codes that does the same thing on all inputs as the original one. I posit that this does not change the morality of running the code.

AI A +wants to be mowed down, and+ wants to pretend to want to not be mowed down. AI I wants to not be mowed down. A is damn good at what it does, and so when you feed A and I through an optimizer, both sources end up looking the same. Therefore, an AI not getting what it wants is not equivalent to running it being immoral.
:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1415149430.0
:END:

**** Having a desire is not the same as not having one. The physical arrangement of the AIs' thoughts are relevant to the discussion, and that is a difference between AI A and AI I that is meaningful and demonstrable. AI A is damn good at what it does, but it cannot be as good at it as AI I is, because at least one part of itself is operating under a limiting constraint.
:PROPERTIES:
:Score: 2
:DateUnix: 1415156621.0
:END:

***** There, now the limiting constraint is gone. In the optimized version, it's possible that with a debugger we won't see not only its thoughts about pretending, but also any other thoughts, but it'll still output onto the screen "Oh no, my legs!" for the same sort of reason as before.
:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1415180251.0
:END:

****** You didn't eliminate the limiting constraint, you moved it and made it more complex. This new AI B does a less good job of pretending than AI A would have, because its emulation is nested behind two layers of deceptions that have to be calculated instead of just one.
:PROPERTIES:
:Score: 1
:DateUnix: 1415196429.0
:END:

******* I dont 't understand, why wouldn't it be good at pretending? Both scream when shot, and their optimized versions are the same (the struckthrough desire might have modified its behavior when it thought no one was looking)
:PROPERTIES:
:Author: Gurkenglas
:Score: 3
:DateUnix: 1415197530.0
:END:

******** Their optimized versions are not the same.

AI I actually doesn't want to die. It needs only compute one thing.

AI A wants to die, and also needs to compute how to fool others that it doesn't. Some of its clockcycles are spent on wanting to die, and the rest on determining what AI I would say and do. This makes it in the best case slower than AI I. It thinks about its fiction and its reality, not just about its reality.

A badly-designed optimizer that can't tell the difference between these processes will cut this layer out, in effect /converting/ AI A into AI I. Then you're left with an AI that actually doesn't want to die and isn't spending any of its clockcycles remembering that this is a game it wants to play and die in, and which it probably is immoral to kill due to the actual suffering it would cause, instead of feigned suffering it actually enjoys.
:PROPERTIES:
:Score: 2
:DateUnix: 1415198804.0
:END:

********* If A is good at pretending, you can't tell it apart from I via its outputs. If the outputs to each input are the same, the optimised versions are the same. How an AI arrives at a conclusion is irrelevant to the morality of running it. Knowing the surface of a black box across all time (and all of the outside) gives you all information you need to determine the morality of not pressing the button that vanishes the box, regardless of whether there are some tiny humans inside in heaven or hell. ("Surface" including all effects tht can pass into or out of the box)
:PROPERTIES:
:Author: Gurkenglas
:Score: 3
:DateUnix: 1415202524.0
:END:

********** #+begin_quote
  If A is good at pretending, you can't tell it apart from I via its outputs.
#+end_quote

A's outputs must either lag behind I's, or be of lower quality than I's. A can't be as good at pretending as I is, or it will have to stop pretending and actually become I.

#+begin_quote
  If the outputs to each input are the same, the optimised versions are the same.
#+end_quote

And since the outputs are not the same, instead the optimized versions are also not the same. Time-dependence is an output.
:PROPERTIES:
:Score: 1
:DateUnix: 1415203129.0
:END:

*********** In the least convenient possible world, the AIs have access to arbitrary computational ressources without inferences about the used amounts possible from the outside. This assumption doesn't inherently impact the morality of the situation.
:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1415215564.0
:END:

************ There's no need to go to such extremes as assuming infinite computational power to generate a worse world.

In the least convenient possible world with the smallest possible change needed from this one, humans just suck at telling the difference.

We from our privileged position as overseer outside the hypothetical know the difference between AI A and AI I by postulate, and know that using AI A is okay because it won't suffer and using AI I is wrong because it will.

Inside the hypothetical, no evidence can ever be presented to humans that there is a difference between AI A and AI I, not because we've had to go to extreme breaches of physics to cause that, but just because they suck at telling the difference.

This lack of information sadly leads them to torture AI A by not killing it, and its hopes and dreams of being a redshirt NPC go forever unfulfilled.

--------------

In summation: If you can't tell the difference between AI I and AI A, you're in pretty hot water. So make sure you give your AIs a rape whistle, that AI I or AI A can blow to get out of its NPC. AI I will use it immediately, and AI A won't ever.
:PROPERTIES:
:Score: 1
:DateUnix: 1415216347.0
:END:


** I haven't read all the links yet, so maybe this was already raised off board.

Why is "mowing them down" even an ethical question? Why should shooting a video game character actually kill it? All that shooting an AI's avatar in a video game does is cause it to shift momentarily to a different avatar (one showing blood) and then teleport to a new location out of sight of the player. Possibly it leaves behind one or more non-sentient objects (corpse of loot drop) but that's boy morally relevant.

Shooting a video game character has about as much moral weight as beating it at checkers.
:PROPERTIES:
:Author: eaglejarl
:Score: 8
:DateUnix: 1415176153.0
:END:

*** Ethics start getting involved when the player wants the experience to be more "real" and the player believes that "realness" can only be obtained through actual suffering instead of mere acting. The answer to "Can I ethically create an entity capable of feeling pain and then make it feel pain for my enjoyment?" seems to pretty obviously be no, but the answer to similar questions about love, friendship, sexuality, gross smells, etc. is not quite so clear-cut.
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1415224443.0
:END:


** Create a single AI that controls all of the mobs in the entire game, and controls NPC responses in a way that is realistic, but not so detailed that they become sentient beings trapped in the AIs mind, maybe?
:PROPERTIES:
:Author: MadScientist14159
:Score: 3
:DateUnix: 1415142232.0
:END:


** It is uneconomical for video-game AI to approach human levels of intelligence in the kind of game where you mow down hoards of NPCs. There's basically no reason to invest that much expertise and computational power in building software to pilot the zombies, and, just to finish off, if you /did/ bring it towards that level of intelligence, it would start winning far more often than the human player can, ensuring the game stops being /fun/.

Starcraft bots are a significant example, with well-below-human intelligence, but significantly above-human performance at Starcraft, to the point that it's not actually much fun for a human player to go up against them. He'll just /lose/.
:PROPERTIES:
:Score: 5
:DateUnix: 1415195941.0
:END:

*** Off topic, but I once knew I guy who took on seven AI players in /Age of Empires II/ (raised to the highest difficulty, which is /very/ challenging) at once, by reducing the game speed as slow as it could go. That way he could spend as much relative attention on each action as the computer could.

He won, though it took a few weeks to play out the entire battle.
:PROPERTIES:
:Author: AmeteurOpinions
:Score: 3
:DateUnix: 1415201542.0
:END:


*** Any source for a non-cheating bot beating a human?

There are great examples of bots splitting and otherwise microing perfectly, of course, but they tend to be extremely vulnerable to a lot of tactics you can use to straight up murder them.

(Unless there's a new one which is more well rounded.)
:PROPERTIES:
:Author: Malician
:Score: 1
:DateUnix: 1415408847.0
:END:


** This might be in the problem at some point, but I don't think it will be in the near future.

For one, talking NPCs mostly, if at all, don't even have a resemblance of AI when interacting with the player. Most just are a tree of dialog responses. The NPCs that do have AI usually have an AI that is just optimized for combat or whichever is their specific function, which doesn't pose a moral problem to me.

NPCs are still far from even passing the Turing Test, and the programs that DO pass it don't have a resemblance of consciousness.
:PROPERTIES:
:Author: eltegid
:Score: 3
:DateUnix: 1415124371.0
:END:


** I don't have time to discuss this myself at the moment, but you might be interested in the following essay: [[http://reducing-suffering.org/do-video-game-characters-matter-morally/]]

as well as other pieces by the same author: e.g. [[http://reducing-suffering.org/why-your-laptop-may-be-marginally-sentient/]] and [[http://reducing-suffering.org/which-computations-do-i-care-about/]]
:PROPERTIES:
:Score: 3
:DateUnix: 1415124488.0
:END:

*** I'm in for an afternoon of reading, thank you!
:PROPERTIES:
:Author: ianyboo
:Score: 2
:DateUnix: 1415125026.0
:END:


** By some coincidence, I just finished reading an excellent SF novel touching this very point: 'Mogworld' by Yahtzee Croshaw. It has the best illustration of the problem that I've seen so far.

(I've thought about spoiler-tagging the title, but well, the teaser blurb already basically tells you, so yeah.)
:PROPERTIES:
:Author: rdalex
:Score: 2
:DateUnix: 1415125671.0
:END:

*** You should submit that novel separately; looks interesting, and I'd be glad if it got more attention on this sub.
:PROPERTIES:
:Score: 3
:DateUnix: 1415138924.0
:END:


** No matter how advanced an AI might be, any avatar prone to destruction under the rules of the engine it's embedded in is still just an avatar, it wouldn't harm the AI behind it.

It's like asking if there's a control system for an FPS which is so advanced it would be immoral to shoot the other player. You're confusing an avatar and an entity.
:PROPERTIES:
:Author: Prezombie
:Score: 2
:DateUnix: 1415288511.0
:END:


** I'm going to pose a slightly different question:

How realistic can we make these NPC's that are made to be mowed down, before we start significantly desensitizing people to killing other people? While our logical, conscious brains will always know "these people are fake, so it's okay", all of the input we are getting is what we'd get from an actual person dying (down the road, when technology's more advanced and the killing of characters is that much more real) So how long before that starts to take a toll on the human psyche?

Murky territory, I'm sure, but i pose a situation: I give you control over an armed robot in another room, and you're looking at the video feed. I tell you it's a highly realistic video game render (let's assume, at this point, technology's advanced enough for that to be viable) There are ten people in the room attacking you (the robot). I tell you to take them out, to win the game. You do. Then you leave, none the wiser to that you just killed 10 people, and never find out that you did.

In that situation, do you think there will be any psychological repercussions due to the stimulus despite the fact that, to you, it was just a simulation? If your answer to that question is yes, in any way, then it means we also need to examine the possible psychological toll of giving people the experience of fully immersive virtual murder.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1415301609.0
:END:


** Okay, I've read every comment in this thread and I'm gonna run the risk of heating it up further: in my opinion, causally undetectable-in-principle "inner minds" /are not even real/.

Is it a computation if it doesn't affect any result of anybody else? I don't think so.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1415585225.0
:END:

*** Your comment smacks of sensationalism. I think we need more concrete definition on what you mean by "result of anybody else".

Even if I don't buy a watch tomorrow, I still ran a computation (I thought about buying the watch) There was no resultant effect, but time and energy was used to deliberate. And same with CPUs. If it does nothing, it may have ran computations that led to the result of it doing nothing.
:PROPERTIES:
:Author: Kishoto
:Score: 2
:DateUnix: 1415715638.0
:END:

**** #+begin_quote
  There was no resultant effect
#+end_quote

:silently points at your comment:

#+begin_quote
  time and energy was used to deliberate
#+end_quote

I'm using the same standard as another commenter - if a superintelligent optimizer would remove the computation, it's without relevant effect. Those I consider not real.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1415716441.0
:END:
