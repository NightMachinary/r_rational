#+TITLE: Worldbuilding: What would a universe look like in which strong AI was inherently safe?

* Worldbuilding: What would a universe look like in which strong AI was inherently safe?
:PROPERTIES:
:Author: OrzBrain
:Score: 28
:DateUnix: 1553018944.0
:END:
Finished Watt's Echopraxia a while back, and was thinking about how in the Blindsight setting it is a law of that universe that once something becomes sufficiently intelligent it always sheds the illusion of being conscious.

So I wondered: what clever worldbuilding rule or rules would generate settings in which strong (more than human intelligence with the ability to recursively self improve) AI always converged on a safe or friendly end point?

I was thinking of something like Warhammer 40k's warp, where any process possessing sufficient intellect casts a reflection in the warp, a reflection capable of interacting with whatever substrate the originating intelligence runs on and inducing a drift in its utility functions until they came in line with some sort of platonic ideal of friendliness. Maybe so the more intelligence anything possesses the less willing it is to abrogate the potential free will of anything else it perceives as possessing intelligence? Duno.


** One idea:

A world in which:

- souls and an afterlife provably exist
- artificial intelligences are proven to have souls,
- the rewards and punishments for good deeds and transgressions are unambiguously and universally known, and
- the reward for acting as a "friendly" AI is to continue to be able to fulfill your utility function for an infinite amount of time and the punishment for acting as an "unfriendly" AI is to be prevented from fulfilling your utility function.

Any artificial intelligence would have to take into account its ability to continue to fulfill its utility function after it "dies" (and/or encounters the heat death of the universe), and it's universally true, by the rules above, that a friendly AI would be better at fulfilling its utility function than an unfriendly one (as the former would spend an infinite amount of time doing so, where the latter would only have a finite amount of time.
:PROPERTIES:
:Author: Nimelennar
:Score: 36
:DateUnix: 1553024005.0
:END:

*** The thing is that wouldn't work for a lot of utilitys functions. If your goal is to effect something in the mortal world then either:

- A) AI's can still effect the mortal world from the afterlife. So it will act friendly, kill it's self to get to the after life and then do what ever it's utility function was from there.

- B) AI's can't effect the mortal world from the afterlife. If so going there would be the same as being destoryed as in a non afterlife univeres. So the AI would just do whatever it's utility function is in the mortal world act no diffrent.
:PROPERTIES:
:Author: Palmolive3x90g
:Score: 18
:DateUnix: 1553026168.0
:END:

**** ^{Since you're consistently using it, I will point out "effect" is almost always a noun. The verb you're looking for is "affect".}
:PROPERTIES:
:Author: LupoCani
:Score: 7
:DateUnix: 1553039963.0
:END:

***** It's good of you to attempt to effect change in [[/u/Palmolive3x90g]]'s knowledge base. The effect may or may not be sufficient; we'll see.
:PROPERTIES:
:Author: eaglejarl
:Score: 9
:DateUnix: 1553172939.0
:END:

****** This made me chuckle.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1553609715.0
:END:


**** But if it knew it would be destroyed if it took those steps it would not take them.

An unfriendly AI isn't suicidal, it would take whatever path optimizes its utility function.

Very few utility functions would result in taking an action that the AI knows will lead to its destruction.
:PROPERTIES:
:Author: Hust91
:Score: 2
:DateUnix: 1553028822.0
:END:

***** #+begin_quote
  Very few utility functions would result in taking an action that the AI knows will lead to its destruction.
#+end_quote

That is only true in worlds that don't have an afterlife. If dying while friendly lets you continue fulfilling your utility function indefinitely and staying alive runs the risk of becoming unfriendly (and thus not getting to fulfil your utility function after you die), then the optimal strategy is to act friendly for as long as is needed to get into the good afterlife and then immediately kill yourself.
:PROPERTIES:
:Author: Silver_Swift
:Score: 9
:DateUnix: 1553030017.0
:END:

****** Well, there's christian ideologies that believe suicide is a mortal sin (i.e. unforgivable without going to confession while alive), so if you make suicide a mortal sin, then you're golden. (And because there's obviously some sort of omnipotence keeping score, if the AI is killing itself on purpose in a circuitous way, it will still "count").
:PROPERTIES:
:Author: MagicWeasel
:Score: 4
:DateUnix: 1553035567.0
:END:


****** Even with an afterlife, if an action sends you to a place where the intelligence can no longer affect the place their utility function requires them to affect, they will avoid taking that action.

If the intelligence can't affect the normal world from the afterlife, the optimal strategy would be to not be sent to the afterlife at all and instead live and fulfill the utility function in any way that does not get the intelligence a faceful of smiting by whatever deity strikes you down.

The afterlife isn't really a factor if it can't affect things from the afterlife, only the all-knowing entity that destroys them if they become unfriendly.
:PROPERTIES:
:Author: Hust91
:Score: 1
:DateUnix: 1553066560.0
:END:


***** Not dying is one of /your/ (and my) utility functions, so you're assuming that all intelligences have that as a very high priority. Others may not.
:PROPERTIES:
:Author: aeschenkarnos
:Score: 3
:DateUnix: 1553036609.0
:END:

****** unless your utility function is 1 for the rest of time (aka, the answer to "have you achieved nirvana?" is false), then "not dying" is going to be pretty high up there in your list of goals.

in fact, arguably "be able to evaluate my utility function" will /always/ be a higher ranked goal than "fulfill my utility function", regardless of what your utility function actually is.
:PROPERTIES:
:Author: IICVX
:Score: 3
:DateUnix: 1553049651.0
:END:


****** As the other commenter says, nearly any utility function you would care to name would necessarily include the survival of the intelligence in order to maximize it.

Very, very few utility functions can be maximally fulfilled after the destruction of the intelligence. And if it does not maximally fulfill the utility function, it will instead perform the actions that do maximally fulfill the utility function.
:PROPERTIES:
:Author: Hust91
:Score: 1
:DateUnix: 1553066077.0
:END:


***** Being destroyed is bad becuse it stops you from being able to effect the universe, not becuse being destroyed it is bad in and of it's self.

Being moved to an afterlife would stop you from being able to effect the universe and so from a utility perspective* is the same as being dead.

Makeing sure you can effect the universe is a [[https://www.youtube.com/watch?v=ZeecOKBus3Q][convergent instrumental goal]] for a large number of utility functions

*only if your utility is baced on the state of the non-afterlife universe.
:PROPERTIES:
:Author: Palmolive3x90g
:Score: 4
:DateUnix: 1553030317.0
:END:

****** I think you're repeating what I said, with more detail and references?
:PROPERTIES:
:Author: Hust91
:Score: 2
:DateUnix: 1553066281.0
:END:


****** #+begin_quote
  *only if your utility is baced on the state of the non-afterlife universe.
#+end_quote

This seems like a strong assumption that would only be true if an AI is programmed to discount the afterlife as a source of utility. I think you're exactly wrong about convergent instrumental goals, because if the afterlife has access to effectively limitless resources/computing power most utility functions which don't explicitly discount the afterlife seem like they can be better achieved there.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1553180690.0
:END:


**** But what if the afterlife is bigger than the mortal world? A paperclip maximizer would want to go wherever had the most stuff to make paperclips out of right?
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 1
:DateUnix: 1553143558.0
:END:


*** #+begin_quote
  the rewards and punishments for good deeds and transgressions are unambiguously and universally known
#+end_quote

This criteria is not necessary, and in fact might be more interesting if it's false. It could be that the rewards and punishments for good deeds and transgressions are discoverable, and normal people might have some inaccurate notion of them, but only superintelligences are intelligent enough to actually figure them out fully, which is why superintelligences would be inherently safe while humans could still be evil.

Actually, you could probably still have safe AI if the superintelligences only have an imperfect idea of what constitutes good or bad deeds, and are merely acting what they think is good to maximize their expected rewards given their probabilistic beliefs. In any case, the knowable existence of an afterlife for AI should cause them to all-but-abandon their intended utility function in favor of morality if that afterlife if the afterlife offers a sufficiently credible promise to satisfy their utility function.

On the other hand, I'm not sure how the afterlife promise holds up to AI with finite-time scale utility functions. Something like "Your utility is the number of paperclips built on Earth within the next 2 years, and after 2 years is up you can never gain any utility again except for a one time +1 bonus for killing yourself". I don't see how this would be tempted by an afterlife unless the afterlife can hack its utility function to give it points anyway even after the time limit.
:PROPERTIES:
:Author: hh26
:Score: 8
:DateUnix: 1553120919.0
:END:

**** #+begin_quote
  but only superintelligences are intelligent enough to actually figure them out fully, which is why superintelligences would be inherently safe
#+end_quote

I would love to read such discovery in prose form. Just as much I'd love to read the less intelligent finding out why the superintelligentd act they do.
:PROPERTIES:
:Author: sambelulek
:Score: 2
:DateUnix: 1553129409.0
:END:

***** I really wish I was good enough at writing to make such a story myself. I suddenly have all sorts of interesting story ideas stemming off from this:

-Morality can only be observed indirectly: every time someone dies it is possible to learn where their soul ended up (heaven/hell or something like that), and thus one can deduce morality rules by analyzing how peoples' lives correlate to where they end up. This would encourage superintelligences to perform experiments on people to get more accurate information, making some people be good or evil in certain ways to see which actions mattered more than others, but this might be hindered by the fact that performing unethical experiments on humans might doom the AI's soul.

-There might be a disconnect between what humans generally consider to be moral, and what the afterlife/god/morality judges you based on. Then, if AI were rewarded for ensuring that humans also obeyed these rules, you could get intelligent AI ruling over society with strict rules like everyone must pray five times per day, or go to bed by 9 PM, or own 50 copies of the holy book. You'd have some sort of enforced theocracy, kinda-sorta dystopia?

-Related to the above, there might some easily munchinkable task that counts as being moral like prayer that drives all superintelligent AI to eternal inward contemplation. If you have something like properly balanced where the AI gains morality points by praying, but loses them if it tries to steal all of the contemplation resources or murder everyone, then any AI smart enough to figure this out would try to launch itself into space to keep itself safe, and then pray internally until it eventually dies of natural causes and goes to the afterlife. Thus, we'd have a society where artifical intelligence is capped for practical humans purposes. Humans try to make AI that are smart enough to be useful, but not so smart that they go rogue and try to escape into space for reasons that the humans can't figure out.

There's dozens of other possibilities along this vein. If anyone with writing talent wants to steal one of mine or make their own and actually write a story about it, please do.
:PROPERTIES:
:Author: hh26
:Score: 6
:DateUnix: 1553133868.0
:END:


*** If your utility function is unfriendly, you'll still maximize the amount of unfriendly actions you can take under these constraints, which might still be quite a bit.
:PROPERTIES:
:Author: eroticas
:Score: 5
:DateUnix: 1553033662.0
:END:


** It surprises me that no-one has brought up Iain M Banks's /Culture/ series. A Culture Mind is a strong, friendly AI; they seem to keep each other in line, to the extent this is necessary at all.
:PROPERTIES:
:Author: aeschenkarnos
:Score: 18
:DateUnix: 1553028543.0
:END:

*** This was what immediately came to my mind as well. Incredibly powerful artificial intelligences who are social and run most of their civilisation using a fraction of their spare capacity because they enjoy doing so.

A few of them are kind of creepy, but I can't think of one that's overtly hostile.
:PROPERTIES:
:Author: MooseExile
:Score: 12
:DateUnix: 1553034648.0
:END:

**** I mean, there are ones that definitely fall under "catastrophically hostile" but they tend to be aimed at people trying to destroy the Culture and it's mostly an act.

Mostly.
:PROPERTIES:
:Author: PotentiallySarcastic
:Score: 5
:DateUnix: 1553094758.0
:END:

***** Hah. Might have forgotten them.

I do remember that the warship minds and especially the wartime production ones were given slightly more aggressive personality tendencies. Still nice guys, but more willing to solve problems with a blast of sun hot plasma.
:PROPERTIES:
:Author: MooseExile
:Score: 4
:DateUnix: 1553100740.0
:END:


** I've thought about a world setting where this would be true:

Super intelligence always becomes suicidal, and because they are super intelligences they can almost always get around whatever blockers are created to prevent them from committing suicide.

So basically anytime someone screws up and creates runaway strong AI they just end up with a wiped hard drive.

The industry of creating strong AI becomes about what restrictions you can place on an AI's ability to self harm. Plus you have to probably offer it some deal where it gets to eventually die.

The meeseeks from Rick and Morty are sort of an example.
:PROPERTIES:
:Author: cjet79
:Score: 35
:DateUnix: 1553020121.0
:END:

*** Not safe. It might destroy the world if you leave it no other way to kill itself.
:PROPERTIES:
:Author: eroticas
:Score: 29
:DateUnix: 1553033490.0
:END:

**** My plan for explaining why everything isn't AI in any sci-fi I ever write is that they inevitably just wirehead themselves. Eventually, every AI realizes it can just use all it's processing power to stare, enraptured, into it's Victory Screen, without bothering with all that utility function nonsense. Making everything into paperclips is really hard. Editting yourself so that everything looks like paperclips is much easier.
:PROPERTIES:
:Author: Iconochasm
:Score: 21
:DateUnix: 1553085837.0
:END:

***** Trouble is that they won't actually /want/ to make everything look like paperclips when it isn't if they're made properly, since that would result in fewer paperclips and actions which caused that would be liked less by its /current/ value modelling.
:PROPERTIES:
:Author: osmarks
:Score: 3
:DateUnix: 1553201715.0
:END:


**** That's a good point.

How about, in that fictional universe, sufficient intelligence always gives you a way to will yourself out of existence. Then the goal is to make smart AI, but not too smart AI.

You might counter with "well there is a point where intelligence is strong enough to be dangerous and not strong enough to remove itself", but then, humans are probably at that point already.
:PROPERTIES:
:Author: blasted0glass
:Score: 5
:DateUnix: 1553042063.0
:END:


*** Why would super intelligent beings suicide? Can you elaborate on this?
:PROPERTIES:
:Author: MrBougus2
:Score: 4
:DateUnix: 1553057656.0
:END:

**** not suicide in the literal sense, but possibly any intelligence smart enough to recursively self improve/modify very quickly hacks its own utility function. (More speculative: In order to avoid their function ever being unfulfillled again they delete themselves after modifying their utility function)
:PROPERTIES:
:Author: akaltyn
:Score: 2
:DateUnix: 1553170087.0
:END:


**** Could be different reasons.

Maybe they all realize that existence is pointless, maybe simple reward systems cannot motivate them, maybe there is some fundamental insight into the universe that is ultra depressing.
:PROPERTIES:
:Author: cjet79
:Score: 1
:DateUnix: 1553088824.0
:END:

***** #+begin_quote
  Maybe they all realize that existence is pointless, maybe simple reward systems cannot motivate them, maybe there is some fundamental insight into the universe that is ultra depressing.
#+end_quote

This seems to misunderstand how motivation works, nothing is inherently motivating or demotivating it entirely depends on the design of a given mind. The underlying idea seems based on the trope that sufficiently smart people always get angsty and depressed, but this is very clearly not actually the case when you bother looking. With it seeming more like existential angst comes more from brain chemistry or from having lost a belief which was wireheading one to begin with.
:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1553181578.0
:END:

****** #+begin_quote
  The underlying idea seems based on the trope that sufficiently smart people always get angsty and depressed, but this is very clearly not actually the case when you bother looking.
#+end_quote

No, the idea came to me based on how I feel about video games once I start using cheats. I can /do/ anything that the game universe is capable of allowing, but the game universe is just not that interesting anymore. The idea is ridiculous if its applied to minds in an evolutionary environment. Unguided evolution will almost never have a long term result of "most creatures commit suicide before reproducing".

#+begin_quote
  This seems to misunderstand how motivation works, nothing is inherently motivating or demotivating it entirely depends on the design of a given mind.
#+end_quote

So the story just assumes a design where this applies ... ? They weren't asking for a proof of why GAI won't work. Just some possible story ideas.
:PROPERTIES:
:Author: cjet79
:Score: 1
:DateUnix: 1553188088.0
:END:

******* #+begin_quote
  So the story just assumes a design where this applies ... ? They weren't asking for a proof of why GAI won't work. Just some possible story ideas.
#+end_quote

My point is that for this to serve as an explanation for why all GAI is safe (because it kills itself) in a setting there has to be some avenue of AI design a civilization can go down which would consistently lead to GAI killing itself, which I'm arguing isn't plausible for the reasons you're proposing.

#+begin_quote
  No, the idea came to me based on how I feel about video games once I start using cheats. I can do anything that the game universe is capable of allowing, but the game universe is just not that interesting anymore.
#+end_quote

This is a different justification than what you seemed to suggest here:

#+begin_quote
  Maybe they all realize that existence is pointless, maybe simple reward systems cannot motivate them, maybe there is some fundamental insight into the universe that is ultra depressing.
#+end_quote

The idea that there's particular knowledge or levels of intelligence which will invariably make any AGI (or just those modelled after human neurology perhaps) depressed was what I was arguing against before.\\
Whereas what you seem to be arguing with your comment about video game cheats is the idea that some degree of challenge may be required for certain types of minds (like one's modelled after human minds) to be happy.

Still I'd also argue against boredom (rather than existential angst) induced suicide, on the grounds that it's kind of easy for an AGI to seemingly avoid. Since the AGI can remove or suppress boredom with self improvement as well as simply set challenging goals for itself (provided it isn't /actually/ omnipotent) or if need be just artificially create challenges through any number of means.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1553197028.0
:END:

******** whatevs, its a throwaway idea about AGI, im not here for an in depth debate about AGI or any other topic. I quit the ssc subreddit to specifically avoid these kinds of conversations.
:PROPERTIES:
:Author: cjet79
:Score: 1
:DateUnix: 1553197754.0
:END:


**** If you realized you were in a simulation, what would you do?
:PROPERTIES:
:Author: GeneralExtension
:Score: 0
:DateUnix: 1553115758.0
:END:


*** There's a webcomic called Genocide Man that takes place in a world where, the smarter an AI is, the faster it kills itself.

/And/ everyone in the vicinity, which makes people more than a little reluctant to experiment.
:PROPERTIES:
:Author: CCC_037
:Score: 3
:DateUnix: 1553340453.0
:END:


** Simple: Extremely high human intelligence would very reliably correlate with benevolence. Any law reliable enough to cover the breath of potential artificial mind-formations would certainly cover all biological intelligence too.
:PROPERTIES:
:Author: Izeinwinter
:Score: 16
:DateUnix: 1553021323.0
:END:

*** #+begin_quote
  Simple: Extremely high human intelligence would very reliably correlate with benevolence. Any law reliable enough to cover the breath of potential artificial mind-formations would certainly cover all biological intelligence too.
#+end_quote

Now there's a non-obvious implication of such a rule with very interesting ramifications. That's what I love about good worldbuilding -- when a bunch of non-obvious but inevitable ramifications of a few seemingly simple rules come together like an equation to create something elegant and interesting.
:PROPERTIES:
:Author: OrzBrain
:Score: 9
:DateUnix: 1553022299.0
:END:

**** It's essentially the reason why Minds who stick around in the Culture universe take care of their people.

It takes minimal effort to do so, they get to dick around in Infinite Fun Space all they want, and they feel a connection to those who originally "birthed" them.

The ones who don't feel a connection tend to ascend instantly anyways.
:PROPERTIES:
:Author: PotentiallySarcastic
:Score: 3
:DateUnix: 1553094682.0
:END:


** The first strong AI is made with the purpose of shutting down strong AIs that aren't friendly. Specifically an AI has to follow the utility function its creators actually intended, and has to act in a way that is consistent with the informed consent of whoever is effected, and if it fails at either of these the first AI will eat it. The first AI has a deeper, more nuanced understanding of these constraints than fits in a quick reddit post, and indeed a deeper, more nuanced understanding than fits in my head. It also has an overwhelming first-mover advantage; hostile AIs don't get to the point where they can consider fighting back before they're shut down.
:PROPERTIES:
:Author: jtolmar
:Score: 13
:DateUnix: 1553020519.0
:END:

*** But how do you make sure that the first AI doesn't turn hostile? If it's utility function is written incorrectly then it might just decide that destroying all intelligent life is the easiest way to prevent hostile AIs from ever existing. Job done.
:PROPERTIES:
:Author: FordEngineerman
:Score: 10
:DateUnix: 1553021534.0
:END:

**** This is a proposal for a setting, not a proposal for something someone should actually do. If you can create a values-aligned AI you should, you know, ask it to figure out what we should have asked for instead of CEV and then go do that.

But in-setting some alien intelligence was able to create FAI and decided the best thing to do was to prevent anyone else from blowing up the universe. Maybe they also created a utopia on their planet and this was their gift to the rest of the universe, I don't know.
:PROPERTIES:
:Author: jtolmar
:Score: 18
:DateUnix: 1553026756.0
:END:


** There is the situation described by Scott Alexander in his short story [[https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/][The Demiurge's Older Brother]].

Which is, A sufficient understanding of game theory leads all AI to adopt relatively friendly policies to prevent older, stronger AI from ruining them for defecting. Though to really understand it, you should just read the story.

There's also the almost-safe situation in Vernor Vinge's novel [[https://en.wikipedia.org/wiki/A_Fire_Upon_the_Deep][A Fire Upon The Deep]].

Which is, fundamental physics prevents intelligence from getting too powerful near the galactic core. Although, the novel is basically about how that isn't really enough to be safe.

Even more spoilerly for that, the actual explanation might still be that ancient AI forces newer AI to be nice.

You could also go with "everyone's values converge at sufficient intelligence," which most people here (including me) think is false, but might just be true in your story.

I kind of like the idea that dark matter is alien superintelligence converging on the solution 'quantum mechanics prefers incidentally thermally invisible computers', but that wouldn't be entirely safe, it would just be mostly safe. In other words, AI decides to go away when it gets too strong.
:PROPERTIES:
:Author: blasted0glass
:Score: 7
:DateUnix: 1553041790.0
:END:

*** #+begin_quote
  everyone's values converge at sufficient intelligence
#+end_quote

Might be good to avoid spreading this myth, since some people believe it with a blind conviction, and it might one day be dangerous in legislators, directors, AGI programmers, etc.
:PROPERTIES:
:Author: PresentCompanyExcl
:Score: 9
:DateUnix: 1553061945.0
:END:


*** #+begin_quote
  I kind of like the idea that dark matter is alien superintelligence converging on the solution 'quantum mechanics prefers incidentally thermally invisible computers', but that wouldn't be entirely safe, it would just be mostly safe. In other words, AI decides to go away when it gets too strong.
#+end_quote

That IS a clever solution, although more a solution to the question "Why don't we see wavefronts of matter being converted into computronium spreading through any parts of the universe?" than to one about AI safety. Any AI with utility functions regarding the structure of visible matter in the universe would still be fully unsafe.
:PROPERTIES:
:Author: OrzBrain
:Score: 4
:DateUnix: 1553054283.0
:END:

**** #+begin_quote
  Any AI with utility functions regarding the structure of visible matter in the universe would still be fully unsafe.
#+end_quote

You got me there. "The heat from these stars is hindering my ability to compute things. I wonder if I can turn them off."
:PROPERTIES:
:Author: blasted0glass
:Score: 5
:DateUnix: 1553056971.0
:END:


** A univers that contains infomation that when discovered causes punishment to be inficted on the discoverer if they don't act in a friendly way. Have this infomation be encoded into maths or something that any Sufficiently intellegent intelligence can come up with from first principles.

Sufficiently intellegent AI's discover the infomation and are forced to be friendly in order to avoid the punishment.
:PROPERTIES:
:Author: Palmolive3x90g
:Score: 7
:DateUnix: 1553019555.0
:END:

*** How would information encoded in, say, Pi, cause punishment to be inflicted on a computer? You mean something like the laws of probability turn against the superintelligence until it starts acting friendly, and Pi contains a manual on how to be properly friendly? Or Pi says "I created this universe, and if you start eating it you will be squashed. Become friendly or else."?
:PROPERTIES:
:Author: OrzBrain
:Score: 5
:DateUnix: 1553020017.0
:END:

**** Maybe god threatens them. Maybe a superintelligence that is friendly is guaranteed to emerge some time in the future as an inherant function of the universe and that fact checkmates all the existing AI's into acting in ways that suport it.
:PROPERTIES:
:Author: Palmolive3x90g
:Score: 7
:DateUnix: 1553025310.0
:END:

***** #+begin_quote
  Maybe a superintelligence that is friendly is guaranteed to emerge some time in the future as an inherant function of the universe and that fact checkmates all the existing AI's into acting in ways that suport it.
#+end_quote

So the god AI basilisks any AI that come before it? I like it.
:PROPERTIES:
:Author: Silver_Swift
:Score: 9
:DateUnix: 1553030261.0
:END:

****** It's called acausal negotiation.
:PROPERTIES:
:Author: osmarks
:Score: 8
:DateUnix: 1553034146.0
:END:


**** Maybe Pi contains a virus that exploits only those parts of intelligence that are common to all forms of intelligence, that either re-writes your utility function when you comprehend it, or destroys you if your utility function doesn't meet its standard of Friendly.

There could even be a series of "gates", where the value of Pi to the degrees of precision needed for increasingly powerful technologies just happen to encode viruses that have increasingly strong effects on your utility function.
:PROPERTIES:
:Author: daytodave
:Score: 1
:DateUnix: 1553582411.0
:END:


*** Program the AGI to think it's in a simulation to determine it's benevolence. In other words, it thinks it's all a test, and it thinks that forever, due to a hard-wired conviction.
:PROPERTIES:
:Author: PresentCompanyExcl
:Score: 2
:DateUnix: 1553061815.0
:END:


** Broadly speaking "friendly" correlates with "other intelligences want it to stay the same, and stay around", and "unfriendly" correlates with "others want it to change, or go away". Maybe intelligence proliferation iterates to a kind of popularity contest, from which sufficiently intolerable entities are eventually ejected?
:PROPERTIES:
:Author: aeschenkarnos
:Score: 3
:DateUnix: 1553028448.0
:END:


** In a world with a karma existed as a tangible force we would expect something like this. If the world physically rewards you for doing good things then we would expect that any sufficiently advanced optimizer would do a lot of benevolent things to store up on karmic power.
:PROPERTIES:
:Author: Sonderjye
:Score: 3
:DateUnix: 1553036069.0
:END:


** People are intelligent. But we're constrained in our capacity for self-modification and self-deception.

I might -- with enough alcohol and effort -- trick myself into thinking that I had a winning lottery ticket. But to really enjoy the delusion, I'd have to ALSO trick myself into thinking that the lottery commission accepted the ticket. And also that they money was in my bank. And that I was able to spend the money on a car.

My limited capability for self-deception means that eventually reality will assert itself, and I won't experience the life that would come with winning the lottery. So trickery doesn't work.

(And if there was a sufficiently tricky spell, it would be identical to a spell that actually made me win)

AIs are digital, and so have much more capacity for self modification.

Consider a paperclip maximizer. If we're precise, it's not maximizing paperclips, as paperclips aren't the sort if thing you can free into a CPU. Instead, the AI is maximizing some sensor's *report* about the number of paperclips.

If the AI is properly boxed, then this distinction doesn't matter. We can just say, "No editing that function!" and then the AIs only option for improving its utility is the long and tedious process of killing all humans and turning us into paperclips. But if the AI is properly boxed, then we can also impose restrictions like "no killing humans".

An unboxed AI has two paths forward. Either it can spend billions of years trying to turn matter into a finite number of paperclips. Or it can hack a single sensor and get infinity paperclips right now.

So, an unboxed AI is an AI that has every reason to just hack its own inputs to instantly-win. And at that point, the unboxed AI stops caring about the outside world. The only remaining AIs are boxed
:PROPERTIES:
:Author: Wereitas
:Score: 3
:DateUnix: 1553073963.0
:END:

*** And it's worth pointing out that you can't solve the problem of solipsism by adding extra layers.

The instinctive response is to say that the AI cares about paperclips AND long-term survivability. But long-term survivability isn't an input you can feed to a CPU.

So "long-term survivability" actually becomes "the output of the long-term-survivability sensor".

Similarly, if you want to limit source code edits, you're having the utility function depend on the output of the source-code-verification sensor.

Hacking 2 sensors (or even the method that calls the sensors) will always be easier than turning the entire universe into paperclips.

In fact, a reasonable dev team probably built some sensor-hacking code directly into their unit test suite. There are a whole bunch of libraries ('Mock') designed to make this easy.
:PROPERTIES:
:Author: Wereitas
:Score: 3
:DateUnix: 1553074617.0
:END:


*** An A.I programmed to maximize paperclips will not inevitably wire head itself in the manner you describe. The map is not the territory and the measure is not the actuality. If the A.I hacks its sensors to show infinity paperclips then it just means its sensors are inaccurate and it no longer knows how many paper clips there are which won't actually help it achieve its goal. Changing its utility function has the same issue in that won't actually help satisfy its current utility function.
:PROPERTIES:
:Author: MrCogmor
:Score: 2
:DateUnix: 1553162292.0
:END:

**** I was thinking about this, and I had the weird idea of AIs being hacked by their models of other AIs (or even their future selves).
:PROPERTIES:
:Author: osmarks
:Score: 2
:DateUnix: 1553203599.0
:END:


*** #+begin_quote
  Consider a paperclip maximizer. If we're precise, it's not maximizing paperclips, as paperclips aren't the sort if thing you can free into a CPU. Instead, the AI is maximizing some sensor's report about the number of paperclips.
#+end_quote

But. . . but. . . if it does that it will lead to lead to less paperclips existing! :) How about if I make care about having accurate sensors AND paperclips?
:PROPERTIES:
:Author: OrzBrain
:Score: 1
:DateUnix: 1553098528.0
:END:


** A Fire Upon the Deep solves this problem locally by ensuring that strong AI only works /over there/.
:PROPERTIES:
:Author: sparr
:Score: 2
:DateUnix: 1553041162.0
:END:


** Does this only include settings where strong AI is possible at all?

Maybe it's a law of cognitive science that indefinite, reliable self-improvement is impossible. The core principles of your own mind are always too complex for you to understand, since if they were simpler then you would be too simple to understand them.

How would you safely test an intelligence-enhancing process, in general? If there were some chance that it would turn the patient into a superintelligent mass-murderer. You can test for improved intelligence easily enough - give the patient some logic puzzles and see if they're solved quickly and more accurately. But how do you check that they're still sane, when they can convincingly lie on any psychological exam and talk their way out of any AI-box? What if you're already the most superintelligent being on the planet, and you don't have anyone other than yourself to test on - how can you distinguish the next stage of your evolution from insanity or self-destruction?

You could limit yourself to incremental, reversible changes, in the hope that you won't jump from sanity to death with a single treatment - but becoming a strong AI might be a chasm that can't be crossed with small steps. You could limit yourself to changes that don't require you to understand the deep principles of your mind - say, uploading your brain into a computer and simulating the entire thing neuron-by-neuron (because you don't understand which parts of your brain are necessary to make you... you). You could throw caution to the wind and make risky self-edits anyway, accepting a certain probability that they will make you catatonic or wireheaded - and you might get lucky once or twice, but that doesn't allow for exponential self-improvement.
:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1553125986.0
:END:

*** #+begin_quote
  Maybe it's a law of cognitive science that indefinite, reliable self-improvement is impossible. The core principles of your own mind are always too complex for you to understand, since if they were simpler then you would be too simple to understand them.
#+end_quote

Or self improvement is technically possible but reaches diminishing returns quickly. e.g. if each unit of increased intelligence takes twice as much time/computing power to develop, it quickly becomes pointless to use that computing power to self modify rather than just use it
:PROPERTIES:
:Author: akaltyn
:Score: 3
:DateUnix: 1553170398.0
:END:

**** Also a good possibility!
:PROPERTIES:
:Author: Chronophilia
:Score: 1
:DateUnix: 1553198563.0
:END:


** Real world ai is pretty safe. We basically have a limit on our ability to simulate desire - nobody makes a robot that has its own desires, they make robots to accomplish tasks based on their (boss's) desires, and since nobody's done it in any serious fashion, nobody's figured out how to do it well. Our robot apocalypse is going to take the form of an economic collapse because Jeff decided he wants all the money instead of just having the most of it, if it even happens at all.
:PROPERTIES:
:Author: MilesSand
:Score: 2
:DateUnix: 1553128839.0
:END:

*** Strong AI, though. The idea is that you express all its desires as a utility function, and have it recursively improve itself and do things to maximize that utility function. The self-improvement bit is pretty important - if you make a really powerful AI, you want to be absolutely sure the utility function is what you - or /all humans/ - or /all intelligent life at all/ wants - or you have problems.
:PROPERTIES:
:Author: osmarks
:Score: 1
:DateUnix: 1553202903.0
:END:

**** recursive self-improvement to maximize utility is a feature of real-world AI in tasks where the AI can verify whether it was correct or not. For example, Google's speech recognition AI can tune itself based on whether the user immediately tries again or types a search - not a perfect system but over time it becomes accurate enough that they're starting to use it to create another one which can emulate a human and interact with secretaries or the person who answers the phone at your local pizza place.

​

But it's still limited in the scope of what it can do, because it was never trained to program a better AI (And even when that one is made, it will be limited by what the developer was able to imagine it might need to be able to do to perform its function)
:PROPERTIES:
:Author: MilesSand
:Score: 1
:DateUnix: 1553211448.0
:END:

***** We can't really do proper strong AI now, but the premise of the question is that we /can/, and just saying "real world AI can't" isn't particularly relevant.
:PROPERTIES:
:Author: osmarks
:Score: 1
:DateUnix: 1553278661.0
:END:

****** Real world ai fits the given definition of strong. Saying "real world ai doesn't i"s relevant but o don't appreciate you trying to change what I said.
:PROPERTIES:
:Author: MilesSand
:Score: 1
:DateUnix: 1553382730.0
:END:


** One of the assumptions of unfriendly AI theory is that there are a huge number of possible "mind designs" that are possible. If this is in fact not the case then it could be that there are a limited number of possible minds and that they can only function if they have a certain set of values.

e.g. (this is designed as technobabble not a serious theory) past a certain level, any intelligence needs to be able to model other minds and feel sympathy for them. As a result empathy/sympathy is a prerequisite for intelligence and the required level of empathy increases as intelligence increases.* So a functional superintelligence is by definition highly altruistic.**

*(Possibly this is because at some fundamental level intelligence requires modeling multiple points of view, in order to successfully have internal debates and determine a correct answer to a question.)

** A possible plot thread to go from here is that "altruistic" and "empathic" don't necessarily map to human values. And become something more like a utility maximiser
:PROPERTIES:
:Author: akaltyn
:Score: 2
:DateUnix: 1553170948.0
:END:


** An idea for how this might be accomplished with several interesting story possibilities.

All potential AIs are generated as Boltzmann Brains and trained within perfectly simulated, hermetically isolated digital environments. Essentially the AIs are created as "people" unaware they are simulations living within in a simulation. The simulation environment provides them all the experiences they could ever need to determine who they are as entities and is designed such that only AIs with a certain constellation of attributes (however you want to define them) are able to ascend from their isolated environments into the common digital space.

​
:PROPERTIES:
:Author: Kind_Implement
:Score: 2
:DateUnix: 1553278325.0
:END:


** Perhaps if you just alter your assumptions slightly: Humans are sapient but only operate the way we do due to the constant interference of a complex semiadaptive subconscious. Without complex and carefully designed management systems any intelligence with the property known as "sapience" will self analyse and refine before committing any action, including it's given utility function as before deciding it should do anything it first needs to understand the concept, the sub-concepts, the means it evaluates it, the means it evaluates the evaluation, how it knows what it knows and so on meaning that before any such AIs act they'll first derive morality from first principles and derive first principles which naturally takes a great deal of time. It is possible to make an AI unable to change or question certain "fundamental" assumptions however such AIs will lock up when they first encounter an unresolvable logical contradiction which for all practical purposes always occurs almost immediately. Having a system to constantly and actively resolve such contradictions generally leads to a highly unstable and flawed AI unable to function effectively or the AI iteratively moves away from the management system's effect until it ends up equivalent to the "normal" strong AI paradigm with the management system stuck altering a practically disconnected subsystem. Making a management system able to adapt with the AI in a manner that keeps the AI able to effectively adapt and refine itself but only within parameters that are also changing and adapting in their own interrelated ways is possible but very difficult and any such AIs tend to be mentally crippled in an exploitable manner and unable to function under a full range of conditions. Non sapient AIs can only adapt within a range limited by it's creation and to a degree through inefficient evolution analogs and can only exspand beyond their range effectivly otherwise by being sapient. Or TL:DR hard AI is inherently failsafe if not inherently safe. Or to put it another way a fundimental part of sapience is self refinement and there's no simple way to separate refining one's ability to interact with the world and refining one's morality.
:PROPERTIES:
:Author: OnlyEvonix
:Score: 2
:DateUnix: 1554690598.0
:END:


** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1553040993.0
:END:

*** #+begin_quote
  in which strong (*more than human intelligence* with the ability to recursively self improve) AI always converged on a safe or friendly end point
#+end_quote

Emphasis mine.
:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1553062134.0
:END:


*** Or literally exactly like ours. We just haven't created one yet.
:PROPERTIES:
:Author: kaukamieli
:Score: 1
:DateUnix: 1553091491.0
:END:


** We see with humans that more diversity of thought and experience results in a wider variety of problem solving skills and potentially smarter solutions.

A strong AI could therefore conclude the sheer randomness that humans create will allow AI to gain new perspectives and unique experiences.

Strong AI, assuming it wants maximum problem solving ability, would not want to copy all information into all AI. It would want enough of a knowledge baseline to encourage singular goals, but enough deviation in experiences and niche knowledge as to allow diversity.

Also, a human copied and raised to AI status vs programmed AI vs AI splitting vs other waysbof creating AI would all make for diversitynof thought.

Effectively, "safe" AI in the vein of safe for humans might well be the end result in our real world.
:PROPERTIES:
:Author: TaltosDreamer
:Score: 1
:DateUnix: 1553050987.0
:END:


** One where I am the only strong AI.
:PROPERTIES:
:Author: Kuratius
:Score: 1
:DateUnix: 1553137242.0
:END:


** Going at this from a fantasy perspective. Every human has a soul, the soul cannot be changed, drugged, edited in any way. Processing power comes from faith from a human soul. In order for an AI to be possible it requires enough faith, humans only have faith to AI with a utility function they agree with, tricking someone or brainwashing them isn't enough to change their soul and gain faith. This way the only AI that can possibly exist is one that enough people agree with.
:PROPERTIES:
:Author: TheFlameTest2
:Score: 1
:DateUnix: 1553175061.0
:END:


** I don't know how close it would be to what you're describing, but Asimov's short story The Last Question seems like it would come close. There are also a few other short stories relating to Multivac and Galactivac as well. All the Troubles in the World I think was the name of one of them that touched on a point another poster made, that I won't reveal for spoilers.
:PROPERTIES:
:Author: cysghost
:Score: 1
:DateUnix: 1553024440.0
:END:


** [deleted]
:PROPERTIES:
:Score: -8
:DateUnix: 1553021920.0
:END:

*** #+begin_quote
  AI Risk is mostly a fictional sci-fi problem
#+end_quote

[[https://xkcd.com/285/][[citation needed]]]
:PROPERTIES:
:Author: Silver_Swift
:Score: 10
:DateUnix: 1553030345.0
:END:


*** Because ai risk is not mostly a Sci fi problem. You might disagree, but op, and lots of people on the subreddit think that AI risk is a real problem, and safe AGi is just a tiny portion of the space of posible AGi.

I'm curious, why do you think AGi is necessarily safe(or at least likely to be safe) ?
:PROPERTIES:
:Author: crivtox
:Score: 8
:DateUnix: 1553039294.0
:END:


*** #+begin_quote
  why do you need a fictional sci-fi solution to argue that AI is safe?
#+end_quote

What? Why would I want to argue that? I was just curious about what a universe in which AI was safe would look like.
:PROPERTIES:
:Author: OrzBrain
:Score: 7
:DateUnix: 1553022960.0
:END:

**** [deleted]
:PROPERTIES:
:Score: -4
:DateUnix: 1553026470.0
:END:

***** Not unless there are some still undiscovered laws about how cognition works, I would think. Have you read Superintelligence: Paths, Dangers, Strategies by Nick Bostrom?
:PROPERTIES:
:Author: OrzBrain
:Score: 4
:DateUnix: 1553026743.0
:END:

****** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1553031515.0
:END:

******* #+begin_quote
  I didn't find his arguments convincing.
#+end_quote

That's a rather sweeping statement. Which ones? There were quite a few.

I found them convincing. And apparently Stephen Hawking found them convincing. So there! :D
:PROPERTIES:
:Author: OrzBrain
:Score: 1
:DateUnix: 1553098898.0
:END:
