#+TITLE: "If you give Frodo a lightsaber..."

* "If you give Frodo a lightsaber..."
:PROPERTIES:
:Score: 30
:DateUnix: 1444493452.0
:DateShort: 2015-Oct-10
:END:
[deleted]


** Depending how similar Clippy's psychology is to my own, and to what extent we're both buffered against the obvious possible ill effects of a speedup, I think we both boost a lot and split the universe between our utility functions. But only if Clippy is similar enough to me that we have common knowledge that it's a /symmetrical/ Prisoner's Dilemma.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 16
:DateUnix: 1444524731.0
:DateShort: 2015-Oct-11
:END:


** double my thought-speed. the paperclip entity is me in mind and human in body, and unless i'm vastly underestimating myself, i couldn't turn the world into paperclips with that, or even significant nations. It will flood the market with paperclips at this point, but it still hasn't reached the dangerous point. When it dies, People can turn the paperclips back.
:PROPERTIES:
:Author: NotAHeroYet
:Score: 13
:DateUnix: 1444495985.0
:DateShort: 2015-Oct-10
:END:

*** It can think and plan. It will probably start with an oil business or something, to get money to make everything paperclips.
:PROPERTIES:
:Author: kaukamieli
:Score: 3
:DateUnix: 1444509922.0
:DateShort: 2015-Oct-11
:END:

**** Yeah, but it won't have the money just yet, and even if it does, it'll need a lot more skill to plan the world's destruction. at the most, it'll be a successful CEO with more... eccentric spending habits. I mean, it might get further, but I doubt it. The demand for paperclips or the supply, depending on how it operates, might skyrocket, but it won't be world-ending or even harmful enough that I would feel morally obligated to turn-down this offer. Now, exponentially boosting it would be foolish, but doubling, trippling, or even quadrupling should be safe.
:PROPERTIES:
:Author: NotAHeroYet
:Score: 2
:DateUnix: 1444522392.0
:DateShort: 2015-Oct-11
:END:


** Why is everybody so concerned with nullifying the paperclip entity, to the point they're willing to sacrifice themselves to stop it?

Even humans that decide their goal is to kill as many people as they can don't usually manage to do that much damage, how much damage would a human-level intelligence paperclip maximizer do?
:PROPERTIES:
:Author: fdar
:Score: 25
:DateUnix: 1444501725.0
:DateShort: 2015-Oct-10
:END:

*** u/sparr:
#+begin_quote
  how much damage would a human-level intelligence paperclip maximizer do?
#+end_quote

If he's as smart as the average redditor thinks they are? A lot.
:PROPERTIES:
:Author: sparr
:Score: 16
:DateUnix: 1444506497.0
:DateShort: 2015-Oct-10
:END:

**** Ah, but the paperclipper isn't as smart as you /think/ you are. It's as smart as you /actually/ are. Which is to say it's at the top of the bell-curve.
:PROPERTIES:
:Author: GeeJo
:Score: 1
:DateUnix: 1444893827.0
:DateShort: 2015-Oct-15
:END:


*** Given a supernatural thinking speed boost? I should think one could make a ton of money and spend it malevolently, at the very least. (Or, you know, just buy paperclips...)

(Not that my answer attempted to nullified it)
:PROPERTIES:
:Author: ishaan123
:Score: 2
:DateUnix: 1444519302.0
:DateShort: 2015-Oct-11
:END:

**** u/fdar:
#+begin_quote
  (Or, you know, just buy paperclips...)
#+end_quote

That doesn't sound too bad, even though

#+begin_quote
  I should think one could make a ton of money
#+end_quote

I haven't seen a plan for this yet...
:PROPERTIES:
:Author: fdar
:Score: 0
:DateUnix: 1444519636.0
:DateShort: 2015-Oct-11
:END:

***** Pro gaming seems like a decent start. A three or four times boost would let you play games dependent on reaction speed or strategy with a nearly insurmountable advantage.

There's a reasonably large amount of money tied up in Counter-Strike tournaments for example, and given a sniper rifle and a x4 speed boost you'd probably seem like some kind of aimbot from the outside.

That's enough to get you some seed money, and I'm sure there are other ways to leverage 'increased clock speed' or whatever you call it into financial success.
:PROPERTIES:
:Author: Jiopaba
:Score: 1
:DateUnix: 1444818000.0
:DateShort: 2015-Oct-14
:END:


*** Human society is a fragile thing and a clever sociopath can do quite a lot of damage sometimes. Especially one with literal superpowers.

Though I take your point, no human is going to be turning the universe into paperclips any time soon.
:PROPERTIES:
:Author: Chronophilia
:Score: 4
:DateUnix: 1444515169.0
:DateShort: 2015-Oct-11
:END:

**** Even more so since sociopaths are generally good people like everyone else.
:PROPERTIES:
:Author: Yasuda1986
:Score: 2
:DateUnix: 1444571718.0
:DateShort: 2015-Oct-11
:END:

***** I was going to question this until I realized that I was coming at this from a virtue ethics perspective even though I know that's usually incorrect.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 2
:DateUnix: 1444577181.0
:DateShort: 2015-Oct-11
:END:

****** How would you have come at this from a virtue ethics perspective?
:PROPERTIES:
:Author: Yasuda1986
:Score: 1
:DateUnix: 1444584623.0
:DateShort: 2015-Oct-11
:END:

******* Sociopaths don't care about humans; thus, they are not caring people. It's ... kind of the usual way to measure goodness.
:PROPERTIES:
:Author: MugaSofer
:Score: 2
:DateUnix: 1444641738.0
:DateShort: 2015-Oct-12
:END:

******** My impression is that it is a lack of empathy, not a lack of caring. People donate to charity even through they can't have empathy for all the people they are helping. But yes, the concept of good or bad people doesn't really make sense to me. Only outcome are good or bad.
:PROPERTIES:
:Author: Yasuda1986
:Score: 1
:DateUnix: 1444656509.0
:DateShort: 2015-Oct-12
:END:

********* I convert it into longform personally: for the purposes of my cognition a bad person is a person who if given more power would do things I (and hopefully humanity in general) consider bad, based on past experience and so forth. Same for a good person doing good things. Basically, my heuristic is to ask can they be trusted with power and what would they do with that power.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1444762260.0
:DateShort: 2015-Oct-13
:END:


**** u/Bowbreaker:
#+begin_quote
  Especially one with literal superpowers.
#+end_quote

Did I miss the superpowers?
:PROPERTIES:
:Author: Bowbreaker
:Score: 0
:DateUnix: 1444548056.0
:DateShort: 2015-Oct-11
:END:

***** Enhanced mental capacity.
:PROPERTIES:
:Author: sephlington
:Score: 1
:DateUnix: 1444555206.0
:DateShort: 2015-Oct-11
:END:

****** It just thinks faster, it isn't more intelligent. Slowing someone's perception of time will not make them a genius, it just gives them more time to think.
:PROPERTIES:
:Author: SpeculativeFiction
:Score: -1
:DateUnix: 1444632386.0
:DateShort: 2015-Oct-12
:END:

******* Still a superpower.
:PROPERTIES:
:Author: MugaSofer
:Score: 2
:DateUnix: 1444641763.0
:DateShort: 2015-Oct-12
:END:


*** You both have practicaly infinite subjective thinking time (as close to immortality as we are going to get IMHO). After subjective tousands of years of preparation you can do a lot of damage with "just" human-level intelligence. Or go mad. Probably both.

If you think that existence of humanity means you can't maximize paperclips - humanity has to go.

How much preparation time do you think you need to start WW3?
:PROPERTIES:
:Author: ajuc
:Score: 1
:DateUnix: 1444724126.0
:DateShort: 2015-Oct-13
:END:


*** No human has ever really wanted to kill "as many people as possible," without regard to who or how. Otherwise they could start by putting something in the water supply and go from there.
:PROPERTIES:
:Author: Rangi42
:Score: 1
:DateUnix: 1444515292.0
:DateShort: 2015-Oct-11
:END:


** We trade: Clippy can have its section of the universe for paperclips (which shall naturally be far away from earth) and I will have mine.

If we ever get to that point. In the mean time there's plenty of room for both humans and lots of paperclips - in fact I think the two rather need each other. We're both better off with respect to maximizing our preferences. Just because we have orthogonal preferences doesn't mean we can't cooperate.

At first, we can just increase it by a small amount - humans are the ones who make paperclips after all, so Clippy will be a temporary ally so long as I don't let him get /too/ powerful. It's in Clippy's best interest to help with ensuring my survival, humanity's survival, and help with ensuring we find a reliable way to enforce our agreement. Plus this way there are two minds working on the problem.

Meaner strategies include: using my greater knowledge of the situation to kill it, using the fact that I control the situation to play the unfair ultimatum game ("I promise to make you a gazillion paperclips if you cooperate, but I get the rest of the universe") and so on.
:PROPERTIES:
:Author: ishaan123
:Score: 9
:DateUnix: 1444501822.0
:DateShort: 2015-Oct-10
:END:


** Eh, maybe a couple percent, enough to give me an edge but not enough to be a major problem. Also I phone the relevant police on the other side of the world and tell them that I think my estranged twin brother might be a danger to himself and others - he called me and started to rant about turning everything into paper clips. Yes, officer, I know it sounds crazy. That's why I'm worried. I'm not in the country, but I'd appreciate it if you kept him on file. You mean you don't have a record of him? Oh, my. I thought he was joking when he talked about illegally immigrating. Well, I don't know his address, but I can give a physical description."
:PROPERTIES:
:Score: 28
:DateUnix: 1444495019.0
:DateShort: 2015-Oct-10
:END:

*** I don't think the paperclipper looks like you. It's only as smart as you.
:PROPERTIES:
:Author: philip1201
:Score: 6
:DateUnix: 1444515695.0
:DateShort: 2015-Oct-11
:END:


** Iff I know the rules of this scenario and the paperclipper doesn't, I would choose Graham's Number%. I would then use my first-mover advantage to locate and obliterate the paperclipper before a serious amount of damage was done by it. Presumably, it would be unprepared for an attack by an entity whose utility function was, at that point, only concerned with obliterating it.
:PROPERTIES:
:Author: Frommerman
:Score: 9
:DateUnix: 1444496354.0
:DateShort: 2015-Oct-10
:END:

*** u/xamueljones:
#+begin_quote
  I would choose Graham's Number
#+end_quote

You'd be trying to kill yourself out of boredom in literally a heartbeat. The speed boost is purely mental and you don't move any faster. You'd be thinking so quickly that the entire world would have looked like it stopped moving and if you can't cancel the speed, then you be bored stiff in the time it takes for your heart to beat even once.
:PROPERTIES:
:Author: xamueljones
:Score: 17
:DateUnix: 1444501304.0
:DateShort: 2015-Oct-10
:END:

**** Alternatively the universe may have a maximum processing speed which he would hit. Probably smaller than graham's number but still kill self boredom. Because you can't move fast you would probably break your desire to kill yourself before you manage to.

You'd spend trillions of years trapped in an unmoving body, I doubt the result of that is anything like human conscious
:PROPERTIES:
:Author: RMcD94
:Score: 9
:DateUnix: 1444502390.0
:DateShort: 2015-Oct-10
:END:

***** On the plus side, the paperclipper would also have that problem. We would both likely take the same first steps in creating a nanotech army, except his would make paperclips and mine would seek out and destroy him. I don't buy that I would die of boredom, given that I would have almost literally infinite time to think.
:PROPERTIES:
:Author: Frommerman
:Score: -1
:DateUnix: 1444502631.0
:DateShort: 2015-Oct-10
:END:

****** u/xamueljones:
#+begin_quote
  I don't buy that I would die of boredom, given that I would have almost literally infinite time to think.
#+end_quote

Yes, but how much of your more interesting thoughts are generated by your experiences and actions? Over time, you will want some new input and what was the survival mnemonic?

#+begin_quote
  Humans can only tolerate 3 minutes without oxygen, 3 hours without warmth, 3 days without water, 3 weeks without food, and *3 months of isolation*.
#+end_quote
:PROPERTIES:
:Author: xamueljones
:Score: 4
:DateUnix: 1444504275.0
:DateShort: 2015-Oct-10
:END:

******* The last one is psychological, though, and there's a huge range. I know people who would go bananas much sooner, and others (like myself) who could go without human contact for years without it having an effect on their psyche.

Honestly, if it were me, I'd use the time to have a computer speed-flip through thousands of books and enjoy being able to read them all with much less in the way of interruptions for physical requirements.
:PROPERTIES:
:Author: Geminii27
:Score: 1
:DateUnix: 1444505985.0
:DateShort: 2015-Oct-10
:END:

******** Good computer screens update at 144Hz. You're experiencing the equivalent of trillions (understatement) of years every second. (I'm assuming you also picked Graham's number). Even if a computer screen was pre-setup for you to use it for this, you would have to go for trillions of subjective years between each page flip.
:PROPERTIES:
:Author: gbear605
:Score: 3
:DateUnix: 1444564426.0
:DateShort: 2015-Oct-11
:END:


**** I take it you've listened to [[http://geeklyinc.com/sayer-episode-6-a-dreamless-sleep/][Sayer EP 6 : A Dreamless sleep]]
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 2
:DateUnix: 1444504967.0
:DateShort: 2015-Oct-10
:END:

***** No, but it looks interesting and I'll be listening to it.
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1444505715.0
:DateShort: 2015-Oct-10
:END:

****** Please be warned it is a horror podcast and the title character is an unfriendly-AI in a Cthulhu universe.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 2
:DateUnix: 1444526060.0
:DateShort: 2015-Oct-11
:END:


*** Same. But in my case it would be mostly because I just want to be as intelligent as possible and I'm willing to take the risk.
:PROPERTIES:
:Author: elevul
:Score: 1
:DateUnix: 1444502416.0
:DateShort: 2015-Oct-10
:END:


*** It's not intelligence, unfortunately, it's just clock speed.

#+begin_quote
  Double your thought speed? Sure! You'll perceive reality as moving half as fast, but it'll be worth it because you'll have twice as long to think of a solution to everything.
#+end_quote

You'd be trapped in a paralyzed body for eternity. Not sure what would /happen/ from an outside perspective, but I doubt you could obliterate Clippy with your first-mover advantage when neither of you can move.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1444641926.0
:DateShort: 2015-Oct-12
:END:


** Can we choose an arbitrary scaling factor for our speed of thought? If so, I pick 0. I'll end up brain-dead, but so will the paperclipper.
:PROPERTIES:
:Author: Rangi42
:Score: 10
:DateUnix: 1444497922.0
:DateShort: 2015-Oct-10
:END:

*** I pick negative one! I guess I live my life backwards from then on.
:PROPERTIES:
:Author: DCarrier
:Score: 5
:DateUnix: 1444506463.0
:DateShort: 2015-Oct-10
:END:

**** help\\
i was asked to input a number but i accidentally pressed a letter and now he isn't responding
:PROPERTIES:
:Author: biomatter
:Score: 15
:DateUnix: 1444507760.0
:DateShort: 2015-Oct-10
:END:

***** If only we had input authentication on these counterfactual scenarios!
:PROPERTIES:
:Author: Transfuturist
:Score: 8
:DateUnix: 1444517047.0
:DateShort: 2015-Oct-11
:END:


** I'd go with the maximum mental speed-up that doesn't result in total boredom. Let's say a couple dozen times acceleration, or so.

Many people seem to be worried about the paperclipper, but it's important to remember that what makes a superintelligent paperclip-maximizing entity dangerous is the */superintelligence/* part. Given effectively infinite power, an entity with a paperclip-maximizing utility function will convert all available matter in the universe into paperclips. But a human-level intelligence, even a fast thinking one? It'll probably just open a factory or something. (Keep it away from nanotechnology and powerful AI, though.)

Anyway, I'd use my newfound time to study math and eventually enter a lucrative career in finance. I'd probably donate most the money earned towards high-impact research or effective charity, but a portion would go towards hiring private investigators to look into any AI or nanotechnology researchers that seem to be unusually interested in paperclips. (Eliezer Yudkowsky, I'm looking at you.)
:PROPERTIES:
:Author: Jace_MacLeod
:Score: 8
:DateUnix: 1444523353.0
:DateShort: 2015-Oct-11
:END:

*** I feel like this is the correct answer. If I know Clippy exists, then its Prime Directive will make it very predictable and fairly easy to identify and neutralize. As long as I don't make myself smart enough to design an Artificial Intelligence (which doesn't seem like a possibility with this power), Clippy won't be able to either, which means the damage he can do is minimal and not necessarily even dangerous.
:PROPERTIES:
:Author: DaystarEld
:Score: 2
:DateUnix: 1444770821.0
:DateShort: 2015-Oct-14
:END:


** If there is no need for me to personally stop the paperclip then I would go with 0%, keeping the paperclip entity as harmless as possible while I gather allies who can help me.
:PROPERTIES:
:Author: VVhaleBiologist
:Score: 7
:DateUnix: 1444493831.0
:DateShort: 2015-Oct-10
:END:

*** [deleted]
:PROPERTIES:
:Score: 8
:DateUnix: 1444495951.0
:DateShort: 2015-Oct-10
:END:

**** u/VVhaleBiologist:
#+begin_quote
  If anything, to guarantee a win you should increase the speed by -50%, take the hit for the team.
#+end_quote

The instructions were only about speeding up the brain and not slowing it down. If it's a more reflective bond than that then my thought was to turn to drugs after gaining allies. I'd like to see the paperclip entity try to to anything constructive while tripping balls.
:PROPERTIES:
:Author: VVhaleBiologist
:Score: 4
:DateUnix: 1444497159.0
:DateShort: 2015-Oct-10
:END:


*** With 0, you should be gathering allies to beat other actual people who right now have gaining money as their goal and who don't care about other things.
:PROPERTIES:
:Author: kaukamieli
:Score: 3
:DateUnix: 1444510133.0
:DateShort: 2015-Oct-11
:END:


** I'd go for 10x speed, maybe a bit slower so I can keep social contact. One human-intelligence paperclipper isn't a real threat, and making it effectively 10x as powerful still wouldn't make it be a threat. There are dozens of people that already want to destroy the world, a few more won't make much of a difference.
:PROPERTIES:
:Author: ulyssessword
:Score: 3
:DateUnix: 1444506304.0
:DateShort: 2015-Oct-10
:END:


** I'm not entirely convinced that just having more time to think is going to make me very much smarter - suspect I'd still find myself constrained by limits on the complexity and depth of my thoughts even with more time to consider.

Might take a small boost just for those times when being a bit more quickwitted /would/ help, but not so much that I start to find conversation annoyingly lag-prone. I doubt Clippy will get very far with that against the combined self-interest of the rest of the human race.
:PROPERTIES:
:Author: noggin-scratcher
:Score: 3
:DateUnix: 1444507807.0
:DateShort: 2015-Oct-10
:END:


** This would be more interesting if the same offer is presented to Clippy!me. In that scenario I'd bide my time, outsourcing personal assistants to spend all their time working for me searching the internet for word on increased paperclip production. If I can find him, and have 90% confidence it's him, I'd make my move.

My winning condition would be his permanent paralysis, so I can take the boost while he still lives, as per whisper's suggestion. If the boost only lasts so long as Clippy!me lives, then we should stop talking about it in a public forum.
:PROPERTIES:
:Author: TennisMaster2
:Score: 2
:DateUnix: 1444500895.0
:DateShort: 2015-Oct-10
:END:

*** I'd probably start by trying to convince my alt self that paperclips are awesome.

If you manage to get yourself as an ally, you can crank up to infinity percent and just win.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 3
:DateUnix: 1444501896.0
:DateShort: 2015-Oct-10
:END:

**** Or the opposite - convince Clippy-you to change their utility function. Perhaps self-sustained biospheres rather than paperclips - they're recursive, too!
:PROPERTIES:
:Author: TennisMaster2
:Score: 1
:DateUnix: 1444502730.0
:DateShort: 2015-Oct-10
:END:

***** The difference is that normal me has a utility function at least vaguely aligned with the rest of humanity. Clippy me /has/ to get converts to get any chance of winning.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 1
:DateUnix: 1444504342.0
:DateShort: 2015-Oct-10
:END:


** 10

I then use this to win the randi prize, and tell the public about my evil twin. Paperclippers are noted and destroyed.

I live the rest of my life in luxury.
:PROPERTIES:
:Author: Nepene
:Score: 2
:DateUnix: 1444505577.0
:DateShort: 2015-Oct-10
:END:

*** This.

I don't think Clippy would actually be found, but they'd be limited enough by everyone else that things should be fine.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1444642242.0
:DateShort: 2015-Oct-12
:END:

**** Clippy may be stupid sometime.

In fact, ten times the thinking speed actually makes it easier to catch Clippy. My brain will look notably abnormal. Just brain scan any candidates.
:PROPERTIES:
:Author: Nepene
:Score: 2
:DateUnix: 1444648387.0
:DateShort: 2015-Oct-12
:END:


**** After that sit in controlled medical einvironment and switch to 100000x the times speed (so you are slowed down so many times). Clippy will be the other person that suddenly stopped communicating or doing anything.
:PROPERTIES:
:Author: ajuc
:Score: 1
:DateUnix: 1444723966.0
:DateShort: 2015-Oct-13
:END:


** My ideal path would be to cooperate with the paperclipper to fight against the empty blackness of space. There's far more to lose through than to gain through competition with an equal with superpowers.

We're not actually equals, however, if its value system is sufficiently inhuman. There are a lot of important unknowns in this scenario. What's the paperclipper's risk aversion? Do paperclips scale linearly in value? Do future paperclips matter more or less than current ones, and by how much?

Quite possibly, the paperclipper would be able to take advantages of relative weaknesses in my motivational system. I get bored easily, but the paperclipper presumably wouldn't. I would give in to torture, while the paperclipper might not. My chief advantage is that I'm a satisficer, and I don't particularly care whether there are ten quadrillion or twenty quadrillion happy humans. Additionally, because I care about other human beings, it's probably easier for me to cooperate with other humans than for the paperclipper to do so.

This suggests that I should set my internal clock at a point where my influence is important but not so strong that I could take over the world without significant help from friends. My goal would not be to carry humanity to a new plateau of technology all by myself, but to speed up our current rate of advancement.

I'd choose to speed up my brain by 5x or so. Interacting with other people would still be bearable at that speed, and I would have a lot of influence but not so much that the paperclipper could destroy the world.
:PROPERTIES:
:Author: chaosmosis
:Score: 2
:DateUnix: 1444507609.0
:DateShort: 2015-Oct-10
:END:


** I attempt to strike an alliance with the paperclipper. I'll use my powers to help produce paperclips, as long as he uses his to help save lives and improve the lot of humanity. We both benefit from cooperation; two heads are better than one.

Then I'll go for a 100x speedup. Enough to give us a powerful edge over normal humans and do things that nobody else could do, but not enough that we're unstoppable if the rest of the world decides we're too dangerous to keep alive and unrestrained.
:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1444514963.0
:DateShort: 2015-Oct-11
:END:


** Processing speed of human brains doesn't work that way -- at least we think it doesn't. Can I choose to draw more samples from my internal generative representations by some factor?
:PROPERTIES:
:Score: 2
:DateUnix: 1444525774.0
:DateShort: 2015-Oct-11
:END:


** +3000%. Then I'd ignore the paperclip maximiser. It won't make much headway anyhow, given that it'll be competing with other maximisers and it's only a 30x increase over my base intellect.
:PROPERTIES:
:Author: Sceptically
:Score: 2
:DateUnix: 1444556764.0
:DateShort: 2015-Oct-11
:END:


** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1444579367.0
:DateShort: 2015-Oct-11
:END:

*** Uh. Even at enormous speeds, you can't simulate universes. There simply isn't the hardware.
:PROPERTIES:
:Score: 1
:DateUnix: 1444659784.0
:DateShort: 2015-Oct-12
:END:


** Without self-replication technology (doesn't need to be nanotech) or a self-improving AI ala Celestia from Friendship is Optimal, the paperclipper is at most an annoyance. At most it could make enough money to build a bunch of paperclip factories, but it would be competing with the rest of humanity for resources, and its product (paperclips) doesn't bring in income to keep the process going -- it has to earn money elsewhere to keep the paperclip factories running.

Self-powered self-replicators or self-improving AI with the ability to take actions in the real world are a win scenario for the paperclipper. Both can be researched (or research can be funded) by the papercliiper without it revealing its aims.
:PROPERTIES:
:Author: therearetoomanydaves
:Score: 2
:DateUnix: 1444525920.0
:DateShort: 2015-Oct-11
:END:


** Go all in. Realistically, the paperclipper will not ever prove a danger, and the benefits to society and to me are well worth the cost.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 2
:DateUnix: 1444501689.0
:DateShort: 2015-Oct-10
:END:


** If I have the ability to increase and then decrease back to normal the speed of my thoughts, then I would go through life using it only in emergencies. For example, giving myself a slight 10% boost when I need to finish thinking about a question on a test or when thinking of a comeback in conversation, and cancel it afterwards. The times I use the speed boost will be variable and while the paperclipper probably could predict a few, like me using it only when I'm awake or at the same time in class, it's still a significant problem to need the speed boost at the same time as I do and only for a minor percentage.

But if I can only chose my speed in the beginning and there's no changing it, then I'd go for a 3% boost. Minor enough to not make me super-smart, but significant enough that for an improvement.

I doubt the paper clipper will be able to damages things significantly, but I'd keep my ears out for someone starting up companies to make paper clips.
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1444501618.0
:DateShort: 2015-Oct-10
:END:


** "...He'd ask for the force to go with it."
:PROPERTIES:
:Author: DCarrier
:Score: 1
:DateUnix: 1444543561.0
:DateShort: 2015-Oct-11
:END:


** This seems pretty easy: don't increase the thought speed of either you or the paperclipper. Think of it this way: we've got a scale, and on one side of the scale sit you and human civilization, and on the other side of the scale sits the paperclipper. Percentagewise the paperclipper is hopelessly outgunned, with 7 billion humans plus you on one side and it on the other. Now let's say you increase the thought speed of yourself and the paperclipper by 1000x. At that point the 7 billion humans start to fade in the background and it becomes more of a fair fight between you and the paperclipper.

My answer might change if I'm allowed to use my superfast thought on other problems too though.
:PROPERTIES:
:Score: 1
:DateUnix: 1444627404.0
:DateShort: 2015-Oct-12
:END:

*** u/MugaSofer:
#+begin_quote
  My answer might change if I'm allowed to use my superfast thought on other problems too though.
#+end_quote

You are. That's the idea, I think.
:PROPERTIES:
:Author: MugaSofer
:Score: 2
:DateUnix: 1444642317.0
:DateShort: 2015-Oct-12
:END:


** Honestly, slowing down your subjective time isn't really the best way to 'be smarter' in my opinion. There are problems I couldn't necessarily solve even with all the time in the world. There are only so many ideas I can hold in my mind at any one time. There's a limit to what I can hold in my memory.

I'd make it so I think slightly faster, between 1.5 and 3x and not really worry at all about contending with my paperclipping double. I mean, he's just one person, and he's still only as smart as a human.
:PROPERTIES:
:Author: Absox
:Score: 1
:DateUnix: 1444683306.0
:DateShort: 2015-Oct-13
:END:


** You have control over both your and his CPU speed - this is a huge advantage.

Sit safely in home watched by someone, and turn the time speed to 100000x for one hour. Wait 1+k4 hours - repeat. Wait another 1+k4 hours - repeat.And so on. When you are reasonably sure you caught your enemy when driving or in other dangerous situation he couldn't possibly survive thinking at 0.000001 the usual speed - stop doing this.

Now you are free to use any time speed you want.

Alternatively - persuade someone to put you into 3 months long pharmatological coma, and connect you to the required medical devices to survive. Swith time to 10000000x just before, switch time to usual after. (S)he most probably died of dehydration or starvation.
:PROPERTIES:
:Author: ajuc
:Score: 1
:DateUnix: 1444723406.0
:DateShort: 2015-Oct-13
:END:


** Anything more than perhaps double or triple speed would make interacting with other humans to be an unbelievable chore. Waiting so long for them to respond, becoming frustrated and such. I don't think I could deal with it without becoming a hermit. Imagine the slowest person you know. Now imagine that everyone you talk to begins to seem slower than them because you're so much swifter. Not fun. Increasing clock speed is only nice if everyone else does it as well.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1444762662.0
:DateShort: 2015-Oct-13
:END:


** I wouldn't worry too much about the paperclipper. I'd figure out how to best use the ability to my advantage, and count on the fact that everyone else on Earth is on my team against clippy, which reduces how threatening it is.
:PROPERTIES:
:Author: psychothumbs
:Score: 1
:DateUnix: 1444508796.0
:DateShort: 2015-Oct-10
:END:


** Can I run the speedup factor in reverse and go 1/2?
:PROPERTIES:
:Score: 1
:DateUnix: 1444524593.0
:DateShort: 2015-Oct-11
:END:

*** To ... slow yourself down? What, as a way of attacking the Paperclipper?
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1444642346.0
:DateShort: 2015-Oct-12
:END:

**** Yes. I will know when I'm going to slow myself down, and can be sure to not be driving or operating heavy machinery at that time. I can actually fuck over the paperclipper pretty good with very little risk to myself until I off him. Every time I take a lunch, or have a break at work, I can make him zone out in the middle of whatever work he does to finance himself. Etc.
:PROPERTIES:
:Score: 1
:DateUnix: 1444653056.0
:DateShort: 2015-Oct-12
:END:
