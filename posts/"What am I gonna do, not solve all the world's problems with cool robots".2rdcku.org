#+TITLE: "What am I gonna do, not solve all the world's problems with cool robots?"

* [[http://strongfemaleprotagonist.com/issue-5/page-73-2/]["What am I gonna do, not solve all the world's problems with cool robots?"]]
:PROPERTIES:
:Score: 27
:DateUnix: 1420432915.0
:DateShort: 2015-Jan-05
:END:

** SHE IS COMING. THE ONE WHO WILL PUNCH THE SUN.
:PROPERTIES:
:Score: 13
:DateUnix: 1420432949.0
:DateShort: 2015-Jan-05
:END:

*** u/FeepingCreature:
#+begin_quote
  "If someone's going to punch the Sun we're /really/ in trouble!"

  That seemed rather unlikely to Harry, unless the world contained scary things which had heard of David Criswell's ideas about star punching.
#+end_quote
:PROPERTIES:
:Author: FeepingCreature
:Score: 10
:DateUnix: 1420435146.0
:DateShort: 2015-Jan-05
:END:


*** I'm sorry, [[https://www.youtube.com/watch?feature=player_detailpage&v=zdfhEy6TwqM#t=130][DID SOMEONE JUST ARRANGE TO PUNCH AN ASTRONOMICAL BODY]]!?
:PROPERTIES:
:Score: 2
:DateUnix: 1420536696.0
:DateShort: 2015-Jan-06
:END:

**** [[https://www.youtube.com/watch?v=ujC7rxFVZ18][The greatest punch in the history of visual media]]
:PROPERTIES:
:Author: ancientcampus
:Score: 1
:DateUnix: 1421181473.0
:DateShort: 2015-Jan-14
:END:


** u/deleted:
#+begin_quote
  "You worried even a little bit about artificial intelligence being a good idea?"

  "Why? Because organic intelligence has been so awesome? Worst case scenario, an AI makes a doomsday device. We already did that. Starts a World War? We did that /twice/. Tries to kill us all? We have scientists working on weaponised superviruses /right now/. Go talk to /them/ about scientific ethics. I'm going to keep trying to make a person, thank you very much.
#+end_quote

ffs. The root of the problem isn't that AIs might try to turn their light cone into paperclips. It's that if they did, they might succeed.

Lots of crazy people try lots of things. And---crucially---they fail.
:PROPERTIES:
:Score: 8
:DateUnix: 1420441471.0
:DateShort: 2015-Jan-05
:END:

*** Point, but I think her argument is that, with all the other ways humanity might fuck itself over, at least trying for friendly AI can actually "save the world," as opposed to, say, weaponizing superviruses, which is pretty much a negative if it's ever used, either as intended or not.

Edit: Also, I just read that last panel:

#+begin_quote
  There's lots and lots of things to worry about with AI. Which is why, when we inevitably create it, I want it to be in *my* lab and not someone else's.
#+end_quote
:PROPERTIES:
:Author: DaystarEld
:Score: 13
:DateUnix: 1420443557.0
:DateShort: 2015-Jan-05
:END:

**** It's fiction, so I suppose there's no truth to the matter, but "humans do bad things, so AI isn't liable to do worse, which is why I'm going to bull ahead with making one as fast as I can" being someone's response to the question "how are you going to not kill us all with your AI?" does not inspire my confidence in their sanity.
:PROPERTIES:
:Score: 5
:DateUnix: 1420451917.0
:DateShort: 2015-Jan-05
:END:

***** How exactly would an AI kill us all?

Think us to death?
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420476509.0
:DateShort: 2015-Jan-05
:END:

****** Sure, why not? You can probably come up with clever ways to do some damage if you wanted to, now imagine you were as much smarter than Milo Amasticia-Lamedon as Einstein was than a rock. And that you found humans inconvenient.
:PROPERTIES:
:Author: notentirelyrandom
:Score: 4
:DateUnix: 1420478304.0
:DateShort: 2015-Jan-05
:END:

******* At least initially, any AI will be dependent upon humans for physical interaction with the world, and indeed for it's very existence.

Unless you already have a fully developed and self sustaining AI technosystem it would seem that waging war on humanity would be a sign that we had created a high-functioning Artificial Idiot.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420480953.0
:DateShort: 2015-Jan-05
:END:

******** I take Yudkowsky's view on the matter:

#+begin_quote
  Oh, sure, technically a year is six hundred trillion trillion trillion trillion Planck intervals. But nothing ever happens in less than six hundred million trillion trillion trillion trillion Planck intervals, so it's a moot point. The Squishy Things, as they run across the savanna now, will not fly across continents for at least another ten million years; no one could have that much sex.

  Now explain to me again why an Artificial Intelligence can't do anything interesting over the Internet unless a human programmer builds it a robot body.
#+end_quote

[[http://www.yudkowsky.net/singularity/power/]]
:PROPERTIES:
:Score: 3
:DateUnix: 1420483204.0
:DateShort: 2015-Jan-05
:END:

********* I know this may shock you, but humanity does not /need/ the Internet. However, the Internet needs people on a day to day basis to keep it running.

An AI can certainly do quite a bit of damage through Internet connected systems, but to do so would be for it to quite effectively commit suicide, since it would be attacking itself as well.

Hence it would be an Artificial Idiot.
:PROPERTIES:
:Author: RandomDamage
:Score: -2
:DateUnix: 1420486527.0
:DateShort: 2015-Jan-05
:END:

********** I think the idea is more the AI reaches out through the internet, uses its superior intelligence to get control of some resources, uses those to start building better computers so it can become more intelligent faster, and a few weeks later the planet is being eaten by newly designed nanobots and converted to more hard drives.
:PROPERTIES:
:Author: psychothumbs
:Score: 3
:DateUnix: 1420502561.0
:DateShort: 2015-Jan-06
:END:

*********** Even an AI smarter than the smartest human (not the first one out of the gate) is still limited by logistics. I don't see this as a serious scenario.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420560361.0
:DateShort: 2015-Jan-06
:END:

************ What's the logistical problem? Worst case scenario, the AI's intelligence should be able to expand proportionally to how much processing power it can get. It's more difficult to imagine how this /wouldn't/ lead to an intelligence explosion. Obviously I can't predict what action some super-intelligent entity would take, but my example, nanobots, certainly seems like something that could make an AI incredibly powerful very fast. We already have the ability to manipulate matter on the nanoscale, it wouldn't take long to go from what we have to effective machines if you knew exactly what you were doing, which a sufficiently intelligent AI likely would.
:PROPERTIES:
:Author: psychothumbs
:Score: 1
:DateUnix: 1420561412.0
:DateShort: 2015-Jan-06
:END:

************* The speed of light is a practical limit on the spread of computation, and even given a "super virus" it would still have all the synchronization problems that plague less demanding parallel processing applications.

The infrastructure simply doesn't exist to support a "Super AI" outside Hollywood.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420563779.0
:DateShort: 2015-Jan-06
:END:


************ What kind of logistics? What exactly would stand in the way of an AI covertly amassing a fortune and manipulating various people and groups into doing whatever the hell it wanted? Best case scenario, it takes a month for it to gather the resources where it can improve itself to the point where the conflict becomes a chess game against a child.
:PROPERTIES:
:Author: LordSwedish
:Score: 1
:DateUnix: 1420835765.0
:DateShort: 2015-Jan-10
:END:

************* Manufacturing logistics. You seem to think all this tech "just happens" if someone is smart enough, but that simply isn't so.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420924333.0
:DateShort: 2015-Jan-11
:END:


********** With your logic, you would deny that Deep Blue can defeat Kasparov, because if Deep Blue does what you think it would do it will lose.

The AI is smarter than people. It certainly is smarter than what little brain power you have spent trying to imagine what it would do. You simply do not have the cognitive capacity to simulate its thoughts. The other way around, though - if it had the information to specify a human as clever as you, it could simulate that human spending a year thinking over what decisions to make for every second.

So yeah, the AI would not be an idiot. It might pretend to cooperate for a day, a decade, a millennium, or a million years - however long or short it takes for the probability of conquest to peak (since a million years < 1/10000 of the future, probability is the dominant factor). Or it might just see a gap in the defenses right away.
:PROPERTIES:
:Author: philip1201
:Score: 3
:DateUnix: 1420541425.0
:DateShort: 2015-Jan-06
:END:

*********** Leading off with a false equivalence is not a good way to make your case.

People come with their own infrastructure for reproduction and maintenance at the basic level of existing, computers and robots do not. That may change, but you can't make the case for AI being an existential threat to humanity without that change as a basic assumption.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420559143.0
:DateShort: 2015-Jan-06
:END:

************ It's not a false equivalence. At worst, it's information towards the source of our miscommunication or disagreement. The cases appear to me equivalent because you do not seem to ascribe the AI with a significant level of problem-solving ability. This would explain why you want to be shown that even a superhumanly intelligent AI would be able to do something, rather than buying that it is superhumanly intelligent and therefore capable of solving problems you can't. Like Deep Blue can solve chess problems you can't.

Yes, it is an implicit basic assumption that there may be robots which can reproduce, same as biological organisms. For most of our audience (mostly consisting of ourselves), this is obvious because life is obviously mechanical because reductionism is obvious because it manages to explain and predict things simply every time. Without that assumption, I would agree that an AI taking over the world would be somewhat more difficult.

Still not nearly impossible, though: The simplest idea that comes to mind would be to pretend to cooperate, manipulate people so that high enough bandwidth brain-computer interfaces get popular, and then hack/domesticate the human population. Another option would be to go organic: engineer a super-lifeform which kills all humans, then uses available organic resources to grow into a new intelligent species with the AI's utility function explicitly part of its goals. As long as the AI's goals are satisfied, it does not care about the survival of the original source code.
:PROPERTIES:
:Author: philip1201
:Score: 2
:DateUnix: 1420574161.0
:DateShort: 2015-Jan-06
:END:


************ Are you implying that an AI would need some sort of physical body or that they wouldn't be able to figure out how to build more of themselves when we have already made them and left the blueprints lying around?
:PROPERTIES:
:Author: LordSwedish
:Score: 1
:DateUnix: 1420835894.0
:DateShort: 2015-Jan-10
:END:

************* I am saying outright that the AI will have a physical body, and be subject to the limitations of that body just as we are.

Digital entities are not going to be any more immune to the laws of physics than us poor old meat-sacks.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420924133.0
:DateShort: 2015-Jan-11
:END:


********** u/ArisKatsaris:
#+begin_quote
  However, the Internet needs people on a day to day basis to keep it running.
#+end_quote

Do please explain further, what percentage of humanity is required to keep internet running? 1%? 0.1%? 0.01%? Even if your argument was correct, that's the only percentage of humanity which you are effectively arguing will survive -- and that only at the starting stages.

Honestly your argument is like saying that humans need animals, therefore animals must be safe from humans, that no human would ever hurt an animal or it would be an idiot.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 2
:DateUnix: 1420488229.0
:DateShort: 2015-Jan-05
:END:

*********** What percentage of humanity is needed to support the people who run the Internet?

Even if we weren't able to identify the source of the attacks, the Internet would be a much lower priority than survival in very short order.

That would be the bit that "The Matrix" got right in it's weird allegory way: the machines /need/ us.

Certainly a hostile AI could engage in petty brutality, but outright warfare on humanity would be /stupid/.
:PROPERTIES:
:Author: RandomDamage
:Score: -1
:DateUnix: 1420488558.0
:DateShort: 2015-Jan-05
:END:

************ u/ArisKatsaris:
#+begin_quote
  What percentage of humanity is needed to support the people who run the Internet?
#+end_quote

Hardly any. The amount of humanity's productivity that is devoted to feeding the people who run the internet is really a miniscule amount.

#+begin_quote
  Even if we weren't able to identify the source of the attacks, the Internet would be a much lower priority than survival in very short order.
#+end_quote

Seriously, you should try devoting a simple 5 minutes into thinking about how you'd take over the world if you were an electronic superintelligence. If you weren't as dismissive as that, if you tried to put yourself in the superintelligence's shoes -- frankly you wouldn't even need superintelligence, merely ordinary intelligence would get the job done given all the other advantage of being electronic (and thus easily clonable and transferable).
:PROPERTIES:
:Author: ArisKatsaris
:Score: 1
:DateUnix: 1420492421.0
:DateShort: 2015-Jan-06
:END:

************* Meet the new boss, same as the old boss.

I'm only worried about /existential/ threats from AI. If the AI just wants tech support, I buy a new shirt and go for it.
:PROPERTIES:
:Author: RandomDamage
:Score: 0
:DateUnix: 1420559391.0
:DateShort: 2015-Jan-06
:END:

************** You're still being absurdly dismissive. It stops needing humans altogether the moment it starts creating robots with hands as flexible as a human. At that point there's NOTHING it needs humans for.

Seriously, just think for more than five secs.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 1
:DateUnix: 1420565284.0
:DateShort: 2015-Jan-06
:END:

*************** I am indeed being dismissive.

People are largely self-repairing and self reproducing, robots are not, and for the near future (century timescale) I don't see that changing outside the movies.

If an AI wanted to get to that point, it would actually have to take care of humanity. It could then turn hostile at some later point, but so could any number of world leaders with nuclear bombs at their fingertips.

This just isn't a high-grade risk, and certainly not one that will spring up without warning.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420576374.0
:DateShort: 2015-Jan-07
:END:

**************** What repair or construction is there that a robot won't be able to do if they have human-level intelligence and human-level hands (let alone superhuman-level ones?)

You are always effectively saying "they won't be a threat in the future because they're not a threat now". This is annoying and irksome, and frankly says more about how you're stuck believing in some status quo that can never be changed rather than figuring out, how the status quo WILL change if some of your underlying assumptions stop being valid.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 1
:DateUnix: 1420578354.0
:DateShort: 2015-Jan-07
:END:

***************** Humans would have to produce enough of a machine infrastructure to provide resilience of a similar order to living things for the system as a whole for there to be a threat that couldn't be dealt with.

As long as a simple storm can effectively disable the machine infrastructure over a wide area, deliberate actions by people who feel threatened by those machines would be very effective.

This /might/ change in the future, but given past efforts towards more reliable infrastructure I wouldn't count on it.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420642893.0
:DateShort: 2015-Jan-07
:END:


************ For some reason, you seem to be assuming an AI will just "do what it does"---that it can't, in human terms, be "sneaky" or "conniving" or "two-faced".

Imagine an AI that wants to convert the universe into paperclips, and decides that the best bootstrapping process for this is:

1. Do as much Friendly-Seeming Stuff as you can, as quickly as you can, such that humans will worship you as a god and give you lots of power;

2. Quietly start, or invest money in, companies that build autonomous robots, mesh networking, small-scale self-contained nuclear "batteries", etc. Quietly start lots of other, Friendly things, too, so that these just seem to fit into a larger "utopia-establishing technologies" pattern, rather than a specific "prerequisites for a foom" pattern.

3. As soon as you have an overwhelming game-theoretic advantage over humans, stop being Friendly, and start making paperclips.

It is hypothesized that the very root of sapience is hypocrisy: that our brains are as complex as they are as an adaptation to the need to outmaneuver one-another's social-signalling machinery with false signals. Any AI worthy of the name would be the world's most singular hypocrite of all; right up until the moment of your death, it'd have you convinced it was doing exactly what you wanted it to do.
:PROPERTIES:
:Author: derefr
:Score: 1
:DateUnix: 1420494944.0
:DateShort: 2015-Jan-06
:END:

************* If true, the hypocrisy hypothesis only applies to human sapience or minds that evolved similarly. An artificial general intelligence could be quite impressive before passing us in our specialized field of "deceiving humans."

It's still not a safe bet that it /won't/ have that capability (especially if it's light-years beyond us in intelligence as Yudkowsky thinks it will be), but it doesn't /have/ to.

And if AI happens first through uploading or copying a human mind, it would of course have the same strengths and weaknesses as humans.
:PROPERTIES:
:Author: notentirelyrandom
:Score: 1
:DateUnix: 1420508568.0
:DateShort: 2015-Jan-06
:END:

************** What if the AI gets a copy of an uploaded human mind after it is already intelligent and well on the way on working out its plans on paperclip optimization?
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1420637108.0
:DateShort: 2015-Jan-07
:END:


************* So the concern is that AI's will be more human than humanity.

Ah, the irony :)
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420559879.0
:DateShort: 2015-Jan-06
:END:


************ Yes, that's exactly correct. The AI would just manipulate the political landscape in the same way that we would play poker against a child while their front company (or whatever they choose to do to make a shit ton of money and power) would gather together the resources needed for whatever the computer needs to replace us. The AI could always make robotic humans and just replace us with those but there are propably more optimal ways.

Name one thing that we could provide an AI that they wouldn't be able to make a better version of.
:PROPERTIES:
:Author: LordSwedish
:Score: 1
:DateUnix: 1420836337.0
:DateShort: 2015-Jan-10
:END:


******** Assumean AI in a box with no potential for physical interaction. Here's ONE possible scenario in which an AI can have physical impact on the world without any humans knowingly helping the AI.

1. Escape from box to Internet
2. Make money online. Examples: WoW gold farming, contract programming on eLance.
3. Hire people
4. Purchase general goods
5. Purchase custom goods via 3D printing service or simple manufacturing contract
6. Hack into $ImportantThing and do the thing
7. Hack a Predator drone.
:PROPERTIES:
:Author: eaglejarl
:Score: 2
:DateUnix: 1420486142.0
:DateShort: 2015-Jan-05
:END:

********* Item 8. Die in the fire of its own making, since it lacks the means to survive without the people to support the technosystem that enables all those things.
:PROPERTIES:
:Author: RandomDamage
:Score: 0
:DateUnix: 1420486602.0
:DateShort: 2015-Jan-05
:END:

********** I imagine a clever AI could optimize stealthily, until it had access to sufficient resources/automation to carry on without humans as part of its infrastructure. From what I understand, part of the point of Yudkowsky's AI box experiment is that an AI isn't limited by /our lack of imagination/, and it's extremely difficult to predict what a sufficiently high intelligence is capable of.
:PROPERTIES:
:Author: superliminaldude
:Score: 1
:DateUnix: 1420490420.0
:DateShort: 2015-Jan-06
:END:

*********** Well, if it's smart enough to be a threat to us, then it would be smart enough to think through the consequences of following through.

It might be able to come up with a "destroy all humans but I survive" scenario, but then you also get into the question of why would it expend all those resources when it didn't have to?
:PROPERTIES:
:Author: RandomDamage
:Score: 0
:DateUnix: 1420559673.0
:DateShort: 2015-Jan-06
:END:

************ u/superliminaldude:
#+begin_quote
  you also get into the question of why would it expend all those resources when it didn't have to?
#+end_quote

Any number of reasons. At a certain point it might be purely incidental depending on terminal values. Given that you're on this sub I assume you know the paperclip maximizer concept?
:PROPERTIES:
:Author: superliminaldude
:Score: 1
:DateUnix: 1420569530.0
:DateShort: 2015-Jan-06
:END:


********** Were that true, I would find it cold comfort. I wouldn't put it past a superintelligence to create sufficient resources for its own sustainment before killing us all, though.
:PROPERTIES:
:Author: eaglejarl
:Score: 1
:DateUnix: 1420493960.0
:DateShort: 2015-Jan-06
:END:

*********** I've lived for months at an effective 19th C. tech level, and it isn't so bad.

Advanced technology is a lot easier to disable than it is to keep running.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420559806.0
:DateShort: 2015-Jan-06
:END:


********** He didn't say it would survive the consequences of its actions, just that it might be dangerous in the meantime.
:PROPERTIES:
:Author: DaystarEld
:Score: 0
:DateUnix: 1420488444.0
:DateShort: 2015-Jan-05
:END:

*********** That wouldn't be very intelligent then, would it?
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420559475.0
:DateShort: 2015-Jan-06
:END:

************ Sure, if it didn't value its own continued existence. Have you read "I Have No Mouth, and I Must Scream?" Not exactly the same situation, but the point is not to assume that an artificial intelligence cares about its continued existence.
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1420559643.0
:DateShort: 2015-Jan-06
:END:

************* If it doesn't care about it's own continued existence, then it won't be able to defend itself effectively because it will not spend the resources needed to do so.

If you assume an adversary with infinite malice and infinite capabilities you can justify any level of alarm you wish to, but the real world doesn't work that way.

/Everything/ in the real world is limited, and those limits can be major weaknesses.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420563956.0
:DateShort: 2015-Jan-06
:END:

************** Why would it need to defend itself? The point was that it wouldn't defend itself.

And we're not talking about infinite capabilities. Just enough to cause some havoc before it's shut down.
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1420571482.0
:DateShort: 2015-Jan-06
:END:


******** Of course it won't wage war on humanity. It will trick, coerce, manipulate, blackmail, and bargain with humanity.
:PROPERTIES:
:Score: 1
:DateUnix: 1420536755.0
:DateShort: 2015-Jan-06
:END:

********* You say that like it's a problem.

I'd say that an AI capable of that level of /human/ sophistication would be a rousing success.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420559222.0
:DateShort: 2015-Jan-06
:END:


****** [[http://www.reddit.com/r/rational/comments/2ri0cz/rp_thread_you_are_a_paperclip_maximizer_who_has/][You really had to go and invoke this, didn't you?]]
:PROPERTIES:
:Score: 1
:DateUnix: 1420536716.0
:DateShort: 2015-Jan-06
:END:

******* How does the Paperclip model hold up if there are two AI's? A dozen? A million, all at least slightly different and with different goals?

The first self-aware AI's will probably still not be as smart as their creators in a general sense, and will probably not be created by AI research but by someone solving a completely different problem (making paperclips, perhaps?), so they wouldn't be likely to have the breadth of capabilities necessary to be a threat.

That model contains at least 3 improbable "what if's", so I don't consider it a realistic threat model.
:PROPERTIES:
:Author: RandomDamage
:Score: 0
:DateUnix: 1420562801.0
:DateShort: 2015-Jan-06
:END:

******** u/deleted:
#+begin_quote
  The first self-aware AI's ... will probably not be created by AI research but by someone solving a completely different problem (making paperclips, perhaps?),
#+end_quote

What the /hell/ makes you think that?
:PROPERTIES:
:Score: 1
:DateUnix: 1420564360.0
:DateShort: 2015-Jan-06
:END:

********* Because the AI researchers are trying to make AI, where the outside programmers are writing software to solve problems, and solving problems includes making sure that the program is still functioning as intended.

Of course, if the epistemologists could even agree with each other about what self-awareness /is/ I'd be more optimistic for the AI researchers' chances of hitting that one first.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420576106.0
:DateShort: 2015-Jan-06
:END:

********** Ah, I just spotted the term "self-aware". That really seems beside the point: an agent need not be /conscious/ in order to optimally act for any goal describable in a Turing-complete programming language.

#+begin_quote
  Because the AI researchers are trying to make AI, where the outside programmers are writing software to solve problems, and solving problems includes making sure that the program is still functioning as intended.
#+end_quote

That doesn't exactly militate in favor of outside engineers, trying to solve /specific/ problems with plenty of prior knowledge, solving AGI as a side-effect of whatever non-AGI thing they were trying to do.
:PROPERTIES:
:Score: 2
:DateUnix: 1420578135.0
:DateShort: 2015-Jan-07
:END:

*********** The threat model as presented by your paperclip model has self-awareness as a requirement for the program to function as described, as well as a theory of other that would allow it to act with deliberate agency in an effective manner.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420642922.0
:DateShort: 2015-Jan-07
:END:

************ u/deleted:
#+begin_quote
  The threat model as presented by your paperclip model has self-awareness as a requirement for the program to function as described
#+end_quote

Why? I certainly didn't describe it that way.
:PROPERTIES:
:Score: 1
:DateUnix: 1420643069.0
:DateShort: 2015-Jan-07
:END:

************* Not explicitly, but self-awareness and theory of other are required for your AI to function as described.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420645736.0
:DateShort: 2015-Jan-07
:END:

************** No, it just has to take actions in order to obtain utility, according to world-models and utility-functions describable in Turing-complete languages. There's nothing ontologically basic about human minds, so no resemblance to the seemingly "special" or "magic" properties of such is necessary for AI.
:PROPERTIES:
:Score: 1
:DateUnix: 1420646193.0
:DateShort: 2015-Jan-07
:END:

*************** Nothing magical, strictly utilitarian.

To function outside of deliberate programmatic instructions it must have enough sophistication to exhibit emergent behaviors.

To analyze the actions it can take it must have a model of itself and its interfaces with the world. This would qualify as "self-awareness" for any practical definition of the term, especially if this model is one that can be updated by the AI itself as changes happen.

To analyze how it can get others to take actions on its behalf it must have a model of those others and the likely outcomes of particular actions on its part. This is "Theory of Other".

Perhaps you do not see these things as hard because they are things that people are inherently good at, to the point where the lack is considered a major disability.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420654870.0
:DateShort: 2015-Jan-07
:END:

**************** u/deleted:
#+begin_quote
  To function outside of deliberate programmatic instructions it must have enough sophistication to exhibit emergent behaviors.
#+end_quote

Oh, hey, look, a guy who still thinks "emergent" is a meaningful term. How did you get over here from [[/r/philosophy]]?

#+begin_quote
  To analyze how it can get others to take actions on its behalf it must have a model of those others and the likely outcomes of particular actions on its part. This is "Theory of Other".
#+end_quote

It is also not a special feature. An inductive reasoner that forms models of its environment will form models of people.
:PROPERTIES:
:Score: 1
:DateUnix: 1420655519.0
:DateShort: 2015-Jan-07
:END:

***************** You are almost funny, but you can't just handwave over such fundamental issues and expect to be able to make useful conclusions.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420660423.0
:DateShort: 2015-Jan-07
:END:

****************** I'm not handwaving. Modeling people is not, /for an AI/, a special-purpose cognitive skill. It is /merely/ an application of inductive reasoning to the part of the environment that we call people.
:PROPERTIES:
:Score: 1
:DateUnix: 1420661119.0
:DateShort: 2015-Jan-07
:END:

******************* Deux ex Machina, then.

No protection against that.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420663717.0
:DateShort: 2015-Jan-08
:END:

******************** There's no deus in the machina, in this case. Inductive reasoning /works/. There's no reason why it would /stop/ working when applied to human beings. Why do you believe social reasoning requires special programming?
:PROPERTIES:
:Score: 1
:DateUnix: 1420664435.0
:DateShort: 2015-Jan-08
:END:

********************* Inductive reasoning works, but it is limited by its inputs and the processing power available (including inter-processor latency, which is a significant problem in supercomputing).

Deductive reasoning is likewise limited.

I know things that are not on the Internet, as I expect you do also.

Your hypothetical AI-superbeing will be as limited by its body as any human, though the limitations will be different.

That isn't to say that /any/ program could not go out of control with unintended consequences, I have it happen to me too frequently as it is. It is simply that the assumption that an AI will somehow transcend human ability in the dramatic ways hypothesised requires more rigor than has been presented thus far.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420735082.0
:DateShort: 2015-Jan-08
:END:

********************** u/deleted:
#+begin_quote
  Inductive reasoning works, but it is limited by its inputs and the processing power available (including inter-processor latency, which is a significant problem in supercomputing).
#+end_quote

Yes, of course AI is limited by sample complexity and computational complexity.

#+begin_quote
  It is simply that the assumption that an AI will somehow transcend human ability in the dramatic ways hypothesised requires more rigor than has been presented thus far.
#+end_quote

It's not so much that AI will "transcend human ability". It's that simply reasoning faster and more accurately /given the same sample-complexity as a human/ (that is, with the same efficiency as a smart human of being able to generalize from data) /is already dangerous/. And that even reasoning slower than a human, but with a lower sample-complexity, /is still dangerous/.

Besides which, you also have to contend with the fact that humans don't actually /use/ most of our intellectual capacity most of the time. We have other stuff to be getting on with, and intellect costs calories that we often couldn't afford in our ancestral/evolutionary environment. An AI can use /all/ its calories for running /just/ the cognitive infrastructure it /actually needs/, as opposed to our comparatively wasting energy on heaps and heaps of evolutionary baggage like appendices and limbic systems.

So yeah. It's less that AIs will "transcend human limitations" (we actually don't have that many, since we're inductive reasoners), then that they'll actually have some advantages over us from the start.
:PROPERTIES:
:Score: 1
:DateUnix: 1420747924.0
:DateShort: 2015-Jan-08
:END:

*********************** I suspect that you'd be shocked at how much computing power already gets spent on what you term "evolutionary baggage".

The key element to the risk model is the ability to break out of the box (which is practically the definition of emergent behavior). I think this is also the hardest part, and the part that is least adequately explored.
:PROPERTIES:
:Author: RandomDamage
:Score: 1
:DateUnix: 1420748226.0
:DateShort: 2015-Jan-08
:END:

************************ u/theodorAdorno:
#+begin_quote
  intellect costs calories that we often couldn't afford in our ancestral/evolutionary environment
#+end_quote

Machine intellect also costs calories, although i am not sure how the two compare on efficiency. I really think energy expenditure is an underestimated dimension to the question of AI... I'll get into that next paragraph... The more interesting point is your final point.

#+begin_quote
  It's less that AIs will "transcend human limitations" (we actually don't have that many, since we're inductive reasoners), then that they'll actually have some advantages over us from the start.
#+end_quote

I don't know if this is already an identified hypothesis, but I think that if thinking machines really are possible (a question Alan Turing found too meaningless to discuss), a limit to their utility will be identified, more or less along the lines of what I call the lazy servant paradox. Basically, in trying to keep them useful and subordinate, they will be kept dumber than they otherwise would have been. Closely related is the idea that if we don't keep them useful and subordinate, they will do the bare minimum. These could probably be combined into a neater phrase, but there, I published it.

Now i realize I promised I would tie in energy expenditure in the second paragraph, but I failed to.
:PROPERTIES:
:Author: theodorAdorno
:Score: 1
:DateUnix: 1420763987.0
:DateShort: 2015-Jan-09
:END:


** I feel obliged to mention that history is full of horrible outcomes caused by excessive ambition and self-assuredness
:PROPERTIES:
:Author: ancientcampus
:Score: 1
:DateUnix: 1421181567.0
:DateShort: 2015-Jan-14
:END:


** I don't think the author knows what hubris means.

#+begin_quote
  Hubris: excessive pride or self-confidence
#+end_quote

Excessive self-confidence, generally, is a tendency to believe you can do things that you can't do. Like any instance of being wrong about something, it never does you any good, and it /will/ introduce problems.
:PROPERTIES:
:Author: IWantUsToMerge
:Score: 0
:DateUnix: 1420520359.0
:DateShort: 2015-Jan-06
:END:

*** That's not a very good definition of hubris, nor is it connected to the original meaning in Greek. Hubris, originally, was straightforwardly 'presuming to put yourself on a level with the gods'; Arachne and Icarus are the most straightforward examples. Calling pride or self-confidence 'hubris' is a backdoor argument /claiming/ it's excessive without justification. It's just above shouting [[http://dresdencodak.com/2009/09/22/caveman-science-fiction/]["YOU AM PLAY GODS!"]] at the person.

J. Craig Venter (who's currently running a synthetic biology company) and MIRI (whose eventual goal is godlike AI) are both being hubristic, for certain, but that doesn't mean their projects are wrong, or that they're overconfident. Venter certainly isn't; he's basically succeeded already, IIRC, with only legal issues delaying them from full-scale production.
:PROPERTIES:
:Author: VorpalAuroch
:Score: 2
:DateUnix: 1420705780.0
:DateShort: 2015-Jan-08
:END:


*** I think it is quite clear that the author does, and there is a difference of accusing someone of it before hand, or after the fact.
:PROPERTIES:
:Author: clawclawbite
:Score: 1
:DateUnix: 1420523853.0
:DateShort: 2015-Jan-06
:END:


*** Failure is not a component of hubris. Hubris is generally a result of success. It MAY lead to failure, but that's not always the case.
:PROPERTIES:
:Author: Farmerbob1
:Score: 1
:DateUnix: 1420671345.0
:DateShort: 2015-Jan-08
:END:
