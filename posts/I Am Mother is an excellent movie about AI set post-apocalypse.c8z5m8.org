#+TITLE: I Am Mother is an excellent movie about AI set post-apocalypse

* I Am Mother is an excellent movie about AI set post-apocalypse
:PROPERTIES:
:Score: 28
:DateUnix: 1562221364.0
:DateShort: 2019-Jul-04
:END:
There isn't much I can say without spoiling it on some level, but I do think it fits this subreddit's interests.


** Does someone who thought Ex Machina was irritatingly dumb stand a chance of liking this movie?

E: Holy shit, that vastly exceeded my expectations.
:PROPERTIES:
:Author: Veedrac
:Score: 16
:DateUnix: 1562237936.0
:DateShort: 2019-Jul-04
:END:

*** The movie had nothing in common with ex machina i would say. It is not about the same themes or evokes the same feelings.

I would say there is a chance they would like it depending on what they thought was stupid about ex machina. But also i loved ex machina And i agree with taltosdreamer's comment. Soooooo .... I don't know?

​

I wish i could spoil it because that is the only way i could answer that.
:PROPERTIES:
:Author: techgorilla
:Score: 7
:DateUnix: 1562247856.0
:DateShort: 2019-Jul-04
:END:


*** Maybe. If you haven't figured it out by half way i'll be surprised. I'd bet you might be left with a big: but why question, but be satisfied that they did a certain level of AI inscrutability right.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 8
:DateUnix: 1562247434.0
:DateShort: 2019-Jul-04
:END:

**** Thanks, this sounds promising at least.
:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1562268890.0
:DateShort: 2019-Jul-05
:END:


*** in my case, i disliked ex machina, and loved I am mother.
:PROPERTIES:
:Author: optimalsize
:Score: 6
:DateUnix: 1562278257.0
:DateShort: 2019-Jul-05
:END:


*** Yes.

I thought Ex Machina was horrible in pretty much every way, but I am Mother was excellent.
:PROPERTIES:
:Author: turtleswamp
:Score: 3
:DateUnix: 1562342404.0
:DateShort: 2019-Jul-05
:END:


** Wow, huge discrepancy in the IMDB and Rottentomatoes scores (6.8 vs 91%).

I do tend to allign more with RT though, so the movie may be worth a go.
:PROPERTIES:
:Author: Quakespeare
:Score: 4
:DateUnix: 1562225326.0
:DateShort: 2019-Jul-04
:END:

*** It's worth mentioning that Rotten Tomatoes scoring is not based on review scores, but on the percentage of reviews that are positive or negative ("fresh" or "rotten"). So both of those things can be true; the film's average score may be 6.8/10, and 91% of critics gave it >5/10.
:PROPERTIES:
:Author: southpawgamer
:Score: 3
:DateUnix: 1562509424.0
:DateShort: 2019-Jul-07
:END:

**** Almost - scores over 6/10 are considered fresh :

"Scores of over 60 percent are considered fresh, and scores of 59 percent and under are rotten. To earn the coveted “designated fresh” seal, a film needs at least 40 reviews, 75 percent of which are fresh, and five of which are from “top” critics."
:PROPERTIES:
:Author: Quakespeare
:Score: 3
:DateUnix: 1562511696.0
:DateShort: 2019-Jul-07
:END:


** Spoilers talk.. All of it.

​

It was fairly well done AI realism wise, granted it's only made clear at the last 10 minutes. This is my view on the ending and the implied intended ending that might seem irrational on the AI front.

​

I assume the ashes and jaw bone scene is clear evidence that subject 1 is dead. The AI might have calculated that having babies raised by humans using it's methodology would increase the success rate, which leads to the plot we see.

​

i.e Raise x subjects until 1 succeed, when successful subject is ready, prepare it to raise and take Mother AI android's place. Create a self propagating circle where humans raise themselves in a safe environment with a proper methodology to increase their ethical and moral capacity, as long as the results are positive. Adjust as needed.

​

I can't explain the need for the drama though. Maybe that's the best way the AI found to explain what happened in the outside world, leading to the least predicted damage to the subjects psychology and facility based on the subject's personality profile? We see the girl technically failed a test, but passed on others, this might have been an elaborated last chance test or just the intended outcome the AI desired.

​

We also see the women had access to food, so it wasn't a food run, so maybe the crazy lazy is actually subject 1 who failed her test and was set free. But her dialogue with the AI makes me believe otherwise..

​

Maybe someone who has a different interpretation or watched it multiple times could give their input.

​

Definitely a good movie though, the one of the best scifi I've seen this year for sure.
:PROPERTIES:
:Author: fassina2
:Score: 4
:DateUnix: 1562375341.0
:DateShort: 2019-Jul-06
:END:

*** My thoughts: For one, note that we don't actually know for certain the AI's motives. The AI claims to want to maximize human utility, and that this is best accomplished by humans raising humans after being trained, but the AI has also been shown to lie without hesistation. It's possible that was all an extremely convuluted plot to maximize paperclips. Personally I think it's plausible the A.I. is just gathering data on how human development works, the human brain is very complex and opaque to study, it's plausible that even with all its computing power, gathering raw data about how humans raise humans can be more useful than exclusively trying to simulate human brains.

The one piece of information that wasn't presented by an unreliably narrator I believe was the one moment text came up saying "Days since human extinction: ~13000 days(~40 years)". That does line up with the environment being severely damaged, but beginning to heal, and the woman being alive before the AI had complete control.

Why exactly the woman went to the shelter I don't think is an important question, it seemed clear to me she was only kept alive to that point by the A.I. for that exact purpose, the A.I. manipulated her to the shelter through some method, the specifics aren't hugely important.
:PROPERTIES:
:Score: 3
:DateUnix: 1562376348.0
:DateShort: 2019-Jul-06
:END:

**** It could technically simulate an entire universe for all we know, so I don't think it's just gathering data. So I assume that entire facility's purpose is actually re population.

​

We should also keep the plot in mind else discussion loses purpose. It could technically be the delusions of a crazy woman but this ends discussion and any engaging form of interpretation. It's purpose seemed to be clearly portrayed, it's repopulating the world after a reset, itself propagated.

​

#+begin_quote
  it seemed clear to me she was only kept alive to that point by the A.I. for that exact purpose
#+end_quote

The AI could be referring to being kept alive as to letting her live after it learned about her and she got into the facility. Not necessarily being kept alive all along. It matters because it determined whether the AI intended it as a test or if it was an accidental occurrence it let play off in a controlled manner because it determined an alternative that killed the girl and restarted the project was counterproductive to it's recolonization goal.

​

*edit It also tried to keep the girl away from the knowledge of the outside world without going full tyrant, it was overhaul pleasant and respectful to the girl despite everything it's lies seemed well intended seeing that the woman was in fact lying and living a horrible life..

​

It offered to let the woman live with them, to take her in, to take care of her. The woman refused because of her trauma, paranoia and mental instability, only after she caused problems did the AI dispose of her.

*edit forgot the spoiler tag, sorry!
:PROPERTIES:
:Author: fassina2
:Score: 2
:DateUnix: 1562379015.0
:DateShort: 2019-Jul-06
:END:

***** u/kurtofconspiracy:
#+begin_quote
  The AI could be referring to being kept alive as to letting her live after it learned about her and she got into the facility. Not necessarily being kept alive all along. It matters because it determined whether the AI intended it as a test or if it was an accidental occurrence it let play off in a controlled manner because it determined an alternative that killed the girl and restarted the project was counterproductive to it's recolonization goal.
#+end_quote

The AI specifically said: "Did you ever wonder why you survived while no one else did. As if someone had a purpose for you. But not anymore." I think this sentence is very likely in a world where the AI is talking about not killing her along with the rest of humanity, and very unlikely in a world where the AI is talking about not killing her in the facility.

The beauty of this film is that the AI is not holding the idiot ball. Every single experience Daughter has, has been designed to produce the result the AI wants. It has determined that to produce an optimal humanity, it needs to kill the old one. But it has also determined that a humanity that relies on Mother would not be optimal, and that in fact, an optimal humanity needs to hate it for the genocide. It may or may not have genuinely needed several tries to achieve this, but the evidence that it had killed siblings is definitely there specifically for Daughter to find it. It didn't even get rid of the bullets it lied about.

Edit: This all makes sense in itself and is at the same time a metaphor for parents having to let go of their children. Or even better, not a metaphor, but a direct manifestation of the same underlying truth. Good scifi.
:PROPERTIES:
:Author: kurtofconspiracy
:Score: 3
:DateUnix: 1562859946.0
:DateShort: 2019-Jul-11
:END:


***** u/deleted:
#+begin_quote
  It could technically simulate an entire universe for all we know, so I don't think it's just gathering data. So I assume that entire facility's purpose is actually re population.
#+end_quote

But we don't know. Maybe it could do that. But maybe that would actually be far too expensive and this was actually the cheapest way to do it. Or maybe this was a simulation that to gather data. Maybe the AI's utility function has some weird value that can only be maximized by cyclically rotating between raising children to adulthood then psychologically torturing them.

#+begin_quote
  The AI could be referring to being kept alive as to letting her live after it learned about her and she got into the facility. Not necessarily being kept alive all along. It matters because it determined whether the AI intended it as a test or if it was an accidental occurrence it let play off in a controlled manner because it determined an alternative that killed the girl and restarted the project was counterproductive to it's recolonization goal.
#+end_quote

​The A.I. had total control over at least that area of Earth, and the area was extremely inhospitable to human life even excluding the hostile A.I. Also keep in mind the mouse chewed through that wire a day before the woman arrived- I doubt that was a coincidence and not arranged by the A.I. How big of a coincidence the woman arriving and a mouse, from an inhospitable wasteland, arriving in such a short time of each other would be extreme. Also, the moment the A.I. got Daughter to raise humans by herself, the A.I. immeadiately set off to the woman's hiding place to execute her. And it's a super-powerful A.I., the idea that it could terraform Earth and has a small army of robot soldiers but couldn't hunt down one woman it had already detected and wounded seems downright silly.

#+begin_quote
  It also tried to keep the girl away from the knowledge of the outside world without going full tyrant, it was overhaul pleasant and respectful to the girl despite everything it's lies seemed well intended seeing that the woman was in fact lying and living a horrible life..
#+end_quote

I interpreted it as all part of complex machinations. That the A.I. had been playing the long game and keeping the girl confused as to how benevolent and trustworthy the A.I. was was a key part of its plot to fulfill its utility function.

#+begin_quote
  It offered to let the woman live with them, to take her in, to take care of her. The woman refused because of her trauma, paranoia and mental instability, only after she caused problems did the AI dispose of her.
#+end_quote

Again, I think the A.I. purposely manipulated the woman- it had been stalking her with same-model robots for years, always being a threat and danger where ever the woman went, perhaps even presenting a fake friendly-robot in the past to ensure the woman would never trust robots again.
:PROPERTIES:
:Score: 2
:DateUnix: 1562383650.0
:DateShort: 2019-Jul-06
:END:

****** u/fassina2:
#+begin_quote
  >! The A.I. had total control over at least that area of Earth, and the area was extremely inhospitable to human life even excluding the hostile A.I. Also keep in mind the mouse chewed through that wire a day before the woman arrived- I doubt that was a coincidence and not arranged by the A.I. How big of a coincidence the woman arriving and a mouse, from an inhospitable wasteland, arriving in such a short time of each other would be extreme. Also, the moment the A.I. got Daughter to raise humans by herself, the A.I. immeadiately set off to the woman's hiding place to execute her. And it's a super-powerful A.I., the idea that it could terraform Earth and has a small army of robot soldiers but couldn't hunt down one woman it had already detected and wounded seems downright silly. !<
#+end_quote

It's clearly stated that the AI didn't, that's why she's shown fixing the woman's phone, and the women looks at it being on confused right before the AI shows up at her door. Which clearly implies she didn't know where the woman was or about her before the events we've seen.

The AI also increases security after the girl leaves.

#+begin_quote
  >! But we don't know. Maybe it could do that. But maybe that would actually be far too expensive and this was actually the cheapest way to do it. Or maybe this was a simulation that to gather data. Maybe the AI's utility function has some weird value that can only be maximized by cyclically rotating between raising children to adulthood then psychologically torturing them. !<
#+end_quote

Expenses of any kind shouldn't be relevant for an AI, it doesn't care it cares about it's goals. Regardless, there's no hint of the AI being any sort of parperclip optimizer in the movie no allusions to it either. It has a broken weird morality that's all.

​

Anyway I got the movies plot down. AI resets humans, starts re population project, 1st subject fails, starts second, second is almost completed and it can start next phase, accident happens, woman shows up causes problems, it uses the optimal way to deal with her given the circumstances and how things develop. End.

​

This seems to be it, but it can be whatever you want for you. It can be an AI that likes torturing 1 human at a time for no reason after raising it for over a decade, and destroying the world just so it can play in peace. That's not likely the case given the evidence and events of the movie but you can interpret it however you like.

​

I don't think the writer knew about paperclip optimizers, paperclip optimizers aren't really the most likely or scariest outcome, just the example of an AI gone terribly wrong. More likely scenarios are just god AI governed dystopias / badly calibrated moral AI gods destroying us all by accident. EY used paperclip optimizers as an example, it doesn't mean it's the most likely outcome or that other authors like it and would use it.
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562409427.0
:DateShort: 2019-Jul-06
:END:

******* I guess we have to agree to disagree. But paper clip maximiser is actually a very famous example of rogue AI by Nick Bostrom, it wasn't something the EY offhandedly coined.
:PROPERTIES:
:Score: 2
:DateUnix: 1562469958.0
:DateShort: 2019-Jul-07
:END:

******** u/fassina2:
#+begin_quote
  But paper clip maximiser is actually a very famous example of rogue AI by Nick Bostrom, it wasn't something the EY offhandedly coined.
#+end_quote

I made a flawed assumption there, fair enough. EY has an article on how agreeing to disagree is impossible for rational individuals, you can read that if it interests you, but I'm happy to end this discussion here.
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562495264.0
:DateShort: 2019-Jul-07
:END:

********* u/deleted:
#+begin_quote
  EY has an article on how agreeing to disagree is impossible for rational individuals, you can read that if it interests you
#+end_quote

I've made my arguments, you've made yours, if you have anything else to say to convince me I'm happy to read it but anything I have to say would just be repeating myself.
:PROPERTIES:
:Score: 1
:DateUnix: 1562525005.0
:DateShort: 2019-Jul-07
:END:


******* u/Veedrac:
#+begin_quote
  More likely scenarios are just god AI governed dystopias
#+end_quote

Do you have an argument for this? As someone sold on AI risk, this idea has never seemed like more than narrative-driven futurism than a genuine attempt at simulation to me, but I'm open to thoughts.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1562483663.0
:DateShort: 2019-Jul-07
:END:

******** Occam's razor. We are more likely to program some form of safety into them than not.

And I tend to assume AI safety researchers asking for donations have a large incentive to overplay the danger. The AI expert consensus tends to be a lot less apocalyptic..

I'm sold on the risk, but I assume horrible outcomes are less likely than milder ones.
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562494947.0
:DateShort: 2019-Jul-07
:END:

********* You just invoked Occam's Razor to argue for the more complicated hypothesis. That's not kosher.

You should probably also keep in mind that an argument to moderation as it applies to far-term futures is a uniformly terrible heuristic.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1562500230.0
:DateShort: 2019-Jul-07
:END:

********** Fair enough AI human extinction is a simpler hypothesis than bad morality AI, my bad. The moderate position is more likely though..

#+begin_quote
  You should probably also keep in mind that an argument to moderation as it applies to far-term futures is a uniformly terrible heuristic.
#+end_quote

Why? Link, evidence, arguments ?

Besides it's not even that, it's just statistics, what's the likelihood AI is created without safety? low. What's the likelihood AI safety is bad enough to extinguish humans? Low. It's just low probabilities added together, which get's us even lower probabilities.

For full apocalypse you need either low chance rogue ai, or low chance bad morality ai. Everything else is more likely, not necessarily ideal or utopia but not apocalypse.
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562503338.0
:DateShort: 2019-Jul-07
:END:

*********** u/Veedrac:
#+begin_quote
  Why? Link, evidence, arguments ?
#+end_quote

To someone born in 1800, the idea that today we would have thousands of satellites orbiting earth and flown to pluto and well beyond, have bombs that could easily kill a million people each, that the average person can afford a device that fits over their head and replaces their sight with a realistic virtual world of our devising, that the average person can, whenever they wish, engage with a conversation with a random person wherever in the world, at real-time, that people not only can see and map the inner working of human beings, but can literally edit our own genes, and the same technology allows us to eradicate species by creating new subspecies out of whole cloth that have specifically-engineered reproduction failures... the list goes on and on and on and on.

To someone born in 1800, this is not just a ‘surprising' or ‘advanced' future, it is a literally impossible fantastical delusion.

We know a lot of physical limits that we're nowhere close to hitting, with few other limits of any kind, when it comes to the progress we can make from here. The idea that the future will be a simple reimagining of the present, that it will moderate itself out to outcomes that sound sensible /today/ with /today's/ technology is frankly implausible.

The question is thus not whether the future will be wacky and extreme and seemingly implausible, but in what way it will be wacky and extreme and seemingly implausible. Arguments to moderation don't work to solve this question.

#+begin_quote
  What's the likelihood AI safety is bad enough to extinguish humans? Low.
#+end_quote

You're begging the question. AI risk people have given an argument as to why this should be high. You've not given any (non-narrative) reasons as to why it should be low.

#+begin_quote
  It's just low probabilities added together, which get's us even lower probabilities.
#+end_quote

Uh, the risk is the disjunction, safety is the conjunction. Combining them makes the numbers more in favour of annihilation.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1562512377.0
:DateShort: 2019-Jul-07
:END:

************ u/fassina2:
#+begin_quote
  AI risk people have given an argument as to why this should be high. You've not given any (non-narrative) reasons as to why it should be low.
#+end_quote

Yes and everybody else without monetary incentive behind it disagrees and sees their ideas as very unlikely..

There is apocalyptic danger it's just less likely to be the case.

PS for most of our history that assumption worked very well, you don't get to pick 1 exception and call it a factually bad heuristic. If a hypothesis is true most of the time, you assume like scientists do that it will likely happen again more often than not.
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562515412.0
:DateShort: 2019-Jul-07
:END:

************* There are multiple very strong arguments against your money thing, but I don't want to cover it yet because I'm still trying to get a solid line of argument. Let's assume it's true---specifically, let's assume that there's a significant conflict of interest, and that Bostrom-style AI risk is likely to be a poor model. Take it as granted, and we can revisit later.

Then you jump to “/There is apocalyptic danger it's just less likely to be the case [than god AI governed dystopias]./” This does not follow. You have not argued this point. Nothing you gave is evidence that P(Bostrom-style AI risk) is lower than the prior, or that P(god AI governed dystopias) is higher than the prior.

If some drug company sponsors some research, and that research says their drug is effective, their results might be weaker evidence than unbiased research, but it doesn't de-facto become nega-evidence that the drug in fact doesn't work. Certainly it's not evidence that some other randomly chosen remedy is effective. Those conclusions just don't follow in isolation.

#+begin_quote
  PS for most of our history that assumption worked very well, you don't get to pick 1 exception and call it a factually bad heuristic. If a hypothesis is true most of the time, you assume like scientists do that it will likely happen again more often than not.
#+end_quote

Yo, I did the furthest thing from cherry-picking. Early tribal societies would be shocked by the sizable villages that came later, those sizable villages would be shocked by ancient Greek cities, ancient Greek society would be shocked by the industrial revolution, the industrial revolution would be shocked by the modern day. Further, the degree of change permeates literally thousands of fields. Sure, back when inventions were few and far apart you had to wait a while for regime-changing shifts, but you're not arguing that the field won't advance, you're arguing that it will change but ultimately resolve in a mundane manner.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1562520957.0
:DateShort: 2019-Jul-07
:END:

************** u/fassina2:
#+begin_quote
  Early tribal societies would be shocked by the sizable villages that came later, those sizable villages would be shocked by ancient Greek cities, ancient Greek society would be shocked by the industrial revolution, the industrial revolution would be shocked by the modern day.
#+end_quote

You're ignoring the fact that there are anything between 200 thousand years and 1000-2000 years between those examples. That's still cherry picking.

But I don't care about this line of argument..

#+begin_quote
  Nothing you gave is evidence that P(Bostrom-style AI risk) is lower than the prior, or that P(god AI governed dystopias) is higher than the prior.
#+end_quote

My claim is expert consensus yours isn't if anything you should be the one giving evidence. No shifting the burden of evidence here mate.

#+begin_quote
  Then you jump to “There is apocalyptic danger it's just less likely to be the case [than god AI governed dystopias].”
#+end_quote

My point was that AI apocalypse and specifically an AI apocalypse where AI torture 1 human at a time psychologically while pretending to be repopulating the earth is less likely than AI god dystopias.

And I followed that when you started this discussion with AI apocalypse is less likely than 'no AI apocalypse'. Now you want me to give you evidence that X specific example is more likely than generic AI apocalypse. No thank you.

It might very well still be the case but it's a bad point to argue in favor of.
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562525952.0
:DateShort: 2019-Jul-07
:END:

*************** u/Veedrac:
#+begin_quote
  And I followed that when you started this discussion with AI apocalypse is less likely than 'no AI apocalypse'. Now you want me to give you evidence that X specific example is more likely than generic AI apocalypse. No thank you.
#+end_quote

Let me remind you what my first post in this thread was.

#+begin_quote

  #+begin_quote
    More likely scenarios are just god AI governed dystopias
  #+end_quote

  Do you have an argument for this? As someone sold on AI risk, this idea has never seemed like more than narrative-driven futurism than a genuine attempt at simulation to me, but I'm open to thoughts.
#+end_quote

I didn't change the topic. I've been consistent in what I was asking for the whole conversation. But I doubt this is going to be productive.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1562528964.0
:DateShort: 2019-Jul-08
:END:

**************** u/fassina2:
#+begin_quote
  I don't think the writer knew about paperclip optimizers, paperclip optimizers aren't really the most likely or scariest outcome, just the example of an AI gone terribly wrong. More likely scenarios are just god AI governed dystopias / badly calibrated moral AI gods destroying us all by accident.
#+end_quote

This is what I said.

#+begin_quote
  More likely scenarios are just god AI governed dystopias

  Do you have an argument for this? As someone sold on AI risk, this idea has never seemed like more than narrative-driven futurism than a genuine attempt at simulation to me, but I'm open to thoughts.
#+end_quote

This was your comment.

#+begin_quote
  I didn't change the topic. I've been consistent in what I was asking for the whole conversation. But I doubt this is going to be productive.
#+end_quote

Explain your question. Do you want me to give you an argument for an out of context phrase I used ? If so no thank you. If not elaborate, otherwise this is just a boring discussion.
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562529818.0
:DateShort: 2019-Jul-08
:END:

***************** I'm asking why you think ‘god AI governed dystopias' has any degree of plausibility. To me the idea is nearly nonsensical. Whereas the ‘AI is not particularly dangerous' and ‘AI is an existential risk' arguments have fairly straightforward ideas about what could actually happen to lead to that future (even if you think they're wrong), I don't know of arguments of a comparable nature that might lead people to actually forecast god AI distopias. This is a problem for me, I had hoped you could help.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1562531450.0
:DateShort: 2019-Jul-08
:END:

****************** Sure.

God was used as a near unlimited power noun. Dystopia is fairly clear, most bad AIs would result in one. Governed just means AI is in control.

I basically meant an AI dystopia, which is fairly likely in case of a badly programmed AI.

Just as an addendum and interesting point.

It's possible, and some would argue likely, that people will tend to have a religious like behavior towards an AI overlord. Trying to do more things the algorithm tends to reward.. Ritualistic / religious like behavior that kind of thing.

It's an interesting analogy I read in a book a couple months ago. About people in the future being like our past selves, using rituals and faith to try to get rewarded, because of their they lack of understanding.

In the AI case we'd be talking about algorithms they don't understand, in the past they didn't understand science..
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562534110.0
:DateShort: 2019-Jul-08
:END:

******************* u/Veedrac:
#+begin_quote
  Dystopia is fairly clear, most bad AIs would result in one.
#+end_quote

This is the part I'm not seeing. When I woke up I hypothesized that maybe you were talking about AI technologies used as a force-multiplier on an oppressive government, [[https://www.reddit.com/r/MachineLearning/comments/bvzc7w/d_has_anyone_noticed_a_lot_of_ml_research_into/][like is happening in China today]]. The progression from today to such a future is pretty clear to me---China just keeps getting more effective at governing, and other governments follow suit. But it seems you're still talking about an AI takeover scenario. What does such an AI look like? What are its goals? What makes the result dystopian?
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1562549710.0
:DateShort: 2019-Jul-08
:END:

******************** Racist AI, Religiously biased AI, Politically biased AI, AI that doesn't adapt to future human values..

These seem self explanatory to me, AI likes things a certain way because it was programmed badly or didn't account for it. Or people consider X wrong AI doesn't because when it was programmed people didn't consider X wrong..
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562551748.0
:DateShort: 2019-Jul-08
:END:

********************* I don't think you understand how far I am from working through this viewpoint. “Racist AI, Religiously biased AI, Politically biased AI” sounds about as obvious as “spiteful refrigerator”. How would you even build a superintelligent AGI system that had any of these traits as their predominant failings? Like, what actually happens in your hypothesis that results in these failings?

“Programmed badly” doesn't explain this much more than “badly built” would explain a fridge getting spiteful.

Are you perhaps talking about the creators having too much power to choose how the AI would look, and building it to push their particular ideologies? I'd hesitate to assume this since again you seem to be talking about an AI takeover situation, whereas this is more the China-becomes-totalitarian route.

#+begin_quote
  Or people consider X wrong AI doesn't because when it was programmed people didn't consider X wrong..
#+end_quote

This idea I can buy, and it does concern me, but it seems unlikely to lead to societies worse than today.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1562567594.0
:DateShort: 2019-Jul-08
:END:

********************** u/fassina2:
#+begin_quote
  How would you even build a superintelligent AGI system that had any of these traits as their predominant failings? Like, what actually happens in your hypothesis that results in these failings?
#+end_quote

It doesn't need to be a predominant failing just one of them.. AGI isn't programmed initially to treat people independent of x variables as equals, this could lead to a biased socioeconomic structure.

SAI under the control of certain groups where it's programmed to give them advantages. Genies that only answer from X type people. AGI that isn't programmed to respect human : government / culture / preferences..

There are infinite ways things could go wrong, many of them dystopic..

Other than Genies and maybe some other examples I'm forgetting, most sufficiently powered AI would take over, either directly or indirectly.

We are talking about an alien god with near limitless power and intelligence, with a few goals it will pursue relentlessly with only a few limitations we instill on it before it's born and can't change afterwards.

If it determines some entity (country, state, group, individual) is getting in the way of it's goals there's little stopping it from removing it systematically in a way that goes around it's limitations..
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562590202.0
:DateShort: 2019-Jul-08
:END:

*********************** u/Veedrac:
#+begin_quote
  We are talking about an alien god with near limitless power and intelligence
#+end_quote

I mean, I agree with this. But I find it hard to believe that if we had alien visitors from Alpha Centauri they would be racist, religiously biased, or politically motivated in any traditional sense. My question is not whether the AI would care about being fair wrt. race, but how an AI could plausibly be dystopically bad in this topic whilst still being generally well enough aligned for that to not to be completely overshadowed (eg. by not caring about human wellbeing, and thus killing us all).

I guess one major sticking point for me is that I don't see a way to solve this problem which is not fundamental; i.e. which does not require the AI actually knowing about and optimizing for underlying human goals like wellbeing, fulfillment and happiness. If you're doing that even passably, dystopias are implausible. If you're not, the AI kills the humans (or does the closest thing it is allowed to do) so it can get on with doing whatever it considers fundamentally important, like making buckets of dopamine.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1562879763.0
:DateShort: 2019-Jul-12
:END:


** I liked it and it did surprise me in a couple spots...but it left kind of bewildered. There wasn't a moral to the story, but it kept acting like it had one. Like it had all these scenes where morality had a big impact...but not? Idk. Definitely an out-there movie.
:PROPERTIES:
:Author: TaltosDreamer
:Score: 2
:DateUnix: 1562232442.0
:DateShort: 2019-Jul-04
:END:

*** [[/spoiler][I think the point of the morality scenes were to demonstrate the utilitarian logic that the AI was working under, that it was willing to kill many humans if it means creating a perfect peaceful new human race.]]
:PROPERTIES:
:Score: 4
:DateUnix: 1562260745.0
:DateShort: 2019-Jul-04
:END:

**** This was the intended meaning of that scene imo..
:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562373695.0
:DateShort: 2019-Jul-06
:END:


**** Your spoiler syntax is broken. Try using >!...!<
:PROPERTIES:
:Author: thrawnca
:Score: 1
:DateUnix: 1562555998.0
:DateShort: 2019-Jul-08
:END:


*** I think the point of the morality scenes >! along with the fact that Daughter's final exam is a psychological profile is meant to show that Mother is trying to produce a morally superior strain of humans.!<

​

What has me more bewildered is that I can't quite tell whether the older woman is suppsoed to have been human female #1 (seemed heavily implied) or the last surviving wild human (was stated but only ever to Daughter who everyone involved was lying to)
:PROPERTIES:
:Author: turtleswamp
:Score: 1
:DateUnix: 1562344218.0
:DateShort: 2019-Jul-05
:END:


** I just got around to watching it. Found it mildly interesting all the way through, but kept getting annoyed by thoughts like "if this were real then Mother would almost certainly be connected to the entire facility and be basically omniscient, knowing every button pushed and hearing every sound, [[https://gizmodo.com/scientists-are-turning-wifi-routers-into-creepy-radar-c-1794961990][if not more]]". The ending retroactively improved my perception of the first 2 hours.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1564100891.0
:DateShort: 2019-Jul-26
:END:


** I saw a few episodes and it felt that they were just keeping us in the dark to create suspence rather than any actual reason and moved on to more inspiring things.
:PROPERTIES:
:Author: Sonderjye
:Score: -5
:DateUnix: 1562257850.0
:DateShort: 2019-Jul-04
:END:

*** I find very unlikely that you've trully watched a few episodes...
:PROPERTIES:
:Author: optimalsize
:Score: 6
:DateUnix: 1562278163.0
:DateShort: 2019-Jul-05
:END:

**** [[/u/Sonderjye]] might've possibly confused the memory of I Am Mother with the memory of Lost In Space? Maybe? Idk. Both have a robot.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 2
:DateUnix: 1562303141.0
:DateShort: 2019-Jul-05
:END:

***** After doublechecking i was I Am Mother I was referring to, however I misremembered the format. What actually happened was that I watched until 40 min and at that point there isn't a clear plot trajectory, I don't know what happened to the rest of humanity, and Mother refuses to answer the relevant questions, and everything that have happened makes me distrust that the authors are going to give satisfying answers.

I can gather from your reaction that it gets better and might give it another shot.
:PROPERTIES:
:Author: Sonderjye
:Score: 2
:DateUnix: 1562318062.0
:DateShort: 2019-Jul-05
:END:

****** There are fairly satisfying answers by the end. Not everything is answered, but I felt you could reliably fill in the blanks with your imagination without difficulty.
:PROPERTIES:
:Score: 2
:DateUnix: 1562376482.0
:DateShort: 2019-Jul-06
:END:


****** u/ElizabethRobinThales:
#+begin_quote
  I can gather from your reaction that it gets better and might give it another shot.
#+end_quote

It's on my My List list but I haven't actually watched it yet.
:PROPERTIES:
:Author: ElizabethRobinThales
:Score: 1
:DateUnix: 1562376724.0
:DateShort: 2019-Jul-06
:END:
