#+TITLE: [Q][D] With an infinite computer, can you derive any large data file from a small hash?

* [Q][D] With an infinite computer, can you derive any large data file from a small hash?
:PROPERTIES:
:Author: lsparrish
:Score: 4
:DateUnix: 1453321669.0
:DateShort: 2016-Jan-20
:END:
Suppose you are stuck in a time loop and need a convenient way to carry information with you. Memorizing a 256-bit integer is easy enough with a mneumonic device, but detailed technical information not so much. Suppose further that you have access to an computing device which instantly produces an output for any rigorously specified algorithm as long as it is a halting algorithm.

Now let's say you have accumulated a gigabyte (compressed) of information through research. Your time loop approaches the end. How do you carry it over?

One possibility is to simply run SHA256 on it and memorize the numbers. Then after the reset, tell your computer to run SHA256 on every number requiring a gigabyte to describe, and from the set of matches, run the decompression algorithm and check for the presence of attributes signifying that the research data is valid (syntax, spacing, metadata, etc).

From what I understand, this should be the exact same file, with incredibly high probability. The algorithm could be run across the entire set of numerical possibilities (which can be done because we have bounded the size) and only one result would be found. And all you needed was a comparatively short hash and a simple and memorable algorithm to get there.


** There are 2^{256} possible outputs from SHA256. There are 2^{8000000000} possible inputs that are exactly 1GB in length. That means that there are [[http://www.wolframalpha.com/input/?i=%282%5E8000000000%29%2F%282%5E256%29][1.77 x 10^{2408239888}]] inputs per output --- or, because we're trying to reverse the process, 1.77 x 10^{2408239888} outputs per input.

Your computer is infinite, so it can check all of them, so the question then becomes how many of these decompressed GB files match your validation attributes. My guess is that even with stringent checks on syntax, spacing, and metadata, there will be more than 2^{256} variations that would look valid on first glance, which means that it's likely you'll get more than one result in that pile of 1.77 x 10^{2408239888} results.

(Unless I'm horribly misunderstanding you, or my math is wrong, both of which are very possible.)
:PROPERTIES:
:Author: alexanderwales
:Score: 18
:DateUnix: 1453323048.0
:DateShort: 2016-Jan-21
:END:


** Why not hash it down to just a 1 or a 0? That would make it even easier to memorize.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 24
:DateUnix: 1453322673.0
:DateShort: 2016-Jan-21
:END:

*** An excellent lens through which to view the original question.

This seems intimately related to [[https://en.wikipedia.org/wiki/Infinite_monkey_theorem][infinite monkeys]] and the [[https://en.wikipedia.org/wiki/The_Library_of_Babel][Library of Babel]]... but I want to focus on the [[https://en.wikipedia.org/wiki/Noisy-channel_coding_theorem][Shannon Limit]] on compression and error-correction.

Keep the ratio the same: Instead of 2^{8} bits being used to encode 2^{33} bits (a binary gigabyte), one bit will be used to encode 2^{25} bits (4 binary megabytes) of information. Also, to continue to keep things simple, we'll assume this information is entirely extended ASCII (2^{3} bit) text printed at 2048 (2^{11)} characters per page.

So one bit has to encode 2048 (2^{11} again) pages. For reference: At that character/page rate you could, in 2^{25} bits, hold the entire million word text of the Harry Potter series. Also any other coherent text of similar size... or, indeed, /every/ other coherent text of similar size. Approximately half of them will hash to 1, and the other half will hash to 0.

So there's the Library of Babel from which you must cull your research. A few difficult-to-detect errors in literature like Harry Potter aren't going to make much of a difference: A different adjective here or there, some swapping of sentences, altered dialogue or description. It would be difficult for all but the most steadfast fan to even notice.

Fishing out exactly the right version isn't easy, but it's hardly crucial.

Let's assume, instead, that what's encoded is a set of tables from the back of the [[https://en.wikipedia.org/wiki/CRC_Handbook_of_Chemistry_and_Physics][Handbook of Chemistry and Physics]]. Maybe part of it is a list of elemental isotopes and their decay rates. An error in even a single character could be catastrophic... and it's not going to be obvious which numbers are wrong without repeating the original experiments that measured them.

How many additional error-checking bits do you need for your single bit hash in order to assure that not one important number is wrong? Go check Shannon's formula. I'd guess no fewer than 2^{20} additional bits. If lossless compression is crucial, there aren't any shortcuts.
:PROPERTIES:
:Author: Sparkwitch
:Score: 9
:DateUnix: 1453365669.0
:DateShort: 2016-Jan-21
:END:


*** Actually, why not forget that too?

With the described computer you've solved the halting problem, so just do a brute-force search for the best possible program to run, then run that program.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 6
:DateUnix: 1453342552.0
:DateShort: 2016-Jan-21
:END:

**** I once saw a paper noting explicitly that dovetailing over Turing machines ought to find strong AI somewhere, somewhen.

That was a small but significant clue-stick for understanding what cognition is really about.
:PROPERTIES:
:Score: 5
:DateUnix: 1453501103.0
:DateShort: 2016-Jan-23
:END:


**** There remains the difficult issue of rigorously specifying an algorithm to identify how optimal each program is. All you've got is a guarantee that your algorithm is executed instantly.
:PROPERTIES:
:Author: seylerius
:Score: 1
:DateUnix: 1453358163.0
:DateShort: 2016-Jan-21
:END:

***** I think that's Eliezer's problem.

^{bad joke I know}
:PROPERTIES:
:Author: PeridexisErrant
:Score: 5
:DateUnix: 1453361837.0
:DateShort: 2016-Jan-21
:END:

****** Well, it's all our problem, but he did sign up for dealing with it, lol.
:PROPERTIES:
:Author: seylerius
:Score: 1
:DateUnix: 1453388635.0
:DateShort: 2016-Jan-21
:END:


*** Why stop at one bit? Compress it down all the way to zero bits, then you don't even need the time machine.
:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1453399028.0
:DateShort: 2016-Jan-21
:END:


** No

I suggest reading up on information theory.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 5
:DateUnix: 1453388825.0
:DateShort: 2016-Jan-21
:END:


** As noted, information cannot be compressed indefinitely. You are trying to recover the information you lose by only memorizing a hash with the check that a random file represents valid research data, and these two need to compensate each other. If you manage to formulate rules that correctly identify only 1% of random data as possible research, and only 1 / 2^{256} ≈ 8.6 * 10^{-78} of files have the correct hash, then you narrow it down to 0.01 * 8.6 * 10^{-78} ≈ 8.6 * 10^{-80,} or one in 1.2 * 10^{79.} There are 2^{512} ≈ 1.3 * 10^{154} files with a length of 512 bits (!), that means that after applying your trick there are 1.2*10^{75} still left, which is a bit to much to manually account for. Making your rules better does not help, doubling their accuracy gets you one extra bit, but you probably had to make the rules at least one bit longer to achieve the improvement. You have to remember the rules, too, so no compressing was done.

/However/, if you somehow manage to get hold of an arbitrarily fast computer and would want to compress information, your best bet may be to choose a turing-complete representation of the data, like (almost) any programming language. Then simply buteforce all possible programs until you find the shortest. (Tip for time-travelers: Choose [[https://en.wikipedia.org/wiki/Shakespeare_Programming_Language][SPL]] to easily hide your valuable research from that pesky Time Police!) Also, automated proof verification is a thing, consider briefly solving all theoretical problems (by iterating over all proofs and checking them). Additionally try simulating all possible laws of physics and output the ones where the observed universe has the highest probability, this may very well solve physics (and by extension everything else).
:PROPERTIES:
:Author: suyjuris
:Score: 6
:DateUnix: 1453325343.0
:DateShort: 2016-Jan-21
:END:

*** u/alexanderwales:
#+begin_quote
  Making your rules better does not help, doubling their accuracy gets you one extra bit, but you probably had to make the rules at least one bit longer to achieve the improvement. You have to remember the rules, too, so no compressing was done.
#+end_quote

I mostly agree with you, but there are a whole lot of easy rules that are easy to remember but /greatly/ reduce the possibility space.

Like for example, "run spell check". There are [[http://wordfinder.yourdictionary.com/letter-words/5][8,887 5-letter words]] but [[http://www.wolframalpha.com/input/?i=26%5E5][11,881,376 possible combinations of five letters]], which means even that simple check reduces the space by 99.93%. In reality, this check is going to work even better than that, because it's going to knock out all data that's missing spaces between words, all data that's got numbers in the middle of a word, all data that's got upper case letters in the wrong place, etc. Same thing applies to "run grammar check".

But that still won't knock out enough of the random data to make much of a difference.
:PROPERTIES:
:Author: alexanderwales
:Score: 6
:DateUnix: 1453327281.0
:DateShort: 2016-Jan-21
:END:

**** Well, you can't really argue with the missing space between words, since then you would have to account for all [[https://en.wikipedia.org/wiki/N-gram][N-grams]].

99.93% equates 10 Bits. I fear that thinking of clever rules may be counterproductive, as the time could be better spent memorizing words (one single 5 letter word is 13 bit of information).
:PROPERTIES:
:Author: suyjuris
:Score: 1
:DateUnix: 1453330467.0
:DateShort: 2016-Jan-21
:END:

***** Spell check already accounts for all that. All you need to do is make sure that your program passes the existing spell-check of your choice before you save and hash it. Spell check doesn't reduce it by 10 bits, it reduces it by a lot more.
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1453330839.0
:DateShort: 2016-Jan-21
:END:

****** After running the spell check, the data is in a certain form (it conforms to to rules of the spellchecker). Let's simplify, and describe this form as 'lowercase words delimited by space'. (Adding extra complexity like punctuation would not increase the information density by much.) What percentage of all possible data of length n is in this form? First, eliminate all data that contains illegal characters, so (26 / 256)^{n} of data. Then, the data is a sequence of words, lets check whether each word is valid. As for every 5 + 1 characters there are only ~8887 valid words, this reduces by 8887 / 26^{6} per six characters, pow(8887 / 26^{6,} 1/6) = 17.5% per character. Together you get 1.8% per character, which is equivalent to multiplying the information length with 8 / log2(256 * 1.8%) = 3.7

You're right; I did not consider that the total percentage scales with message length.
:PROPERTIES:
:Author: suyjuris
:Score: 3
:DateUnix: 1453332898.0
:DateShort: 2016-Jan-21
:END:

******* Since this is in the context of time loops, what if you had to invent some terminology for something? And that terminology wasn't recognized by whatever spell check program you run, since if you're running it then you're in a different loop than the one in which you saved whatever word you made up into the spell check program.
:PROPERTIES:
:Author: somnolentSlumber
:Score: 1
:DateUnix: 1453757583.0
:DateShort: 2016-Jan-26
:END:

******** You could choose old words for new concepts (e.g. an acyclic undirected graph is called forest), the meaning can be inferred from context. Of course, that only works as long as the amount of concepts you need is smaller than the amount of builtin concepts (~words) of the language, however there is always the possibility of using multiple words for one concept. As there are infinite possible combinations of words is it quite unlikely that you would run out. The amount of bits needed to encode the concepts would grow, but that is only natural - you are trying to encode more information after all. (The fact that the representation doesn't grow if you use old words is only an artifact of using an imperfect compression method.)
:PROPERTIES:
:Author: suyjuris
:Score: 1
:DateUnix: 1453822248.0
:DateShort: 2016-Jan-26
:END:


** Entropy rate of the English language is about one bit per character. This tells you the lowest size to which you can compress your research if the test for correctness is "seems vaguely coherent written in English". Think of what randomly generated research papers look like (e.g, [[http://thatsmathematics.com/mathgen/]]), or maybe Markov chain English sentences.

Now, if you can write a test that distinguishes /correct/ research from /incorrect/ research, you can do better than this. But this should be hard. The question to ask is "is it possible to write a nonsense research generator that passes all these tests?" If the answer is yes, then most of the possible 1-gigabyte files corresponding to your 256-bit integer will be nonsense research than legitimate research, because there's a lot more nonsense research.

I think the most plausible actual method would be to use the 256-bit integer to encode successful research paths. Say you memorize 64 050 022 778 895 360 168 845 298 619 307 079 864 103 618 688 689 436 920 572 106 993 648 082 329 355. This could mean "Think of the problem you're trying to solve. Spend five minutes listing the 9 likeliest possibilities. The correct one turned out to be #6 in alphabetical order. Next, think of 9 logical further questions to ask. The most promising one is #4 in alphabetical order. Next, think of 9 possible answers. It turns out (#0) none of them are correct. So think of 9 more..."

(In general, this doesn't uniquely give you a research path, because your answers won't be perfectly consistent. In a time loop, we can achieve this. Just go to your favorite brilliant scientist and ask them to follow this algorithm. Assuming the scientist is not looping, they will list the same 10 possibilities each time. Spend the first 10 digits of your number on a lottery ticket to convince the brilliant scientist. Using someone else in this way is even better, because the brilliant scientist's brain is probably your best source of a really good compression algorithm for brilliant research, and this method lets us access it easily.)
:PROPERTIES:
:Author: SpeakKindly
:Score: 5
:DateUnix: 1453333429.0
:DateShort: 2016-Jan-21
:END:


** Just simulate the universe you live in up to the n-1st time loop. Use something similar to AIXI, which is computable on your device, to identify the properties of the universe you live in.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1453382544.0
:DateShort: 2016-Jan-21
:END:

*** Warning: if you do this, you are quite possibly [[http://qntm.org/responsibility][a simulation of the real you embedded arbitrarily deep in an infinite stack of recursively-simulated universes.]]
:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1453398563.0
:DateShort: 2016-Jan-21
:END:

**** Eh, everything is real. And nothing is. It's complicated. (The characters in that story are wrong, though, or rather insufficiently patternist. Most of humanity has not yet diverged; due to moral uncertainty, the only unambiguously moral choice is to turn the simulation off immediately. (And next time, don't fucking /change/ things - that's the one thing that makes it complicated.))
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1453399011.0
:DateShort: 2016-Jan-21
:END:

***** Plus if infinite computational power is possible and your universe is a simulation anyway, then there's a good chance someone is running every possible program, and you'll live on in one of those instances even if your most immediate instance is halted.
:PROPERTIES:
:Author: gabbalis
:Score: 2
:DateUnix: 1453753939.0
:DateShort: 2016-Jan-26
:END:

****** Yeah, that's one possibility, but it's conceivable that there's some issue with that view that forces us back into a morality where computation is important. That's what I meant by "moral uncertainty".
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1453755050.0
:DateShort: 2016-Jan-26
:END:


** It's impossible because Kolmogorov copmlexity.

On the other hand your computer clearly isn't a Turing machine (it allows you to break halting problem by waiting for example 1 nanosecond and reseting it with the next combination if there's no output). For optimal performance install regular PC as a watchdog to do the resetting for you.

So who knows what's possible in this universe if basic mathemathical laws don't work :)
:PROPERTIES:
:Author: ajuc
:Score: 2
:DateUnix: 1453418320.0
:DateShort: 2016-Jan-22
:END:


** I'm pretty sure you're dealing with a ream of random symbols /so large/ that even after correcting for this and that, you'll find multiple instances of valid looking "research" with perfect syntax and everything within the noise. Intuition breaks at such sizes.
:PROPERTIES:
:Author: glowingfibre
:Score: 2
:DateUnix: 1454012322.0
:DateShort: 2016-Jan-28
:END:

*** Now that I have read the responses, I agree with those who are saying that the number of results that seem like valid research, grammatically correct, and so on, would be too great to sort through by hand.

I think what threw my intuition off was realizing that there is a lot of specificity in language (or just about any other system of encoding useful information) which does increase the compression quite a bit beyond what a normal compression algorithm would be able to do. For example, the research being written in English words makes it compressible by a factor based on the difference between random assortments of characters and actual words. But it wouldn't be a big enough factor compared to the difference between 8 billion and 256 bits.
:PROPERTIES:
:Author: lsparrish
:Score: 1
:DateUnix: 1454085195.0
:DateShort: 2016-Jan-29
:END:

**** Yeah. I mean your intuition is right in most senses - this is not entirely dissimilar how real compression algorithms actually work - you just overestimated the /degree/ of compression possible.

(Or, perhaps the real flaw is that you maybe didn't fully realize that processing power is not really the limiting factor to compression, that all lossless compression has a strict upper bound, and that all lossy compression must get more lossy with scale)
:PROPERTIES:
:Author: glowingfibre
:Score: 1
:DateUnix: 1454113440.0
:DateShort: 2016-Jan-30
:END:


** them hash collisions though...
:PROPERTIES:
:Author: protagnostic
:Score: 1
:DateUnix: 1453453482.0
:DateShort: 2016-Jan-22
:END:


** If you can memorize an extra number, you could use the index in the ordered set of data that satisfy the hash and filtered by a grammar syntax. Perhaps, if you are lucky, that number will be short too?
:PROPERTIES:
:Author: long_void
:Score: 1
:DateUnix: 1453682057.0
:DateShort: 2016-Jan-25
:END:


** You still need to get the information transferred somehow. You can get 256 bits with the hash, another couple dozen (?) with the exact size, and a more with the filetype. It's already compressed, so you can't save anything there, but you could get more out of the fact that it is English writing about physics (or whatever).

In short, no. But it is probably the best compression you can get, as you already have a bunch of the information without needing to pay for it.
:PROPERTIES:
:Author: ulyssessword
:Score: 1
:DateUnix: 1453324699.0
:DateShort: 2016-Jan-21
:END:


** With an infinite computer? I think this setup should work:

Convert the binary of the large data file into a single large number. Obtain the prime factors of that single large number. Now, you have a set of numbers, all of which are prime. Replace all of the actual numbers in that set with their position in the set of all primes.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 0
:DateUnix: 1453324473.0
:DateShort: 2016-Jan-21
:END:

*** This should not, in general, produce a sequence which is any shorter to write down than the original large number. The Pigeonhole Principle is a cruel mistress.
:PROPERTIES:
:Author: SpeakKindly
:Score: 7
:DateUnix: 1453333935.0
:DateShort: 2016-Jan-21
:END:

**** It seems like, the larger the file is, the more likely it is that it should lead to significant compression.

1) On average, the size of a number's largest prime factor increases with the size of a number.

2) On average, prime numbers occur less frequently as we move from 2 to infinity - and therefore, the larger a prime number is, the more it's compressed by simply saying "the nth prime number".
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 1
:DateUnix: 1453410372.0
:DateShort: 2016-Jan-22
:END:

***** Well, we can dig out our asymptotic estimates of the size of these effects. The number of primes less than n is around n/log(n); thus, if we know that x is prime, instead of writing it down in log(x) bits as we usually would, we can use merely log(x/log(x)) = log(x) - log(log(x)) bits, saving log(log(x)) bits on each prime.

But your encoding method also needs to separate the encoded prime factors you write down somehow. The typical n has around log(log(n)) prime factors; we need roughly this many bits to specify how many prime factors n has and how large they are (when writing down "this is the k-th prime number", you need to know how many bits you need to write down k). This happens to magically more or less exactly compensate for our earlier savings. Some numbers will become shorter; some longer.

I did not do this computation yesterday, because your method will not work on general principles. There are 1024 times as many 1000000-bit integers as there are 999990-bit integers. Thus, if you apply any compression algorithm of your choice on a random 1000000-bit integer, you have a less than 0.1% chance of getting a 999990-bit result and saving so much as 10 bits. If your compression algorithm actually compresses some 1000000-bit integers to 999990-bit integers, though, then not all 999990-bit integers can be given 999990-bit results, so some of those will have to be "compressed" to longer sequences. Some numbers will become shorter; some longer.

Lossless compression algorithms work by exploiting properties of the input. A randomly chosen text in English has lots of properties to exploit, so a text file can be compressed by a lot. A randomly chosen integer has no properties whatsoever to exploit, so a randomly chosen integer cannot be compressed.
:PROPERTIES:
:Author: SpeakKindly
:Score: 1
:DateUnix: 1453413973.0
:DateShort: 2016-Jan-22
:END:
