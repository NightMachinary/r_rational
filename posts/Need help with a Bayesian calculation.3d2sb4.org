#+TITLE: Need help with a Bayesian calculation

* Need help with a Bayesian calculation
:PROPERTIES:
:Score: 0
:DateUnix: 1436752133.0
:DateShort: 2015-Jul-13
:END:
Ok guys, so I have this question but I don't know where to post it. Is there a subreddit or another place where I could ask this? Is there one of you who's willing to help me out even if this is off-topic?. The question:

#+begin_quote
  What's the optimal choice in a situation in which you have only two outcomes |Either the whole human race goes extinct, or all the others lifeforms on earth (here humanity has no need of those for its sustenance) do>?
#+end_quote

I've tried to solve this problem, but I feel like having smart people as a sounding board would improve my take on the problem.

Probably, inside this problem lay the hidden questions of how much an animal life is worth compared to a human one, how much the potential of the whole of earth ecosystem is worth compared to our race's potential, and probably the meaning of life, or at least the inferable purpose of life according to life-form's priorities (like, they prefer life over death, death over eternal suffering, freedom over life and other such).

Please tell me if you think this is too much a heavy question.

EDIT: just to clarify, I don't need anyone to do a Bayesian calculation for me, I just want to know your take on the problem.

EDIT2: Deleted the "Bayesian" bit since I misused it.


** Yes, there are some deep questions there, and no, they aren't related to the math. Bayes law doesn't tell you how to value things, it's only useful for making decisions and updating beliefs once you've figured that out yourself.
:PROPERTIES:
:Author: davidmanheim
:Score: 12
:DateUnix: 1436754884.0
:DateShort: 2015-Jul-13
:END:

*** So, is there a way to figure out values?
:PROPERTIES:
:Score: 2
:DateUnix: 1436798339.0
:DateShort: 2015-Jul-13
:END:

**** Ask a philosopher. Statistics cannot help you here.
:PROPERTIES:
:Author: Chronophilia
:Score: 3
:DateUnix: 1436836274.0
:DateShort: 2015-Jul-14
:END:

***** Didn't ask for statistics, I asked for the help of smart people.

Anyway, It's not like someone forces you to help, if you are so against the idea.
:PROPERTIES:
:Score: 1
:DateUnix: 1436837164.0
:DateShort: 2015-Jul-14
:END:

****** Sorry, I didn't mean to be rude! Your top-level post said this was a question about Bayesian probability calculations.

Determining the relative value of a human life and an animal life is /way/ outside the realm of probability theory. It doesn't even have an objective right answer; you'd need to ask an expert to get a good idea of the different perspectives. Maybe [[/r/askphilosophy]] can help?

And while I'm flattered that you consider this subreddit to be full of "smart people", being smart in one area doesn't make a person smart in everything. You wouldn't ask Albert Einstein for help with a plumbing problem, because he's a theoretical physicist. You'd ask a plumber. The people in this subreddit are generally pretty good at mathematics, but an important part of your question is outside our collective expertise.
:PROPERTIES:
:Author: Chronophilia
:Score: 5
:DateUnix: 1436837563.0
:DateShort: 2015-Jul-14
:END:

******* Well, I made this mental connection in which Mathematics is included in Logic, and Philosophy is also included in Logic, and maybe people good at logic would feel at home, like Einstein could feel dealing with molecular bonds.

I'm versed in philosophy anyway, so I think it was some sort of bias on my part. That's why I asked if there would have been a better place to pose the question. Thank you for answering that, by the way.
:PROPERTIES:
:Score: 2
:DateUnix: 1436838797.0
:DateShort: 2015-Jul-14
:END:


*** From HPMOR

#+begin_quote
  Of course, the point of a subjective Bayesian calculation wasn't that, after you made up a bunch of numbers, multiplying them out would give you an exactly right answer. The real point was that the process of making up numbers would force you to tally all the relevant facts and weigh all the relative probabilities. ... One version of the process was to tally hypotheses and list out evidence, make up all the numbers, do the calculation, and then throw out the final answer and go with your brain's gut feeling after you'd forced it to really weigh everything.
#+end_quote
:PROPERTIES:
:Author: thyratron
:Score: 4
:DateUnix: 1436761912.0
:DateShort: 2015-Jul-13
:END:

**** Yes. Thinking about a problem is helpful. But you need to think through it yourself before the process quoted makes any sense.

Also, as intended to be applied here, it badly conflates expected utilities and "raw" utilities.
:PROPERTIES:
:Author: davidmanheim
:Score: 3
:DateUnix: 1436767010.0
:DateShort: 2015-Jul-13
:END:

***** Trying to learn here. Please explain what you mean by "raw" utilities.
:PROPERTIES:
:Score: 1
:DateUnix: 1436798507.0
:DateShort: 2015-Jul-13
:END:


** Bayes doesn't do values, only probabilities. This is a utility problem, and without a utility function to be compared to, you can't make a choice either way.
:PROPERTIES:
:Author: Transfuturist
:Score: 8
:DateUnix: 1436765380.0
:DateShort: 2015-Jul-13
:END:

*** Well, it is a bayes problem, but like most bayes problems the big challenge is not the math but the fact it relies on utilities or probabilities that are very hard to measure. Which the theorem itself does not help you with.
:PROPERTIES:
:Author: ancientcampus
:Score: 1
:DateUnix: 1436802316.0
:DateShort: 2015-Jul-13
:END:


** What are your assumptions on the odds of some sapient form of life other than humans ever coming into existence? Is there a non-zero chance that humanity is the only form of sapience that would ever exist? If so, is there any utilitarian value to the extinction of all minds which can ever /give/ value to anything other than 'negative infinity', which would mean that the appropriate answer would always be 'keep humanity alive', no matter how much value there is in keeping other non-sapient species alive?
:PROPERTIES:
:Author: DataPacRat
:Score: 3
:DateUnix: 1436758127.0
:DateShort: 2015-Jul-13
:END:

*** Mmm, well, we know we're not the only case a life form has reached the level of intelligence of australopithecus, because we have chimpanzees and bonobos, and we know these two have the potential for abstraction and language, because captive primates can learn the sign language.

So that level of intelligence, at least, is not a single exception composed by humans. We can infer that the probabilities of intelligent life are not so incredibly low as to never happen but magically in us (like many theists seem to suppose), this the collective potential of the millions of species could give us a probability very close to one of multiple intelligent life in the future.

On the other hand, the probability of intelligent life in humans is one...

Well, is it intelligent life with many variations and differences preferable over a single type? And if yes, is this difference enough to justify the lost of a certainty over a (possibly) nearly-certain possibility? Mmmm, probably not.
:PROPERTIES:
:Score: 1
:DateUnix: 1436798961.0
:DateShort: 2015-Jul-13
:END:

**** u/codahighland:
#+begin_quote
  On the other hand, the probability of intelligent life in humans is one...
#+end_quote

The POSTERIOR probability is 1. The PRIOR probability (that is, if you didn't already know that humans were intelligent life) is less than that.
:PROPERTIES:
:Author: codahighland
:Score: 1
:DateUnix: 1436809505.0
:DateShort: 2015-Jul-13
:END:

***** Is one in the moment of your choice, that's what I meant.
:PROPERTIES:
:Score: 1
:DateUnix: 1436820778.0
:DateShort: 2015-Jul-14
:END:

****** Oh, fair enough then.
:PROPERTIES:
:Author: codahighland
:Score: 2
:DateUnix: 1436829067.0
:DateShort: 2015-Jul-14
:END:


** Correct me if I'm wrong, but I don't think your problem is a Bayesian calculation. It seems more like a morality question.

 

Remember that Bayes theorem states:

P (A|B)= P (B|A) *P (A)/P (B)

where A and B are events, P (A) is the probability of event A, and P (A|B) is the probability of event A given that event B has already happened.

 

A common example of someone using Bayes theorem is a doctor finding the probability that a patient really has breast cancer given a positive mammogram finding. [A would be the event of having breast cancer, B the event of having a positive mammogram finding. ] The best I can summarize is that bayes theorem let's us "update" our belief in the probability of some event after being given new information about the conditions that influence said event.

 

In your post you present a choice between two scenarios, one in which humans are made extinct, and one in which every life form save humans are made extinct--we're not given new information that let's us update beliefs on probabilities, but rather with situations that examine what we value.

 

I think the obvious choice would be the one in which humans live. Most people value their own existence. However, this existence might be pretty bleak. Though you tell us to assume humans would be able to live without other life forms, I imagine it would be quite difficult. Even if we ignore the problem of getting food without plants, animals, and fungi, human life would be very different without the almost unimaginable number of microorganisms that live on earth and throughout our very own bodies. (You may know that there are more bacterial cells than human cells in what you typically think of as "you"). That said, we would be freed from communicable diseases such as malaria, pneumonia, meningitis, etc. This scenario might make a pretty good story--would humans resort to cannibalism to survive? Could we synthesize enough amino acids, glucose, fatty acids, and vitamins using non living resources on earth? And what about the environmental effects? Without microorganisms and plants, the composition of the atmosphere would change--I think we'd eventually have to dedicate a lot of resources to splitting oxygen out of water.

 

Anyway, I hope this helps. Sorry for any spelling, formating, or factual mistakes--I'm typing this from mobile.
:PROPERTIES:
:Author: DrPresidentMD
:Score: 2
:DateUnix: 1436823083.0
:DateShort: 2015-Jul-14
:END:


** The question boils down to what you ascribe moral weight to and how much. If a human has 100 'moralons', does a blade of grass have any number of moralons greater than 0? How about an insect? A bacteria? A blue whale?

If you can set those numbers, the choice is trivial -- just add up the total for humans and the total for non-humans and compare them. You obviously end up with three scenarios:

- Humans have greater moral weight; kill everything else.
- Plants/animals have greater moral weight; kill humans.
- They are exactly the same. Kill all the non-humans because we are the ones making the choice and therefore we get to win ties.

How to assign moralon values is left as an exercise for the reader.
:PROPERTIES:
:Author: eaglejarl
:Score: 2
:DateUnix: 1436919408.0
:DateShort: 2015-Jul-15
:END:

*** Interesting. So what would you have done if you where in that situation right now? And how did you come to the conclusion?
:PROPERTIES:
:Score: 1
:DateUnix: 1437898542.0
:DateShort: 2015-Jul-26
:END:

**** Me personally? I'd kill everything else. I believe that animals and plants have moral weight but I'm still going to side with humans. If I'm being honest, I'm siding with humans because I'm not perfectly rational. I could justify that decision if I wanted to, but the fact is that I would not be making it based on moral weight.

"What would you do?" wasn't the question I thought you were asking though -- you were asking about the optimal choice. I don't know that my decision is optimal, and it may well not be. It's probably not optimal from a moral perspective -- I /do/ assign moralons to non-humans, so the combined weight of all the other critters on earth might believably outweigh that of humans.

Even on pragmatic grounds, my choice might not be optimal. Objectively speaking, the only reasons to assign greater moral weight to humans is that we can create things (art, music, poetry, science....) and because the person doing the assigning is (presumably) human. If all humans dropped dead right now, it's not improbable that another sapient species would evolve and do a better job (for whatever value of "better") than we have. So, perhaps the pragmatically better thing to do is kill the humans. I don't care. I'm still choosing the humans.

Marginally offtopic: I once asked my co-founder "if you had the chance to sacrifice your life in order to save the entire human race, would you do it?" Now, to me, this is a no-brainer: yes, of course I would. He said no. This took me very much by surprise.
:PROPERTIES:
:Author: eaglejarl
:Score: 2
:DateUnix: 1437953082.0
:DateShort: 2015-Jul-27
:END:

***** It takes me by surprise too, would he want to live in a world where he's the only person alive? How did the conversation go?

EDIT: don't know why I took it like a "me or Humanity" thing, I guess in my mind there was a box like in that movie, /The Box/, and This scarred man told me "either you die or Humanity dies, choose.
:PROPERTIES:
:Score: 1
:DateUnix: 1439589045.0
:DateShort: 2015-Aug-15
:END:

****** There was mutual incomprehension--he couldn't imagine why I would do that, and I couldn't imagine why he wouldn't. Even when I said "you realize that 'the entire human race' includes you, right? You're dying in either scenario, it just depends what's happening to everyone else"...even then he wouldn't do it.

I'm really glad I got out of that company.
:PROPERTIES:
:Author: eaglejarl
:Score: 1
:DateUnix: 1439593738.0
:DateShort: 2015-Aug-15
:END:


*** Have you read the articles on Foundational research institute that are related to this:

- [[http://simonknutsson.com/files/The%20Moral%20Importance%20of%20Invertebrates%20Such%20as%20Insects.pdf][The Moral Importance of Invertebrates Such as Insects]]

- [[http://foundational-research.org/publications/importance-of-wild-animal-suffering/][The Importance of Wild-Animal Suffering]]

- [[http://reducing-suffering.org/bacteria-plants-and-graded-sentience/][Bacteria, Plants, and Graded Sentience]]
:PROPERTIES:
:Score: 1
:DateUnix: 1438018592.0
:DateShort: 2015-Jul-27
:END:

**** I ascribe moral weight to those things, yes. I ascribe moral weight to anything that is sentient. I ascribe more moral weight to anything that is sapient. If it's a question of sentient vs sapient death, under non-contrived scenarios I will choose sapient survival.
:PROPERTIES:
:Author: eaglejarl
:Score: 1
:DateUnix: 1438021414.0
:DateShort: 2015-Jul-27
:END:


** Ok, it seems nobody else is trying to answer the question, so I'll give it a go.

The issue seems analogous to the argument for vegetarianism: There are a flagrilion more animals than humans. THEREFORE it is either the case that animals have significant moral worth which through the virtue of multiplication then eclipses all human moral worth, OR they have no moral worth at all.

Then to find your utility function you could look at your own revealed preferences: You (probably) currently act as if even just one animal has some moral worth, which means that the answer would almost certainly be that letting all humans die is preferable.

BUT the problem arises when you realize that human preferences are not consistent. We are willing to pay 1 dollar to save one bird, but not a thousand dollars to save a million of them. So something does not add up but the question is where the problem lies: Either we give up on trying to use math for moral questions OR we give up on trying to make our moral preferences consistent. Either way the prospect of moral progress becomes rather bleak...

Personally I think the second option is the better one, which implies that choosing for the humans to live is probably better, unless you're an extreme animal rights activist, since that is what most of us would intuitively prefer.
:PROPERTIES:
:Author: Sophronius
:Score: 2
:DateUnix: 1437815048.0
:DateShort: 2015-Jul-25
:END:

*** Two weeks and thirty-four comments later we finally have a winner! Thank you, user who actually tried to find a solution instead of putting the question aside because it's impossible to answer or only bashing the op because he used the wrong terms, even if the meaning of the question is pretty evident!

I'll fondly remember your attitude towards problems, and I'll let you know that it's inspiring for me, even if there's only one person out of, well I don't really know how many read the post, but just one in the *rational subreddit* that's willing to use rationality to solve problems, even hypothetical ones.

I'll let you know that if I ever, in the future, were to build a giant company of whatever, I won't come short of proposing you a position as big-ass manager with a shitload of pay, because you're the one of the very few people I've ever found who's willing to use his brain to /really/ figure problems out.
:PROPERTIES:
:Score: 2
:DateUnix: 1437898216.0
:DateShort: 2015-Jul-26
:END:

**** Well I'm very flattered, thank you! If I ever get to realize my dream of forming a team of rationalists dedicated to improving the world, I'll contact you as well. :-)

I share your frustration with the rationalist community, largely because I strongly believe that rationalists should win: [[http://lesswrong.com/lw/7i/rationality_is_systematized_winning/]]

I've always felt that there must be a team of competent people working in secret to save the world somewhere, but somehow that doesn't seem to be the case.
:PROPERTIES:
:Author: Sophronius
:Score: 2
:DateUnix: 1438363040.0
:DateShort: 2015-Jul-31
:END:

***** Unless Illuminati.
:PROPERTIES:
:Score: 1
:DateUnix: 1439589747.0
:DateShort: 2015-Aug-15
:END:


** Just to help better define the problem:

Does choosing in favor of humanity give it immunity to all future extinction events until the death of the universe? Or does it just dodge this one particular extinction event? (Ditto for Earth, but that's less important)
:PROPERTIES:
:Author: ancientcampus
:Score: 1
:DateUnix: 1436802446.0
:DateShort: 2015-Jul-13
:END:

*** just this one
:PROPERTIES:
:Score: 1
:DateUnix: 1436806317.0
:DateShort: 2015-Jul-13
:END:


** Gosh, but that's hard. Moral uncertainty is awkward.

You /might/ be able to pull a Fermi estimate out of some experiments with people's instinctive reaction to harm to different animals, and then use Bayes to evaluate your the expected utility of different possibilities for humanity's future ... but honestly, there's so much uncertainty I doubt you could get an answer with any degree of confidence.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1436778269.0
:DateShort: 2015-Jul-13
:END:


** u/appropriate-username:
#+begin_quote
  all the others lifeforms on earth (here humanity has no need of those for its sustenance) do>?
#+end_quote

The latter, obviously. That way, only mosquitoes and possibly some germs die because of how linked and twisted the food chain is.

Or if this is interpreted as saying that only those we can eat get spared, then the choice is a moot one because either one will end up with us dying because things we can eat will die because they won't have sustenance and depending on how quickly this happens, we won't have time to recover.
:PROPERTIES:
:Author: appropriate-username
:Score: 0
:DateUnix: 1436834141.0
:DateShort: 2015-Jul-14
:END:

*** Omega whisks all humans away from Earth into a giant spaceship. It will either (a) kill all humans on the spaceship and leave Earth alone or (b) destroy all life still on Earth and provide a vast array of replicators that mimic farming in terms of costs, side effects, and output. (You miss out on truffles. Oh well.) Furthermore, we can ask Omega to provide more replicators at any location we want, and it will. In either case, it returns all humans to Earth, dead or alive, exactly where it found them, with no apparent passage of time.

Do you prefer it to execute plan A or plan B?

Or are you going to try to outsmart my revised scenario instead of probing the issue the hypothetical question is trying to probe? You /know/ what it's trying to ask, after all, and it's just a waste of everyone's time to try to work around the real issue.

And you misunderstood the original question in trying to avoid answering it. The original question said:

#+begin_quote
  humanity has no need of those for its sustenance
#+end_quote

Suggesting that humans can already synthesize food without the help of any living organisms, making your objection null and void.
:PROPERTIES:
:Score: 3
:DateUnix: 1436848087.0
:DateShort: 2015-Jul-14
:END:

**** Appropriate username, thanks. Your scenario makes more sense than OPs.

Though even in your scenario, we get alien tech if we live? Then whether we continue to live depends on how good the alien tech is. If it never breaks down and is compatible with our style of life, etc., etc., such that we don't actually die within days of the change and may even survive for approximately the same time as we would've otherwise, I pick this scenario, since it shouldn't really make life that much different.

Plus, no mosquitoes or germs. No AIDS/HIV or any other viruses.

#+begin_quote
  You know what it's trying to ask
#+end_quote

Honestly have no clue. Is it supposed to be some moral question about whether human life is more valuable or that of other organisms?

#+begin_quote
  Suggesting that humans can already synthesize food without the help of any living organisms
#+end_quote

I read that as being set in present time, and OP being simply unaware of the percentage of organisms needed for sustenance. If it's set in a future when we can synthesize food without living organisms, I'd need a lot more details about that future to be able to answer.
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1437002828.0
:DateShort: 2015-Jul-16
:END:

***** u/deleted:
#+begin_quote
  Plus, no mosquitoes or germs. No AIDS/HIV or any other viruses.
#+end_quote

Well, considering that the alternative is all humans dying...

#+begin_quote
  Is it supposed to be some moral question about whether human life is more valuable or that of other organisms?
#+end_quote

Yes. If you were confused, you should have asked.
:PROPERTIES:
:Score: 2
:DateUnix: 1437014566.0
:DateShort: 2015-Jul-16
:END:

****** u/appropriate-username:
#+begin_quote
  If you were confused, you should have asked.
#+end_quote

That's what this entire conversation was about lol. I pointed out inconsistencies to get at what OP wanted to talk about instead of the false dilemma OP presented.

And really this is a fairly shallow question. No life has any more inherent worth than any other life, what's the point in discussing it.
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1437173896.0
:DateShort: 2015-Jul-18
:END:

******* u/deleted:
#+begin_quote
  That's what this entire conversation was about lol.
#+end_quote

Actually, you answered a question you were confused about, rather than asking for a clarification. This is a good strategy for taking tests at school (unless your teacher says otherwise), but it's a bad strategy at understanding.

#+begin_quote
  No life has any more inherent worth than any other life
#+end_quote

Well, no sophont's life, yes.
:PROPERTIES:
:Score: 2
:DateUnix: 1437319789.0
:DateShort: 2015-Jul-19
:END:


*** I meant that humans can live on without any other life-form's help. Every other living being would die. Disappear. Whatever.
:PROPERTIES:
:Score: 1
:DateUnix: 1436838250.0
:DateShort: 2015-Jul-14
:END:

**** We can't. Without germs we'd die fairly quickly. Hell, mitochondria is an adapted alien lifeform.

We'd also not have any real food since most of what we eat was once alive.
:PROPERTIES:
:Author: appropriate-username
:Score: -1
:DateUnix: 1436838398.0
:DateShort: 2015-Jul-14
:END:

***** u/deleted:
#+begin_quote
  Without germs we'd die fairly quickly
#+end_quote

In the real world, yes. In this hypothetical scenario, instead...
:PROPERTIES:
:Score: 2
:DateUnix: 1436839009.0
:DateShort: 2015-Jul-14
:END:

****** Well if we have an alien biology in this hypothetical scenario, one can't really make /any/ conclusions without further specification as to how it works.
:PROPERTIES:
:Author: appropriate-username
:Score: 0
:DateUnix: 1437002444.0
:DateShort: 2015-Jul-16
:END:

******* We don't have alien biology, we simply have the technology to compensate for lack of other lifeforms.
:PROPERTIES:
:Score: 1
:DateUnix: 1437065527.0
:DateShort: 2015-Jul-16
:END:

******** We have technology to compensate for lack of mitochondria. We have technology that replaces part of /every single cell in our body./ If it's not biology, it's not too functionally different.
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1437173344.0
:DateShort: 2015-Jul-18
:END:
