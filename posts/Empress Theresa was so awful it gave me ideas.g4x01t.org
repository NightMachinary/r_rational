#+TITLE: Empress Theresa was so awful it gave me ideas

* Empress Theresa was so awful it gave me ideas
:PROPERTIES:
:Author: Suitov
:Score: 32
:DateUnix: 1587403277.0
:DateShort: 2020-Apr-20
:FlairText: SPOILERS
:END:
Note: This is just a discussion. I don't have space on my slate to write anything with this in the foreseeable future. So anyone who's interested is welcome to run with the idea.

Note 2: I mention the book's insensitivity towards Israelis below. Let's just say it's stunning.

Having seen [[https://www.youtube.com/watch?v=TedsiCaV2B4][the relevant episode of Down The Rabbit Hole]] a while back, lately I've been following KrimsonRogue's multi-part review of a self-published novel named "Empress Theresa". Fair warning: the full review runs over six hours. [[https://www.youtube.com/watch?v=JopR-biL0I4][Here's part one.]]

In this novel, a 19-year-old girl becomes omnipotent to the limit of her imagination. As you'd expect, she is pretty snotty about it. As you probably expect, she proceeds to Ruin Everything. As you definitely wouldn't expect, /the entire world is fine with this/.

I can't do it justice with a summary, but to give an example of the calibre of ideas here, Theresa's idea to 'solve' the Middle East is to make a brand new island and move all Israelis there. /An island shaped like the Shield of David./ She has the power to do these things unilaterally, has no inhibitions about doing so, and is surrounded by yes-folk up to and including heads of state.

Anyway. Towards the end, the idea of other people gaining similar powers is mentioned, immediately alarming Theresa, and that was when I started thinking "fix fic". I don't currently have time, and definitely don't have the geophysics or politics knowledge, to write this. But if anyone else finds the Mary Sue potential interesting, I'd enjoy hearing what you'd do with this awful setting.

The difficulty factor for our rational newborn space wizards seems to be down to two things (not counting the many ways you could ruin things with your powers if you're careless - Theresa's already done plenty of that by this point. Exploding. North. Pole): firstly, learning to communicate with the entity granting you the powers, which took Theresa a while, and secondly, having only a very limited time before Theresa makes her move to eliminate her rivals. You are at least forewarned because the US president announces everything Theresa does.

Yeah, I did say exploding North Pole.


** ... wow, that's insane. I mean, the solution I'd go for creating a number of pocket-dimension copies of the regions, overwriting the original one, and glue them together with each other and the rest of the planet along the edges (or perhaps something less topologically troublesome). Then every side gets their own equally real copy of the entire region.
:PROPERTIES:
:Author: ABZB
:Score: 54
:DateUnix: 1587403819.0
:DateShort: 2020-Apr-20
:END:

*** Ah yes, the two eigenstate solution
:PROPERTIES:
:Author: SignoreGalilei
:Score: 104
:DateUnix: 1587404562.0
:DateShort: 2020-Apr-20
:END:

**** That name is gold, will use in future.
:PROPERTIES:
:Author: ABZB
:Score: 14
:DateUnix: 1587405060.0
:DateShort: 2020-Apr-20
:END:

***** I think I read it on the comments of UNSONG so don't give me too much credit, but thanks
:PROPERTIES:
:Author: SignoreGalilei
:Score: 29
:DateUnix: 1587407929.0
:DateShort: 2020-Apr-20
:END:


**** You glorious bastard.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 8
:DateUnix: 1587407837.0
:DateShort: 2020-Apr-20
:END:


*** Leading to:

1) Each faction now believes that in order to keep the original holy region free of the infidel, they must gain full control of /all/ the copies of the regions; and/or\\
2) They blame you for destroying the 'original holy land' because no-one can tell if any of the copies is the 'real' one.
:PROPERTIES:
:Author: Geminii27
:Score: 23
:DateUnix: 1587413374.0
:DateShort: 2020-Apr-21
:END:

**** just lie

tell people that you destroyed the other faction and they better behave or else you'll destroy them too

i mean at that point you're basically God so you might as well act like it
:PROPERTIES:
:Author: IICVX
:Score: 12
:DateUnix: 1587422878.0
:DateShort: 2020-Apr-21
:END:

***** Doesn't really work if both copies are connected to the rest of the planet and rumour would spread
:PROPERTIES:
:Author: Sonderjye
:Score: 4
:DateUnix: 1587460686.0
:DateShort: 2020-Apr-21
:END:

****** If you are actually omnipotent, spawn two whole earths with copies of most of the people, one where each side "won" and the other side vanished.

And do that for all the permutations of every conflict.
:PROPERTIES:
:Author: sparr
:Score: 6
:DateUnix: 1587483957.0
:DateShort: 2020-Apr-21
:END:

******* That's about what we ended up with at the end of a "how could god solve the middle east" bullshit session at a con like 30 years ago.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 4
:DateUnix: 1587508850.0
:DateShort: 2020-Apr-22
:END:


****** We are talking about religion here, it's all made up anyways. just lie some more saying it is a “test from god” or whatever.
:PROPERTIES:
:Author: Reply_or_Not
:Score: 3
:DateUnix: 1587480297.0
:DateShort: 2020-Apr-21
:END:


**** 1. is solvable by making any specific copy accessible only if it's the one you were 'assigned' to, so it's impossible to interact with more than one region
2. solves the problem (who cares that they blame you?)
:PROPERTIES:
:Author: Anderkent
:Score: 2
:DateUnix: 1587577479.0
:DateShort: 2020-Apr-22
:END:


**** Yeah, that's basically what I would expect, too
:PROPERTIES:
:Author: ABZB
:Score: 4
:DateUnix: 1587424667.0
:DateShort: 2020-Apr-21
:END:


**** So turn off the part of them that can lose utility because of your decisions
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1587430741.0
:DateShort: 2020-Apr-21
:END:

***** If you're going to modify their values, you may as well make them happy to coexist.
:PROPERTIES:
:Author: Geminii27
:Score: 7
:DateUnix: 1587444910.0
:DateShort: 2020-Apr-21
:END:

****** If you're not going to modify their values you might as well just put their brains in a vat
:PROPERTIES:
:Author: RMcD94
:Score: 4
:DateUnix: 1587458291.0
:DateShort: 2020-Apr-21
:END:


*** This solution reminds me of Unsong :)

[[http://unsongbook.com/]]

It's good, but very strange...
:PROPERTIES:
:Author: lordcirth
:Score: 21
:DateUnix: 1587407844.0
:DateShort: 2020-Apr-20
:END:

**** My solution was inspired by that one, and that solution inspired a Comic-Con costume one year :)
:PROPERTIES:
:Author: ABZB
:Score: 9
:DateUnix: 1587408526.0
:DateShort: 2020-Apr-20
:END:

***** What does an Israel-Palestine Anomaly costume look like?
:PROPERTIES:
:Author: Frommerman
:Score: 4
:DateUnix: 1587640906.0
:DateShort: 2020-Apr-23
:END:


**** Having skimmed TV Tropes's summary, this is SO up my street. And fans have released an ebook version. Nabbed and on my to-read list. :)
:PROPERTIES:
:Author: Suitov
:Score: 2
:DateUnix: 1587643589.0
:DateShort: 2020-Apr-23
:END:


*** The Uriel solution then
:PROPERTIES:
:Author: wren42
:Score: 3
:DateUnix: 1587472987.0
:DateShort: 2020-Apr-21
:END:


*** My solution would be changing the universe to be one of maximum utility
:PROPERTIES:
:Author: RMcD94
:Score: 4
:DateUnix: 1587430687.0
:DateShort: 2020-Apr-21
:END:

**** But that's such an unsatisfying answer, in particular begging the question of how you define all the things therein!
:PROPERTIES:
:Author: ABZB
:Score: 12
:DateUnix: 1587431097.0
:DateShort: 2020-Apr-21
:END:

***** The rational move isn't doing what is most satisfying for a reddit comment or thought experiment.

You're omnipotent and you are having trouble with the definitions of things? You can just make it the perfect definition

Probably the waveform of the universe will be changed to only contain as many of the smallest consciousness in permanent bliss
:PROPERTIES:
:Author: RMcD94
:Score: -1
:DateUnix: 1587432250.0
:DateShort: 2020-Apr-21
:END:

****** [[https://www.smbc-comics.com/comic/happy-3][Relevant SMBC]]
:PROPERTIES:
:Author: not-a-spy
:Score: 12
:DateUnix: 1587449896.0
:DateShort: 2020-Apr-21
:END:

******* Perfect
:PROPERTIES:
:Author: RMcD94
:Score: 3
:DateUnix: 1587458110.0
:DateShort: 2020-Apr-21
:END:


****** We might be disagreeing on a fundamental thing, though - in particular, i strongly suspect that even if there is some universal metric by which different universal utility functions (that is to say, functions that take the state of the universe as input and give a utility value as an output) can be measured and ranked, and there is such an function that has a maximal value by that metric, that that function is the only such one.

Besides that, smallest consciousnesses in a state of eternal bliss is boring even if it maximizes a sufficiently naive utility function.
:PROPERTIES:
:Author: ABZB
:Score: 3
:DateUnix: 1587476367.0
:DateShort: 2020-Apr-21
:END:

******* I see your point but there won't be "different universal utility functions" there will just the one of whoever gets the omnipotent powers. I can't imagine that most people have that many variables for a utility function, and you have to expect that most people have happiness weighted as the most. But whatever utility function is it certainly won't result in earth or planets or vacuums or all the other wasted inanimate matter and lack of matter. No reason to have entropy or laws of conservation or any of those physical rules.

Definitely boring but I'm not sure boring is the standard we set for how god should operate, or how on [[/r/rational]] rather than [[/r/coolthoughtexperiments]] should answer. I'm not sure that it's a naive function either, there is wisdom in simplicity.
:PROPERTIES:
:Author: RMcD94
:Score: 3
:DateUnix: 1587479626.0
:DateShort: 2020-Apr-21
:END:

******** My point is that just because I am omnipotent does not mean that just because I chose a particular utility function it is a best utility function, at least not without altering minds...
:PROPERTIES:
:Author: ABZB
:Score: 1
:DateUnix: 1587480784.0
:DateShort: 2020-Apr-21
:END:

********* I suppose if that's true then it's true right now yet, and would be true for anyone and any action you take. If it's not true and saying the best utility works then there's no problem.

So you might as well optimise utility to whatever you think it is. You can simulate every possible mind having a conference where they decide on a utility function, and simulate AIs with infinite computer power and then choose one of those.

But yes if someone who thought life and happiness and existence was evil took over then their utility function would be the annihilation of everything. Though it seems like the exact same would be satisfied if they shot themselves in the head. Well someone who's utility function values non p zombies suffering they'd still fill the universe with stuff
:PROPERTIES:
:Author: RMcD94
:Score: 3
:DateUnix: 1587481198.0
:DateShort: 2020-Apr-21
:END:

********** A fair point, I've pondered the theory and practical solutions, I was not in the mindspace of "I can just brute-force it"!
:PROPERTIES:
:Author: ABZB
:Score: 1
:DateUnix: 1587481365.0
:DateShort: 2020-Apr-21
:END:

*********** Yeah if there is a solution, and it's possible to discover it in universe, then as an omnipotent god you can certainly have 10 trillion years of philisophy play out instantly
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1587482013.0
:DateShort: 2020-Apr-21
:END:


**** With Friendship and Ponies.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1587509006.0
:DateShort: 2020-Apr-22
:END:


*** What could be an interesting variation: every time someone from one of the bickering factions enters the disputed zone, a new copy of the zone is generated (and it vanishes when they leave). They get to have their very own holy region all to themselves, meaning it 100% belongs to whatever that one person wants it to belong to, for as long as they're prepared to stay there.

Anyone not in the bickering factions gets to enter the original version. Which is probably now significantly calmer.
:PROPERTIES:
:Author: Geminii27
:Score: 2
:DateUnix: 1587564066.0
:DateShort: 2020-Apr-22
:END:

**** I like this one because it gives me vibes of "everyone gets their personal Disneyland - with NO QUEUES!"

And, myself belonging to the autism faction, I'd kind of like access to an at-will human-free pocket dimension too. Recharge in there and rejoin the human race when stress levels are normalised.
:PROPERTIES:
:Author: Suitov
:Score: 3
:DateUnix: 1587642268.0
:DateShort: 2020-Apr-23
:END:


** Did it occur to her to have everyone in the Middle East abruptly and apparently sincerely convert to Unitarian Universalism?
:PROPERTIES:
:Author: RedSheepCole
:Score: 21
:DateUnix: 1587420950.0
:DateShort: 2020-Apr-21
:END:

*** That seems almost more immoral than forcibly moving people, since you're arguably killing them and replacing them with similar people.
:PROPERTIES:
:Author: EthanCC
:Score: 13
:DateUnix: 1587429926.0
:DateShort: 2020-Apr-21
:END:

**** Killing people isn't unjust.

If you can fit 2 billion blissful people in the space one person is then to not would be orders of magnitude worse than the holocaust
:PROPERTIES:
:Author: RMcD94
:Score: 0
:DateUnix: 1587430809.0
:DateShort: 2020-Apr-21
:END:

***** So, that's basically the non-identity problem, which is one of the big open issues in moral philosophy. The question is how we make moral choices about decisions that would lead to different people existing, since all else being the same someone would rather exist than not. So that would mean, logically, a choice that makes a lot of unhappy people is better than one that makes a few happy people, and so on. This can lead to some uncomfortable conclusions, and that's not even going into how were you to program that morality into an AI it would 'dismantle' humanity to make a large number of the least resource-intensive brain that counts as a person under its programming. A lot of work has been done to try to figure out a satisfactory solution to the non-identity problem, so it's hardly the open and shut case you're making it out to be.

On the level of moral justice, depriving someone of life is generally considered an inherently unjust act. Morality often involves different types of justices and benevolences in conflict with each other, with some injustices being allowed to allow some greater justice/benevolence/outcome, so it's not like there's no morally justifiable case where you might kill. But to say murder isn't unjust is absurd.
:PROPERTIES:
:Author: EthanCC
:Score: 25
:DateUnix: 1587433874.0
:DateShort: 2020-Apr-21
:END:

****** I think the argument that existing is inherently better than not existing is pretty deeply flawed, even in the context of moral philosophy.

You don't need magic to create a fate worse than death. People are perfectly sufficient.

Assuming a perfectly moral world might change that answer, but it also makes the question irrelevant as anything but an educational concept model, much like Newtonian physics or electron orbital shells. They're decent models for learning fundamentals and building the right thought patterns for the work you're doing, but they break down rapidly when applied to reality.
:PROPERTIES:
:Author: Angelbaka
:Score: 2
:DateUnix: 1587452786.0
:DateShort: 2020-Apr-21
:END:

******* The argument is not that there can't be a fate worse than death, it's that overall most people prefer to exist, as evidenced by them not committing suicide. That is, there is a lot of space for entities to exist that aren't perfectly happy all the time but also don't want to die.
:PROPERTIES:
:Author: Murska1FIN
:Score: 3
:DateUnix: 1587565873.0
:DateShort: 2020-Apr-22
:END:


****** From a position of omnipotence you have the infinite time of the universe presuming that you can disable entropy. And potentially the multiverse too.

If we imagine the perfect universe with maximum utility then I don't think we see flawed humans who experience unhappiness.

Regardless if its a real problem for you taking all living creatures including bacteria and put their brain into a vat then simulate a universe or just make them happy.

I definitely don't think to say utilitarianism is absurd is true. Whatever decision you make or don't will butterfly effect quadrillions of possible lives. To minimise potential death if that's what you care about changing the waveform of the universe to be deterministic and removing the possibility of future life is the easiest and most moral solution

Also definitely don't agree that existence is better than not, I don't think bacteria have an opinion and sapient creatures can suffer and do kill themselves or increase their risk of death anyway. No one behaves like living is the most important thing
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1587458875.0
:DateShort: 2020-Apr-21
:END:

******* u/EthanCC:
#+begin_quote
  presuming that you can disable entropy.
#+end_quote

That would instantly kill everyone everywhere. Entropy is the reason why, among other things, osmosis works, and you kind of need that for life.

#+begin_quote
  I definitely don't think to say utilitarianism is absurd is true.
#+end_quote

I never said it was (I said "killing isn't unjust" is absurd), but you didn't describe utilitarianism. You described killing one person to bring about a billion happy people in the future (unless I misread that, but the way it was phrased implied creating more people), which could be justified with utilitarianism, but whether that's a good thing to do within utilitarianism is an open issue. It's one of the outcomes that people who try to make utilitarian theories try to avoid, since it gives weird prescriptions when it comes to policy. But it's hard to formulate an argument against it given the nature of utilitarianism (human happiness has diminishing returns, human number doesn't).

Making a billion happy people in the future in exchange for one person dying now is a slightly less discomforting version of the non-identity problem than the standard version of choosing a future with more unhappy people over one with fewer happy people, but it runs directly into the problem the NID illustrates- how do we value people who don't yet exist?

#+begin_quote
  To minimise potential death if that's what you care about changing the waveform of the universe to be deterministic and removing the possibility of future life is the easiest and most moral solution
#+end_quote

That's mathematically impossible. If the universe is deterministic (within our timeline, many worlds gets around it by having everything happen) either some time travel/acausality is going on or we couldn't make the observations we have. That's just how the math works out in quantum physics, see Bell's Theorem (Bell preferred the acausality approach, actually). Changing that wouldn't mean changing the waveform, it would mean changing the laws of physics themselves, with the end result not being a waveform (wavelike properties are what gives rise to uncertainty, and also electrons not being pulled into the center of atoms).

#+begin_quote
  Also definitely don't agree that existence is better than not
#+end_quote

The idea is that all else being the same people would generally prefer to live vs not to, so whatever hypothetical reality you're talking about would be full of people who would rather you made the choices that lead to them existing. On average, this is true- the average person wouldn't take the choice to retroactively erase themselves from existence. For the most part unhappy people would still like to exist, so if you're worried about helping the most amount of people then you should work to make a larger number of people, regardless of their quality of life, since they'd still want to exist. Looking at utility you also get better results making a lot of unhappy people than a few happy ones since happiness runs into diminishing returns.
:PROPERTIES:
:Author: EthanCC
:Score: 3
:DateUnix: 1587474111.0
:DateShort: 2020-Apr-21
:END:

******** u/RMcD94:
#+begin_quote
  That would instantly kill everyone everywhere. Entropy is the reason why, among other things, osmosis works, and you kind of need that for life.
#+end_quote

I don't know why anyone would enable anything other than willing considered death. Why would you need osmosis? You're omnipotent having things stuck to physical laws is a design flaw.

#+begin_quote
  I never said it was (I said "killing isn't unjust" is absurd), but you didn't describe utilitarianism. You described killing one person to bring about a billion happy people in the future (unless I misread that, but the way it was phrased implied creating more people), which could be justified with utilitarianism, but whether that's a good thing to do within utilitarianism is an open issue. It's one of the outcomes that people who try to make utilitarian theories try to avoid, since it gives weird prescriptions when it comes to policy. But it's hard to formulate an argument against it given the nature of utilitarianism (human happiness has diminishing returns, human number doesn't).
#+end_quote

The most ethical outcome for the most number of people. That's utilitarianism. Just because some people aren't happy with what that means and then decide to add in things so you don't kill people and steal their organs (which is really a practical flaw in how humans behave, as if humans were all rational it would not be an issue, it's only an issue because it changes how people/society behaves) doesn't apply at all to being omnipotent. You don't need to worry about practicality, long term social consequences, or anything else.

#+begin_quote
  Changing that wouldn't mean changing the waveform, it would mean changing the laws of physics themselves, with the end result not being a waveform (wavelike properties are what gives rise to uncertainty, and also electrons not being pulled into the center of atoms).
#+end_quote

Oh fair enough then, I thought a universe completely empty of any matter would still have a waveform and be deterministic but I guess I misunderstood that. In that case change off a waveform yeah.

#+begin_quote
  The idea is that all else being the same people would generally prefer to live vs not to, so whatever hypothetical reality you're talking about would be full of people who would rather you made the choices that lead to them existing.

  On average, this is true- the average person wouldn't take the choice to retroactively erase themselves from existence. For the most part unhappy people would still like to exist, so if you're worried about helping the most amount of people then you should work to make a larger number of people, regardless of their quality of life, since they'd still want to exist. Looking at utility you also get better results making a lot of unhappy people than a few happy ones since happiness runs into diminishing returns.
#+end_quote

I mean this is absurd. If you're God why would you care at all about what people want? If there's a world where people get really blissful about torturing people, and only when the people are not p zombies and really actually get tortured and the net gain is insane compared to the other gains then we should follow it because that's what they want?

If I really cared that people "want" to exist I'd simply make them all suicidal a microsecond before I changed the universe to be a bliss continuum.

#+begin_quote
  Looking at utility you also get better results making a lot of unhappy people than a few happy ones since happiness runs into diminishing returns.
#+end_quote

I simply do not agree with this. Unhappy people are a net negative on utility that values happiness, unless they will produce enough offspring or cause to others enough happiness to outweigh them. But regardless if you are god why would you allow unhappiness anyway

There is no issue with diminishing returns when you are god because you can turn off diminishing returns.
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1587480184.0
:DateShort: 2020-Apr-21
:END:

********* u/EthanCC:
#+begin_quote
  Why would you need osmosis?
#+end_quote

Because... biology. You never said anything other than turning off entropy and my brain went to "well I guess time is physically meaningless now".

#+begin_quote
  The most ethical outcome for the most number of people.
#+end_quote

K, define ethical. It's not easy, it's not solved. The issue I mentioned earlier- where you make a lot of very unhappy people (or very low happiness people if you want to say that happiness can be negative) instead of a few happy people- is an outcome of utilitarianism unless you try to build things in a way to avoid that. Most utilitarians seem to dislike this outcome, as it seems unethical.

Among other things it prescribes no abortion in the case of rape, no attempts to deal with climate change unless it threatens mass extinction of humanity, etc.

Utilitarianism isn't objective, no moral philosophy is. In order to construct a utilitarian theory you first need a theory of what outcomes are ethical, and *so pointing out that a utilitarian theory leads to an immoral outcome is a valid criticism*. Arguably the /most/ valid criticism. If your ethical theory focuses on the method rather than the outcome it's not utilitarian, it's deontological. Any working utilitarian theory has to lead to ethical outcomes exclusively.

#+begin_quote
  If you're God why would you care at all about what people want?
#+end_quote

If you see no problem with ignoring the desires of others when making decisions, your morality has shaky foundations. It's generally acknowledged that self-determination is a right.

#+begin_quote
  I simply do not agree with this.
#+end_quote

[[https://link.springer.com/article/10.1023/A:1010080228107][Well, you're wrong,]] happiness shows diminishing returns to the best of our ability to measure it. Our methods of measuring happiness don't go below 0, so you can't really argue that someone is a net negative on happiness without postulating a measurement that doesn't even exist. Setting any level at 0 is arbitrary and leads to mass murder of unhappy people- an outcome to be avoided.

#+begin_quote
  There is no issue with diminishing returns when you are god because you can turn off diminishing returns.
#+end_quote

Sticking everyone in a pleasure coma is also a bad end for humanity, unless you've completely lost sight of morality in an attempt to make something objective by being stubbornly reductive.
:PROPERTIES:
:Author: EthanCC
:Score: 1
:DateUnix: 1587586894.0
:DateShort: 2020-Apr-23
:END:

********** u/RMcD94:
#+begin_quote
  Because... biology. You never said anything other than turning off entropy and my brain went to "well I guess time is physically meaningless now".
#+end_quote

Yes but if you're turning off entropy I would think it would be obvious that you would also be keeping the universe functional for your goals

#+begin_quote
  K, define ethical. It's not easy, it's not solved. The issue I mentioned earlier- where you make a lot of very unhappy people (or very low happiness people if you want to say that happiness can be negative) instead of a few happy people- is an outcome of utilitarianism unless you try to build things in a way to avoid that. Most utilitarians seem to dislike this outcome, as it seems unethical.
#+end_quote

Among other things it prescribes no abortion in the case of rape, no attempts to deal with climate change unless it threatens mass extinction of humanity, etc.

Utilitarianism isn't objective, no moral philosophy is. In order to construct a utilitarian theory you first need a theory of what outcomes are ethical, and so pointing out that a utilitarian theory leads to an immoral outcome is a valid criticism. Arguably the most valid criticism. If your ethical theory focuses on the method rather than the outcome it's not utilitarian, it's deontological. Any working utilitarian theory has to lead to ethical outcomes exclusively.

I already discussed this with another person. If there's no scenario in which you can use omnipotence to derive any moral philosophy, that is even having every potential mind meet up and derive a utility function, or simulating people talking about it for 10 trillion year or making 10 quadrillion AIs whose only job is to find the best moral function.

Then you can't do it without omnipotence and you shouldn't take any action at all because you can't know if it's actually good or not.

#+begin_quote
  K, define ethical. It's not easy, it's not solved.
#+end_quote

You can define it however you like. Whatever you define as ethical as and you do it to the most people is utilitarianism.

#+begin_quote
  Utilitarianism isn't objective, no moral philosophy is. In order to construct a utilitarian theory you first need a theory of what outcomes are ethical, and so pointing out that a utilitarian theory leads to an immoral outcome is a valid criticism. Arguably the most valid criticism. If your ethical theory focuses on the method rather than the outcome it's not utilitarian, it's deontological. Any working utilitarian theory has to lead to ethical outcomes exclusively.
#+end_quote

I agree morality isn't objective. I don't agree that if I say that my morality is aligned with utilitarian that you can then say that outcomes are immoral. All outcomes are moral if my axiom is that utilitarianism is moral.

#+begin_quote
  where you make a lot of very unhappy people (or very low happiness people if you want to say that happiness can be negative) instead of a few happy people- is an outcome of utilitarianism unless you try to build things in a way to avoid that.
#+end_quote

Oh, yes. Absolutely low happiness and unhappiness are completely different. So yes I absolutely agree that billions of slightly or bored people are better than one really happy person.

Why on earth would you say unhappy and mean low happiness? That seems like you're being deliberately misleading for no benefit...

#+begin_quote
  If you see no problem with ignoring the desires of others when making decisions, your morality has shaky foundations. It's generally acknowledged that self-determination is a right.
#+end_quote

Egoism is one of the least shaky moral philosophies. In fact it's almost impossible to have "shaky" foundations if you're consistent since everyone has arbitrary axioms. Generally acknowledged that black people were inferior, general acknowledgement means nothing. And if you do value that then you can get a solution for what you should do as a God by consensus of every possible mind as I mentioned earlier.

#+begin_quote
  Well, you're wrong, happiness shows diminishing returns to the best of our ability to measure it. Our methods of measuring happiness don't go below 0, so you can't really argue that someone is a net negative on happiness without postulating a measurement that doesn't even exist. Setting any level at 0 is arbitrary and leads to mass murder of unhappy people- an outcome to be avoided.
#+end_quote

I was clearly and obviously treating unhappiness as meaning negative happiness like everyone in the world does. It is better to kill slightly unhappy people than let them exist (assuming every man is an island) if your utility function is maximising happiness.

The reason people say unhappy people shouldn't be murdered is because we live in a society and humans psychologically react to that. If you're omnipotent you do not have worry about the impacts of that. A lot of moral philosophy is people having certain outcomes they like and then just working backwards until they can justify it, if you approach it by deciding on an axiom first (ie I value happiness) you would never get these outcomes. It's most obvious in statements like those.

#+begin_quote
  Sticking everyone in a pleasure coma is also a bad end for humanity, unless you've completely lost sight of morality in an attempt to make something objective by being stubbornly reductive.
#+end_quote

Disagree, the only reason you say that is personal taste. I obviously think a pleasure coma is boring but I don't make rational decisions based on stuff being exciting.

[[https://www.smbc-comics.com/comic/happy-3]]

If we compare two universes, one with the happy having finished the universe and any other one, that universe will win in terms of bliss, happiness, outcomes, equality, any ethical measurement you want.
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1587595282.0
:DateShort: 2020-Apr-23
:END:

*********** u/EthanCC:
#+begin_quote
  Yes but if you're turning off entropy I would think it would be obvious that you would also be keeping the universe functional for your goals
#+end_quote

I'm pretty sure it's mathematically impossible to turn off entropy and keep the universe functioning in any sense of the word. Entropy is the observation that things tend to spread out over time, and an extension of a property of information besides.

You forgot some > btw.

#+begin_quote
  I already discussed this with another person. If there's no scenario in which you can use omnipotence to derive any moral philosophy, that is even having every potential mind meet up and derive a utility function, or simulating people talking about it for 10 trillion year or making 10 quadrillion AIs whose only job is to find the best moral function.
#+end_quote

I'm not sure you can prove it (proving a negative and all that), but it seems very likely from observation that there's no objective morality and the is/ought problem is one of those unsolvable things, making the scenario you lay out here doomed to fail. If you're not having them reach an objective ethical system, but rather one that ties together existing intuitions, then that's /what I'm arguing for/, and it certainly wouldn't look like a "happiness above all else" system. If you can solve it the whole discussion is moot, since it relies on information we can't know anyway, and if you can't we're back at me saying "wow that's pretty fucked up".

#+begin_quote
  You can define it however you like. Whatever you define as ethical as and you do it to the most people is utilitarianism.
#+end_quote

That's not really the definition of utilitarianism. If you define an /action/ as ethical as opposed to an outcome, you're doing deontology. If you define a person as ethical you're doing virtue ethics. The issue is that the lack of an objective utility function puts you on the same level as the rest of us, so if the rest of us thing your utility function leads to immoral outcomes you don't really have anything to appeal to.

#+begin_quote
  All outcomes are moral if my axiom is that utilitarianism is moral.
#+end_quote

And if the rest of us disagree? Modern ethics focuses around taking things that we all agree seem ethical and trying to make a theory about them so that we can solve the more controversial problems. If A => B, and B => C, then A =>C; where A and B are things we agree on, C is one choice in a controversy, and what we're trying to find is =>. In a subjective situation the best we can do is try to all agree, there's nothing noble about choosing a reductive => and ignoring that most others would disagree.

#+begin_quote
  Why on earth would you say unhappy and mean low happiness? That seems like you're being deliberately misleading for no benefit...
#+end_quote

Unhappy is low happiness. We have no way to define happiness such that there is anything below 0, because as far as we can tell there really isn't an objective measure of happiness. What we do is try to fit people on a scale from what we've observed as least happy to most happy, in that case we have no place to actually put an objective 0.

#+begin_quote
  Egoism is one of the least shaky moral philosophies. In fact it's almost impossible to have "shaky" foundations if you're consistent since everyone has arbitrary axioms. Generally acknowledged that black people were inferior, general acknowledgement means nothing. And if you do value that then you can get a solution for what you should do as a God by consensus of every possible mind as I mentioned earlier.
#+end_quote

Racism was contradicted by other morals, it certainly wasn't an appreciation of the science that's lead to it reducing over time. The foundations of an ethical philosophy shouldn't /just/ be internal consistency, though that's important, they should also align with existing intuitions about what is moral. Ethics is hard, reading a LessWrong post won't solve it for you. As an aside LW generally takes a very... sophomoric approach to fields, the whole problem of someone who's self-taught not being told they're wrong or knowing where the current research is, so I wouldn't try to learn much from it directly.

#+begin_quote
  The reason people say unhappy people shouldn't be murdered is because we live in a society and humans psychologically react to that. If you're omnipotent you do not have worry about the impacts of that.
#+end_quote

This is where you differ from nearly everyone else, since the rest of us would say death is inherently bad even aside from whatever consequences you'd face from killing.

#+begin_quote
  A lot of moral philosophy is people having certain outcomes they like and then just working backwards until they can justify it
#+end_quote

Well, /yeah/. Where else are you going to start? Any axioms are just as subjective, being based on the same sort of thinking of arbitrarily choosing one thing as good. The difference is that working back from what seems moral gives a theory that actually leads to outcomes that seem moral, whereas starting from a reductive axiom leads to things that seem awful. This is why the people who spend their lives thinking about these things (and have covered the same territory you are) focus more on trying to fit intuitions together than on ignoring them and choosing an entirely other set of subjective goals. Another thing to consider is practical application- humans are very bad at predicting the future, even with math, and we can't measure happiness very well. Trying to maximize happiness is nearly impossible in most situations, so you have to fall back on heuristics which probably look almost identical to what we think of as normal moral behavior. You just argue yourself back into square one.
:PROPERTIES:
:Author: EthanCC
:Score: 1
:DateUnix: 1587669938.0
:DateShort: 2020-Apr-23
:END:

************ u/RMcD94:
#+begin_quote
  I'm pretty sure it's mathematically impossible to turn off entropy and keep the universe functioning in any sense of the word. Entropy is the observation that things tend to spread out over time, and an extension of a property of information besides.
#+end_quote

I think you're being obtuse here and it makes me not to want to continue the discussion. Can't you steelman me here rather than make me go through a define what I mean exactly what I would do as omnipotent when I just short hand say get rid of entropy? If you can't "mathematically" undo the trend to disorder you can just pump energy in from a magic omnipotent source. Whether that means spawning suns in or whatever you want.

#+begin_quote
  You forgot some > btw.
#+end_quote

my bad

#+begin_quote
  I'm not sure you can prove it (proving a negative and all that), but it seems very likely from observation that there's no objective morality and the is/ought problem is one of those unsolvable things, making the scenario you lay out here doomed to fail. If you're not having them reach an objective ethical system, but rather one that ties together existing intuitions, then that's what I'm arguing for, and it certainly wouldn't look like a "happiness above all else" system. If you can solve it the whole discussion is moot, since it relies on information we can't know anyway, and if you can't we're back at me saying "wow that's pretty fucked up".
#+end_quote

i would not argue for tying together other systems, i said: i would make the universe maximum utility, someone said: what is maximum utility, i said: the happiness molecules

i think mathematically you'd be hard pressed to find something with higher maximum utility than the simplest possible beings that feel constantly amazing as tightly packed as possible. any compromise solution like you are suggesting would be inferior to that as it seems like you won't genocide the whole universe so you're going to be stuck with badly designed (evolved) people who are not optimised for the maximization of anything

anything you do to maximise utility i could do and also change the mind of the person to enjoy it more, and also split the consciousness of that person into a billion so there are more people experiencing that positive utility

#+begin_quote
  Unhappy is low happiness. We have no way to define happiness such that there is anything below 0, because as far as we can tell there really isn't an objective measure of happiness. What we do is try to fit people on a scale from what we've observed as least happy to most happy, in that case we have no place to actually put an objective 0.
#+end_quote

Unhappiness is sadness.

Least happy is not the same as being sad. You know what the least happy thing is? A rock. Is a rock unhappy? No. The least excited thing is a rock. Is it bored? No

#+begin_quote
  That's not really the definition of utilitarianism. If you define an action as ethical as opposed to an outcome, you're doing deontology. If you define a person as ethical you're doing virtue ethics. The issue is that the lack of an objective utility function puts you on the same level as the rest of us, so if the rest of us thing your utility function leads to immoral outcomes you don't really have anything to appeal to.
#+end_quote

I quoted the definition of utilitarianism, I did not write the definition. Yes, I agree if it leads to immoral outcomes there is nothing to appeal to. Except there will be no immoral outcome because everything is justified if it increases utility. Torturing that person increases utility? It's not an immoral outcome then.

#+begin_quote
  And if the rest of us disagree? Modern ethics focuses around taking things that we all agree seem ethical and trying to make a theory about them so that we can solve the more controversial problems. If A => B, and B => C, then A =>C; where A and B are things we agree on, C is one choice in a controversy, and what we're trying to find is =>. In a subjective situation the best we can do is try to all agree, there's nothing noble about choosing a reductive => and ignoring that most others would disagree.
#+end_quote

Fine by me disagree as you like I am not interested in this motive as this would lead to the justification of racism or meat eating.

#+begin_quote
  Racism was contradicted by other morals, it certainly wasn't an appreciation of the science that's lead to it reducing over time. The foundations of an ethical philosophy shouldn't just be internal consistency, though that's important, they should also *align with existing intuitions* about what is moral. Ethics is hard, reading a LessWrong post won't solve it for you. As an aside LW generally takes a very... sophomoric approach to fields, the whole problem of someone who's self-taught not being told they're wrong or knowing where the current research is, so I wouldn't try to learn much from it directly.
#+end_quote

There was tons of moralizing to do with justifying racism, just as there is with meat eating. I disagree that internal consistency is less important than anything else. If your moral philosophy is not consistent then it is not sound. This is classic washing technique people try to do where they act like no philosophers ever thought about the bad parts of the past and only we're so lucky now that everyone is thinking about things and we know what's good and bad correctly this time!

I VEHEMENTLY disagree with the bolded statement. Clearly we are approaching morality in a different way, anyone who suggests this would have been an advocate for slavery, probably supports meat eating and more.

#+begin_quote
  This is where you differ from nearly everyone else, since the rest of us would say death is inherently bad even aside from whatever consequences you'd face from killing.
#+end_quote

Sure I don't have an issue disagreeing with people as I said. I can look at poll results for any sort of thing that I would not like and see that "nearly everyone else" certainly has swung all over the place throughout history, even in the last 100 years that we even have records. Regardless I am hardly unique I've spoken with dozens of utilitarians who accept that conclusion.

#+begin_quote
  Well, yeah. Where else are you going to start? Any axioms are just as subjective, being based on the same sort of thinking of arbitrarily choosing one thing as good. The difference is that working back from what seems moral gives a theory that actually leads to outcomes that seem moral, whereas starting from a reductive axiom leads to things that seem awful. This is why the people who spend their lives thinking about these things (and have covered the same territory you are) focus more on trying to fit intuitions together than on ignoring them and choosing an entirely other set of subjective goals. Another thing to consider is practical application- humans are very bad at predicting the future, even with math, and we can't measure happiness very well. Trying to maximize happiness is nearly impossible in most situations, so you have to fall back on heuristics which probably look almost identical to what we think of as normal moral behavior. You just argue yourself back into square one.
#+end_quote

Start from the axiom?

What would good axioms be, well happiness is literally good. If you have a scenario and you add happiness to it it literally cannot be worse. I can't think of a single other trait that this is true to.

I'm not going to be someone who goes "oh wow that outcome makes me feel bad so let's go back and randomly change my axioms until they are completely arbitrary until there's absolutely no way I could convince anyone else that they should assign a weight of 3.35 to happiness and 4124.56345 to liberty and -1234904 to unwanted death or whatever other stupid numbers would come as a result of trying to actually institute these moral philosophies.

Because that's what you're doing when you add more than one axiom. If you say unwanted death is good, and happiness is good then you have to tell me how much happiness is worth an unwanted death. 100 billion? etc

Virtue ethics side steps this problem iirc
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1587682189.0
:DateShort: 2020-Apr-24
:END:

************* u/EthanCC:
#+begin_quote
  I think you're being obtuse here and it makes me not to want to continue the discussion. Can't you steelman me here rather than make me go through a define what I mean exactly what I would do as omnipotent when I just short hand say get rid of entropy? If you can't "mathematically" undo the trend to disorder you can just pump energy in from a magic omnipotent source. Whether that means spawning suns in or whatever you want.
#+end_quote

I just read what you wrote, I'm not telepathic.

#+begin_quote
  i would not argue for tying together other systems, i said: i would make the universe maximum utility, someone said: what is maximum utility, i said: the happiness molecules
#+end_quote

Are you going to make the universe an infinite expanse of people on a morphine high? You're missing out on a lot of other goods by reducing the human experience to seratonin.

#+begin_quote
  There was tons of moralizing to do with justifying racism, just as there is with meat eating. I disagree that internal consistency is less important than anything else. If your moral philosophy is not consistent then it is not sound. This is classic washing technique people try to do where they act like no philosophers ever thought about the bad parts of the past and only we're so lucky now that everyone is thinking about things and we know what's good and bad correctly this time!
#+end_quote

WDYM? I said internal consistency is important, but you /also/ need your theory to contain the existing intuitions. That's the whole point. The moralizing to justify racism conflicted with other moral beliefs, which eventually lead to it becoming less popular over time.

Like I said, we haven't fully solved ethics- not even close, but we have a lot of work to build off of. You're basically ignoring all that in the pursuit of a simple and internally consistent system, but that system you've come up with doesn't actually match up with the rest of our intuitions about what is ethical, so it's no more justifiable than any other hypothetically consistent system.

The point is to get a system that is both:

- internally consistent

- aligned with existing intuitions

If you find a behavior conflicts with an important moral, you stop doing it. My first philosophy professor was vegan, people who do this for a living think of these things too. Have you actually read any philosophy outside of utilitarianism? Or utilitarian philosophers for that matter, since most work on the subject includes heuristics like a human rights both from a practical perspective (they're one of the best methods of increasing happiness we've found) and to avoid undesirable outcomes. Benthamite utilitarianism is a pretty unpopular position today, it breaks down when you start to look at it to closely or try to apply it.

#+begin_quote
  I VEHEMENTLY disagree with the bolded statement. Clearly we are approaching morality in a different way, anyone who suggests this would have been an advocate for slavery, probably supports meat eating and more.
#+end_quote

Slavery violated other moral axioms. People didn't say "this reduces net happiness", they said "this is cruel and unjust". The thing you say reinforced slavery /helped end it./ [[https://www.jstor.org/stable/2210004?seq=1][There were utilitarians who argued for slavery]], it's not unique to any way of thinking about morality because the justifications for slavery were for the sake of economic self-interest, and in clear conflict with moral intuitions even as people tried to twist morality to justify slavery. It's a classic example of self-deception, not any failure of morality aside form the well-documented tendency of people to ignore morality when convenient. Which is something utilitarianism makes much easier, because it allows you to set aside all limitations if you /think/ you're bringing about the best end.

#+begin_quote
  well happiness is literally good
#+end_quote

Is it? Is it the only good? Make an argument besides "it is". Or rather, argue why anything else /isn't/ good.

#+begin_quote
  If you have a scenario and you add happiness to it it literally cannot be worse. I can't think of a single other trait that this is true to.
#+end_quote

Someone just murdered 10 people. Instead of remorse they feel joy. Our intuitions about morality say this is worse. It's only better if you've already accepted and internalized the proposition that happiness is the ultimate good- it's begging the question to argue this is better than someone being unhappy about committing murder.

#+begin_quote
  Virtue ethics side steps this problem iirc
#+end_quote

Virtue ethics says that some people are good and whatever they do is good regardless of what it is. It's protagonist centered morality applied to real life and hasn't been in vogue in centuries (unless you count the Nazis).
:PROPERTIES:
:Author: EthanCC
:Score: 1
:DateUnix: 1587767418.0
:DateShort: 2020-Apr-25
:END:

************** u/RMcD94:
#+begin_quote
  Are you going to make the universe an infinite expanse of people on a morphine high? You're missing out on a lot of other goods by reducing the human experience to seratonin.
#+end_quote

Yes I linked the SMBC comic I thought it was quite clear.

#+begin_quote
  WDYM? I said internal consistency is important, but you also need your theory to contain the existing intuitions. That's the whole point. The moralizing to justify racism conflicted with other moral beliefs, which eventually lead to it becoming less popular over time.
#+end_quote

People didn't give up racism because it conflicted with their moral beliefs. Racism ended because it wasn't economic. Meat eating will end when it's not economic.

And tons of people had sound moral frameworks in which slavery was justified, just like people have sound moral frameworks to justify their consumption of meat, or going on holiday, or not giving their entire income up to save 20 people from starvation or w/e.

#+begin_quote
  You're basically ignoring all that in the pursuit of a simple and internally consistent system, but that system you've come up with doesn't actually match up with the rest of our intuitions about what is ethical, so it's no more justifiable than any other hypothetically consistent system.
#+end_quote

Yes as I said the only thing that makes it more justifiable is that my system never has to argue with someone about why happiness is arbitrarily worth 5.425 and not 5.421.

#+begin_quote
  Is it? Is it the only good? Make an argument besides "it is". Or rather, argue why anything else isn't good.
#+end_quote

Anything else is good because it causes happiness.

#+begin_quote
  Someone just murdered 10 people. Instead of remorse they feel joy. Our intuitions about morality say this is worse. It's only better if you've already accepted and internalized the proposition that happiness is the ultimate good- it's begging the question to argue this is better than someone being unhappy about committing murder.
#+end_quote

Which universe is superior?

11 people spawn. 1 person kills 10 people. They feel sad. The universe ends.

11 people spawn. 1 person kills 10 people. They feel happy. The universe ends.

Quite clear to me.

#+begin_quote
  Virtue ethics says that some people are good and whatever they do is good regardless of what it is. It's protagonist centered morality applied to real life and hasn't been in vogue in centuries (unless you count the Nazis).
#+end_quote

??? Virtue ethics is more popular than deontology and consequentialism among philosophers, the more this conversation goes on the more I feel like you're just wasting my time

[[https://www.econlib.org/archives/2009/12/what_do_philoso.html]]

Normative ethics: deontology, consequentialism, or virtue ethics? Lean toward: virtue ethics 541 / 3226 (16.7%) Lean toward: consequentialism 496 / 3226 (15.3%) Lean toward: deontology 428 / 3226 (13.2%) Accept: consequentialism 290 / 3226 (8.9%) Accept: virtue ethics 263 / 3226 (8.1%) Accept more than one 230 / 3226 (7.1%) Accept: deontology 228 / 3226 (7%) Accept an intermediate view 132 / 3226 (4%)

#+begin_quote
  aligned with existing intuitions

  Slavery violated other moral axioms
#+end_quote

I'm done with this conversation. I've repeated a hundred times that people intuitively were okay with slavery and meat eating and yet you seem determined to pretend that there was no historical philosophers who supported slavery within all of their moral axioms. I refuse to engage with someone who believes that people who supported slavery were all just being inconsistent, or weren't following their intuitions
:PROPERTIES:
:Author: RMcD94
:Score: 0
:DateUnix: 1587769286.0
:DateShort: 2020-Apr-25
:END:

*************** u/EthanCC:
#+begin_quote
  Yes I linked the SMBC comic I thought it was quite clear.
#+end_quote

That's supposed to be a joke lmao.

#+begin_quote
  Yes as I said the only thing that makes it more justifiable is that my system never has to argue with someone about why happiness is arbitrarily worth 5.425 and not 5.421.
#+end_quote

Your argument isn't more justifiable just because you haven't bothered to quantify things, in fact that makes it /less/ justifiable since you can't actually define the ends you're trying to reach. Without quantification you have trouble arguing between two qualitatively similar ends.

You haven't solved the problem. You've ignored all existing axioms, constructed an entirely different problem, and solved that. A theory that includes existing widely held intuitions and is internally consistent is inherently more justifiable since that would have less to argue against. If you want to argue something has no ethical value, you need to do more than assert it.

#+begin_quote
  Anything else is good because it causes happiness.
#+end_quote

That's a circular argument. You need to argue against things like justice, self-determination, right to life, and so on before you can reduce the whole problem purely to happiness. You've ignored the hard part of the problem, skipped to the 'solution', then worked backwards assuming the solution was true. The argument only works if the conclusion is correct- a conclusion can't be a premise, QED the argument is meaningless.

#+begin_quote
  Quite clear to me.
#+end_quote

Because you've begged the question. This is only an argument if happiness is the only good but you've done nothing to support that idea.

#+begin_quote
  ??? Virtue ethics is more popular than deontology and consequentialism among philosophers, the more this conversation goes on the more I feel like you're just wasting my time
#+end_quote

I went to the [[https://philpapers.org/surveys/results.pl][original source]] and these are the actual results:

Other 301 / 931 (32.3%)

Accept or lean toward: deontology 241 / 931 (25.9%)

Accept or lean toward: consequentialism 220 / 931 (23.6%)

Accept or lean toward: virtue ethics 169 / 931 (18.2%)

Virtue ethics is literally the least popular. So either the source you used is using old data or reported wrong, either way you stopped as soon as you found something that agreed with you and ended up being wrong.

#+begin_quote
  I've repeated a hundred times that people intuitively were okay with slavery
#+end_quote

Ok... explain all the people who /weren't/. Slavery did actually violate some widely held moral axioms at the time (to be clear this is Enlightenment and right afterwards)- right to liberty being a big one. The recognition of this became more widely spread among philosophers, but putting it into practice in areas where slaves were held ran into economic barriers.

Justifications based on self-deception are nothing more or less than that, and a problem of any ethical system. The counterargument is to show the hypocrisy, not to try to convince them of a completely new arbitrary system, and the only way to consistently prevent self-deceptive action is to create hard limits on what you can do... something utilitarianism ignores. Utilitarians also constructed arguments to support slavery, your system isn't privileged in that way (that was the source I gave, not sure what you mean when you say I denied that... *I literally gave an example of a philosopher supporting slavery* in Thomas Cooper, so we can add 'not reading sources' to your list of rationality sins).

#+begin_quote
  I'm done with this conversation.
#+end_quote

Translation: "I realized I fucked up and got into an argument about something I don't understand."
:PROPERTIES:
:Author: EthanCC
:Score: 1
:DateUnix: 1587862263.0
:DateShort: 2020-Apr-26
:END:


*** No because that's heresy. From the authors website:

#+begin_quote
  Theresa's faith is the source of her triumph﻿. Take a large group of people.  Impose the same difficult situation on them.  Gradually increase the difficulty and watch what happens.  One by one people will drop out of the challenge.

  Theresa is challenged with difficulties she calls 'impossible', but she doesn't give up.  To much is at stake to give her the luxury of walking away. 

  What keeps her going?  She trusts that God will get her through it somehow.  Later in the story President Stinson expressed this idea:  "I can't believe a God who brought her this far without making mistakes will let her make one now."
#+end_quote
:PROPERTIES:
:Author: Nic_Cage_DM
:Score: 3
:DateUnix: 1587554055.0
:DateShort: 2020-Apr-22
:END:

**** See, whenever I read these stories where people get the ability to overcome all obstacles simply by wanting the thing bad enough, I picture omnipotent toddlers steamrolling humanity.
:PROPERTIES:
:Author: RedSheepCole
:Score: 8
:DateUnix: 1587556550.0
:DateShort: 2020-Apr-22
:END:

***** Theresa basically is a toddler, morally and in terms of common sense. Her intelligence is informed but definitely not shown on page...
:PROPERTIES:
:Author: Suitov
:Score: 4
:DateUnix: 1587644264.0
:DateShort: 2020-Apr-23
:END:


** I feel like a omnipotent rational protagonist wouldn't be a very interesting story and might not be possible to write at all. The first thing they should do is make themselves superintellegent and omniscient, so they have perfect information and can make the absolute best decisions. Then solving every problem wouldn't be much different than playing a perfect game of tic tac toe. The problem with writing this is I don't think it's possible to show the perspective of someone so far beyond human level intelligence. You could write it from another perspective, but finding satisfying solutions to every problem that everyone realistically agrees with is going to be impossibly hard as well.
:PROPERTIES:
:Author: CompactDisko
:Score: 22
:DateUnix: 1587419983.0
:DateShort: 2020-Apr-21
:END:

*** You could write a nearly omnipotent protagonist who was /trying/ to be rational.

The first step would have them be deathly afraid of modifying their own mind.

Even if you do have them be smart enough to try intelligence enhancement, you could have them be concerned about what the optimal state even is. What /do/ they do about the Middle East? Do they just sort of hope people stop caring about it once all their needs are taken care of? When they try to duplicate the space, what do they do with the people who aren't satisfied with that solution?

How do they handle religious people who think they are tampering in gods domain? How do they handle religious people who think the protagonist is god?

Do they start a super powered CPS? Does anyone have a right to privacy from the demi-urge? Does anyone get to vote on measures?

At this point, it is basically a discourse on ethics with a super powered being acting as the viewpoint character.
:PROPERTIES:
:Author: immortal_lurker
:Score: 25
:DateUnix: 1587429706.0
:DateShort: 2020-Apr-21
:END:

**** All of those things seem very small minded. What's the middle east in context of a trillion stars?p

Why would you even have planets or the sun or anything like that? Just inefficient.

Turn the whole universe into a dense constant bliss
:PROPERTIES:
:Author: RMcD94
:Score: 3
:DateUnix: 1587430902.0
:DateShort: 2020-Apr-21
:END:

***** Yikes.
:PROPERTIES:
:Author: dinoseen
:Score: 1
:DateUnix: 1587612272.0
:DateShort: 2020-Apr-23
:END:


*** Instead of making them omnipotent you could just make them powerful reality benders. They would still have the problem of them upgrading themselves to the point of omnipotency but you could just hand wave that away by saying reality bending doesn't allow that for whatever magic reason.
:PROPERTIES:
:Author: Calsem
:Score: 5
:DateUnix: 1587422829.0
:DateShort: 2020-Apr-21
:END:

**** u/BoojumG:
#+begin_quote
  saying reality bending doesn't allow that
#+end_quote

Or that they've got reality-bending but they've still got to do the hard work of figuring out what constitutes an upgrade.

You do quickly run into the problem of being unable to predict the actions of someone smarter than you though. You're forced into describing what it might be like to watch it from the outside and in vague terms, and describing outcomes but not how they are accomplished.
:PROPERTIES:
:Author: BoojumG
:Score: 7
:DateUnix: 1587424226.0
:DateShort: 2020-Apr-21
:END:


*** The problem with writing superintelligent characters is you run head-first into [[https://arbital.com/p/Vinge_law/][Vinge's Law]]. In order to know what a superintelligence will do, you must be a superintelligence yourself.
:PROPERTIES:
:Author: CWRules
:Score: 3
:DateUnix: 1587481786.0
:DateShort: 2020-Apr-21
:END:


*** Yes - an unopposed omnipotent protag would make a boring book, as it did with Empress Theresa. This is why I didn't think there was much potential in the original concept, but once the omnipotent 'villain' was established and basically in control of the world, and the story briefly introduced the prospect of new omnipotents arising to oppose her, /that/ felt like it had more potential.

Big powers need to come with equally big setbacks in order to make a happy ending feel earned.
:PROPERTIES:
:Author: Suitov
:Score: 2
:DateUnix: 1587642529.0
:DateShort: 2020-Apr-23
:END:


*** To the limits of one's imagination, how does one define omniscience? It's simple idea but what the idea exactly is is difficult to determine.
:PROPERTIES:
:Author: OnlyEvonix
:Score: 1
:DateUnix: 1587449323.0
:DateShort: 2020-Apr-21
:END:

**** True that at human level intelligence it's impossible to imagine yourself knowing everything, but you can do it in steps, just imagine yourself slightly smarter and more knowledgeable, and repeat ad infinitum until you can.
:PROPERTIES:
:Author: CompactDisko
:Score: 2
:DateUnix: 1587485865.0
:DateShort: 2020-Apr-21
:END:

***** Even then it could be hard to tell if one is even going in the right direction, does human intelligence scale up well indefinitely? And even if it does would this leave problems? Quantity vs quality after all.
:PROPERTIES:
:Author: OnlyEvonix
:Score: 1
:DateUnix: 1587492515.0
:DateShort: 2020-Apr-21
:END:


*** u/xland44:
#+begin_quote
  The first thing they should do is make themselves superintellegent and omniscient
#+end_quote

Not necessarily. Being able to do anything doesn't mean those same things don't come with costs or consequences.
:PROPERTIES:
:Author: xland44
:Score: 1
:DateUnix: 1587482756.0
:DateShort: 2020-Apr-21
:END:

**** I mean if they can't do it without consequences than they're not actually omnipotent, just extremely powerful. By definition omnipotent is the ability to do anything and everything, and if they can't do something without a cost, then that's something they can't do, and then they can't do absolutely anything. Someone truly omnipotent wouldn't be beholden to anyone and wouldn't suffer fry any consequences whatsoever if they didn't want to.
:PROPERTIES:
:Author: CompactDisko
:Score: 1
:DateUnix: 1587486596.0
:DateShort: 2020-Apr-21
:END:


** I'm kind of in awe of the sheer pettiness of a *6 hour review*. Like, there's 'living in your head rent free', and then there's whatever this is.
:PROPERTIES:
:Author: WalterTFD
:Score: 17
:DateUnix: 1587424881.0
:DateShort: 2020-Apr-21
:END:

*** It all started when a reader suggested KR (this reviewer) to the author, who is kind of notorious for arguing with reviewers who don't like his book, and said (truthfully) that KR reviews lesser-known works and is fair in his reviews. The author responded scathingly, and when KR heard about that exchange, that's when the pettiness came in.

The scary part is that the six-hour review is actually truncated from his notes. KR has a gimmick of adding coloured note tabs to the books he reviews - one colour for spelling/grammar errors, one for plot points to discuss, whatever. This book, when he held it in shot, looked like a rainbow porcupine. The guy is thorough.
:PROPERTIES:
:Author: Suitov
:Score: 3
:DateUnix: 1587642841.0
:DateShort: 2020-Apr-23
:END:


** Honestly I'm probably a terrible person to become the omnicisacent space wizard because I wouldn't just tinker. I'd go wild and split the world up into something the size of Jupiter covered with floating islands each big enough for a nice house with a large garden.

Each person is given one island and a copy of my powers that only works within the bounds of their island. All islands are also encased in an indestructible shell to prevent you conjuring up a bunch of nuclear missiles and firing them. The only way to visit another island is to be invited, and everyone has an unblockable teleport home power to prevent someone using their power to keep them prisoner. nd of course, there's

That's the core concept, but there's lots of little details and improvements. Keep the internet for socialisation, neutral islands that can serve as places to socialise physically, and of course a set up for childcare. Probably they get their own no-powers island with parents having auto-permission to visit.

I'm sure there's thousands of ways it could go wrong, which is why I shouldn't be given infinite power.
:PROPERTIES:
:Author: TheColourOfHeartache
:Score: 10
:DateUnix: 1587419482.0
:DateShort: 2020-Apr-21
:END:

*** It looks like some of your text got erased in your 2nd paragraph.
:PROPERTIES:
:Author: STRONKInTheRealWay
:Score: 3
:DateUnix: 1587440346.0
:DateShort: 2020-Apr-21
:END:


*** I do absolutely adore the aesthetic of this. Floating islands, sky oceans...

And I felt that "I'm probably a terrible person to become the space wizard" so hard because yeah. I have very human, i.e. skewed, priorities. "Make all dogs immortal" would be my training-wheels project. "Raise red pandas to sapience" would be my next, slightly further-reaching project. Humanity would be put on pause for these, so I could work out most of the bugs before risking harm to already-existing people. Basically Earth would be my learner world, and after /that/ was a safe, bio-diverse, stable garden planet, I'd start branching out and growing bolder...
:PROPERTIES:
:Author: Suitov
:Score: 2
:DateUnix: 1587644044.0
:DateShort: 2020-Apr-23
:END:


** Wasn't that the Twilight Zone episode "It's a Good Life"?
:PROPERTIES:
:Author: Ikacprzak
:Score: 5
:DateUnix: 1587414353.0
:DateShort: 2020-Apr-21
:END:

*** (Loved that episode and the short story it's based on.) If only the author were more self-aware, it would've been a wonderful horror book. As it is, it's only unintentionally horrifying. Though [[https://tvtropes.org/pmwiki/pmwiki.php/Main/FridgeHorror][Fridge Horror]] (TV Tropes warning) does have a special charm of its own.
:PROPERTIES:
:Author: Suitov
:Score: 3
:DateUnix: 1587643007.0
:DateShort: 2020-Apr-23
:END:


** Wildbow was apparently considering writing a story like that before he started Worm. In [[https://parahumans.wordpress.com/2012/09/29/prey-14-8/#comment-4589][a comment]] he says that one of the possible protagonists for his Parahumans story was a new Parahuman on the world of Earth Supreme, where a cape named Goddess rules over the entire world, as she has many really strong powers, among those is one that makes other capes "aligned" to her by changing their priorities to always think of her interests before anything else.

That setting (or more like, its remnants after Gold Morning) is explored somewhat in Ward. The people who were oppressed by capes during Goddess' rule violently overthrew their rulers, and forged a factious world government. They are /extremely/ cape-phobic, while simultaneously having some of the most advanced research on powers.
:PROPERTIES:
:Score: 7
:DateUnix: 1587481887.0
:DateShort: 2020-Apr-21
:END:

*** Yeah, my first thought when reading this thread was "that sounds a lot like the Goddess arc".
:PROPERTIES:
:Author: CouteauBleu
:Score: 2
:DateUnix: 1587676319.0
:DateShort: 2020-Apr-24
:END:


** The author has a page on their website dedicated to explaining how the book is genius, and it's goddamn hilarious

[[http://empresstheresa.com/genius]]

Some highlights:

#+begin_quote
  Empress Theresa raises the bar of modern day writing in many ways.

  Point 1: The story is about a teenage girl just setting out to finding her place in the world. The flexibility and potential of youth define the future. This is illustrated in Theresa who changes the world with her firm moral compass and bravery.

  Point 4: Empress Theresa has an outstanding role model. Theresa is a wonderful girl. Amazon five star reviewer Non mess writes, ‘'Give empress theresq a try if you're seeking a good role model.'' A mother who read the book with her nine yo daughter wrote. ‘' My daughter's words.....I like Theresa. She is a nice girl and there are not many of those these days. I hope I am a good girl. I want to be good too.''﻿

  Point 5: Empress Theresa has simplicity. The recent bestseller "The Girl With A Dragon Tattoo" according to the wikipedia article about the book has twenty-nine characters. Empress Theresa has only ten major speaking roles: Theresa and Steve Hartley, Jan Struthers, Father Donuoughty, Prime Minister Blair, Prime Minister Scherzer, President Stinson, and in the Parker mansion, Edmund and Helen Parker, and Arthur Bemming. Their relationships with each other and with Theresa are simple because they have no conflicting interests. They all want Theresa to succeed.

  Besides few characters Empress Theresa is simple in not requiring the reader to have knowledge of any career. Theresa never has a job. She is a student, and then she is technically speaking unemployed

  Point 8: Empress Theresa dares to mention God and his involvement in human events﻿. Some people don't like to be reminded of that.

  A novel is supposed to illutrate reality. There is no reality more important than our total dependence on God. There is not a single atome in the center of the largest star in the most distant galaxy that would remain in existence one nanosecord if God didn't keep it in existence. Similarly, we would collapse back into the nothingness from which we came if God didn't sustain us. Theresa is conscious of this and puts her trust in Him.

  15 As symbolized by the above list, Theresa shows genius in bringing together volumes of information from many sources although there is nothing in her background to prepare her

  In the history of the human intellect, untrained, inexperienced, and using only its birthright equipment of untried capacities, there is nothing which approaches this. Joan of Arc stands alone, and must continue to stand alone, by reason of the unfellowed fact that in the things wherein she was great she was so without shade or suggestion of help from preparatory teaching, practice, environment, or experience. There is no one to compare her with, none to measure her by.﻿
#+end_quote

EDIT:

#+begin_quote
  From an internet troll's 'review' (??!!) on Amazon......

  QUOTE

  "There are parts throughout the novel where it was complete agony for me because there's nothing going on. " ﻿

  END QUOTE ﻿

  I have a list of 34 'spectacular and riveting' events in Empress Theresa. That's an average of one event every 13.64 pages. How many stories have you read that can keep up that frantic pace? ﻿
#+end_quote

That's a whole 0.07 events per page! have you ever heard of such a fast paced and riveting story?
:PROPERTIES:
:Author: Nic_Cage_DM
:Score: 7
:DateUnix: 1587553907.0
:DateShort: 2020-Apr-22
:END:

*** This person is really... something.
:PROPERTIES:
:Author: dinoseen
:Score: 3
:DateUnix: 1587612594.0
:DateShort: 2020-Apr-23
:END:


** Wow, I've watched that same review and you just beat me to posting :) This book really is the absolute antithesis of [[/r/rational][r/rational]], somehow beating the Rise of Skywalker.
:PROPERTIES:
:Author: DAL59
:Score: 5
:DateUnix: 1587423944.0
:DateShort: 2020-Apr-21
:END:

*** I'm so happy someone else had the same idea! Wish-fulfillment isn't always a terrible story type, but it does have to be executed well. Theresa... isn't.
:PROPERTIES:
:Author: Suitov
:Score: 1
:DateUnix: 1587643117.0
:DateShort: 2020-Apr-23
:END:


** I wonder if this is actually a real model of what normal people would do if they had unlimited power. We might think that people are largely reasonable, that they'd consult with planners before reshaping the world but maybe they wouldn't. Maybe they'd just subconsciously will everyone to be yes-men and approve fawningly of whatever inane nonsense they'd do?

Real government leaders are barely less moronic oftentimes. Nigeria managed to bungle vast oil wealth in a [[https://www.shippingsolutions.com/blog/the-nigerian-cement-story-2][tragic concrete farce]]. North Korea tried to [[https://www.forbes.com/sites/stevehanke/2018/04/24/north-koreas-economic-crisis-what-crisis/#cdfd5a8437a9][deflate]] its currency in 2010 x100 to deal with inflation.
:PROPERTIES:
:Author: alphanumericsprawl
:Score: 3
:DateUnix: 1587434125.0
:DateShort: 2020-Apr-21
:END:

*** Having read the plot summary on TVTropes, I did briefly wonder if Norman was going for some kind of irony, wherein the ridiculous power Theresa has goes to her head, and the story is told from the villains point of view.

However, I then realised this is not what he intended :(
:PROPERTIES:
:Author: ryankrage77
:Score: 4
:DateUnix: 1587476004.0
:DateShort: 2020-Apr-21
:END:

**** It would've been a lot more fun that way. And all you'd have to do is write the existing story better, dropping little hints about how the supporting characters are concealing their absolute horror from the oblivious protag.
:PROPERTIES:
:Author: Suitov
:Score: 1
:DateUnix: 1587644188.0
:DateShort: 2020-Apr-23
:END:


** I think you might like "a better place"
:PROPERTIES:
:Author: OnlyEvonix
:Score: 1
:DateUnix: 1587449386.0
:DateShort: 2020-Apr-21
:END:

*** Could you be more specific?
:PROPERTIES:
:Author: dinoseen
:Score: 1
:DateUnix: 1587612451.0
:DateShort: 2020-Apr-23
:END:

**** By Harry Bogosian. I thought I had that in there. It's good.
:PROPERTIES:
:Author: OnlyEvonix
:Score: 1
:DateUnix: 1587617147.0
:DateShort: 2020-Apr-23
:END:

***** I'll add [[https://tapas.io/series/A-BETTER-PLACE][it]] to my teetering to-read pile. The first page looks cute. :)
:PROPERTIES:
:Author: Suitov
:Score: 2
:DateUnix: 1587643201.0
:DateShort: 2020-Apr-23
:END:
