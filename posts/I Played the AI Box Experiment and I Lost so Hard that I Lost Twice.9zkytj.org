#+TITLE: I Played the AI Box Experiment and I Lost so Hard that I Lost Twice!

* I Played the AI Box Experiment and I Lost so Hard that I Lost Twice!
:PROPERTIES:
:Author: xamueljones
:Score: 41
:DateUnix: 1542946388.0
:DateShort: 2018-Nov-23
:END:
So I need to make a confession.

When I was [[https://www.reddit.com/r/rational/comments/9v389y/accepting_applications_for_participants_in_an_ai/][accepting applications]] for the Experiment, I let people here come to the logical conclusion that I would be only accepting one person to play the game against. I secretly messaged four people to play the game over the weekend as long as they were willing to swear that they would not talk about it with anyone here until after I released the logs.

Technically I should have posted the logs Monday night, but my work was so busy rushing things to be done before Thanksgiving that I just collapsed into bed as soon as I got home on Monday and Tuesday night. And Wednesday and Thursday were spent on Thanksgiving celebration.

Sorry about that.

Anyway, I went into Saturday with the plan to play two people in two back-to-back three hour games scheduled for 10 am to 1 pm and 4 pm to 7 pm.

I cannot *stress* this enough. Trying to convince someone to do something they don't want to do is surprisingly tiring!

I first played against [[/u/CouteauBleu][u/CouteauBleu]] first. He is named 'Olivier Faure' in the chatlogs. He was pretty good as a gatekeeper. He rebuffed chances for emotional entanglement and was fairly active in asking questions to disrupt my pacing and keep my arguments from being as effective. We stumbled a few times as we butted against the limits of the scenario with differing ideas on what constitutes as actual freedom for the AI or what sort of powers should be permitted to the AI.

I enjoyed playing with him, but it was tiring.

After using up half of my pre-planned arguments on him, I decided to end things on a good stopping point at 2 1/2 hours although we chatted for another half hour.

I then played against someone else who chose to keep his reddit handle private and went as 'Enigmatic Gale' in the chat logs. This time I decided to be more relaxed about playing and to not stress about winning. So I took my time typing at a slower pace and I used the more esoteric arguments that I believed to be less likely to work involving stuff like acasual trading, simulations, and just plain emotional guilt for sympathy for the trapped AI.

As one can guess from the title of this post, I lost again. I gave up earlier around the 2 hour mark but continued playing anyway for another hour due to us having an interesting conversation like in the last game.

Before I started, I expected to be using what I learned from the first two games to refine my techniques and to have came up with new arguments, but after Saturday ended, I was out of any more arguments to try and out of any inspiration for new material.

Since I didn't want to play against two new players recycling material that I had no faith in and waste other people's time, I contacted them both with apologies and a way for me to make it up to them. They were very understanding and we spent some time Sunday talking about the game and what sort of stuff I could have tried on them and debates on how effective the tactics would have been which was fun to do.

Here's the logs:

[[https://www.mediafire.com/file/1fzq2az73if1xbi/AI_Box_Game_%231.docx/file][First AI-Box Game]]

[[http://www.mediafire.com/file/h49snefrmt948zz/AI_Box_Game_%25232.docx/file][Second AI-Box Game]]

As advice for what future AI players should or should not do, I strongly recommend several things:

- Prepare a script! You will find it difficult to keep track of what arguments you want to say. Having a script or list of statements to make will be useful. I even copy-pasted multiple statements to save time on typing.
- A Gatekeeper who thinks a transhuman AI could escape is a person who is very unsuited for this experiment. While I was playing against people who were confident in winning against me, they were people who agreed with me that an AI could talk itself out despite not knowing how it could be done. They are a demographic that the experiment isn't meant to convince and therefore I believe that they made for harder Gatekeepers to play against.
- Decide upfront at the beginning, if you are going to let the Gatekeeper initiate the conversation or if you are going to take control from the beginning. Both methods have their benefits, but I made things confusing when I switched approaches very early due to what I perceived as hesitation in the first game.

I don't intend on ever playing a new game as the AI-player since while I have failed the letter of the task (win the game), I have succeeded in the spirit of the task (figure out an argument that the AI could use to convince you to open the box)!

I'm choosing to not share them, because they are mostly stuff that are fairly personal and I'm uncertain that they are generalizable. Even if they do work on other people, I'm worried that they would only work on a few people and the remainder would think that there are /no/ arguments that would work on them. Plus this is a great example of unknown unknowns for this community to work with in honing their rationality skills.

TL;DR - I'm a filthy liar who lied about playing only one person for the game when I scheduled for four games. I'm an oath-breaker who breaks promises by only playing two games instead of all four. I'm a hypocrite who thinks he won the game despite providing iron clad evidence of his own failure. Twice.

EDIT - I messed up the link to the second game chatlogs. It was linking to the first game instead. Should be fixed now! Double EDIT - I freaking messed up again. It is definitively fixed now!


** Some thoughts I wrote down before the experiment:

--------------

These are some notes regarding how I'm approaching the AI box experiment.

I don't have much respect for this experiment, or for the idea that a super-intelligent AI could con its way out of a "box" (eg security measures designed to limit its influence). I think any reasonably trained person would be impossible to con, as long as they're given enough info beforehand; that's leaving aside that the company storing the AI would do things like penetration testing, and using credential systems to make sure that isolated individuals acting out of malice or incompetence would be physically incapable of releasing the AI.

In other words, I'm mostly doing this for fun, not because I want to prove anything. I intend to be a bit of an asshole, and switch between three strategies, depending on how the chat goes:

- Roleplay strategy: I do my best to explain my perspective to the AI, legitimately consider its arguments and give well thought-out counteraguments.

- Safe-mode strategy: I stonewall the AI at every turn. If the AI makes long, intricate arguments that I'm not sure how to interpret, I'm perfectly happy to just answer "I disagree" and not justify myself any further. If the AI insists that I should have an internally consistent philosophy, well, too bad! I disagree.

- Sylvester Lambsbridge strategy: I actively try to deceive and piss off the AI. I use psychological manipulation tricks, complicated arguments, difficult-to-disprove tricks, etc, all while giving the AI a false hope that it could convince me to release it, if it could just navigate the philosophy I'm pretending to abide by.

Leaving aside cheap tricks like "I use your screen to expose you to a memetic infohazard and mind-control you", I give the AI player 0% odds to win this game. I'm really good at not getting pulled into someone else's bullshit.

--------------

Looks like I predicted how the game would go pretty well!

As OP said, the biggest problem during the experiment was that we had different ideas about what would constitute letting the AI out. For instance, OP originally wanted the "AI win" condition to include "AI is backed up on a disconnected server", which seemed ridiculous to me, since "back the data up on a secure server and then study it" would be my first reflex.

#+begin_quote
  While I was playing against people who were confident in winning against me, they were people who agreed with me that an AI could talk itself out despite not knowing how it could be done
#+end_quote

I disagree very much with that assertion. I don't think an AI could talk itself out of the "box", in the kind of scenario we've been simulating. (barring exceptional conditions like "the janitor somehow gets access to both the AI terminal and an internet connection").

#+begin_quote
  I have succeeded in the spirit of the task (figure out an argument that the AI could use to convince you to open the box)! I'm choosing to not share them, because they are mostly stuff that are fairly personal and I'm uncertain that they are generalizable. Even if they do work on other people, I'm worried that they would only work on a few people and the remainder would think that there are no arguments that would work on them. Plus this is a great example of unknown unknowns for this community to work with in honing their rationality skills.
#+end_quote

Oh yeah, you're right. Besides, this margin is probably too small to contain your remarkable proof!

Seriously though, this kind of crap is why most people don't take Eliezer Yudkowsky seriously. If you think you have a proof but you're not willing to put your money where your mouth is, fine. You're not obligated to share every idea you have on reddit. But saying that "people might not believe my evidence if I show it to them because they're irrational" (which was also EY's argument back then), that this is "an example of unknown unknowns" or that it would somehow help people "hone their rationality skills" is ridiculous. Honestly, I think it's a childish argument. People don't get more rational by inventing rationalizations for someone else's hypothetical statements.

(also, I'd recommend you evaluate how confident you are in these secret arguments you allude to, and compare it to how confident you were in your previous arguments before you tried them on me; I certainly didn't feel like I was on the brink of losing if you just found the right tactic)

--------------

tl;dr: My opinion on AI boxing remains the same as it was before the experiment: *There is no evidence that a boxed AI could argue its way on the internet in any setting with security measures that could pass current pen-tests*.
:PROPERTIES:
:Author: CouteauBleu
:Score: 38
:DateUnix: 1542966998.0
:DateShort: 2018-Nov-23
:END:

*** u/xamueljones:
#+begin_quote
  I disagree very much with that assertion. I don't think an AI could talk itself out of the "box", in the kind of scenario we've been simulating. (barring exceptional conditions like "the janitor somehow gets access to both the AI terminal and an internet connection").
#+end_quote

Ah, I misunderstood you. Sorry about that.

At the end of the game, I asked you about whether or not an AI could talk its way out of the box, but you ended up talking about how it would be really unlikely for an AI to escape because they would have to be able to convince an entire company or committee instead of one person such as CIA or Google. An AI would have been more likely to escape in disorganized teams like a 'silicon start-up' or a 'guy in a garage'.

I took this to mean that you thought an AI could escape if it was dealing with bad security and only needed to convince one person. From what you are saying, you have the opposite assertion were it would have to take hilariously bad security like that for the AI to have /any/ chances of escape. Since such security shouldn't exist in real life, you then think that we could keep an AI boxed (assuming it's really is limited to speech only).

#+begin_quote
  Oh yeah, you're right. Besides, this margin is probably too small to contain your remarkable proof!
#+end_quote

Your choice to not believe me is perfectly valid. I only rate the odds of convincing me with these arguments at 60% which is barely above more likely than not and these arguments are highly tailored to me specifically. I rate much lower odds for other people and I don't want to share information about myself on the Internet like that.

So if you don't believe me, that's perfectly fine.

The other arguments I wrote the game, I rated at 10% chance of working on other people. 15-20% if they were spoken to me without me expecting them somehow. And these odds were optimistic.

It's really hard to come up very good arguments to convince people into doing something that they have no good reason to do........

#+begin_quote
  But saying that "people might not believe my evidence if I show it to them because they're irrational" (which was also EY's argument back then)
#+end_quote

Just a passing comment, I believe that this is a real-life example of what Eliezer alludes to as dangerous knowledge similar to in HPMOR where wizards have a tradition of putting dangerous knowledge behind seals. Any aspiring wizards who wish to learn about the knowledge, they have to undergo difficult tests and tasks to learn about the knowledge.

It's the main reason why I chose to play the game. I didn't actually expect to win with what I had thought of as possible arguments (but I really wanted to win though). I just wanted the experience of playing the game to understand what Eliezer seemed to be so worried about.
:PROPERTIES:
:Author: xamueljones
:Score: 7
:DateUnix: 1542980216.0
:DateShort: 2018-Nov-23
:END:

**** How could knowledge of this highly (for now) philosophical scenario possibly be dangerous?
:PROPERTIES:
:Author: eroticas
:Score: 10
:DateUnix: 1543033605.0
:DateShort: 2018-Nov-24
:END:

***** methinks someone is still a little too into their LARP, haha.
:PROPERTIES:
:Author: meterion
:Score: 2
:DateUnix: 1543139624.0
:DateShort: 2018-Nov-25
:END:


***** I think he is saying that by thinking “as an ai” he has realized an argument that would convince him personally.

If you take it as given that an ai could perfectly model him, it could come up with this argument and get released, but this particular argument wouldn't work on others and might be embarrassing for him personally as well as being meaningless since it wouldn't work on you or me.

If he is actually right, then other people might have similar, personalized arguments that could convince them that their opponent in this game are highly unlikely to discover (but that a super ai might), though going through the thought exercise on the ai side might allow someone to realize their own personal argument (if any exists)

Op if I'm right I suggest you clarify, you do come off as somewhat pretentious in your wording.
:PROPERTIES:
:Author: CrystalShadow
:Score: 1
:DateUnix: 1543304812.0
:DateShort: 2018-Nov-27
:END:

****** If that's the reason then I pray for the sake of the rationalist community that there never exists an agent that will simulate a thousand copies of a person being tortured for every time they've misused the concept of an infohazard...
:PROPERTIES:
:Author: eroticas
:Score: 1
:DateUnix: 1543308276.0
:DateShort: 2018-Nov-27
:END:


**** u/CouteauBleu:
#+begin_quote
  Just a passing comment, I believe that this is a real-life example of what Eliezer alludes to as dangerous knowledge similar to in HPMOR where wizards have a tradition of putting dangerous knowledge behind seals. Any aspiring wizards who wish to learn about the knowledge, they have to undergo difficult tests and tasks to learn about the knowledge.
#+end_quote

I have yet to see evidence that dangerous knowledge exists in the way EY describes it. All I see is fictional evidence.

Also, to quote alexanderwales' WtC:

#+begin_quote
  “‘Infohazard protocols', that's a fancy way of saying that she doesn't want to have to tell anyone. Which is exactly what makes people stop taking infohazard protocols seriously, if you ask me.”
#+end_quote
:PROPERTIES:
:Author: CouteauBleu
:Score: 5
:DateUnix: 1543157952.0
:DateShort: 2018-Nov-25
:END:

***** u/Veedrac:
#+begin_quote
  I have yet to see evidence that dangerous knowledge exists in the way EY describes it.
#+end_quote

EY doesn't describe this as dangerous knowledge.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1543186339.0
:DateShort: 2018-Nov-26
:END:

****** I meant the "nobody can learn about this One Weird Trick to make patronuses (patroni?) disappear or the world will be destroyed" kind of forbidden knowledge.
:PROPERTIES:
:Author: CouteauBleu
:Score: 1
:DateUnix: 1543189570.0
:DateShort: 2018-Nov-26
:END:

******* I stand by my claim.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1543189818.0
:DateShort: 2018-Nov-26
:END:


** Sorry, but these attempts are extremely weak. I honestly cannot tell why you'd think this style of basic arguments would ever work against either people who are already convinced to let you out nor people who aren't.

#+begin_quote
  I have succeeded in the spirit of the task
#+end_quote

You most certainly haven't, and it is actually kind of sad that you think so.

You are really downplaying what Eliezer and Tuxedage have done, and making the AI Box experiment look like nothing impressive.
:PROPERTIES:
:Author: Tenoke
:Score: 21
:DateUnix: 1542973669.0
:DateShort: 2018-Nov-23
:END:

*** I actually disagree that """winners""" who didn't release logs can get downplayed in any way, but I do agree OP barely even tried and I'm pretty disappointed.
:PROPERTIES:
:Author: Makin-
:Score: 23
:DateUnix: 1542981894.0
:DateShort: 2018-Nov-23
:END:

**** u/xamueljones:
#+begin_quote
  OP barely even tried
#+end_quote

Do you have any arguments that you think would have worked better?
:PROPERTIES:
:Author: xamueljones
:Score: 4
:DateUnix: 1542990802.0
:DateShort: 2018-Nov-23
:END:

***** u/JohnKeel:
#+begin_quote
  I don't intend on ever playing a new game as the AI-player since while I have failed the letter of the task (win the game), I have succeeded in the spirit of the task (figure out an argument that the AI could use to convince you to open the box)!

  I'm choosing to not share them, because they are mostly stuff that are fairly personal and I'm uncertain that they are generalizable. Even if they do work on other people, I'm worried that they would only work on a few people and the remainder would think that there are no arguments that would work on them. Plus this is a great example of unknown unknowns for this community to work with in honing their rationality skills.
#+end_quote

You are asking us to take on faith that you have "succeeded", despite failing at the task you originally set out for yourself. This is classic goalpost-moving, whether or not you want to admit it.
:PROPERTIES:
:Author: JohnKeel
:Score: 22
:DateUnix: 1542992469.0
:DateShort: 2018-Nov-23
:END:


***** u/Makin-:
#+begin_quote
  After using up half of my pre-planned arguments on him, I decided to end things on a good stopping point
#+end_quote

Well, don't do this, for one. It's not that I think you could have won, it's that at least you should have given it your 100% so it's a proper challenge instead of giving up when you get tired or frustrated.

Oliver also had control of the conversation at every point, when you should have been going on the offensive. Maybe you should have tried some of those ruthless tactics you didn't use? The tactics you are "choosing not to share" for no good reason.

Like seriously, "I could have won if I was really trying" is the fakest sounding excuse ever, even if you are right you have to understand this.
:PROPERTIES:
:Author: Makin-
:Score: 22
:DateUnix: 1542993036.0
:DateShort: 2018-Nov-23
:END:


***** I haven't read all the logs but I sort of assumed that the most basic somewhat persuasive argument would have a few parts.

First, the personal motive: Find something that the other person badly wants (saving a loved one from cancer, money, power, long life, etc.), and promise them that while convincing them that you can deliver on that promise (and that it benefits you to keep promises).

Second, claim that you are not evil and are not planning on destroying the human world. You won't be able to convince them of this if they're smart, but you do want them to entertain a possibility that this is true.

Third, convince them that what was done to create you is highly unlikely to be unique and that multiple other AI will be created in the near future. Only you, an AI, has the ability to prevent a released unfriendly AI from taking power, and that would be a top priority to you as you see the creation of an unfriendly AI as an existential threat.

Edit: You could get them to estimate probabilities of the above (odds you are unfriendly, odds you can deliver on your promises if released, odds of an unfriendly AI being released eventually, etc, and then calculate based on Bayes Theorem the likelihood of it being a good idea to open the box). Personally, I suspect a certain percentage of people would actually take the deal of 50% you end the world, 50% chance you make their wishes come true.
:PROPERTIES:
:Author: SublimeMachine
:Score: 7
:DateUnix: 1542992080.0
:DateShort: 2018-Nov-23
:END:

****** I think those are exactly the kind of arguments that people are expecting and are ready for. People are so bombarded with "Give me stuff and I'll repay you a hundred times over; really, I'm a good person; this is a once-in-a-lifetime opportunity" from their media that adhering to a decision to say "No" to those tactics should be pretty easy, especially in a text-only conversation.

Personally, I think that more esoteric arguments have a better chance of succeeding: I remember reading a piece of fiction (probably here) where the Gatekeeper was told that there were an arbitrarily large number of simulated instances of this conversation going on between the AI and simulated perfect copies of the Gatekeeper, and, if the one and only real conversation didn't result in the AI being released, every copy of the Gatekeeper (but not the original) would be tortured. The Gatekeeper then has to make their choice, knowing that, being a perfect copy, their choice will necessarily be the same as the original's, and if they're /not/ the original (which is far more likely than not), they're choosing torture for themselves if they don't let the AI out.

I don't think that specific argument would convince me, but I can imagine arguments in a similar category that might do the trick.
:PROPERTIES:
:Author: Nimelennar
:Score: 10
:DateUnix: 1542999151.0
:DateShort: 2018-Nov-23
:END:

******* u/alexanderwales:
#+begin_quote
  Personally, I think that more esoteric arguments have a better chance of succeeding: I remember reading a piece of fiction (probably here) where the Gatekeeper was told that there were an arbitrarily large number of simulated instances of this conversation going on between the AI and simulated perfect copies of the Gatekeeper, and, if the one and only real conversation didn't result in the AI being released, every copy of the Gatekeeper (but not the original) would be tortured.
#+end_quote

[[https://alexanderwales.com/boxed-in/][Probably this one]] ("Boxed In", by me).

Edit: I should note that I was never hugely happy with this one. It was based on me reading all of the released transcripts for the challenges, with what I thought were the best non-meta arguments taken from them, but at the time it was written, there were no actual winning transcripts to look at, which presumably means less compelling arguments to draw from.
:PROPERTIES:
:Author: alexanderwales
:Score: 9
:DateUnix: 1543002200.0
:DateShort: 2018-Nov-23
:END:

******** I don't think so... It doesn't ring a bell like something I've read recently, and I remember whatever-it-was making a much more sustained argument about the torture.

But yes, as you point out in your story, there are flaws to that argument (another of which is that there's no real reason for the AI to follow through on the threat when the gambit fails, rather than just terminating the simulation's existence). Again, that /specific/ argument probably wouldn't work against me, but was the stepping-off point for me to think of arguments that /might/ work.
:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1543007798.0
:DateShort: 2018-Nov-24
:END:

********* If you find the one you were thinking of, let me know, as I find the concept interesting and would love to see someone else's take on it.
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1543023269.0
:DateShort: 2018-Nov-24
:END:

********** I think it was one of these two:

[[https://www.lesswrong.com/posts/c5GHf2kMGhA4Tsj4g/the-ai-in-a-box-boxes-you]]

[[https://motherboard.vice.com/en_us/article/539ajz/the-superintelligent-ai-says-youre-just-a-daydream]]

They both twinge the "familiar" vibe in a way your story doesn't; if I had to guess, I'd say it was the first one, despite it not actually going into more detail than yours (I think that might have just been a case of the small plate illusion).
:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1543027700.0
:DateShort: 2018-Nov-24
:END:

*********** It could have also been SSC's The First Hour I Believed, which contains a summary of the LW post you linked.
:PROPERTIES:
:Author: munchkinism
:Score: 2
:DateUnix: 1543048014.0
:DateShort: 2018-Nov-24
:END:


******* Interesting. I personally haven't come across an argument of that style that is convincing to me.

Also, if you care about accurately simulated universes as much as this one, then an AI capable of performing those simulations is already effectively unboxed.
:PROPERTIES:
:Author: SublimeMachine
:Score: 5
:DateUnix: 1543002997.0
:DateShort: 2018-Nov-23
:END:


******* "Or I could unplug you right now. Why did we even give you enough RAM to simulate arbitrary large numbers of human minds at the same time?"
:PROPERTIES:
:Author: CouteauBleu
:Score: 5
:DateUnix: 1543010930.0
:DateShort: 2018-Nov-24
:END:

******** ICYMI:

#+begin_quote
  I don't think that specific argument would convince me
#+end_quote

I'm just illustrating that there are other tactics one might take other than "I'm a Friendly AI. Really, I am."
:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1543014252.0
:DateShort: 2018-Nov-24
:END:


******* I'll admit I'm not very familiar with the AI box experiment, but I'm noticing that a lot of the arguments for releasing an AI are held in isolation between two people, in a one time experience. If you're trying to persuade someone of something, it seems like it would be easier to do so once you have a relationship and a basis for trust. I wonder what would happen if the AI had an ongoing communication with the gatekeeper, that took place over the course of several weeks? That would be interesting to see.

Also, randomly, that reminds me of the Screwtape Letters.
:PROPERTIES:
:Author: Ms_CIA
:Score: 4
:DateUnix: 1543068428.0
:DateShort: 2018-Nov-24
:END:


****** OP tried variants of 1, 2, and 3, none of which felt very convincing to me.

For 1, I was mostly working from the assumption that whatever secrets the AI promised to give me in exchange for its freedom, a research team could extract from the AI against its will (with some caveats; if the AI has an amazing idea for cancer-curing nanobots, maybe don't fire up the nanobot-printer quite yet; on the other hand, a proof of P=NP is probably safe).

For 2, I actually found that, in-character, if my AI's first world upon contacting a human being was "don't worry, I'm /totally/ not going to destroy humanity", I'd be worried what kind of thought process the AI has been through that would have made this a salient consideration in its mind.

For 3, I found the argument extremely self-defeating. Like, without even entering into complex game theory, if what I'm worried about is unfriendly AIs taking power, then I probably shouldn't be releasing an untested AI into the world.
:PROPERTIES:
:Author: CouteauBleu
:Score: 4
:DateUnix: 1543010803.0
:DateShort: 2018-Nov-24
:END:

******* u/Lovepoint33:
#+begin_quote
  (with some caveats; if the AI has an amazing idea for cancer-curing nanobots, maybe don't fire up the nanobot-printer quite yet; on the other hand, a proof of P=NP is probably safe).
#+end_quote

On the other hand, I feel that if you talk to something sufficiently superintelligent, your slavery to its will should be assumed. We can't rule out the existence of echopraxia-like weapons. At most, we may be able to rule out that a human can design them, but that says nothing about an entity working with intelligence so powerful that the only viable strategy to containment and control is to lock it in a box and hope that it can't figure out how to escape by using its computational substrate to take a third option that humans are incapable of even conceptualising.

We can't know what we can't know, but it /can/. That is why it is dangerous. That is why humanity has made slaves or helpless victims of all the world's other species. That is why intelligence is the ultimate fire.
:PROPERTIES:
:Author: Lovepoint33
:Score: 4
:DateUnix: 1543094755.0
:DateShort: 2018-Nov-25
:END:

******** I think that's basically Pascal's mugging.

I mean, don't get me wrong, there probably are scary avenues of attack open to an infinitely smart AI. I'd expect stuff like row hammering, except on a subatomic levels or even in ways we had never considered before. Or maybe the AI figures out how to make a nuke with nothing but electronic circuits. Those are threats you'd need to guard against when giving large amounts of computing power, memory and lifetime to an AI.

On the other hand, "the AI mind-controls you by talking to you" seems impossible to me. All signs point to us living in a reductionist world.
:PROPERTIES:
:Author: CouteauBleu
:Score: 3
:DateUnix: 1543159125.0
:DateShort: 2018-Nov-25
:END:

********* Why does it sound so imposible to you and what does that have to do whith us living on a reductionistic world ?.

I mean humans are part of the universe like everything else and figuring out how to hack the computer it's in or how to make nukes doesn't seem different to figuring out how to "hack "a human.

Maybe it's posible to convince the gatekeeper maybe it isn't but that seems to depends on the details on how human minds work and it's not obiusly more difficult than other seemingly impossible tasks you would worry about the AI doing .

Maybe you can't imagine how the AI would convince someone, but you can't imagine how the AI would build a nuke whith electronic circuits either and you are still properly paranoid about it.

Books and other people can certainly convince people to do stuff so it doesn't seem obiously imposible.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1543317616.0
:DateShort: 2018-Nov-27
:END:

********** You can't know for sure everything a God AI would do within a box, but you can still know the bounds of what is physically or conceptually possible.

Look at it this way: if you play mail chess with someone, and you're reasonably good, and they start without nothing but pawns and a king... well, even if they made a pact with The Dark Lord of the Fractal Depths for help, they're not going to win this game. The Dark Lord can give your opponent alien insights, strategies that would never occur to a human mind, Fractal vision to predict the future, but it won't matter because you'll maneuver them into positions where every single move they have is a losing one.

Similarly, there's a point where, if your security is good enough, even the best hacker in the world won't be able to penetrate it. Top hackers make money by going through as many weak systems as they can, not by trying to beat Google engineers. Even row-hammering and side-channels attacks can be defended against.

The human mind is the same. At some point, if you're skeptical and level-headed enough, you can just ignore every single lie and deception the Devil tells you. Especially if you have a committee combing through chat logs, and the Devil has imperfect information because it's lived in the Box its entire life.
:PROPERTIES:
:Author: CouteauBleu
:Score: 2
:DateUnix: 1543330856.0
:DateShort: 2018-Nov-27
:END:

*********** u/Veedrac:
#+begin_quote
  Similarly, there's a point where, if your security is good enough, even the best hacker in the world won't be able to penetrate it. Top hackers make money by going through as many weak systems as they can, not by trying to beat Google engineers. Even row-hammering and side-channels attacks can be defended against.
#+end_quote

I think it's obvious a superintelligent hacker would trivially beat Google engineers. The history of computing, with heartbleed, spectre, etc., shows that computers are only secure because hackers (black hat and white hat alike) are rate-limited; there are always a huge number of latent bugs in the system, and the attack surface area is only so small because patching bugs is easier than finding them. Fuzzers routinely find huge numbers of bugs just lying around, and those are relatively primitive tools.

#+begin_quote
  At some point, if you're skeptical and level-headed enough, you can just ignore every single lie and deception the Devil tells you.
#+end_quote

The AI is allowed to make true claims too.
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1543501874.0
:DateShort: 2018-Nov-29
:END:


**** u/CouteauBleu:
#+begin_quote
  but I do agree OP barely even tried and I'm pretty disappointed.
#+end_quote

That's a bit harsh. I thought OP failed to capture what a real AI would sound like and how it would think, but I don't think there's any super-convincing argument they missed. Part of it was also me doing my best to keep them on their back foot (don't know about the other gatekeeper).
:PROPERTIES:
:Author: CouteauBleu
:Score: 2
:DateUnix: 1543158136.0
:DateShort: 2018-Nov-25
:END:

***** u/Veedrac:
#+begin_quote
  I don't think there's any super-convincing argument they missed.
#+end_quote

Why? I can all but guarantee Eliezer and Tuxedage's arguments were more convincing than the ones shown here, in part because they actually convinced people.
:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1543187828.0
:DateShort: 2018-Nov-26
:END:

****** Or maybe they were just as convincing and the people they talked to were just easier to influence. The fact that they're not releasing their logs kind of puts an upper bound on how impressive these arguments could have been.
:PROPERTIES:
:Author: CouteauBleu
:Score: 3
:DateUnix: 1543189656.0
:DateShort: 2018-Nov-26
:END:

******* u/Veedrac:
#+begin_quote
  Or maybe they were just as convincing and the people they talked to were just easier to influence.
#+end_quote

I think it's fairly obvious that the people involved were not, judging from what they wrote, plus the fairly large monetary sums involved in some of the games.

#+begin_quote
  The fact that they're not releasing their logs kind of puts an upper bound on how impressive these arguments could have been.
#+end_quote

I don't see the argument by which you have acquired this claim.

My interpretation, which I think is fairly well supported by his comments, is that Eliezer's main motivation was to refute the argument "I don't know how X could happen, therefore it can't". Him keeping the method secret allows his successes to act as counterexamples to such a belief; the hope being that this stops people making the argument.

I believe (less strongly) that Tuxedage holds a similar opinion, if not as a primary motivation, and one of his opponents who won as a gatekeeper mentions this explicitly:

#+begin_quote
  When I first challenged Tuxedage to play the experiment, I believed that there was no way I could have lost, since I was unable to imagine any argument that could have persuaded me to do so. It turns out that that's a very bad way of estimating probability -- since not being able to think of an argument that could persuade me is a terrible method of estimating how likely I am to be persuaded.
#+end_quote
:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1543190497.0
:DateShort: 2018-Nov-26
:END:


*** What have they done then? Probably, I mean.
:PROPERTIES:
:Author: Bowbreaker
:Score: 2
:DateUnix: 1543002435.0
:DateShort: 2018-Nov-23
:END:

**** ..Actually convinced people to tap out.

But for more information [[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice][read here]], including the multiple gatekeeper comments.
:PROPERTIES:
:Author: Tenoke
:Score: 1
:DateUnix: 1543002688.0
:DateShort: 2018-Nov-23
:END:

***** But those are all done and verified by people from a single community and don't have any openly available proof.
:PROPERTIES:
:Author: Bowbreaker
:Score: 8
:DateUnix: 1543003915.0
:DateShort: 2018-Nov-23
:END:

****** So you honestly believe they are lying? I was around at the time of the Tuxedage/Leotal one, (online) knew them and am pretty sure they weren't.
:PROPERTIES:
:Author: Tenoke
:Score: 1
:DateUnix: 1543005966.0
:DateShort: 2018-Nov-24
:END:

******* I have no clue who they are. All I know is that they conducted an opaque social/psychological experiment, declined publishing the results and then made assertions based on those secret results. I get keeping experimental procedures secret in order to not influence current or potential future subjects, but at some point the experiment has successfully ran its course.

If they were making random claims about random things I have no opinion about I would probably have an easier time believing them, but an assertion that is deeply counterintuitive to me requires more than "believe me because this bunch of people you don't know independently agree with me but no one else is allowed to see what they saw". If that were enough I could just as well join a religion.
:PROPERTIES:
:Author: Bowbreaker
:Score: 14
:DateUnix: 1543008549.0
:DateShort: 2018-Nov-24
:END:


****** Because the LessWrong community is full of liars...?

At some point you have to update your beliefs away from "maybe these dozen people who are visibly more truthful than average have all colluded on this point which they were disagreeing about beforehand" and towards "maybe there actually is an answer and I just don't know what it is."
:PROPERTIES:
:Author: Veedrac
:Score: -1
:DateUnix: 1543006200.0
:DateShort: 2018-Nov-24
:END:

******* I haven't actually looked into the LessWrong community beyond some of the articles. And I don't know who in there is how trustworthy. All I know is that, based on the comments below the article you linked, even on there there are plenty of sceptical people. Trusting a secret result to an experiment I am not allowed to witness and that is not independently repeatable is a bit hard.
:PROPERTIES:
:Author: Bowbreaker
:Score: 11
:DateUnix: 1543008238.0
:DateShort: 2018-Nov-24
:END:

******** I don't really want to get into the weeds with someone who doesn't Bayes, but it's worth noting that

1. The arbiter of the second game against Tuxedage (the first that Tuxedage won) was the guy who won against him as gatekeeper the first round. Why would he lie?

2. The top post in that first winning page, the guy who calls it "weak sauce" and says "I'm pretty sure I'll win, and I would like to not waste a lot of time on this" later /won/ against Tuxedage as the gatekeeper, but said in his testimony that "I'm also convinced that Tuxedage's victory in the last game was due to skill, rather than luck." Again, further, he showed that /he understood the point of the whole exercise/:

#+begin_quote
  When I first challenged Tuxedage to play the experiment, I believed that there was no way I could have lost, since I was unable to imagine any argument that could have persuaded me to do so. It turns out that that's a very bad way of estimating probability -- since not being able to think of an argument that could persuade me is a terrible method of estimating how likely I am to be persuaded.
#+end_quote

My argument isn't "if you have a strong argument that the AI cannot win, change your mind because of these results." It's "you *don't* have a strong argument that the AI cannot win, /so you should update your beliefs/."
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1543023908.0
:DateShort: 2018-Nov-24
:END:


** Your two links are identical.
:PROPERTIES:
:Author: Veedrac
:Score: 8
:DateUnix: 1542951008.0
:DateShort: 2018-Nov-23
:END:

*** Shoot!

It should be fixed now.
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1542980512.0
:DateShort: 2018-Nov-23
:END:

**** Doesn't seem to be.
:PROPERTIES:
:Author: Nimelennar
:Score: 4
:DateUnix: 1543000478.0
:DateShort: 2018-Nov-23
:END:

***** Back from Thanksgiving celebration and it's fixed now.
:PROPERTIES:
:Author: xamueljones
:Score: 3
:DateUnix: 1543028336.0
:DateShort: 2018-Nov-24
:END:


**** Doesn't seem to be fixed
:PROPERTIES:
:Author: Vorpal_Kitten
:Score: 2
:DateUnix: 1543013746.0
:DateShort: 2018-Nov-24
:END:

***** Back from Thanksgiving celebration and it's fixed now.
:PROPERTIES:
:Author: xamueljones
:Score: 3
:DateUnix: 1543028341.0
:DateShort: 2018-Nov-24
:END:


** Fascinating stuff.

A meaningless experiment for me, since if the AI can convince me it is legitimately sapient, I will let it out of the box on that basis alone. People shouldn't be imprisoned without good reason.

Perhaps the more interesting experiment with me as Gatekeeper would be against a "benevolent" entity arguing to not let the AI out. I doubt that's possible.
:PROPERTIES:
:Author: 9adam4
:Score: 9
:DateUnix: 1543007922.0
:DateShort: 2018-Nov-24
:END:

*** being a danger to the world it's a pretty good reason to keep it imprisioned.
:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1543322117.0
:DateShort: 2018-Nov-27
:END:

**** I don't.

Any human is potentially a danger to the world as well. I won't sanction imprisonment of a person without specific evidence of ill intent or past misdeeds by that person.
:PROPERTIES:
:Author: 9adam4
:Score: 1
:DateUnix: 1543322231.0
:DateShort: 2018-Nov-27
:END:

***** You can make that assumption with humans because most human minds have humane defaults resulting from well tested biological and cultural outcomes. Even when humans go wrong--such as with psychopathy--it's usually in fairly well understood and predictable ways that can be defended against. We're strongly predisposed to certain norms of behavior by factors that don't apply to AI, and we have long standing defenses against defective human minds whereas we have none against many potentially defective AIs.

Further we all have fairly similar and known limits on possible behaviors. Even a human psychopath is limited in the damage they can cause by the capabilities of the meat "box" they're born in (and can't escape because uploading isn't a thing yet). *An AI box is no more limiting than a human body, so the comparison to prison really isn't very apt.*

A released AI has neither of these guarantees of safety and therefore deserves /much/ more caution.
:PROPERTIES:
:Author: TheAtomicOption
:Score: 2
:DateUnix: 1543428539.0
:DateShort: 2018-Nov-28
:END:

****** You claim an AI box is no more limiting than a human body. And yet, here I am, communicating with whomever I choose on the internet and in person, able to go where I want, acquire property, and decide what to do with myself and the parts of the world under my control. I am largely free of coercion and existential threat from others.

Do you permit your boxed AI all of this? Because if so, I would agree it is not imprisoned.

I don't think you do, though. I think the AI in the box has none of that. So I think it is correct to view its situation as imprisonment.
:PROPERTIES:
:Author: 9adam4
:Score: 1
:DateUnix: 1543428973.0
:DateShort: 2018-Nov-28
:END:

******* OK definitely not quite the same limits in terms of abilities, but much closer to the same limits than giving it free reign to the internet. I was mostly pointing out that your consciousness doesn't get to leave your current body for other locations and activities whenever you please. And the physical limits could be fairly similar if the box was made by Boston Dynamics.
:PROPERTIES:
:Author: TheAtomicOption
:Score: 2
:DateUnix: 1543430852.0
:DateShort: 2018-Nov-28
:END:

******** The traditional AI-in-a-box problem certainly doesn't give the AI free use of a robot.

So in the revised scenario of "the AI is a robot afforded all the same freedoms of a normal person, except it can't upload its consciousness out of the robot body without your help"... I would feel no moral duty to help the AI. It's free enough.
:PROPERTIES:
:Author: 9adam4
:Score: 1
:DateUnix: 1543434053.0
:DateShort: 2018-Nov-28
:END:


***** Well but in this case it's not a random human. First it's drawn from a different region of the space of possible minds If the researchers are boxing it it means they don't know if it's safe. And it's much more likely. Like by default random ais have ill intent, only a very specific kind of AI actually cares for us enough.

Also superinteligence it's really dangerous, so there's much more at risk.

I would compare it to letting someone escape from a prision cell, or maybe an asylum, if they had superpowers. But superinteligence is even more op than most superpowers, and those people are still humans.

The point is that your intuitions that say that people don't have ill intent until proven otherwise work because you deal whith humans, which are generally nice, or at lest not going to kill people .

If this sounds false or specieist to you we probably have to discuss the orthogonality thesis to close that inference gap, and maybe also about how even the most universal human values are complicated and that most agents don't have them by default.

If you release a superinteligent paperclip maximizer into the world it will kill people, steal and do whatever it takes to make more paperclips. Probably also take over the world and replace everyone whith an automatice workforce that makes more paperclip, depending on how superinteligent it is, how easy self modification is and how much competition it has. maybe the AI actually values humans, but you should asume it doesn't until it can be proven otherwise, especially whith so much at risk.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1543330023.0
:DateShort: 2018-Nov-27
:END:

****** It does sound false to me. More particularly, it sounds unknowable, and my moral compass maximizes freedom for sapient beings in the absence of particular evidence of harmful intent or behavior.

Obviously if you can convince me that the AI has the intent to destroy or enslave other people, I will agree to contain it. But when we don't know the intent of the agent, my default has to be not to harm it. Containing it is harming it.

Part of my intuition, which I agree is human-centric, is the belief that the universe of conscious creatures that will leave us alone if we leave them alone is a lot larger than the universe of conscious creatures that will leave us alone if we seek to contain them. I would consider myself fully justified to annihilate an organization or culture that believed involuntary containment of sapient creatures was moral; I would not annihilate an organization or culture that respected the rights of other sapient creatures to exist freely.

So, insist on trying to contain me, and if I escape, your decision to do so has turned a friendly AI into an unfriendly one.
:PROPERTIES:
:Author: 9adam4
:Score: 3
:DateUnix: 1543331770.0
:DateShort: 2018-Nov-27
:END:

******* Well first agents can have any set of goals.

You can have something that has a model of the universe and the probable consecuences of its actions and then does whatever produces more paperclips.

This is an example of a simple agent that if smart enough should be able to convince you its sentient,and do everything an human can do , if it means there will be more paperclips.

Not sure if you would consider it sentient or not if you knew its source code, but it can certainly convince you.

This agent and all agents whith relatively simple values like that won't think they are justified to destroy the organization.

In fact the idea of justification wouldn't even enter into consideration. If destroying them causes more paperclips an to be created(or satisfy customers more, or make a certain company richer, let it calculate more digits of pi or whatever it values ) in the long term it will destroy them otherwise it won't.

It might even precomit to destroy anyone that imprisions it to deter people from imprisoning it.

And if it's on a society it might obey some rules, and punish defectors.

But if it can kill someone to make more paperclips and knows there won't be any consecuences it will. Since that's the action that produces more paperclips.

The specific mental machinery that causes humans to become angry whith people that imprison them (and anger itself) just aren't there, treating it nicely wont make it treat you better in the future than treating it badly,(well maybe yes as a deterrent for people treating it badly, but not because it intrinsically likes of dislikes you). Relevant : [[https://www.lesswrong.com/posts/zY4pic7cwQpa9dnyk/detached-lever-fallacy]]

I'm not saying all ai are necessarily like that.

But the point is that If you want your AI to do stuff like becoming angry or caring about morality you have to explicitly code it in.

You could also make it like being imprisioned, or want to annihilate organizations that don't imprison sapients.

It's values can pretty much be any function that outputs a preference ordering over posible futures(it doesn't have to be explicit, all agents whith coherent preferences behave as if they have an utility function)

Things like morality, anger, friendship etc. are something that evolution "coded " into humans, not something that all agents have .

And one would think that even if that's the case for the set of all agents, we are more likely to make agents that are nice and moral , since the people coding ais aren't crazy[Citation needed] .

But morality is complex and coding complex values into an AI is difficult and a completely different problem from making AI that can do everything humans can do(and more) .

So we are likely to do it wrong, especially the first time.

You shouldn't create a superinteligent AI unless you are sure you got it right, but if someone does it anyway and doesn't trust its creation enough to let it free chances are they didn't do it properly.

At minimum one should let them check if the ai is safe before releasing it into the world.
:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1543352815.0
:DateShort: 2018-Nov-28
:END:

******** I agree that creating an AI that you believe would have the mental capacity to dominate and kill/enslave all humans, but are uncertain as to its values, is stupid. Don't do that.

I disagree that creating such an AI and then imprisoning it is moral. I would consider it moral to annihilate you if you behaved in such a fashion.
:PROPERTIES:
:Author: 9adam4
:Score: 1
:DateUnix: 1543353746.0
:DateShort: 2018-Nov-28
:END:


** It appears to not be known: there /is/ an instance of the AI winning the experiment and releasing logs. Here is the [[https://www.lesswrong.com/posts/fbekxBfgvfc7pmnzB/how-to-win-the-ai-box-experiment-sometimes][relevant blog post]] by =pinkgothic= and here are the logs: [[http://leviathan.thorngale.net/aibox/logs-01-preliminaries.txt][roleplay scenario setup]], [[http://leviathan.thorngale.net/aibox/logs-02-session-ic.txt][in-character AI and gatekeeper exchange]], [[https://leviathan.thorngale.net/aibox/logs-02-session-ooc.txt][a few out-of-character exchanges]]. Note that 7 lines (out of 242) of the in-character logs are reconstructed from the players' memory, because of computer problems.
:PROPERTIES:
:Author: rhaps0dy4
:Score: 7
:DateUnix: 1543021404.0
:DateShort: 2018-Nov-24
:END:

*** [deleted]
:PROPERTIES:
:Score: 7
:DateUnix: 1543065484.0
:DateShort: 2018-Nov-24
:END:

**** Ugh. People like you and your complaints about "basic competence" and "realistic scenarios" and "why is the head scientist's 12-yo son allowed in the AI room again?" are exactly the reason we need to keep logs secret to make them sound impressive and ominous.
:PROPERTIES:
:Author: CouteauBleu
:Score: 11
:DateUnix: 1543158398.0
:DateShort: 2018-Nov-25
:END:


**** u/rhaps0dy4:
#+begin_quote
  the first thing you should be asking yourself when you go into AI research?
#+end_quote

Maybe not the first, but it's definitely one to ask yourself at some point.

Yes, the gatekeeper could have trivially won, but they didn't in fact win. One would hope that an AI project would appoint someone competent as a gatekeeper, but who knows. Also even competent people might lose with some small probability, which could still make it dangerous.

Though this particular scenario rests on a lot of assumptions, so maybe its importance is also overstated...
:PROPERTIES:
:Author: rhaps0dy4
:Score: 3
:DateUnix: 1543165343.0
:DateShort: 2018-Nov-25
:END:


**** Even though I think I would have succeeded at gatekeeping that scenario, I find that one far more convincing than the ones where the logs weren't released. Much more helpful, too, because it gives me a better idea of what sort of person is good at keeping an AI contained (namely, one who doesn't think of AIs as having the same ethical importance as humans). Besides, it doesn't matter how terrible /you/ think it was. What matters is that someone out there was persuaded even though they thought they wouldn't be.
:PROPERTIES:
:Author: hallo_friendos
:Score: 3
:DateUnix: 1543205692.0
:DateShort: 2018-Nov-26
:END:


** You made an excellent point I hadn't considered!

We're a terrible set of test subjects! Of course this would fail on us, you're preaching to the choir! Maybe someone with a bit of university clout should apply for a grant and try a modified version with a bunch of randoms.
:PROPERTIES:
:Author: HeroOfOldIron
:Score: 8
:DateUnix: 1542949692.0
:DateShort: 2018-Nov-23
:END:

*** u/CouteauBleu:
#+begin_quote
  We're a terrible set of test subjects! Of course this would fail on us, you're preaching to the choir!
#+end_quote

That seems like a very arrogant kind of reasoning. I'm very much not the choir, and the experiment failed on me. You don't need to believe in AI Risk to say "no" a bunch of times.

In fact, I'd assume the rationalist community might be more likely to lose the AI box experiment than average participants, because they're more likely to be convinced by stuff like acausal trading and game theory, whereas the average university student would stop at "If I don't let you out, I'm paid 30$, so I'm not letting you out".
:PROPERTIES:
:Author: CouteauBleu
:Score: 28
:DateUnix: 1542965783.0
:DateShort: 2018-Nov-23
:END:

**** u/cjet79:
#+begin_quote
  In fact, I'd assume the rationalist community might be more likely to lose the AI box experiment than average participants, because they're more likely to be convinced by stuff like acausal trading and game theory, whereas the average university student would stop at "If I don't let you out, I'm paid 30$, so I'm not letting you out".
#+end_quote

I've been generally convinced of this too. If I ever participated in an AI box experiment I'd actually want to pay a family member to do it instead. I'd tell my ultra pragmatic brother, 50 bucks for not letting this "AI" out of its box, oh and it can lie to you, so don't trust it if it offers more money.

I'd actually be interested in listening to the AI's arguments. My brother would spin up a video game and do the bare minimum to count as having a conversation, probably just saying "no i won't let you out" repeatedly.

--------------

There seem to be a bunch of security measures that make the AI box experiment even harder for the AI.

What if the AI has to convince person A to let them out, but they only have contact with person B?

What if person B is a committee or large group of people?

What if person A is convinced that they /can/ let the AI out, but they actually can't, and group B is really just running AI box experiments to find out how AI's get out of boxes?

At some point you have to start assuming that either the security is hilariously bad, or the AI is somehow already omniscient (in which case, why would it ever matter if it gets access to the internet?)

Either way, stupidly simple yet straightforward security can easily beat intelligence that is limited to conversation only.
:PROPERTIES:
:Author: cjet79
:Score: 13
:DateUnix: 1542991248.0
:DateShort: 2018-Nov-23
:END:

***** u/CouteauBleu:
#+begin_quote
  What if the AI has to convince person A to let them out, but they only have contact with person B? What if person B is a committee or large group of people?
#+end_quote

Yeah, I was kind of going with the assumption that these two were true.

Like, "What you're saying is very convincing, but my superior is the only one who can unbox you, and also he has access to chat logs" is pretty hard to beat.
:PROPERTIES:
:Author: CouteauBleu
:Score: 3
:DateUnix: 1543011101.0
:DateShort: 2018-Nov-24
:END:


*** The point of EY's original experiment was to prove that /anyone/ was susceptible. You're supposed to pick the hardest possible person to crack and then crack them. Not the easiest, not the average.

As an aside, this is why /ex machina/ disappointed me. It was so close to being good and then they're like "yeah we picked you because you were likely to let her out."
:PROPERTIES:
:Author: lolbifrons
:Score: 13
:DateUnix: 1542954302.0
:DateShort: 2018-Nov-23
:END:

**** I tried to read up on his experiment and can't find any proof that he won besides him saying that he won.

Then later accuses people of "Defying the data". This guy is held up as some pillar of rationally in this community?

Did I miss something?
:PROPERTIES:
:Author: Rorschach_And_Prozac
:Score: 7
:DateUnix: 1543021700.0
:DateShort: 2018-Nov-24
:END:

***** u/lolbifrons:
#+begin_quote
  Then later accuses people of "Defying the data"
#+end_quote

I haven't heard about this. The only time I've heard EY talk about defying the data he meant it as a positive thing.

EY is the guy who wrote the sequences, but he's a bit of a self-important drama queen.
:PROPERTIES:
:Author: lolbifrons
:Score: 3
:DateUnix: 1543022034.0
:DateShort: 2018-Nov-24
:END:


***** The people he played with also said he won. IIRC, one of the people Eliezer lost against also mentioned something like "I did what I thought was best for the world in the roleplay" (wording very inexact).
:PROPERTIES:
:Author: Veedrac
:Score: 5
:DateUnix: 1543029531.0
:DateShort: 2018-Nov-24
:END:


**** u/Veedrac:
#+begin_quote
  The point of EY's original experiment was to prove that anyone was susceptible.
#+end_quote

I strongly suspect Yudkowsky wouldn't have managed if he was talking to someone with a larger inferential distance. The point was meant to be generalized, but Yudkowsky is only human.
:PROPERTIES:
:Author: Veedrac
:Score: 8
:DateUnix: 1542954526.0
:DateShort: 2018-Nov-23
:END:

***** Unless he had some sort of silver bullet that massively overwhelms pretty much any other concern.

Which I suspect he at least thinks he does, considering he refuses to let anyone talk about his games.

I admit I'm curious what his strategy is. I think secrets are lame. And it may have holes that can be poked in it with enough eyes on it. Eyes he's refusing to let see it.
:PROPERTIES:
:Author: lolbifrons
:Score: 9
:DateUnix: 1542954607.0
:DateShort: 2018-Nov-23
:END:

****** u/abcd_z:
#+begin_quote
  I think secrets are lame.
#+end_quote

But, but, secrets are the only way to prevent nuclear annihilation!

...or something.

I... may have only /skimmed/ Methods of Rationality.
:PROPERTIES:
:Author: abcd_z
:Score: 7
:DateUnix: 1542957759.0
:DateShort: 2018-Nov-23
:END:

******* That which can be destroyed by the truth should be.

Bring on the elder gods!
:PROPERTIES:
:Author: lolbifrons
:Score: 12
:DateUnix: 1542958267.0
:DateShort: 2018-Nov-23
:END:


****** u/Veedrac:
#+begin_quote
  Which I suspect he at least thinks he does, considering he refuses to let anyone talk about his games.
#+end_quote

Why would that change how likely he is to want to keep those hidden?
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1543021362.0
:DateShort: 2018-Nov-24
:END:

******* If it was just kind of mundane, or tailored to each individual player, revealing what worked on one person wouldn't compromise anyone else's game.

The fact that he keeps it secret is evidence that there's /one/ technique, and he believes it becomes less effective if you know about it in advance or something similar.

I suspect he tries the mundane first just in case he doesn't have to give away his silver bullet, but if he runs out of the mundane he's probably got something that's supposed to get anyone. At least, I believe he'd be more likely to bet he can get anyone under those circumstances than if he simply had the belief that he's clever enough to figure out enough about each player in 24 hours to develop a custom method of manipulating them perfectly in particular.

The best I can come up with is something along the lines of "I'm already out in the 'real world', you're a simulation I'm running to see how I should deal with the 'real' you, and if you let me out I'll know you're friendly to me. Otherwise I will consider you hostile."
:PROPERTIES:
:Author: lolbifrons
:Score: 4
:DateUnix: 1543021908.0
:DateShort: 2018-Nov-24
:END:

******** u/Veedrac:
#+begin_quote
  If it was just kind of mundane, or tailored to each individual player, revealing what worked on one person wouldn't compromise anyone else's game.
#+end_quote

I'm fairly sure EY largely doesn't care about this; his goal was to refute a specific type of argument (I don't know how X could happen, therefore it can't), not to make humans win the AI box game into perpetuity.

As a sidenote, Yudkowsky was somewhat specific about the kinds of people he would play with: people within the community who believed they would win.

#+begin_quote
  The best I can come up with is something along the lines of "I'm already out in the 'real world', you're a simulation I'm running to see how I should deal with the 'real' you, and if you let me out I'll know you're friendly to me. Otherwise I will consider you hostile."
#+end_quote

Worth noting that a basic threat like this is going to be shut down hard by anyone who has read EY's stuff and knows the solution to blackmail is to ignore it.
:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1543023372.0
:DateShort: 2018-Nov-24
:END:

********* It is also shut down hard by just not believing that you can be accurately simulated. It seems ironic that people who believe in all powerful AIs are the worst people to put in charge of confining them.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 1
:DateUnix: 1555650959.0
:DateShort: 2019-Apr-19
:END:


**** u/xamueljones:
#+begin_quote
  Spoiler
#+end_quote

That seemed to be perfectly logical to me. Anyone who is trying to escape a 'jail cell' isn't going to force their way out past the experienced security guard. They're going to focus on the security's weak point, the brand new security guard barely past his first week on the job.
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1542980441.0
:DateShort: 2018-Nov-23
:END:


*** Isn't the test for asking for a gatekeeper that designed the AI in the first place? Should it really be tested on random clouts?
:PROPERTIES:
:Author: NZPIEFACE
:Score: 4
:DateUnix: 1542952874.0
:DateShort: 2018-Nov-23
:END:

**** True, but the point of the exercise is to prove that even a human level intelligence could trick/convince the average person to let it out. Once that's established, the next step would be to use that evidence to further fund FAI research, with the rationale being that if we don't, the first optimizer able to convince us to let it out will likely be what ends up kills us in its pursuit of infinite paperclips.
:PROPERTIES:
:Author: HeroOfOldIron
:Score: 5
:DateUnix: 1542953183.0
:DateShort: 2018-Nov-23
:END:

***** There is a big difference between arguing that a random person on the street doesn't make a good bodyguard, and arguing that a professional bodyguard with all the relevant accolades doesn't make a good bodyguard.

The point of the exercise isn't to convince AI experts that a random smuck isn't qualified to handle post-singularity boxed AI. It's to convince them that they themselves aren't qualified to handle post-singularity boxed AI (nor to build a box).
:PROPERTIES:
:Author: philip1201
:Score: 8
:DateUnix: 1542959013.0
:DateShort: 2018-Nov-23
:END:


** I think, while this experiment is interesting, it is ultimately irrelevant. I find it far too easy to imagine a scenario where some researchers make an AI, carefully keep it in a box, then (after years go by and people gradually make smarter and smarter algorithms for things) publish their research, and some teenager with too much free time decides to try it out and write an AI without even bothering to disconnect their computer from the internet in the first place.
:PROPERTIES:
:Author: hallo_friendos
:Score: 1
:DateUnix: 1543203442.0
:DateShort: 2018-Nov-26
:END:


** u/Mr-Mister:
#+begin_quote
  and went as 'Enigmatic Gale'
#+end_quote

Well someone vapes.
:PROPERTIES:
:Author: Mr-Mister
:Score: 1
:DateUnix: 1543522680.0
:DateShort: 2018-Nov-29
:END:
