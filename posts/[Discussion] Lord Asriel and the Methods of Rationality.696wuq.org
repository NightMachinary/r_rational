#+TITLE: [Discussion] Lord Asriel and the Methods of Rationality

* [Discussion] Lord Asriel and the Methods of Rationality
:PROPERTIES:
:Author: TheUtilitaria
:Score: 60
:DateUnix: 1493897050.0
:END:
I've always had very mixed feelings about Philip Pullman's His Dark Materials trilogy, a sequel to which is coming out soon. On the one hand, it's basically the bible of Secular Humanism and it's very well written, but on the other hand it stabs directly at the differences between the rationalist and secular humanist community. The story is atheistic and anti religion but also decidedly anti-rationalist.

Lord Asriel in the story is a good rendition of a flawed rationalist hero, to the extent that I wonder if Philip Pullman deliberately wanted to deconstruct the idea. His plan is to build a republic of heaven, defeat the authority, merge magic and technology and have everyone live forever. If LAMOR were ever to exist, Asriel would only need some minor tweaking to basically become the adult Harry Potter from the HPMOR sequel, Significant Digits. Yet Asriel is constantly presented as hubrisic and even a little evil for wanting these things, and his ambition causes his downfall.

I can't have been the only person, reading those novels, to be angry at the authorial choices, when after defeating the forces of Heaven, Lord Asriel dies and all his plans come to nothing. All the souls are liberated from hell, only to evaporate into nothingness instead of trying to get on with living as free people.

From a rationalist point of view, the His Dark Materials trilogy ends with a nasty pro-death message, and the victory against the authority is nothing compared to the continued volume of avoidable death and suffering the story perpetuates. More than anything, these books made me reluctant to self-identify as a humanist precisely because they highlighted so sharply how pro-death so much of the humanist movement is. [[https://youtu.be/pR7e0fmfXGw][(I'm not exaggerating this problem, watch at your own risk.)]] Seeing this attitude in a book so beloved of sceptics and atheists was confusing.

If any story needed the rational treatment, His Dark Materials does.

*TL;DR - His Dark Materials is humanist but strongly anti-rationalist, because of its pro-death message. Reading it made me realise the problems with secular humanism and is part of what led me here.*


** [deleted]
:PROPERTIES:
:Score: 35
:DateUnix: 1493898271.0
:END:

*** I'd like to know how long a life is too long. They don't have to be specific about it, I'd be fine with a ballpark figure.

But how long is too long? 100 years? 200? 1,000? 10,000? 1,000,000? Can these pro-death people pick any finite time and say, "Yep, this is definitely far too long to live."?

When I was younger, I couldn't imagine being old. Mind you, I only recently turned 28, but 10 years ago the idea of being 100 and still wanting to live flummoxed me. Why would anyone want to live when they were old and frail and needed help toileting, eating, etc.?

Well, turns out that when you're in that situation you don't suddenly start wanting to die (though untreated depression is distressingly common in the elderly). And that's with our current quality of aged life. Imagine if you didn't lose mobility, dexterity, and cognition as you aged. Would a fit and healthy 200 year old beg for death? What if they were 1,000, 10,000, or 1,000,000? When would such a person decide they wanted to die?

And what if they never made that decision, if they just went on and on until the heat death of the universe, or even beyond? Would that somehow be a terrible thing? I've never seen a good argument that it would be.

Humanists who oppose immortality annoy me so much, because they're /so close/ to the revelation. Ask them if they're happy about any specific unwilling death. With such an example, figure out what their problem is.

Was the person "evil"? Well, what if that person was rehabilitated?

Were they in pain or otherwise hurting? What if they were treated?

Were they significantly impaired, unable to do the things they wanted to do? What if we fixed their bodies/minds, or provided alternate means?

Point is, eventually you'll figure out that their criterion for the acceptability of a death isn't really related to age. It may be things that are related to age (like infirmity and senility), but age on its own is scarcely relevant.

But they don't seem to make that leap themselves, to tease apart their feelings about age from their feelings about death. That frustrates me, especially when otherwise moral people seem to be totally okay with somebody dying.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 35
:DateUnix: 1493904228.0
:END:

**** It makes no sense to me to wish for oblivion/nonexistence. If that's what you expect after death, then death is unquestionably the worst thing that could happen to you.

Even the Bible is not a fan of death. Right back at the beginning, Adam and Eve are told not to eat the fruit because why? /They'll die if they do/. The most positive thing the book says about death is that it has been defeated and is therefore less scary. Indeed, Paul indicated that if it weren't for the hope of resurrection - in other words, eternal life with perfect health, as you say - then Christians would be the most miserable people on earth.
:PROPERTIES:
:Author: thrawnca
:Score: 16
:DateUnix: 1493932920.0
:END:

***** "The last enemy to be destroyed is death." - 1 Corinthians 15:26
:PROPERTIES:
:Author: THEHYPERBOLOID
:Score: 16
:DateUnix: 1493942432.0
:END:


***** I think it only really makes sense when you predict that the future is expected to hold net negative utility for you.

E.g. if the "fire and brimstone" hell was real and you ended up in it, any satisfaction you derive from life would probably be heavily outweighed by all the torment.

In a more practical example, suicide makes sense as a desire for people who are depressed. That's not to say it's a good idea; depression is frequently treatable, and if you haven't given treatment a genuine attempt because you believe it won't work, any prediction about the future is unfounded.

So while I'd say suicidal ideation is almost always a poor solution, the error of logic is the lack of other attempted solutions, not the desire to die in the face of an expected lifetime of agony.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 7
:DateUnix: 1493955171.0
:END:

****** I'd argue that when available, cryonic suspension (basically reversible death) is strictly preferable to death no matter what, since you get all the benefits that death would provide, plus the possibility of revival later on.

Of course, there are some situations in which this might conceivably lead to a worse outcome, such as the "hell" scenario where your torturers revive you in order to inflict more pain on you--but in such cases you're unlikely to have the option of cryonic suspension available in the first place. Hence, the "when available" qualifier I placed at the beginning of this comment. (Not to mention the fact that outright death probably /also/ won't be available to you as an option in such situations, making the point a mostly moot one.)
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 10
:DateUnix: 1493957654.0
:END:


**** In our current technological state, deathist humanism is pragmatic. While being in favor of the concept of death is obviously wrong, right now it is unavoidable.

And there are many people for whom death is the best option available. Those for whom all remaining life will be unutterable agony should not be forced to experience that if they don't want to.
:PROPERTIES:
:Author: Frommerman
:Score: 6
:DateUnix: 1493916707.0
:END:

***** Well, there is cryonics. But i suppose that isn't actually possible for most people, not even considering how many people could do it but won't.

Actually, has anyone ever done or seen an estimate of how many people we could hypothetically put through cryonics given a 100% willing humanity? Could we theoretically save everyone in that scenario?
:PROPERTIES:
:Author: Sarkavonsy
:Score: 3
:DateUnix: 1493917479.0
:END:

****** I doubt that's possible just from a logistical perspective. Cryonics on death requires several highly trained people to perform specific actions within less than a minute to work properly, and those people have to be already there when you die. This means they are either waiting for you to die, or you are chosing euthanasia. There isn't really a way to get enough people trained to perform the procedures for absolutely every person dying even in a nursing home or hospital, much less in accidents or at home.

Barring some insane-to-expect cultural, economic, and technological advances, we can't realistically attempt to save everyone this way.
:PROPERTIES:
:Author: Frommerman
:Score: 6
:DateUnix: 1493918052.0
:END:

******* You underestimate the power of a cultural shift. I think it would be sufficient.

"Several highly trained people" and "it's quick" means that cryonics is about as hard to implement as the most minor of surgical procedures that require anesthesia, or comparable to the level of medical care that people receive /in the ambulance on the way to the hospital/. Medical teams in the OR and ER learn many such procedures, and, if much of our entire society where deeply interested in cryonics (such that, when someone died in a hospital and wasn't frozen, their relatives would write an angry op-ed and the papers would publish it - a sequence of events that might happen at the 5-10% sign-up level) they'd learn that one too.

With 5-10% of the population signed up with the idea that cryonics is worth trying, you get near-universal /availability/ of the procedure over the entire west and much of China and India. At 70%, it has become the dominant belief about death, causality, and the afterlife in those regions of sub-sahara Africa currently torn apart by religious struggles, and they start trying to save people with automotive antifreeze, old bike tires, and $5 dewars distributed by charity groups.
:PROPERTIES:
:Author: BoilingLeadBath
:Score: 8
:DateUnix: 1493934115.0
:END:

******** #+begin_quote
  the level of medical care that people receive /on the ambulance on the way to the hospital./
#+end_quote

Heh, no. I'm an EMT, and the procedure required for proper vitrifaction of a human brain is /far/ more involved and requires far more expensive equipment than currently exists in the EMS sphere. Cycling all the blood out of the body and replacing it with antifreeze requires a dialysis machine, and even with current technology those things are large, bulky, and very finicky to move and operate safely. In addition, the trucks which actually transport a prepared body to be stored in liquid nitrogen are equipped almost nothing at all like ambulances.

Also, the fact of the matter is that human bodies take up a ton of space. You know how the catacombs of Paris are a labyrinthine boneyard? Imagine that but every body is fully intact and must be kept under liquid nitrogen at all times, and the nitrogen must be refilled at a constant rate that increases for each additional bit of volume the crypt comprises. It would be a /gargantuan/ engineering challenge to create enough space to store even ten people per minute worldwide. That's over 5.25 /million/ bodies per year, and that's only saving 10% of people.

And all of this is ignoring the fact that /we have no idea if this will even work./ Which tends to discourage people from spending thousands of dollars on life insurance to hand over to a cryogenic freezing charity.
:PROPERTIES:
:Author: Frommerman
:Score: 10
:DateUnix: 1493937955.0
:END:

********* Sorry for the poor assumption Re: Cryonics and EMS. It is not offered in my area, so I had not read about the implementation details...

Which is just as well. I sincerely doubt that contemporary cryonics is using very efficient tools. For instance, it would seem that for /replacing/ blood rather than filtering it, a dialysis machine is the wrong tool, and if we had a market for a million of these operations per year, you'd get a simpler and cheaper purpose-built tool. Perhaps an open loop injection machine.

As for the cryocrypt: tunnels are for transit; tanks are for storage. 52 million heads fit in 30 or so medium sized (IE, 20m tall x 20m dia) insulated tanks, kept cold with maybe 50 megawatts of electricity. An unusual chemical installation, to be sure, being focused on storage rather than production... but probably less that a billion dollars to construct. Call it 5 billion to fund future maintenance and... well, it's still a tiny amount of money.
:PROPERTIES:
:Author: BoilingLeadBath
:Score: 2
:DateUnix: 1493947545.0
:END:

********** Yeah, that's a billion dollars to store every head of every dead person...for one year. Then you need another billion dollars to store next year's heads, and another plot of land to drop them on, and, and...

And the reason you need specifically a dialysis machine is perfusion. Draining all blood from the major arteries and veins is pretty easy: cut someone's jugular and their own heart will do a significant fraction of the work for you. But we aren't trying to do that. We're trying to preserve the most complicated neural networking computer currently known to exist, and in order to do that, the antifreeze absolutely /must/ go everywhere. Every vein and artery, every microscopic capillary, every single fragment of the maze that is the circulatory system must be flushed, if you fail the patient gets numerous clots in the brain, which at the very least would significantly cut down on the number of options for revival, and at worst could make it impossible if they didn't know about those clots as the attempt proceeded. The only way to do this is slowly and carefully. Additional pressure just ruptures the capillaries and gives your patient countless tiny aneyurisms, you can't accelerate the process. Dialysis machines clean all of the blood, and therefore they are designed to carefully pull blood out and put it back in at exactly the pressures that the body's systems can withstand, no more and no less.

So unfortunately, there's no way to simplify the process.

By the way, would you happen to be an engineer?

Edit: Actually, I'm remembering this wrong. It's not a dialysis machine you need, it's a heart - lung bypass machine. Some /hospitals/ don't have those, mostly smaller rural ones who expect to air evac anyone who might need them. A heart - lung bypass machine is the one which can perfuse fluids through the entire circulatory system without a functioning heart. These machines are...not really simplifiable. You could remove the 'lung' part, but the heart part is the toughest bit.
:PROPERTIES:
:Author: Frommerman
:Score: 3
:DateUnix: 1493950046.0
:END:

*********** Uh, not to sound insensitive or anything, but we currently spend way way more than that on torturing people horribly in the last year of their lives instead of giving them a quiet, graceful, hopeful departure to the future with their senses and dignity intact.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 8
:DateUnix: 1494038466.0
:END:

************ This is absolutely true, and I see way too much of that at work. However, that is for the most part by choice. People scrabble for every chance at survival no matter what, no matter the pain, not even considering for an instant that death might be preferable. Maybe because they know that they /don't want to die/, no matter their protestations that immortality would be terrible.

You are completely correct that everyone wants to be immortal and just won't admit it to themselves. And I don't think we should deny people that. Usually there is no hope of course, we overtreat the stage four cancer or amputate the last limb of the diabetic, but that doesn't mean people should be forced into a position where there's no hope at all. I do think choice is valuable.

And sometimes, people choose pain before death. I can't begrudge them that.
:PROPERTIES:
:Author: Frommerman
:Score: 5
:DateUnix: 1494040753.0
:END:


*********** A billion of dollars per year isn't that munch money for the world and de are already buriying a important part of this people, the amount of money and Solace neccesary is big but we are already spending a smaller but not multiple orders of magnitude smaller amount of money and space in funerals , buriying people , caring for terminally ill people.I expect cryonics to become way cheaper if it becomes mainstream and big companies start offering cryonics services and spending millions in research and development .I'm sure the world is spending absurdly bigger amounts of money on sillier things than potentially saving the life's of a big percentage of the people that die, and if most people actually wanted cryonics I think our current economy Is perfectly capable to eventually giving them cryonics like it was perfectly capable of gradually giving most of them other things.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1494150529.0
:END:


******* Right, that's about what I expected. /googles/ yep, global rate is about a hundred deaths per minute. Totally outside the current realm of possibility.
:PROPERTIES:
:Author: Sarkavonsy
:Score: 2
:DateUnix: 1493918255.0
:END:


**** #+begin_quote
  But how long is too long? 100 years? 200? 1,000? 10,000? 1,000,000? Can these pro-death people pick any finite time and say, "Yep, this is definitely far too long to live."?
#+end_quote

2^{{#} ^{of} ^{neurons} ^{in} ^{the} ^{human} ^{brain}.}
:PROPERTIES:
:Author: Daneels_Soul
:Score: 7
:DateUnix: 1493914214.0
:END:

***** Missing the point. What if the mind were expanded? Cybernetic implants added; consciousness uploaded, etc.

Yes, a human brain can only cycle through so many states, but immortality discussions tend to almost instantly become trans-human immortality discussions.
:PROPERTIES:
:Author: ben_oni
:Score: 14
:DateUnix: 1493915887.0
:END:

****** In consideration of mind-expansion, I am willing to replace replace by bound by (the probably much more restrictive bound) the number of possible mental states that can be had be beings that could reasonably be considered to be "you".
:PROPERTIES:
:Author: Daneels_Soul
:Score: 1
:DateUnix: 1494000298.0
:END:


***** What's the significance of that figure? Each neuron has far more than two possible states, after all, given its complexity...
:PROPERTIES:
:Score: 6
:DateUnix: 1493920887.0
:END:

****** Actually that figure indicates the number of connections possible between the neurons in the brain. So 2^{{#} ^{of} ^{neurons} ^{in} ^{the} ^{brain}} indicates the number of every possible brain that can ever exist which is a vastly larger space than the space of all possible human minds in every possible environment.

Just to make it clear how that figure was reached, let's say that you have 2 neurons, then there is only two ways the neuron can connect to each other (there is a connection or no connection). Add one for 3 neurons, then there are 8 different connections (8 different groups with unique ways to connect or not connect the neurons). Continuing the pattern leads to 4 neurons-16 connections, 5 neurons-32 connections. According to this pattern, the group of possible brain states apparently doubles for every neuron added.

EDIT: I made a mathematical mistake earlier with the numbers and did a little editing.
:PROPERTIES:
:Author: xamueljones
:Score: 4
:DateUnix: 1493929419.0
:END:

******* But why would that number be "years" rather than "milliseconds" or "millenia?"
:PROPERTIES:
:Author: electrace
:Score: 7
:DateUnix: 1493940803.0
:END:

******** It almost doesn't matter. The difference between milliseconds and millennia amount to a difference of about 25 neurons in the exponent.
:PROPERTIES:
:Author: Daneels_Soul
:Score: 4
:DateUnix: 1494000088.0
:END:


**** Beyond the heat death of the universe?

Gotta love how these discussions always hit the point where godlike powers are invoked. This is also missing the point.

If a being has the power of a god, then I would hope that immortality is okay. Presumably, they also have the wisdom to use it (and all the accompanying powers) responsibly. And if they don't, our little discussion isn't likely to matter to them.

On the other hand, lacking the powers of a god, mortality is assured. The "pro-death" crowd, as you call them, generally aren't arguing that people should die, or that old people need to die; rather, it's a belief that we should come to accept death when the inevitability is upon us rather than rage against it. This should never be a value judgement for other people, but a way to assess our own morals.

The fundamental question is how far someone should go to extend their existence?

And for bonus points, in what circumstances should we violate the "Do Not Resuscitate" wishes of patients?
:PROPERTIES:
:Author: ben_oni
:Score: 5
:DateUnix: 1493917278.0
:END:

***** #+begin_quote
  Beyond the heat death of the universe?

  Gotta love how these discussions always hit the point where godlike powers are invoked.
#+end_quote

I'm not presupposing the existence of such an outcome. I included the possibility for completeness.

#+begin_quote
  The "pro-death" crowd, as you call them, generally aren't arguing that people should die, or that old people need to die;
#+end_quote

In the linked video, Stephen Fry said that an infinite life would have no meaning. Assuming he thinks that meaning is a good thing for a life to have, he would appear to be arguing that death is good as a concept.

#+begin_quote
  rather, it's a belief that we should come to accept death when the inevitability is upon us rather than rage against it.
#+end_quote

I agree. That doesn't mean that we shouldn't take steps to delay that stage for as many people as possible. If we could push that date back to hundreds or thousands of years, should we then complain that humans are simply living too long?

#+begin_quote
  The fundamental question is how far someone should go to extend their existence?
#+end_quote

As far as they can without impeding on the ability of others to do the same, or until the point at which they do not wish to go on. This is what humans currently do, and have been doing for a hundred thousand years, only we've been limited to decades for most of that time.

#+begin_quote
  And for bonus points, in what circumstances should we violate the "Do Not Resuscitate" wishes of patients?
#+end_quote

When those wishes were not made in sound mind. What would qualify for that situation is a question for psychologists and lawyers, but in general if somebody has an untreated mental illness or is being coerced, the DNR shouldn't be accepted.

The issue gets murkier if somebody has a mental illness and has been offered and informed of treatment, but declined. Even if you accept their refusal of treatment, you may not want to grant their request for euthanasia. Again, this is probably a question for the experts.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 11
:DateUnix: 1493923218.0
:END:


**** I think I have a very precise answer for you!

"Eternal life" gets either boring or disturbing when it passes the functional Busy Beaver number for universe simulations containing everything in our Hubble Sphere. There exists an amount of time where the universe itself must end or enter a loop (probably a boring loop like everything being so far from everything else there is no meaningful interaction). I'm not sure if that is a close enough ballpark for you, but, while the busy beaver number is incomputable in general, it is computable for specific numbers, one of which I've supplied (essentially). Or (for those of you more interested in complex loops) when all usable energy I could possibly interact with has been exhausted and there is some maximal entropy state for my hubble sphere. Since that is likely the longest we could get this machine to go, it is assuredly a boring fate.

Certainly at that point I, as a constituent element of the universe, would not have any particular time preference -- more time would by definition go unnoticed by me (as the noticing part of my brain would be in a loop as well), and I cannot fathom how I would prefer an unchanged and unnoticed loop over an end.
:PROPERTIES:
:Author: thepublicinternet
:Score: 4
:DateUnix: 1493934749.0
:END:


**** Reasons not to want to live forever: 1) being alone (obviously if immortality was large-scale this is mitigated) 2) infirmity (as we age so does our body, and they aren't really designed to last even 100 years at present, although they can... I guess this would be solved if someone managed to crack eternal youth or something) 3) overpopulation 4) lack of resources (both of these solvable by terraforming...I guess? to be honest though, resource distribution already sucks with the current number of people on the planet, I couldn't see it being more equal if more people were vying for it) 5) lack of societal change. Can you imagine if people from 500 years ago were still alive today? We already have enough problems with generational political shifts, aka old people being in favour of more traditionalist values while young people are generally more liberal/ left-wing. There would be more of a problem of horrible old fashioned values sticking around (based on my country: views on women, people of colour, people of other religions including the 'wrong' strain of christianity, slavery, war, medicine, philosophy...) How could we overcome this barrier? 6) related to the above: sometimes dictatorial regimes only end with the old-age death of the dictator (cf: Franco in Spain). Immortality = the people under Franco would have been stuck under Franco. Indefinitely.
:PROPERTIES:
:Author: obbets
:Score: 1
:DateUnix: 1494497175.0
:END:


*** #+begin_quote
  I really hate the comparison about the book that would be boring if it went on and on.
#+end_quote

It's an absurd argument.

What if you could only read 80 books in your life? Would it bring more joy because there is a finite number, or less joy because you miss out on so many good books?
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 13
:DateUnix: 1493908190.0
:END:


** It's common knowledge that His Dark Materials is meant to be a deconstruction of Narnia, but I almost see Unsong as like a deconstruction of His Dark Materials - God really is good after all.
:PROPERTIES:
:Author: TheUtilitaria
:Score: 18
:DateUnix: 1493897276.0
:END:

*** Well kinda. It's complicated. I mean there's still Thamiel and stuff. Then again it might just be that god's good side is not very smart when he crosses the Panama Canal. Since he turns into a dog. Then again, if Thamiel crosses the Panama Canal what would happen?
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 17
:DateUnix: 1493901900.0
:END:

**** He would at least have liveD.
:PROPERTIES:
:Author: RMcD94
:Score: 12
:DateUnix: 1493907322.0
:END:


**** Actually, it was Nemo who turned into an omen, not God into dog.
:PROPERTIES:
:Author: ShareDVI
:Score: 7
:DateUnix: 1493915206.0
:END:

***** Or that was a red herring. Or it means that Nemo is God.
:PROPERTIES:
:Author: XerxesPraelor
:Score: 5
:DateUnix: 1493922468.0
:END:

****** I think that nemo being god or at least metatron makes sense.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1493987028.0
:END:


** #+begin_quote
  Yet Asriel is constantly presented as hubrisic and even a little evil for wanting these things
#+end_quote

I mean, he does torture-murder a kid [[https://www.youtube.com/watch?v=D8aBP-JOZsU][to power his machine]]. And yes, blah blah utilitarianism greater good blah, but [[http://econlog.econlib.org/archives/2016/01/the_invisible_t.html][he wasn't too bothered]] by the whole affair.
:PROPERTIES:
:Author: Roxolan
:Score: 12
:DateUnix: 1493906479.0
:END:


** In defense of HDM...just because death is bad doesn't mean that someone can't be considered hubristic or even evil for their own particular, flawed implementation. While immortality is a goal and a prize, that doesn't mean that any steps taken toward that goal are justified. The difficulty of implementation and high stakes also mean that more corners are likely to be cut in some kind of direct A to B search for immortality than there would in some large but better scoped goal, such as implementing universal health care.
:PROPERTIES:
:Author: Amonwilde
:Score: 12
:DateUnix: 1493915373.0
:END:


** Honestly the thing that always bothered me about His Dark Materials is that it's /so/ much the anti-Narnia that it kind of feels like it falls into the same traps to me? Like, it's definitely nowhere near as bad as Narnia's shouting "THE LION IS JESUS" in your ear every five seconds, but it still kind of feels to me like it's more allegorical than it needs to be to tell a good story. :/
:PROPERTIES:
:Author: The_Magus_199
:Score: 9
:DateUnix: 1493917548.0
:END:


** The beauty of seeing the world in materialistic terms is that all bad things, all things that plague humanity suddenly become solvable problems. The video was built out of standard arguments against living forever. It's the worst kind of contrarianism, the religious folk believe that living forever in an afterlife is great so we have to argue against that.
:PROPERTIES:
:Author: Lexabyte
:Score: 10
:DateUnix: 1493921773.0
:END:


** Yeah, it's definitely a problem with HDM that I only really noticed after I started my journey toward Transhumanism.

Rational!Golden Compass would be really interesting, to me, and I'd love to see Lyra as a rationalist, where Asriel and Coulter represent two potential failure modes of rationality and the anti-death value is included as well as the anti-God one. But the biggest hurdle is the titular Golden Compass itself... when you have a device (and by extension, sentient all-knowing magic particles) that can answer any question you pose to them, a rational protagonist has the potential for explosive munchkinry unless some major parts of the lore/world are altered from canon.
:PROPERTIES:
:Author: DaystarEld
:Score: 9
:DateUnix: 1493972651.0
:END:

*** #+begin_quote
  when you have a device (and by extension, sentient all-knowing magic particles) that can answer any question you pose to them, a rational protagonist has the potential for explosive munchkinry unless some major parts of the lore/world are altered from canon.
#+end_quote

Canonically, you can only use the alethiometer instinctively if you are 'innocent', ie childlike and unlikely to use it for this kind of thing.

Adults need substantial training and careful interpretation, but they're fantastically valuable artefacts for good reason!
:PROPERTIES:
:Author: PeridexisErrant
:Score: 15
:DateUnix: 1493988697.0
:END:

**** Are... are you the same PeridexisErrant that made the DF starter pack? If you are, then I really need to thank you for that because it really lowered the barrier to entry for me.

Sorry for off topic, I'll go back to lurking now.
:PROPERTIES:
:Author: ardetor
:Score: 9
:DateUnix: 1493992820.0
:END:

***** That's me - I'm glad it helped :D
:PROPERTIES:
:Author: PeridexisErrant
:Score: 7
:DateUnix: 1493993748.0
:END:


**** Hmm. I remember her losing the ability to use it at the end of the series, but you think r!Lyra wouldn't be able to use it instinctively from the beginning?

Also I forget, was it ever cleared up whether ANY child could use it that way, and everyone was just ignorant of that until Lyra picked one up, or if it was something special about her as well as the fact that she was a child?
:PROPERTIES:
:Author: DaystarEld
:Score: 3
:DateUnix: 1494008353.0
:END:

***** #+begin_quote
  you think r!Lyra wouldn't be able to use it instinctively from the beginning?
#+end_quote

I think it depends on how rational Lyra is, and why. Rationality, if one is fully committed, has a certain ruthlessness about it that seems fundamentally incompatible with innocence. It's hard to imagine, for example, anyone "childlike" [[http://lesswrong.com/lw/ur/crisis_of_faith/][deliberately provoking a crisis of faith]] just to make sure they held empirically true beliefs. Then again, it's not too hard to imagine a precocious child who thinks they know everything about rationality, but who hasn't ever been in a situation where they needed to apply it.

It would be very moving, I think, if Lyra had a shock (maybe the death of the boy at the end of the first book) that required her to apply all her rationalist art, and thereby lost the ability to read the alethiometer much earlier than in the source material. Or, maybe even better, if the alethiometer itself told her she had to use rational thinking to prevent some bad thing, and as a result she lost access to its answers. It's narrative necessity, I think, that rational!Lyra's childhood end suddenly and traumatically. Which is a real shame when you consider how nicely and gently source Lyra's did.
:PROPERTIES:
:Author: bassicallyboss
:Score: 6
:DateUnix: 1494138698.0
:END:

****** Yeah, I think that would work well. It would be interesting to see how Will is adapted in such a setting too, considering he's the much more methodical thinker in canon.
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1494146124.0
:END:

******* Yeah. I can't think of any really great ideas for Will, just a bunch of okay ones.

Their roles could be reversed, with Will being the emotional one and moral compass. I kind of like the idea of Lyra as the ruthless goal-achiever and Will as the voice of restraint. But I think running with this too far would uninteresting. You'd end up with Will as a passive conscience for Lyra, and he was definitely more than that.

Will could mentor Lyra, since his world has more advanced science than hers. This seems like a better fit for their relationship in the source material, but I'm not sure it would be all that useful. Lyra's world already has the scientific method, so her mind's in the right place, and ~1850s or so level technology, iirc. Apart from teaching her about germ theory and washing her hands, maybe some basic chemistry, I'm not sure there's a ton of knowledge that Will would have that Lyra wouldn't that two fleeing children without infrastructure could take advantage of. Then again, Lyra wouldn't have learned the best knowledge available to her world. If her training skewed more toward philosophy or the humanities, and Will had attended a science school or something, then she could teach him about rationality and thinking and he could teach her about empiricism and experimentation. That could be a fruitful relationship, I think.

It would be important to make sure Will couldn't read the compass, unless the story is just the two of them munchkining their way through everything. I don't remember if he could in the books--I think he was a few years older than Lyra, so maybe not?--but either way, he needs to be worldly enough that this isn't possible for him.
:PROPERTIES:
:Author: bassicallyboss
:Score: 2
:DateUnix: 1494201687.0
:END:


***** I honestly don't know. Supposedly it was the onset of puberty that removed Lyra's ability to intuitively use the alethiometer. She was a very intelligent, cunning girl even in canon, so it's not like her ability came from naivety or innocence.

Also, I remember there being an Effulgence arc set in the HDM-universe, so if you haven't, maybe look at that as something relevant?
:PROPERTIES:
:Author: Detsuahxe
:Score: 2
:DateUnix: 1494136546.0
:END:

****** Hmm, I never read the Effulgences, I should probably check them out.
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1494144536.0
:END:


** I'm surprised [[/u/DayStarEld][u/DayStarEld]] hasn't given his thoughts on this yet.
:PROPERTIES:
:Author: trekie140
:Score: 5
:DateUnix: 1493924821.0
:END:

*** Whelp, that's what I get for avoiding Reddit most of the day.
:PROPERTIES:
:Author: DaystarEld
:Score: 5
:DateUnix: 1493972248.0
:END:


** I always disliked how Mrs. Coulter was allegedly redeemed by the third book. She would have been more interesting as a psychopathic ally.
:PROPERTIES:
:Author: TimTravel
:Score: 3
:DateUnix: 1494129266.0
:END:


** that sequel has been coming out soon since 2006
:PROPERTIES:
:Author: flagamuffin
:Score: 2
:DateUnix: 1493959335.0
:END:


** I'm confused. What part of rationalism is "anti-death"?

Also, "well-written"? You've got to be joking. I found the author-tract to be incredibly annoying, among it's many other flaws.
:PROPERTIES:
:Author: ben_oni
:Score: 4
:DateUnix: 1493901470.0
:END:

*** Rationalism is a /meta/epistemology. Transhumanism isn't a part of rationalism per se, rationalists just happen to usually be transhumanist because transhumanism is right. If death was a good thing and living longer could only make us suffer, then there would be more rationalists who weren't transhumanists.

Just like how if biological evolution was a hoax, there wouldn't be so many scientists who believe it's real.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 19
:DateUnix: 1493902722.0
:END:

**** And nobody is talking about forcing people to live forever. A lot of rationalists are pro assisted suicide (sometimes with the caveat that they need to seek psychological help, sometimes without).

I'd characterize us a generally pro-choice.
:PROPERTIES:
:Author: traverseda
:Score: 12
:DateUnix: 1493904764.0
:END:

***** #+begin_quote
  I'd characterize us a generally pro-choice.
#+end_quote

Death is always an option. We think life should be too.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 19
:DateUnix: 1493908364.0
:END:


**** I'm not convinced that a rationalists necessarily have to be a transhumanist. I am a transhumanist, but I see that as a rational implication of my values rather than my heuristics. Transhumanism is a logical consequence of utilitarian humanism, but I don't know of any requirement that you must be a utilitarian humanist to be a rationalist. It's a moral philosophy that I believe is good, but I don't think it's fair to say you can't be considered rational if you don't subscribe to it.
:PROPERTIES:
:Author: trekie140
:Score: 13
:DateUnix: 1493922921.0
:END:

***** Transhumanism is humanism extrapolated to an absurd conclusion. Singularitarianism and AI alarmism are technological progress extrapolated to an absurd conclusion. Rationality trains people to take ideas seriously. Taking ideas seriously tends to make people transhumanists.
:PROPERTIES:
:Author: FeepingCreature
:Score: 5
:DateUnix: 1493938870.0
:END:

****** To be clear, you're not using "absurd conclusion" in its colloquial sense of "false conclusion", right?
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 5
:DateUnix: 1493941062.0
:END:

******* More in the sense of "Beware the Absurdity Heuristic." :)
:PROPERTIES:
:Author: FeepingCreature
:Score: 6
:DateUnix: 1493941503.0
:END:


****** I still think it's about values. In general, I've found rationality very good at arguing over ways to optimize values, but utterly useless at changing the values people wish to optimize. For instance, I intrinsically value human autonomy so I oppose requiring all cars to be self-driving even though I want people to always use the safer option and know that allowing them not to will put people at greater risk than it would otherwise.

I have a fundamental philosophical view on what the relationship between humans and technology should be and aim to optimize that belief since I consider it morally right. I know all the completely rational reasons people have to believe otherwise, but I still believe in what I do. I've seen the same thing happen in arguments over religion. People do not change what they see as right and wrong, only how they serve their morals.

I think anyone who became a utilitarian humanist is someone who never intrinsically valued anything except reducing human suffering in the first place. Meanwhile, people like me who intrinsically value other things and get accused of being poser rationalists. I don't think that's a useful worldview since evidence, as catalogued in /The Righteous Mind/ by Jonathan Haidt, indicates that rationality can't change someone's values.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1493948818.0
:END:

******* I think you're mischaracterizing self-driving car supporters here. Like you, I intrinsically value autonomy. The problem isn't that I /don't/ view autonomy as something valuable and worth preserving. The problem is that I /also/ value human life, and the case of self-driving cars is a specific instance of a more general issue: the issue of trading off one value for another. When confronted with such a choice, you have to choose which value you think is more important (weight-adjusted for the specific numbers of people in question, of course). I support self-driving cars, in other words, not because I think the ability to make a choice for yourself is intrinsically worthless, but because I think the potential for saving lives is worth the sacrifice of that particular choice. (You could also refuse to make the choice entirely, but that basically amounts to leaving things as-is, which is equivalent to letting the universe make the choice for you. The universe tends to have a poor track record of optimizing my values, so I typically view this as a poor idea.)

This is, admittedly, still a consequentialist way of thinking--no deontologist with "do not violate human autonomy" as a rule would ever be willing to sacrifice any amount of autonomy for any number of lives--but in my experience, deontology tends to be less successful than consequentialism when it comes to /actually describing how humans reason/. Actual humans do appear to me to be largely consequentialistic in our reasoning (although admittedly we are not 100% consequentialistic). Thought experiment: suppose [[https://wiki.lesswrong.com/wiki/Omega][Omega]] swooped down and told you that it was keeping a running tally of the number of car accidents that had ever occurred in history, and that once this number reaches a certain value--say, 100 million--it would destroy the Earth. Would you still be against self-driving cars? I won't go so far as to claim that /everyone/ would be for self-driving cars in this case, but I'd be pretty confident that the /vast majority/ would. It's in this sense that I view consequentialistic reasoning as more "psychologically realistic" than deontological reasoning.

This is also, I think, what [[/u/FeepingCreature]] is arguing. He's not claiming that rationality implies transhumanism directly, but rather that rationality plus /humanism/ in turn leads to transhumanism (plus the unspoken empirical assumption that humanism is a fairly common philosophy). This is, so far as I can tell, not a sentiment you disagree with (the only plausible point of disagreement I can think of is the empirical assumption itself), so I'm not sure why you seem to think the two of you are in disagreement about something.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 7
:DateUnix: 1493954060.0
:END:

******** Plus, you know, we could just require a very high standard of training (and alertness, etc) for human drivers on public roads, and encourage more fun driving on private tracks.
:PROPERTIES:
:Author: PeridexisErrant
:Score: 3
:DateUnix: 1493988974.0
:END:


**** #+begin_quote
  rationalists just happen to usually be transhumanist because transhumanism is right.
#+end_quote

Lol. Right, like there aren't any hidden biases or confounding variables between rationalism and transhumanism. It's not like Eliezer Yudkowsky, the person whose fictional story led, albeit indirectly, to the creation of this subreddit, is an avowed transhumanist or anything. No, you figured it out from first principles, because being a rationalist makes you infallible, and you are a transhumanist, and therefore transhumanism is correct.

That's a good one. Tell me another.
:PROPERTIES:
:Author: abcd_z
:Score: 1
:DateUnix: 1493966713.0
:END:

***** Let's keep this civil please.

Obviously I'm not saying I'm infallible. But you don't need to be infallible to be right about something that's blatantly obvious in retrospect which most people are totally wrong about. Case in point, most people are religious and think that faith is a good thing. And just because I didn't figure this one out on my own doesn't mean that I'm just blindly agreeing with EY. Some things really are obvious once someone is thinking critically about them instead of blindly accepting common wisdom.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 4
:DateUnix: 1493972714.0
:END:

****** That's great, but you completely missed or avoided the central point of my argument. I'll rewrite the relevant portions, so there's no misunderstanding:

There are hidden biases and confounding variables between rationalism and transhumanism. Eliezer Yudkowsky, the person whose fictional story led, albeit indirectly, to the creation of this subreddit, is an avowed transhumanist. Trying to pretend otherwise ("transhumanists just happen to usually be rationalists") is factually incorrect.

Additionally, I'm rather concerned that you're taking an opinion and treating it as fact. "Transhumanism is right" is exactly as defensible as "feminism is right" or "social democracy is right", or "patriotism is right".

On top of that, you're basically using majority consensus to prop up your argument, with a dash of appeal to authority. "A lot of smart people believe it, therefore it must be true."

Between this and the heavy [[/r/rational/comments/5as909/spoilers_doctor_strange_is_the_antirational_movie/d9jc7we/][hero-worship of EY]], I'm quite disappointed with this community of so-called rational thinkers.
:PROPERTIES:
:Author: abcd_z
:Score: 5
:DateUnix: 1493975165.0
:END:

******* When a bunch of smart people believe something that doesn't automatically make it true but it does make it more likely.

Also there are some facts that are so completely obvious that people think they need more justification than is actually necessary because they've deceived themselves into believing something else, or they've been indoctrinated into it. If almost everyone you'd ever known always told you the sky is made of rainbow sprinkles ever since you were a child you'd probably believe it even if it was really obvious that it wasn't if you just looked. Likewise, if everyone you'd ever known always told you that death was a good thing and it was inherently morally wrong not to die at some point and that letting people live as long as they want to if possible was evil ever since you were a child, then you'd probably believe that too.

Just because EY was the first person I observed pointing out how silly those claims are doesn't mean that I only agree with him because he says it.

What specific biases do you think are being exhibited in rationalism and transhumanism, and how?
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 5
:DateUnix: 1493977926.0
:END:

******** So I think the issue under contention is that there are a lot more possible explanations for a correlation between tranhumanism (philosophy encouraging directed evolution) and rationalism (set of heuristics which are slower but more rigorous than our evolution-granted heuristics) than just "rationalist techniques encourage transhumanist beliefs". For example:\\
- transhumanist beliefs encourage use of rationalist techniques\\
- transhumanists who don't use rationalist techniques don't participate in [[/r/rational][r/rational]]\\
- rationalists who don't share transhumanist beliefs don't participate in [[/r/rational][r/rational]]\\
- participation in [[/r/rational][r/rational]] encourages use of rationalist techniques and transhumanist beliefs

Given my light participation in this subreddit, I am in no place to make meaningful observations about which if any of the above possible factors are actually in play.
:PROPERTIES:
:Author: Esryok
:Score: 6
:DateUnix: 1494020258.0
:END:

********* This! Thank you, yes! This is what I was initially trying to get at, and you said it much better than I could.
:PROPERTIES:
:Author: abcd_z
:Score: 0
:DateUnix: 1494029166.0
:END:


******** And you're still not hearing me.
:PROPERTIES:
:Author: abcd_z
:Score: -1
:DateUnix: 1493978465.0
:END:

********* Sorry accidentally pressed post before I was done talking. Mind refreshing the page and then responding to what I specifically said?
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 1
:DateUnix: 1493978540.0
:END:

********** And you're still not hearing me.
:PROPERTIES:
:Author: abcd_z
:Score: 0
:DateUnix: 1493979378.0
:END:

*********** What specifically did you say that I didn't hear, and how do you know that I didn't hear it?
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 2
:DateUnix: 1493990864.0
:END:


*********** In some sense you can say that transhumanism is "true" if you are talking about people that have the same preferences , if two persons have the same values (and most people do have similar values )one can be wrong about how much a future state of the universe rates in their "utility function" (yes human preferences aren't consistent whit any utility function but you know what I mean) especially because of things like cognitive dissonance , some people maybe have different values (but at least things like the death is bad part of transhumanism seem to be based in values that are common in most humans , otherwise they wouldn't invent religion ) and things like transhumanism aren't the kind of things people normally use the word true for, so what Sailor Vulcan said does sound kind of weird( but I'm not sure if I understand or agree whith the distintion you are making between "facts" and "opinion") , and yes its true that the argument that something is more likely to be true because one group of people believes in it its only valid if you think that group of people is less biased than the other side of the discussion, and i think that its reasonable that you don't think that's the case with rationalists and transhumanism. But I do have a problem with you apparently judging a whole community based on (apparently) almost no evidence. Also saying "And you're still not hearing me" is clearly not helpful if you think someone isn't understanding something about your argument you tell them what you think they aren't understanding.
:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1494018957.0
:END:


*** This is something I find myself saying rather often these days, but:

You should consider the possibility that your opinion of a particular work is not, in fact, the be-all and end-all in terms of quality judgment.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 6
:DateUnix: 1493907991.0
:END:

**** Really earning your flair, there.
:PROPERTIES:
:Author: abcd_z
:Score: 0
:DateUnix: 1493966482.0
:END:

***** I'm afraid I haven't the slightest idea of what you're talking about. (Or rather, I do, but I'm not in the mood to let you insinuate things.) If you're going to attempt to insult me, the least you could do is be explicit about whatever insult you're trying to convey.
:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 5
:DateUnix: 1494001351.0
:END:


*** It's more that the Less Wrong descended rationalist community that HPMOR comes from and this sub was founded by is than anything too intrinsic about people who try to think rationally.
:PROPERTIES:
:Author: psychothumbs
:Score: 10
:DateUnix: 1493904973.0
:END:


*** What specifically annoyed you about the way he tried to communicate his ideas? What are some of the many other flaws?

I haven't heard many criticisms of the books, so I'd really appreciate hearing from someone with a different view. After all, it'd be really weird if it /didn't/ have flaws, and I'm probably blinded by nostalgia.
:PROPERTIES:
:Author: rochea
:Score: 1
:DateUnix: 1493960159.0
:END:
