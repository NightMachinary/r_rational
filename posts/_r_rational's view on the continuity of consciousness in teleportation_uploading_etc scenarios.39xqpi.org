#+TITLE: /r/rational's view on the continuity of consciousness in teleportation/uploading/etc scenarios

* /r/rational's view on the continuity of consciousness in teleportation/uploading/etc scenarios
:PROPERTIES:
:Author: RMcD94
:Score: 27
:DateUnix: 1434389757.0
:DateShort: 2015-Jun-15
:END:
After seeing the popular discussion in [[/r/AskReddit]] around this [[http://www.reddit.com/r/AskReddit/comments/39wkcu/what_scientific_breakthrough_would_be_the_most/cs73gds][comment]] and seeing what seems to be a fairly large consensus against sacrificing their consciousness I wonder what the opinion of [[/r/rational]] subscribers who are I feel far more likely to be pro transhumanism (which usually involves supplanting consciousness) is on this matter, I couldn't find any thread discussing this already and so:

The question:

Bill Gates perfects teleportation and releases a world wide transportation system, the way it functions is by replicating the exact atomic (or quantum whatever you want) configuration inside the teleporter and replicating it elsewhere. As part of this process the things inside the teleporter are broken into a subatomic mush (which then reforms randomly or whatever).

Do you use this teleporter?

What if they scanned your brain onto a computer? What do you do if you're the consciousness left behind?


** * /Pick your poison!/
  :PROPERTIES:
  :CUSTOM_ID: pick-your-poison
  :END:
In the philosophy corner ... [[http://existentialcomics.com/comic/1][*EXISTENTIAL COMICSS* with a short but explanatory cartoooon!!!]]

In the science corner ... [[http://lesswrong.com/lw/r9/quantum_mechanics_and_personal_identity/][*EEEEEEEELIEZER YUDKOWSKY* with a super-long quantum essay series!!!!!]]

In the fantasy corner ... [[http://tardis.wikia.com/wiki/Souffl%C3%A9][*STEVEN MOFFATTTT* with a shitty, unjustified, one-sentence explantion!!!!!!!]]

No matter which route you prefer, *the answer is clear*: personal identity and continuity of consciousness are evolutionarily-helpful features of the map that have no concrete basis in the territory! A soufflé isn't a soufflé; a soufflé is a recipe - you/!cryoniced/, you/!teleported/, you/!uploaded/, you/!positive-spin-branch/, and you/!negative-spin-branch/ are all just as *"you"* as you/!tomorrow-morning/ or you/!five-minutes-ago/! No death involved!

*/!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!/*
:PROPERTIES:
:Score: 61
:DateUnix: 1434392570.0
:DateShort: 2015-Jun-15
:END:

*** [[http://www.smbc-comics.com/?id=3262]]
:PROPERTIES:
:Author: Jules-LT
:Score: 6
:DateUnix: 1434458723.0
:DateShort: 2015-Jun-16
:END:


*** u/deleted:
#+begin_quote
  No matter which route you prefer, the answer is clear: personal identity and continuity of consciousness are evolutionarily-helpful features of the map that have no concrete basis in the territory! A soufflé isn't a soufflé; a soufflé is a recipe - you!cryoniced, you!teleported, you!uploaded, you!positive-spin-branch, and you!negative-spin-branch are all just as "you" as you!tomorrow-morning or you!five-minutes-ago! No death involved!
#+end_quote

Oh no. It gets worse. When you get right down to it, the particular recipe for personal identity that /matters/ is a matter of your own /preferences/. It's really all about which potential future versions of me I /want/ to be, which is actually /more/ interesting and disturbing: the highest-value potential future me's might be quite substantially different from present!me.

But there's also the psychology corner: personal identity = moral character/alignment, which is where our intuitive notion of identity actually seems to cash out in experiments.

Anyway, you win the contest! Pick your +poison+ prize!
:PROPERTIES:
:Score: 4
:DateUnix: 1434466589.0
:DateShort: 2015-Jun-16
:END:


*** Can I interdict and say that exactly because consciousness and continuity are materialistic concepts, it should be possible to extend consciousness to your teleportation partner?
:PROPERTIES:
:Author: goocy
:Score: 3
:DateUnix: 1434468602.0
:DateShort: 2015-Jun-16
:END:


*** But be careful: you!positive-spin-branch is not you!negative-spin-branch, they merely [[https://youtu.be/dYAoiLhOuao?t=6m18s][share a section of their past]]
:PROPERTIES:
:Author: Jules-LT
:Score: 1
:DateUnix: 1442999986.0
:DateShort: 2015-Sep-23
:END:


** Yeah, I think the pro-transhuman crowd will go for "the teleport clone is still the same person". It's almost a prerequisite for talking about immortality or enhanced intelligence or human life outside the narrow range of temperatures and pressures we evolved in. Or any other cool sci-fi ideas like that. (For comparison's sake, you should ask this question on [[/r/scifi]] and see how they like the idea.)

Uploads open a /massive/ can of worms that AIs or teleportation individually do not. For example, small errors in the upload's brain chemistry - due to imperfections in the simulation - might change a person's mind enough that I would no longer consider them the same person.

Copies are cool. I think I could get on quite well with my doppelgänger. We'd consider it rude to ask which of us is the real one.
:PROPERTIES:
:Author: Chronophilia
:Score: 23
:DateUnix: 1434391958.0
:DateShort: 2015-Jun-15
:END:

*** Someone who professes to believe the clone would be the same person only because otherwise their dreams of immortality could never come true would be doing it wrong. It should just add another hard problem to be solved onto the pre-existing problem of death.
:PROPERTIES:
:Author: Gurkenglas
:Score: 7
:DateUnix: 1434455464.0
:DateShort: 2015-Jun-16
:END:


*** u/deleted:
#+begin_quote
  For example, small errors in the upload's brain chemistry - due to imperfections in the simulation - might change a person's mind enough that I would no longer consider them the same person.
#+end_quote

Aging, addiction, and food consumption also have measurable effects on brain chemistry. Do you think, for instance, that 10-year-old [[/u/Chronophilia]] is the same person as today's [[/u/Chronophilia]]? What about [[/u/Chronophilia]] before and after eating a chocolate bar?
:PROPERTIES:
:Score: 9
:DateUnix: 1434393138.0
:DateShort: 2015-Jun-15
:END:

**** I think that "same person" tries to condense a real number into a boolean value. It glosses over a spectrum in search of a yes or a no. I'm 99.999% the same as I was yesterday. I'd be nearly as happy with my present self being replaced by that self as I would if I survived. I'm so different from myself twenty years ago that I would be indifferent as to whether I died or simply regressed twenty years.

Similarly, there are potentially versions of uploading that would result in a me that is similar enough to me today that I would prefer uploading now rather than risking death. There are potentially other versions where I'd rather wait and see if they improve them a bit.

But the means of change are important, too. I am generally happier with changes, even ones that aren't an improvement, that are a result of my experiences.
:PROPERTIES:
:Score: 15
:DateUnix: 1434399193.0
:DateShort: 2015-Jun-16
:END:


**** Sorry, I misspoke. I don't mind becoming a different person, I'd just like to become a /better/ person. Since I was 10, I've learned skills and gained experiences which I wouldn't take that back if I had the option.

Also I've been through adolescence with all the brain chemistry changes that implies, but that's a normal and well-understood part of life. When I was 10 I /wanted/ to become an adult, even if I didn't really understand what it meant. I'd seen other adults and I wanted to do the things they did.

So, there are some changes I would make to my brain, and some I wouldn't. I don't ever drink myself into unconsciousness. I don't want to be hit in the head or otherwise brain-damaged. If there's a way to prevent Alzheimer's disease I'll take it. And uploading would be a much more faster and more complete change than any of those. Maybe after the long-term consequences had been studied in real-world situations for a long time, after I'd met other people before and after uploading and knew what changes to expect...

... I'm rambling, of course we know this is all science fiction and I'll never have to make the choice anyway.
:PROPERTIES:
:Author: Chronophilia
:Score: 7
:DateUnix: 1434394943.0
:DateShort: 2015-Jun-15
:END:


**** It's a good point that due to the fundamental nature of humanity having always existed on a more, shall we say, visceral medium that hundreds of thousands of small changes happen constantly. From that point of the view the line becomes quite difficult to distinguish when you compare you uploaded and you unuploaded. After a radical event in your life you might compare that to the death of your previous self far more than a small change in the uploads behaviour.
:PROPERTIES:
:Author: RMcD94
:Score: 0
:DateUnix: 1434393994.0
:DateShort: 2015-Jun-15
:END:


*** u/TimeLoopedPowerGamer:
#+begin_quote
  Yeah, I think the pro-transhuman crowd will go for "the teleport clone is still the same person".
#+end_quote

#notalltranshumanists
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 7
:DateUnix: 1434419493.0
:DateShort: 2015-Jun-16
:END:


*** I'm a transhumanist and i don't go for that concept.

My explanation is fairly easy and i think the logic is sound :

Your consciousness is not only the cells and atoms in your brain : it is the continuous biological and electrical processes that happen. They never stop , even when you sleep there's no interruption in your brain firing.

In my humble opinion if you stop the processes and then re-start them somewhere else *they have not the same unbroken continuum* , there's someone else that is complety convinced to be you because the brains are identical.

And anyway since i really really really want to become immortal i would rather avoid any kind of potential risk.

Anyway i don't understand why such concepts are equated with sleep.. Sleep is a loss of awareness but there's not any kind of disruption of the normal processes.
:PROPERTIES:
:Author: Zeikos
:Score: 6
:DateUnix: 1434401286.0
:DateShort: 2015-Jun-16
:END:

**** It's usually associated with sleep because sleep is a normal part of most people's lives, and because it's actually a very good example.

Your sense of self is based around physical continuity? That's strange. Continuity can behave very counter-intuitively in extreme situations - many functions that can seem continuous are not, and vice-versa.

Suppose I replaced each of your neurons with a synthetic one that acts identically. One by one, over the course of hours or days, until your brain is entirely synthetic. That would be a continuous change, wouldn't it? But doing the same thing instantaneously is discontinuous. Now, what if the process takes a nanosecond? That's continuous again. But surely lightspeed delays make it impossible for me to scan and copy your entire brain in less than a nanosecond? How do you know a discontinuity is even something that can physically exist?
:PROPERTIES:
:Author: Chronophilia
:Score: 9
:DateUnix: 1434403278.0
:DateShort: 2015-Jun-16
:END:

***** First, you suggest serial, single neuron brain damage plus functional repair and replacement on brain-functional time scales. Remember, in this model new takes over old by continuing to function in the same way, not by simply having the same state as when replacement occurred.

The way this one goes is, it can't be death of personal identity because that level of die-off happens all the time. The brain naturally reroutes and uses redundancies and broad activation networks to fix this issue all the time. Repeating the process shouldn't be any different.

Damage is done. Very minor. It is instantly repaired, faster than any cognitive process could notice. Faster than the brain naturally functions. Matter and energy from the original is replaced. Maybe it is perfect, continuing one electrical beat to the next, with chemical decays already mapped out. Maybe a single signal state is lost. Maybe a dozen. That's fine, as far as it goes. We know small losses and activation issues happen all the time. For consciousness, personal identity in this sense, to mean anything at all, death of self can't be constantly occurring at this scale. That's logical.

Then you're saying, why not take all that brain damage really fast, with no time to electrochemically integrate old with new over time. Replace it all perfectly, but don't give the old time to send a single signal state to the new. That's where the problem with the model lies.

*The replacements are still brain damage.*

There is no time for integration of new parts into the original neural network. There is *damage* and then *repair* in the "slow replacement" model. That's not happening in the faster model. It can't happen in the fast model. It happens faster than the brain works.

That isn't just an emotional factor, which is what that flawed argument is attempting to ferret out. Yes, I've seen this one before, and it doesn't hold up. You are reducing the personal identity to a single moment in time, not a continuing process. You are removing matter and energy and replacing it and asking why it isn't the same, without thinking about what that means for the system.

Yes, the new brain would function the same afterward in the fast model, but it wouldn't be the same person because it did happen so fast. You'd have killed the original process-substrate and replaced it with another, not done a sub-natural-brain-damage level replacement over time.

That's the point of continuous process being identity. If you replace the brain faster than the brain functions, by necessity you'll be doing so with new chemical states (simulated or otherwise) and electrical charges from outside the original brain structure in addition to new matter.

If that is all we are, matter and energy, replacing both all at once faster than natural repair, rerouting, and signaling happens already means, by physical necessity, the death of the original structure. There was no processing being continued from old matter and energy to new. You're just pumping the old blood around a clone brain, with no continuing physical process in common with the original.

Just because I don't have a rate at which neuronal replacement would be acceptable and not murdering someone doesn't mean it can't be worked out. I'd suggest starting at something less than levels similar to brain damage which causes measurable cognitive malfunctions, but faster than background loss. That will only slightly kill someone over and over again, and very slowly replace the damage with functional bits that will be used by the much larger originals as a continuing process.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 7
:DateUnix: 1434412208.0
:DateShort: 2015-Jun-16
:END:

****** I have two problems with this argument (I think).

- First the idea of identity relying on continuum is worrying to me. What if in 50 or 100 years we discover that time is discrete? Does this disprove consciousness (or continuity of consciousness)?

- Second is related to the first, but continuity is a very fragile thing. How can you know that super-powerful alien beings aren't stopping all electrochemical activity in your brain every time you sleep? Or even just once? You (presumably) feel like the same person you were yesterday but how can you ever know? I prefer to base my decisions on what is "me" on whether it feels like me both to itself and to others.

Edit: discreet/discrete /stupid homophones/
:PROPERTIES:
:Author: duffmancd
:Score: 5
:DateUnix: 1434442189.0
:DateShort: 2015-Jun-16
:END:

******* u/deleted:
#+begin_quote
  What if in 50 or 100 years we discover that time is discreet?
#+end_quote

Well I would hope so, since I don't want her and Wen the Eternally Surprised turning exhibitionist.
:PROPERTIES:
:Score: 7
:DateUnix: 1434472579.0
:DateShort: 2015-Jun-16
:END:


******* u/Jules-LT:
#+begin_quote
  Does this disprove consciousness
#+end_quote

Only a specific concept of it. Even if all I experience is false, there is an "I" to experience it: I know of no surer thing.

#+begin_quote
  or continuity of consciousness?
#+end_quote

If time is discrete, doesn't it disprove continuity in general?
:PROPERTIES:
:Author: Jules-LT
:Score: 2
:DateUnix: 1434457769.0
:DateShort: 2015-Jun-16
:END:


******* u/TimeLoopedPowerGamer:
#+begin_quote
  What if in 50 or 100 years we discover that time is discreet?
#+end_quote

The point isn't that time has some special property. The physical matter that makes up the brain and originally ran the copied state used on the clone has no causal brain relationship with the new brain matter. Anything that could be self is in the human brain.

The new clone brain isn't the original brain, even if it is loaded with a software configuration that is exactly the same. It is a new person. If you killed the original brain, you killed that person because they were their brain. Very straight forward.

Neurons change and die, but they were still part of a functioning system, closely related and part of a continuing process. Taking a snapshot of that informational structure and then claiming a clone programmed with it is the same person is just stupid, and ignores how physical processes work. It is part of a seductive tech-dream lie people are telling themselves, one carefully designed to make uploading seem easier and more approachable to better stave away the terrors of death. Ironic (in the original sense) that this meme could very well result in such "thinkers" killing themselves, giving the world a much more efficient copy of their mind.

In short, if the original brain is destroyed as part of some one-to-one copying, you've only copied information to make a murder-clone. It's that simple. Calling the information divorced from the physical substrate it was running on the person and ignoring the destruction of a thinking, living physical being is intentionally misunderstanding the complex process that is the human brain. Or blindly ignoring the rights of individual intelligent beings in favor of some optimized informational strategy. Either way, really fucking evil.

--------------

#+begin_quote
  How can you know that super-powerful alien beings aren't stopping all electrochemical activity in your brain every time you sleep?
#+end_quote

Occam's razor. No proof, and reality can be explained just as well without that. The rest is all nihilistic Psych 101 nonsense that makes the very discussion moot. And is totally unproven, untestable, and unuseful in the real world.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434447004.0
:DateShort: 2015-Jun-16
:END:

******** Interesting. I'd consider myself the process being performed by my brain, rather than the brain itself. The software, not the hardware. And software can be forked, downloaded, backed up, etc. without any problem.
:PROPERTIES:
:Author: Chronophilia
:Score: 3
:DateUnix: 1434457029.0
:DateShort: 2015-Jun-16
:END:

********* u/TimeLoopedPowerGamer:
#+begin_quote
  Interesting. I'd consider myself the process being performed by my brain, rather than the brain itself.
#+end_quote

Then you have a lot of proving to do. The current scientific model is that the matter thinking of itself is your brain, not the pattern that matter makes. Anything flying in the face of that holds the burden of proof.

You're concluding that you must be /just/ the software, the information, seemingly because otherwise you can't get what you want. Which is deeply irrational reasoning.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434494767.0
:DateShort: 2015-Jun-17
:END:

********** u/Chronophilia:
#+begin_quote
  Then you have a lot of proving to do. The current scientific model is that the matter thinking of itself is your brain, not the pattern that matter makes.
#+end_quote

And what "scientific model" is that?

Of course my brain is made of matter, there's nothing else it could be. But my brain doesn't think of itself. When I introspect, I don't think "Hmm, neuron #65336 is a little slow today" or "I don't like these atoms I'm made of, I should get some new ones". I think "I'm feeling sad lately" or "I'm not thinking enough about my friends". My /thoughts/ are self-referential and self-aware. My brain is not, except where it's a substrate for my thoughts.

Sleep interrupts your conscious awareness, but you don't consider that to be death. The atoms of your brain are replaced every few weeks, but you don't consider that to be a slow death. How convenient for you that evolution should have created a form of life that naturally dies only once! Does it provide some sort of reproductive advantage?

Why exactly is being killed-and-recreated a bad thing, anyway? Death is bad because of the loss of memories, because it makes your loved ones sad, because you might have benefited society if you were still alive, and other reasons like that... but if there's a near-identical copy of you, then none of those reasons apply any more. We are always being replaced by our future selves. If that's death, I've died (12 hours)/(Planck Time) times so far today.
:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1434539284.0
:DateShort: 2015-Jun-17
:END:

*********** Death semantics plus reductionist nihilism both unsupported by neuroscience. And some crypto-deathist stuff thrown in for good measure. Boring. Done now.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: -2
:DateUnix: 1434580600.0
:DateShort: 2015-Jun-18
:END:


********** u/Jules-LT:
#+begin_quote
  the matter thinking of itself is your brain, not the pattern that matter makes
#+end_quote

So... the hardware that executes the software is the hardware, not the software?\\
Agreed, but the question is whether your identity lies in the hardware or the software
:PROPERTIES:
:Author: Jules-LT
:Score: 1
:DateUnix: 1434552504.0
:DateShort: 2015-Jun-17
:END:

*********** There is no software that is unexpressed in hardware states. Thinking otherwise is magical thinking at worst, and misapplication of complex math models at best. It is the process of your brain as a whole system that results in a sense of self. That is why thinking pure informational context equal to the original hardware system in identity is ignorant.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434580901.0
:DateShort: 2015-Jun-18
:END:

************ Do you think that simulating a brain is not just very hard, but actually physically impossible?
:PROPERTIES:
:Author: Jules-LT
:Score: 2
:DateUnix: 1434610095.0
:DateShort: 2015-Jun-18
:END:

************* Glad you asked. People are wasting time arguing for software minds being the same operationally as the meaty originals instead of the issues actually I have with OP's suicide-clone teleporters. This confuses the issue, when it is really straight forward.

In fact, I'm the *strongest* strong AI supporter you're ever likely to meet. I think software brain simulation, uploading of human minds, human-level AI, IA-to-uploading, non-human superintelligent "Virtual" Intelligences (a dumb term), and all the ways in which self-aware and self-reflective thought can occur and be represented in software are not just scientifically possible, but likely in the relatively near-term.

There is nothing /unique/ about the human brain that makes it non-simulatable or unapproachable by science. Worst case, use chopped up brain matter to make a wetware computer add-on to run your software. But I don't think even that is necessary. The process of human cognition is likely much, much more redundant and parallel than a computer program would need to be.

There is, however, something unique about the continuing process of a specific human brain in operation. It is self-aware and has, like other physical objects, unique identity. If you simply outright destroy it, it doesn't matter if you have a copy for other people (who aren't now dead) to enjoy. You've killed someone.

I don't want to die simply to get a copy of what I /was/ uploaded or cloned, and lots of my peers are seeing that and then thinking with their unsupported and illogical ideologies, not their rational skills. It's frustrating.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434614211.0
:DateShort: 2015-Jun-18
:END:

************** Thanks for the clarifications.\\
Speaking of software in general, now: if you pause a program, copy the state, then continue execution on other hardware. Did you destroy it?
:PROPERTIES:
:Author: Jules-LT
:Score: 1
:DateUnix: 1434619373.0
:DateShort: 2015-Jun-18
:END:

*************** No, but I've already said that the human mind is a brain. Subjective reality is contained entirely within that physical process. Saying otherwise is positing a second nature, an informational soul. The copied program is simply not the same instance of even the software.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434662515.0
:DateShort: 2015-Jun-19
:END:

**************** What about if a person is split in two, then before anything has time to stop functioning, each half is completed by a copy atom for atom of the other half? (like cellular mitosis)
:PROPERTIES:
:Author: Jules-LT
:Score: 1
:DateUnix: 1434837325.0
:DateShort: 2015-Jun-21
:END:

***************** Then each half of the original brain has suffered instantaneous 50% brain damage then had it repaired with new matter and energy. They have almost certainly died and been replaced by a clone twice over, as humans don't actually survive losing literally half their entire brain through all structures.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434837796.0
:DateShort: 2015-Jun-21
:END:

****************** u/Jules-LT:
#+begin_quote
  They have almost certainly died
#+end_quote

What would be your criterion to decide that? Picture a full-body mitosis.
:PROPERTIES:
:Author: Jules-LT
:Score: 1
:DateUnix: 1434924384.0
:DateShort: 2015-Jun-22
:END:


************** Imagine the world is a simulation with discrete units of time. If for one of these instants you blink away from existence then come back.\\
Technically, you could say you did die. But what's the harm?

I tend to think that the potential for uploading and duplication makes our previous views on death mostly obsolete...
:PROPERTIES:
:Author: Jules-LT
:Score: 1
:DateUnix: 1434625866.0
:DateShort: 2015-Jun-18
:END:

*************** Then you are suggesting different levels of death. Prove destructive uploading is a lesser death. Show your work. Otherwise, admit to magical thinking.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434662678.0
:DateShort: 2015-Jun-19
:END:

**************** What do you mean, "prove"? It's not a matter of checking what reality is, it's a matter of deciding what labels we want to apply to what.\\
Based on a pretty precise idea of what reality could be, I'm saying that I feel that this is different enough from our previous concept of death that many of our ideas and feelings about death don't apply any more. I really don't see what one could "prove" to change the other's position.
:PROPERTIES:
:Author: Jules-LT
:Score: 1
:DateUnix: 1434837002.0
:DateShort: 2015-Jun-21
:END:

***************** u/TimeLoopedPowerGamer:
#+begin_quote
  What do you mean, "prove"?
#+end_quote

Cite some neuroscience on brain damage, information theory on complex systems changing over time, theoretical physics on matter state duplication, even a logical argument showing a likely scenario. Not a bunch of "I feel" and "what's the harm" when it comes to destroying a human brain. Otherwise, cute ideas lose out to known proven scientific models.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434838307.0
:DateShort: 2015-Jun-21
:END:

****************** There's nothing "proven" about whether individuality/personality reside in the brain instance or the brain pattern. It's not a physics or medical question, it's a definition/philosophical question.
:PROPERTIES:
:Author: Jules-LT
:Score: 1
:DateUnix: 1434923992.0
:DateShort: 2015-Jun-22
:END:


***** My sense of self is based also on /functional/ continuity. It can't be only purely physical you also have to take into consideration the electrical component . I want to avoid "zero moments" , i define them as moments in when there's a break in the continuum of brain activity.

About teleportation you have both breaks, the physical one is obvious the functional one is less apparent. I use this example to explain myself : think about a hypotetical computer that can keep all his cache loaded even when not powered. You do your thing, turn it down , copy all the things to another machine and boot it up. You as an external observer would see no difference whatsoever from the outside but continuity has been broken .

Now bringing this to human beings, we never break continuity. Sure we arent conscious all the time but from bith to death you have a system that is in a permanent state of flow. Any process that's disruptive breaks the continuity, destroying the consciousness.

I highly doubt we will ever have to worry about disruptive teleportation, even if it would be feasible by the point we are at that level there will be far better options.

My end comment is that we lack knowledge, the processes in the brain are still highly unknown so i can easily be wrong but with the knowledge at the moment at our disposal consciousness is an arising phenomena and by definition it's caused by many lower level phenomenas this implies that if you shut them down you shut consciousness down, and since the ammount of possible consciousnesses per given brain is infinite (yes it is an assumption, but not a far fetched one : "you" could not have ever existed, there could be someone completly identical to you in your stead ; the universe would objectively be the same) when you restart them you cannot be certain that you restart the old consciousnesses , sure the new one would genuinely believe to be it.

I may be paranoid, and i have no problems admitting that but isn't it the most rational thing to do? Preservation of subjectivity from a subjective point of view is the end goal of any kind of immortalism. I'm for substitutive uploading, no else. And by the way, i see it way more feasible than the destructive route.

Sorry for the huge rant but i wanted to be complete. I'm open to any criticism (i would be more than happy to be wrong)
:PROPERTIES:
:Author: Zeikos
:Score: 1
:DateUnix: 1434447751.0
:DateShort: 2015-Jun-16
:END:


***** u/deleted:
#+begin_quote
  How do you know a discontinuity is even something that can physically exist?
#+end_quote

Planck units.
:PROPERTIES:
:Score: 1
:DateUnix: 1434472506.0
:DateShort: 2015-Jun-16
:END:


*** u/Transfuturist:
#+begin_quote
  I think I could get on quite well with my doppelgänger.
#+end_quote

My heart's not of stone,

As I've frequently shone

When alone with my own little X.

And after we've dined,

I am sure we will find

Better incest then Oedipus Rex.

Why should such sex vex,

Or disturb or perplex,

Or induce a disparaging tone?

After all, don't you see,

Since we're both of us me,

When we're having sex, I'm alone.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1434553007.0
:DateShort: 2015-Jun-17
:END:

**** I said get on, not get it on.
:PROPERTIES:
:Author: Chronophilia
:Score: 1
:DateUnix: 1434555660.0
:DateShort: 2015-Jun-17
:END:

***** You say that like I wasn't talking about myself.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1434563600.0
:DateShort: 2015-Jun-17
:END:


*** u/RMcD94:
#+begin_quote
  Copies are cool. I think I could get on quite well with my doppelgänger. We'd consider it rude to ask which of us is the real one.
#+end_quote

It's a question that bothers me greatly because when discussing clones and the such people can't get it out of their head that they are the original. I tend to do something like "You go in a dark room and fall asleep, when you wake up there are two of you" so that they might finally realise that the clone is replica.

A runaway cloning machine would be a problem for a lot of people based on some of their attitudes towards a "lesser" clone whereas I'm with you on the boat that I hope to be committed to going so far as to die to save two+ of my clones.
:PROPERTIES:
:Author: RMcD94
:Score: 3
:DateUnix: 1434394185.0
:DateShort: 2015-Jun-15
:END:

**** [[http://smbc-comics.com/index.php?db=comics&id=1879#comic]]
:PROPERTIES:
:Author: blazinghand
:Score: 3
:DateUnix: 1434401363.0
:DateShort: 2015-Jun-16
:END:

***** Which of the two resulting cells of a mitosis is the original?
:PROPERTIES:
:Author: Jules-LT
:Score: 3
:DateUnix: 1434458306.0
:DateShort: 2015-Jun-16
:END:


**** u/noggin-scratcher:
#+begin_quote
  I hope to be committed to going so far as to die to save two+ of my clones.
#+end_quote

Nah, screw those guys - they wouldn't do the same thing for me.
:PROPERTIES:
:Author: noggin-scratcher
:Score: 1
:DateUnix: 1434407994.0
:DateShort: 2015-Jun-16
:END:


** The question isn't really "is the copy of you you". Yes, yes, it is you. The question is "did you die when you were disintegrated". The basic premise of copying people perfectly decouples those two questions.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 8
:DateUnix: 1434397232.0
:DateShort: 2015-Jun-16
:END:


** Yes and yes. I don't believe in continuity of consciousness. If you like the illusion of that, then go ahead and use anything that preserves the illusion.
:PROPERTIES:
:Author: DCarrier
:Score: 6
:DateUnix: 1434405607.0
:DateShort: 2015-Jun-16
:END:

*** u/deleted:
#+begin_quote
  Yes and yes. I don't believe in continuity of consciousness.
#+end_quote

So you think consciousness has something like a "frame rate"?
:PROPERTIES:
:Score: 1
:DateUnix: 1434474943.0
:DateShort: 2015-Jun-16
:END:

**** I think you're conscious at every moment in your life, but the fact that they're all lined up has no intrinsic importance.
:PROPERTIES:
:Author: DCarrier
:Score: 2
:DateUnix: 1434478509.0
:DateShort: 2015-Jun-16
:END:


** If it were perfect, then sure, I'd use it. However, you have to put a lot of trust in people if you're talking about the core of your being.

If someone intercepts the transmission, they have their own private copy of you. If someone makes a copy of your brain-state, they have their own private copy of you /and/ can probably make their own edits to it. That's the sort of stuff that I find truly horrifying; not the continuity of consciousness stuff, which (to me) isn't that distinct from sleeping and then waking up.
:PROPERTIES:
:Author: alexanderwales
:Score: 11
:DateUnix: 1434392188.0
:DateShort: 2015-Jun-15
:END:

*** I think the only goal in the future is that there would be no personal motivation, at the end of the day I can't see another way to stop such a thing as the power of personal computing grows if it ever reaches a point where people can simulate other people you've already lost that battle. Whether it ends up being you simulated or a random person someone is going to be under the control of someone else.
:PROPERTIES:
:Author: RMcD94
:Score: 3
:DateUnix: 1434394272.0
:DateShort: 2015-Jun-15
:END:

**** Yeah, but I care a lot more about me than I care about a random person.
:PROPERTIES:
:Author: alexanderwales
:Score: 1
:DateUnix: 1434395223.0
:DateShort: 2015-Jun-15
:END:

***** Well the idea is that once one person is simulated it's potentially only a matter of time and random chance before it ends up being you simulated without you having to do anything.

After all when we finally turn the human brain into data there will only be so many combinations of it, though I guess it's theoretically possible that those combinations would be functionally infinite in our universe.
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1434395538.0
:DateShort: 2015-Jun-15
:END:

****** The human brain has [[https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=number%20of%20neurons%20in%20the%20human%20brain][100 billion neurons]] and [[https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=number+of+neuronal+connections+in+the+brain][100 trillion connections]]. If we assume that neurons are interchangable, and that each neuron has 1000 connections, then ... there's some math to be done there, but I'm pretty sure the answer is that it's always going to be prohibitively expensive to just try to simulate a specific human brain by brute force.

If someone wants to know my opinion on custard, they're going to have to ask me, dammit.

Edit: The naive math is just 100,000,000,000^{1,000} which is the same as 1 × 10^{11000} but obviously there are going to be some brain configurations that aren't valid, and some that are so close as to be functionally identical. Still, there are only 1 x 10^{80} atoms in the universe, so ...
:PROPERTIES:
:Author: alexanderwales
:Score: 6
:DateUnix: 1434396183.0
:DateShort: 2015-Jun-15
:END:

******* Time-looped computing solves that issue for the most part, if we ever get stable wormholes that connect two different points in time. It's still a finite amount of computation, and a time-looped system that sends information back to itself would complete the calculation instantly from the perspective of an outsider. So it's not out of the realm of scifi.
:PROPERTIES:
:Author: Kuratius
:Score: 1
:DateUnix: 1434399198.0
:DateShort: 2015-Jun-16
:END:

******** Idk about you, but I feel like there are different levels of "scifi". And true time-travel is on the never-ever-going-to-happen end of the scale, since it violates causality.
:PROPERTIES:
:Author: SublimeMachine
:Score: 3
:DateUnix: 1434411747.0
:DateShort: 2015-Jun-16
:END:

********* There's no stable universelle where the time loop doesn't complete. Thus it ends uo giving you the result in all possible universes. It is compatible with the theory of relativity, and whether it violates causality is a matter of interpretation.
:PROPERTIES:
:Author: Kuratius
:Score: 1
:DateUnix: 1434432292.0
:DateShort: 2015-Jun-16
:END:


*** u/Transfuturist:
#+begin_quote
  If someone intercepts the transmission, they have their own private copy of you.
#+end_quote

>not using /secure/ self-transport transfer protocol

>2163
:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1434553135.0
:DateShort: 2015-Jun-17
:END:


** Conditional on the teleporter both encrypting the data, and transmitting it over hard wire only, yes.

I'm not at all interested in my quantum brain state being intercepted by third parties. If somebody has a copy of that, they have a copy of me, can rebuild me, can scan my brain for information, can put me in a simulated torture device for 9^{9^{9}} simulated years, anything, really, and I will have a continuous conscious experience of it.
:PROPERTIES:
:Author: trifith
:Score: 4
:DateUnix: 1434390948.0
:DateShort: 2015-Jun-15
:END:

*** While I understand your fear I can't contemplate any motivation that anyone could have ever to commit such an act. It would have to be a crazy run away AI singularity that really got messed up programming to do such a thing.
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1434394049.0
:DateShort: 2015-Jun-15
:END:

**** Maybe not simulated torture for that long, but there are all the standard motivations:

- Theft. You have a copy of a person, you can probably extract their accounts and passwords, and take all their money (as well as use their credit).
- Retribution. You were arguing with someone online, and they were a dick to you, well now you can get a physical revenge.
- Obsession. Emma Watson can be your sex slave. You can run your fan theories by Brandon Sanderson. You can get your copy of Stephen King to write a book for you that no one else will ever see. Sure, they might not like it, but they'll come around, eventually. Or else.
- Lulz. Whenever I think about a technology, I think about what 4chan would do with it. In this case, probably finding ever-more-creative ways to inflict humiliation/pain/etc. on people whose security has been breached.
:PROPERTIES:
:Author: alexanderwales
:Score: 10
:DateUnix: 1434394864.0
:DateShort: 2015-Jun-15
:END:

***** u/trifith:
#+begin_quote
  Obsession. Emma Watson can be your sex slave. You can run your fan theories by Brandon Sanderson. You can get your copy of Stephen King to write a book for you that no one else will ever see. Sure, they might not like it, but they'll come around, eventually. Or else.
#+end_quote

Story idea: Famous people such as these deliberately sell copies of themselves for these purposes.
:PROPERTIES:
:Author: trifith
:Score: 10
:DateUnix: 1434397088.0
:DateShort: 2015-Jun-16
:END:

****** I like that. As a riff on that, a fading star trying to recapture the limelight has her brainstate "leaked" onto the internet under advice of her manager, in the same way that celebrities have their sex tapes come out on "accident".

Edit: Which should not to be taken to imply that all or even most sex tapes are released with the consent of the parties involved.
:PROPERTIES:
:Author: alexanderwales
:Score: 9
:DateUnix: 1434397793.0
:DateShort: 2015-Jun-16
:END:

******* I can't quite put my finger on it but I think there was some anime with a star leaking a copy of herself into the internet or something similar to that concept..
:PROPERTIES:
:Author: IomKg
:Score: 2
:DateUnix: 1434443191.0
:DateShort: 2015-Jun-16
:END:


******* I agree that those leaks aren't accidents, but I doubt that it's according to the celebrities' intent most of the time. Rather, celebrities determine that they can't do anything to stop it, especially as their publicists insist that being angry about it won't help.

The victims here are invariably women. Women being angry are assumed to be overreacting and bitchy. One episode of that can ruin a person's career, if that person is a public figure. And there's this impression that giving any response is just pandering to trolls, bringing more attention to something you'd rather hide -- Streisand effect bait.

So a few people capitalize on it because it's that or abandon their public career, and most try to ignore it, and one or two speak out against it, typically to little effect.

Then people like you assume that, because some people have profited in the end from their partners deliberately distributing pornographic images of them without their consent, the majority of people in similar situations got their deliberately in order to increase their fame.

It reminds me of the parable in the [[http://lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/][Policy Debates Should Not Appear One-Sided]] article, just one step further removed. It's implausible that a person would deserve having images of her having sex in private distributed without her permission, but the victim must somehow have gotten what she deserved, so people come up with a story that she actually chose to send them around. And once that story makes the rounds, other people have it as a cached thought that they can apply to situations that look similar.
:PROPERTIES:
:Score: 0
:DateUnix: 1434406429.0
:DateShort: 2015-Jun-16
:END:

******** I think you're reading /way/ too much into what I said. I think it's hard to argue that /some/ celebrity sex tapes were released intentionally under the guise of being accidental, and it was the comparison to those specific instances that I think would be compelling for a story.
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1434407222.0
:DateShort: 2015-Jun-16
:END:

********* Communicate with care. Your statement was:

#+begin_quote
  celebrities have their sex tapes come out on "accident"
#+end_quote

Without any qualifiers, this is a generalization or a universal statement. If you don't want to add to the sexist misconception that all or the majority of "leaked" sex tapes are distributed on purpose by the celebrity, the /very/ least you should do is make it hypothetical or provide a qualifier.

That's not nearly enough to avoid promoting that misconception, but it does signal that you've spent half a second trying not to appear sexist.
:PROPERTIES:
:Score: -1
:DateUnix: 1434408000.0
:DateShort: 2015-Jun-16
:END:

********** Fair enough; to me, it's not a universal, because it's like saying, "in the same way that people get hit by cars 'on accident' as part of an insurance scam". That doesn't imply (to me) that /all/ people who get hit by cars are doing it as part of an insurance scam, or that this is always (or even mostly) how people get hit by cars.

If what I meant wasn't clear from context though, that's on me. I'll offer a clarifying edit.
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1434409320.0
:DateShort: 2015-Jun-16
:END:

*********** Your simile isn't quite as apt as it could be because there is no pervasive, systemic bias against people who get hit by cars.
:PROPERTIES:
:Score: 1
:DateUnix: 1434414812.0
:DateShort: 2015-Jun-16
:END:


****** Futurama, /I Dated a Robot/
:PROPERTIES:
:Score: 1
:DateUnix: 1434475085.0
:DateShort: 2015-Jun-16
:END:


****** LOL

Then the copy thinks: oh shit, I'm the copy. Let's make another copy of me to suffer instead of me!
:PROPERTIES:
:Score: 1
:DateUnix: 1434496845.0
:DateShort: 2015-Jun-17
:END:

******* Procrastination paradox in one.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1434553486.0
:DateShort: 2015-Jun-17
:END:


***** And this is why mind uploading is actually a really bad idea.
:PROPERTIES:
:Score: 2
:DateUnix: 1434475239.0
:DateShort: 2015-Jun-16
:END:


**** Obvious answer is a person who has economic or military secrets of high value, and a less than ethical opponent who wants those secrets.

There's also the possibility of alien intelligences with vastly different moral systems. Or simply a broken upload system.

I'm not very interested in taking chances.
:PROPERTIES:
:Author: trifith
:Score: 3
:DateUnix: 1434394742.0
:DateShort: 2015-Jun-15
:END:


**** NSA style spying - set up a universal teleport system and build back doors into the handshake in order to be able to instantiate a copy of any person who might be of interest to you for the purposes of +national+ intergalactic security.

(Idea blatantly stolen from the webcomic Shlock Mercenary)
:PROPERTIES:
:Author: nicholaslaux
:Score: 2
:DateUnix: 1434398871.0
:DateShort: 2015-Jun-16
:END:


** Instead of writing it up again, lemme [[http://www.reddit.com/r/AskScienceFiction/comments/39mmgi/star_trek_why_doesnt_anyone_use_transporter/cs555d9][just link my last post on the subject]].

I've talked about this on Reddit /a lot/.

#+begin_quote
  What do you do if you're the consciousness left behind?
#+end_quote

Upload party. Cyberself is invited, of course!

I'd consider taking some cyanide or something, but I'd probably stick around in case there's some systematic problem with the uploading process and they need to rescan me. But long-term, there's no reason to keep running on a platform that can't even fork.
:PROPERTIES:
:Author: FeepingCreature
:Score: 8
:DateUnix: 1434390791.0
:DateShort: 2015-Jun-15
:END:

*** It's just about sidebar-worthy at this point.

The debate is one I no longer enjoy, because it spirals back around to the same tired intuition pump every time.

"But... is it /REALLY/ *YOU*????? Not like... a COPY of you..."

Yeah. It is. Yawn.
:PROPERTIES:
:Author: gryfft
:Score: 9
:DateUnix: 1434399237.0
:DateShort: 2015-Jun-16
:END:


** My instinct is that it's fine - and, in practice, I predict that I and most other people would do it - but ...

Here's the thing. AFAICT, the right way to deal with being copied is to view /both/ copies as "really you"; if you're going to be copied, and one of the copies will be given a cookie, you should anticipate a 50% chance of receiving a cookie. (We can kinda-sorta empirically test this via the Sleeping Beauty problem.)

Which means ... if I'm going to be copied, and one of the copies will then be /killed/, shouldn't I anticipate a 50% chance of dying?

That's clearly true in the case of one copy being killed by Unnecessarily Painful Slow Death Rays. But is it true if they're killed instantly? Apparently, from a quantum mechanics standpoint, that question is incoherent (which seems intuitive to me; I don't worry that having the atoms in my body replaced over time will kill me, so I don't /seem/ to care about the atoms.)

If I've used this machine fifty times, and I know for a fact it kills me instantly in the process of scanning me, what should I anticipate going in for the 51st time?

I *remember* emerging seemlessly at the destination, but that would be /equally true/ if it killed me via Unnecessarily slow Painful Death Rays. So should I anticipate a 50% chance of everything going black, the same way I would anticipate a 50% chance of Unnecessarily Slow Painful Death if that was what would happen to me?

Again, I don't think this would stop me from doing it. But it would bother the heck out of me. Hell, it bothers me right now.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1434470160.0
:DateShort: 2015-Jun-16
:END:

*** u/deleted:
#+begin_quote
  if I'm going to be copied, and one of the copies will then be killed, shouldn't I anticipate a 50% chance of dying?
#+end_quote

No, you should anticipate a near 0% chance of dying, barring malfunction of the machine. Mario doesn't really /die/ when he falls off a cliff, he has extra lives to keep going. You should anticipate a 100% chance of losing an extra life. You should view this as a death-warp to a desired destination, with a 1up placed nearby to the source.

If the death isn't painless or instant though, that does change matters significantly.
:PROPERTIES:
:Score: 1
:DateUnix: 1435755503.0
:DateShort: 2015-Jul-01
:END:


** One condition: The subatomitizational-mushimizer must be instant and painless, or the iteration of myself being mushed must be otherwise rendered free of consciousness before being destroyed. I won't abide by any iterations of myself (or anyone else for that matter) suffering intense mortal pain from the simple use of public transport, no matter how much the next iteration of myself won't know about it.

That condition given? Sure.
:PROPERTIES:
:Author: drageuth2
:Score: 5
:DateUnix: 1434408656.0
:DateShort: 2015-Jun-16
:END:


** There is no question here, it has nothing to do with transhumanism. Identical atoms are identical.

And anyone who finds the idea of being made of different atoms scary should notice what is happening to them on a constant basis.
:PROPERTIES:
:Author: wendigo_days
:Score: 5
:DateUnix: 1434416633.0
:DateShort: 2015-Jun-16
:END:


** u/TimeLoopedPowerGamer:
#+begin_quote
  Do you use this teleporter?
#+end_quote

Do I become a first adopter of new, potentially very dangerous tech *products? Obviously not.

Do I /ever/ use it? Still an emphatic no.

It is logical, in a sense, for people not to care about the original versions of other people getting killed since they get a perfect copy to continue interacting with, but it is *very* irrational for me to not care about such things in relation to myself. And others in relation to their suicided originals, as well, but I have long given up on this level of rational thought from most people.

--------------

#+begin_quote
  What if they scanned your brain onto a computer?
#+end_quote

Non-destructively? Sure. Obviously that's a good thing.

Look under your chair. You get a me, and you get a me, and you get a me: everyone gets a me!

Otherwise, obviously not. That's still suicide.

--------------

#+begin_quote
  What do you do if you're the consciousness left behind?
#+end_quote

Assuming non-destructive uploading? Nod and do what my plan for the day was all along. I knew that was going to happen, because that's /all that can ever happen in that scenario/. Some really poorly thought-out body-horror sci-fi might tell fantastic stories to the contrary, but that doesn't affect how *reality* works.

I wouldn't ever become a teleport clone, because I am my brain *and* the history of regular electrochemical processes that lead it to its present state, and which will continue into the future for some time to come. Sure, someone will wake up at the destination, but they'll know what happened just as well as I do. I think a small, brief rejoicing of more me in the world is likely, on both ends, but it's not some big existential crisis. Looking at it any other way is some wishy-washy, soft sci-fi magical thinking.

The continuity of processing substrate plus the continuously updating and modifying programs in an operational brain is all any fundamental identity /can/ be. You are the past-present-future program /and/ the past-present-future physical substrate it is running on. Everything else is, at best, pseudo-religious technobabble and self-comforting mechanisms which can and should be shaved off with Occam's razor.

Don't unnecessicarily multiply entities, and certainly don't kill the original just because your misapplied pseudo-rational, neo-altruistic reasoning results in dumb ideas about personal identity.

Some /carefully/ designed forms of additive hardware uploading--where the original brain substrate becomes a less and less important percentage of total processing potential--are the only safe ways to upload.

Anything else is the gift of a brand new person to the future, possibly with a sad little suicide thrown in.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 5
:DateUnix: 1434408185.0
:DateShort: 2015-Jun-16
:END:

*** u/philip1201:
#+begin_quote
  What if they scanned your brain onto a computer?

  Non-destructively? Sure. Obviously that's a good thing.

  Otherwise, obviously not. That's still suicide.
#+end_quote

Sure, but what's so terrible about suicide in that case?

Suppose you're going to die in 60 years if you don't get brain-uploaded now, and your brain-uploaded version gets to live 60 trillion years if you do. Even if suicide is somewhat bad, you might still choose to sacrifice yourself so that your brain-uploaded version may live.

And if you accept that, consider that your body contains a massive amount of negentropy which can power your simulated self for simulated millennia. If entropy can't be reversed, you would have the choice to keep your old self alive, or to sacrifice it in order to extend your brain-uploaded version's life by thousands of years per year lost to suicide.
:PROPERTIES:
:Author: philip1201
:Score: 1
:DateUnix: 1434409334.0
:DateShort: 2015-Jun-16
:END:

**** u/TimeLoopedPowerGamer:
#+begin_quote
  Even if suicide is somewhat bad, you might still choose to sacrifice yourself so that your brain-uploaded version may live.
#+end_quote

This is the techno-altruism trap. Accept it and the value of the individual to society becomes zero. Anything can be justified by saying the new task schedule and resource allotment will benefit from the sacrifice of the old order. It is the path to one of the darkest possible future digital hells.

If your answer to the question of what action is optimal returns *suicide*, you aren't rational. Despite whatever dark little thought games internet intellectual tough-guys play to prove how altruistic and sexable they are, reality has no exceptions. Death is the loss of everything someone is and has been. Anything else is a comforting lie people tell themselves, one that isn't supported by science and objective reality. Having another copy */at the cost of the original/ only matters to the most selfish and solipsistic.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 5
:DateUnix: 1434413280.0
:DateShort: 2015-Jun-16
:END:

***** u/philip1201:
#+begin_quote
  If your answer to the question of what action is optimal returns *suicide*, you aren't rational. Despite whatever dark little thought games internet intellectual tough-guys play to prove how altruistic and sexable they are, reality has no exceptions.
#+end_quote

*Put up or shut up.* I'll give you $100 if I can't find someone whose suicide (deliberate destruction of self) was rational within a week of you accepting, and you donate $10000 to MIRI in my name if I can. If no exceptions exist, there's no chance of you losing the $10000, is there?

As for why the stakes are that high: that's to make it so you actually care about being right.

#+begin_quote
  Accept it and the value of the individual to society becomes zero.
#+end_quote

I don't follow and you don't seem to explain.

#+begin_quote
  Anything can be justified by saying the new task schedule and resource allotment will benefit from the sacrifice of the old order.
#+end_quote

No: If the new task schedule is inferior, for example, we should simply not adopt it. Please be more exact.

#+begin_quote
  Death is the loss of everything someone is and has been.
#+end_quote

Except, in this case, literally everything they are and have been, since no atoms are lost (not that those are relevant) and no information is lost if a backup exists.
:PROPERTIES:
:Author: philip1201
:Score: 3
:DateUnix: 1434421838.0
:DateShort: 2015-Jun-16
:END:

****** u/TimeLoopedPowerGamer:
#+begin_quote
  I'll give you $100
#+end_quote

Fuck off. You're not even doing it right. I wouldn't drive an hour to spit on you for that much, especially not with that sort of weaselly risk profile on the other side. Certainly not with your apparent ability to think rationally.

Rationally debate or get the fuck out of the pool. I don't have time for your sophomoric internet dick-measuring wastes of time.

#+begin_quote
  I don't follow and you don't seem to explain.
#+end_quote

Read the other replies to me. I can't be arsed to link any further, given how you're treating this debate.

#+begin_quote
  Please be more exact.
#+end_quote

You and your ilk don't value continued existence of intelligent beings, and only value the functional output of their processes. Thus, you will murder anything if that results in exact copies of processes, even if the original process and substrate don't continue with a casually uninterrupted chain.

You monsters.

#+begin_quote
  since no atoms are lost (not that those are relevant)
#+end_quote

A person is not matter running a program which can be copied at will, even with a magically perfect process. *A person is the specific continuing collective process of the matter and the program running on it.* The same way rearranging every electrochemical connection in the brain kills a person, so does simply copying information (even perfectly) from them and then destroying their body. You've got a nice clone that does the same things, but you still killed a living, thinking person to do it.

They don't "wake up" in the clone, or some other trashy sci-fi nonsense. Not that you've said so, but that's pretty typical a delusion for futurists to have, even subconsciously.

But if all you care about is having something that does the same things, why should it matter? Self-preservation. Obviously. Because /you/ will die if you do that.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: -4
:DateUnix: 1434423564.0
:DateShort: 2015-Jun-16
:END:

******* u/philip1201:
#+begin_quote
  I wouldn't drive an hour to spit on you for that much,
#+end_quote

I'm fine with raising the stakes. How does $1,000 / $100,000 sound to you? If you don't have the money lying around, I'm fine if you pay at least $5,000 now and pay 4% interest rate over whatever you don't pay.

#+begin_quote
  especially not with that sort of weaselly risk profile on the other side
#+end_quote

Explain.

#+begin_quote
  Certainly not with your apparent ability to think rationally.
#+end_quote

The way you've phrased it, it almost seems like you're being sensible and acknowledging I'm capable of good. I'm pretty sure I'm misunderstanding you.

#+begin_quote
  I don't have time for your sophomoric internet dick-measuring wastes of time.
#+end_quote

But surely you have time for money? If you're confident, this would be the easiest $1,000 you ever make.

#+begin_quote
  You and your ilk don't value continued existence of intelligent beings, and only value the functional output of their processes.
#+end_quote

False. We value the content of the processes, not the output. (except to the extent that the content is a succession of infinitesimally separated outputs).

#+begin_quote
  A person is not matter running a program which can be copied at will, even with a magically perfect process. A person is the specific continuing collective process of the matter and the program running on it.
#+end_quote

Physics is a mathematical process of information exchange as well, which means matter can be simulated. If it is necessary to preserve the person - if it is necessary to have the program engage in the same fundamental behaviour - then the matter that makes up a person's brain would have to be simulated as well, and we would aim to do so.

#+begin_quote
  The same way rearranging every electrochemical connection in the brain kills a person, so does simply copying information (even perfectly) from them and then destroying their body.
#+end_quote

No. Rearranging the electrochemical connections changes the information-theoretical behaviour of the brain. Copying the brain perfectly means keeping the information-theoretical behaviour the same.

#+begin_quote
  but you still killed a living, thinking person to do it.
#+end_quote

I can't believe you're still upset about this. Surely you can understand that /if/ someone isn't dead after being killed, being killed isn't so bad?

#+begin_quote
  They don't "wake up" in the clone, or some other trashy sci-fi nonsense. Not that you've said so, but that's pretty typical a delusion for futurists to have, even subconsciously.
#+end_quote

Correct. In my way of viewing things, "me" isn't a conserved quantity: there may be several people in the future who all are equally me, or there may just be one. When I say I don't want to die, I mean that I want at least one of me to exist in the future. Teleportation creates one of me who is in the right location and kills one of me who is in the wrong location, so I'm okay with that trade. The wrong me does die, and for that reason I would want it to be quick and painless even if I don't remember it (though remembered pain does have priority). The decision to kill the wrong me is independent from the decision to create the right me, but if it's good for there to be several of me, it makes no sense to wait for them to be randomly created/not-killed in teleporters.

I don't want to die because I want to continue existing, because I think it is good for me to exist, and because it causes pain in me and those I care about. (1) is taken care of if there is another one of me elsewhere with all identity I care about preserving. (2) may be true or false depending on circumstances; but if true it's not a teleporter but a duplicator. (3) is also not true if there's another one of me.

I think one of the weird things about teleporters is that in most settings where they're used the answer to (2) is true: there really isn't a good reason to kill the originals. It's hard to imagine a universe so optimised, so /good/ that human beings are the morally best fuel source. At least to imagine something that would be that good; the consequences for physical humanity are a little more obvious.

#+begin_quote
  But if all you care about is having something that does the same things, why should it matter? Self-preservation. Obviously. Because you will die if you do that.
#+end_quote

I am not an extraphysical object with no physical method of detection or preservation, I am a process which is being "done". If I have something that "does" me exists, I exist.
:PROPERTIES:
:Author: philip1201
:Score: 3
:DateUnix: 1434429827.0
:DateShort: 2015-Jun-16
:END:

******** u/deleted:
#+begin_quote
  I'm fine with raising the stakes. How does $1,000 / $100,000 sound to you? If you don't have the money lying around, I'm fine if you pay at least $5,000 now and pay 4% interest rate over whatever you don't pay.
#+end_quote

I'm with [[/u/TimeLoopedPowerGamer]] on this constituting dick-waving. Neither of you is making a prediction that can be checked by a third party doing a physical experiment in the near-term, so there's no point trying to beef up your position by signaling a willingness to bet.
:PROPERTIES:
:Score: 1
:DateUnix: 1434476101.0
:DateShort: 2015-Jun-16
:END:

********* u/philip1201:
#+begin_quote
  Neither of you is making a prediction that can be checked by a third party doing a physical experiment in the near-term
#+end_quote

Words have meaning. Even if they aren't exact, they create testable expectations and "suicide is never rational" is a reasonably clear prediction which can be falsified by demonstrating that people tend to value other lives as more valuable than their own in [[https://en.wikipedia.org/wiki/Kin_selection#Hamilton.27s_rule][ev-psych theory]], [[https://en.wikipedia.org/wiki/John_R._Fox#Military_service][fact]], and thought experiment. I would be surprised if you wouldn't consider this sufficient third-party-verifiable evidence that I'm correct, at least for casual discussion.

Is that what he meant with "weaselly risk profile"? Vagueness? I agree increasing the stakes isn't a good way to deal with vagueness, and I'd wanted to make the terms more clear if he had agreed to bet on them or said something I recognised as wanting to go into more detail, but I saw neither.

It isn't my intent to wave my dick or even to give (more than a few percent of) casual viewers the impression that I'm waving my dick, so would you please explain what I did wrong/missed in this case?
:PROPERTIES:
:Author: philip1201
:Score: 2
:DateUnix: 1434495492.0
:DateShort: 2015-Jun-17
:END:

********** u/deleted:
#+begin_quote
  Even if they aren't exact, they create testable expectations and "suicide is never rational" is a reasonably clear prediction which can be falsified by demonstrating that people tend to value other lives as more valuable than their own in ev-psych theory, fact, and thought experiment.
#+end_quote

Well, you two were sufficiently vague that I had no idea this was what you meant. So clear things up, and/or act more courteous.
:PROPERTIES:
:Score: 1
:DateUnix: 1434495610.0
:DateShort: 2015-Jun-17
:END:


******** u/TimeLoopedPowerGamer:
#+begin_quote
  I'm fine with raising the stakes. How does $1,000 / $100,000 sound to you? If you don't have the money lying around, I'm fine if you pay at least $5,000 now and pay 4% interest rate over whatever you don't pay.
#+end_quote

It sounds like, at best, a complex and misunderstood internet dick-waving competition, not an attempt to discover the nature of reality through rational dialog. I was around when that was invented in these online circles. It was made as a technique to shut-up whiny internet rationalist kids and their terribly unlikely theories. No one told them that, and some people seriously used it later on, but it was always consensual when used that way, not some weird point of order.

I am massively offended to have it used on me here, something I thought I made clear. Your continued misuse of it makes it even more offensive. This is my problem not yours, the offense I take, but I don't see any compelling advantage in slogging forward regardless.

I'm not learning anything new and I consider you not to be arguing in good faith because of that and other inabilities to debate clearly, such as failing to echo back the points I've been making in an honest and clear-cut way to avoid confusion and digression on either side.

You know, how friendly debate is typically run instead of internet peecocking.

Yeah, I think we're done here.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: -1
:DateUnix: 1434436165.0
:DateShort: 2015-Jun-16
:END:

********* I was somewhat offended by your description of your opponents' thinking as "whatever dark little thought games internet intellectual tough-guys play". I have no problem with offensive language or offensive debates, so I took your lack of decorum as an indication that you have no problem with roughhousing. I am sorry I assumed you wouldn't be a hypocrite.

#+begin_quote
  It was made as a technique to shut-up whiny internet rationalist kids and their terribly unlikely theories.
#+end_quote

Correct. I'm sorry you're not a kid anymore but still behaving like that, but eh, maybe you're tired or something.

#+begin_quote
  No one told them that, and some people seriously used it later on, but it was always consensual when used that way,
#+end_quote

Are you implying you are unable to consent?

When I posted the bet I saw four main possibilities, in decreasing order of preference:

1. You accept the deal, MIRI gets $10k or $100k, you learn a hard lesson in humility, and there's a 0.00001% greater chance humanity isn't killed by unfriendly AI.

2. You acknowledge that you're being overly confident when saying "suicide is irrational, no exceptions", and we can continue the discussion while you have been prompted to find arguments against your own theories.

3. You accept the deal, but chicken out. I win status, and a fraud gets exposed.

4. You demonstrate irrationality by claiming certainty while simultaneously refusing a bet which would have a serious profit margin if you were as certain as you say or even think you are.
:PROPERTIES:
:Author: philip1201
:Score: 3
:DateUnix: 1434441652.0
:DateShort: 2015-Jun-16
:END:


********* u/philip1201:
#+begin_quote
  I am massively offended to have it used on me here, something I thought I made clear.
#+end_quote

Oh yeah, I was nerd-sniped for that. You said you wouldn't want $100 for driving an hour and spitting on me, but with the price of gas and vehicle maintenance that's about $60 for an hour's work where you learn nothing, which is a pretty mediocre deal for me and probably many others in the [[/r/rational]] community as well. I was so distracted by how the insult didn't make sense that I forgot to wonder what prompted it.
:PROPERTIES:
:Author: philip1201
:Score: 1
:DateUnix: 1434443800.0
:DateShort: 2015-Jun-16
:END:


********* Both of you tone down the ego.
:PROPERTIES:
:Score: 1
:DateUnix: 1434476189.0
:DateShort: 2015-Jun-16
:END:


***** u/FeepingCreature:
#+begin_quote
  This is the techno-altruism trap. Accept it and the value of the individual to society becomes zero. Anything can be justified by saying the new task schedule and resource allotment will benefit from the sacrifice of the old order. It is the path to one of the darkest possible future digital hells.
#+end_quote

That seems a stretch.
:PROPERTIES:
:Author: FeepingCreature
:Score: 4
:DateUnix: 1434417983.0
:DateShort: 2015-Jun-16
:END:

****** If the personal answer is to sacrifice one's life for a better, longer-lived version of that person, what inherent value is actually being placed on continued existence of any single person by a society consisting of such-minded individuals?

If continuance isn't valued at its most basic point, singular individual consciousnesses and self identities, then why not murder-clone those wasters of energy and use their physical or informational resources to do something more efficient? Why not murder-clone seven billion flesh and blood people to allow a hundred trillion digital people to live faster, better lives at the same cost? They'll be copies of the old ones, and might be made to not even notice their new simulated lives--ones that have no continuous physical and informational relationship to the dead bodies left behind.

This line of thought, that all processes that function the same are equivalent, is the dumbest thing I've seen since religion, and for many of the same reasons. And it is just as dangerous to my being, as technology advances and idiots who think this way come into power, shaping laws, social mores, and technology.

The only way to fight it is this. Point out how much magical thinking and fuzzy-headed idiocy is involved in thinking your murder-clone is just as good to have around as you. Otherwise, some day they'll try to murder-clone you, sure that your +spirit+ informational process will live on in the +afterlife+ clone.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434419232.0
:DateShort: 2015-Jun-16
:END:

******* u/philip1201:
#+begin_quote
  Why not murder-clone seven billion flesh and blood people to allow a hundred trillion digital people to live faster, better lives at the same cost?
#+end_quote

Consent is one thing, though in my opinion if people would consent when properly informed, you don't need to wait for them to physically move their lip-flaps in order to express consent. (This rule only applies to rational superbeings who can accurately predict if people would consent. Humans would just trick themselves into believing consent would be given).

And I would guess that people would all consent when properly informed. (If they understood quantum mechanics, reductionism, and the structure of the human mind).

The other is unknown unknowns. Nothing in known physics or theoretical physics that I know of indicates that something would go wrong, but this kind of thing often goes wrong because of weird reasons.

#+begin_quote
  The only way to fight it is this. Point out how much magical thinking and fuzzy-headed idiocy is involved in thinking your murder-clone is just as good to have around as you.
#+end_quote

So actually do so?
:PROPERTIES:
:Author: philip1201
:Score: 5
:DateUnix: 1434421887.0
:DateShort: 2015-Jun-16
:END:

******** u/TimeLoopedPowerGamer:
#+begin_quote
  So actually do so?
#+end_quote

[[http://www.reddit.com/r/rational/comments/39xqpi/rrationals_view_on_the_continuity_of/cs7s1qs?context=2][You are not a single snapshot of your brain, but the result of a continuous process of changing electrochemical functions acting on a specific organization of matter.]]

To believe otherwise is to add magical complexity to the experience of consciousness just so uploading fantasies give a happy, sci-fi afterlife. As we see ITT. More likely than not also causing one to think two processes that function the same are in all ways equivalent, morally speaking.

I swear, if I have to live on the run from a bunch of serial teleport murderers in a dark sci-fi future, I'm blaming you idiots.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 2
:DateUnix: 1434422463.0
:DateShort: 2015-Jun-16
:END:

********* Are you under the impression that continuous processes of changing electrochemical functions acting on a semi-static brain can't be simulated/emulated?

Or is it that your definition of self is non-local? That you can be told everything about an entire time-segment of a person down to the movement of the electrons (quantum mechanics notwithstanding), and not tell if they are your friend or a mere /duplicate/ with little moral value?

Or are you highly confident that an essential part of someone's identity is stored in continuously changing patterns which are lost using the uploading/duplicating methods you assume I'm talking about?

I would guess option 2. Which is what [[http://existentialcomics.com/comic/1][the comic linked by the most upvoted post]] addresses. More particularly the "It's no different from being knocked unconscious or falling asleep?" bit.

#+begin_quote
  I swear, if I have to live on the run from a bunch of serial teleport murderers in a dark sci-fi future, I'm blaming you idiots.
#+end_quote

Don't worry. The (f)AI will be good enough at its job that you won't have to run. :)
:PROPERTIES:
:Author: philip1201
:Score: 5
:DateUnix: 1434426782.0
:DateShort: 2015-Jun-16
:END:

********** I am confident that one bit of matter is different from another, even if functionally identical. They have different *causal history and location, for example. This is independant of any theory of mind I mind have.

If one brain hasn't been running a brain software process for the temporal history of that informational result, it isn't the same brain as originally collected that informational result. You can have two brains, different collections of matter, with the same information. The brain with the original causal history is the original brain. A person is a brain (for now).

As the brain is the person, you need a causal link between processes on the brain matter in one location and in a second location. Gradual replacement is thus permitted in this model. Otherwise, anything that might be called consciousness is guaranteed to be lost to the original.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 4
:DateUnix: 1434431508.0
:DateShort: 2015-Jun-16
:END:

*********** u/philip1201:
#+begin_quote
  I am confident that one bit of matter is different from another, even if functionally identical. They have different *causal history and location, for example.
#+end_quote

Good. You've set yourself up for falsification, and the universe sees fit to award you with your assumption being provably incorrect, giving you a learning opportunity.

Bits of matter have no causal history. Everything is blobs of amplitude of quantum fields. An electron which came from A to B via C will interfere with an electron which came via D (if they have the same phase) in exactly the same way as it interferes when it's split into two halves which take both routes.

Position is indeed a piece of information, same as time. However, since people don't die every femtosecond or as the planet rotates, position is not morally relevant.

#+begin_quote
  If one brain hasn't been running a brain software process for the temporal history of that informational result, it isn't the same brain as originally collected that informational result.
#+end_quote

Assuming you mean "if one brain is created in the exact state as another, it isn't the same brain":

Factually, quantum mechanically incorrect. Both brains are identical and if they could maintain quantum coherence they would interfere just as surely as simple electrons. This does take some extrapolation from available data: so far we haven't been able to maintain quantum coherence on objects larger than single molecules, but it's theoretically possible.

Well, that is if they're quantum mechanically identical. Same phase on every quark, same photons and phonons and everything. Merely containing the same person is luckily a much laxer standard, but the important thing is that history is irrelevant to present identity. Now, since you are the same person as you are a second from now when all your spins are out of whack, you are also the same person as someone who is only slightly different from someone with an entirely different history from you but the same quantum mechanical state as you. Therefore you are identical to someone who was teleported back and forth too fast for you to notice.

Now consider that you would still be you if you physically walked from the entrance site of the teleporter to the exit site. In that case, you have physically changed more in every way than if you just teleported from one location to the other. Either you just shift the spatial coordinates of every cell in your body a little, or you literally replace half of them, shift the spatial coordinates by the same amount, and also the time coordinate by a lot.

#+begin_quote
  As the brain is the person, you need a causal link between processes on the brain matter in one location and in a second location.
#+end_quote

How else would you expect to know what kind of person to create at the exit site?

#+begin_quote
  Otherwise, anything that might be called consciousness is guaranteed to be lost to the original.
#+end_quote

Of course: the original is destroyed. Am I missing something here?
:PROPERTIES:
:Author: philip1201
:Score: 7
:DateUnix: 1434438950.0
:DateShort: 2015-Jun-16
:END:


*********** I just want to second the other commenter and point out that there's an experiment called a [[https://en.wikipedia.org/wiki/Quantum_eraser_experiment][quantum eraser]] that would not be physically possible if particles had causal history.

We don't live in a world that has a factual past. We live in a world that is the superposition of /all/ pasts that have led to it. There is no "fact of the matter" which past is "actually" ours.
:PROPERTIES:
:Author: FeepingCreature
:Score: 4
:DateUnix: 1434463106.0
:DateShort: 2015-Jun-16
:END:

************ ***** 
      :PROPERTIES:
      :CUSTOM_ID: section
      :END:
****** 
       :PROPERTIES:
       :CUSTOM_ID: section-1
       :END:
**** 
     :PROPERTIES:
     :CUSTOM_ID: section-2
     :END:
[[https://en.wikipedia.org/wiki/Quantum%20eraser%20experiment][*Quantum eraser experiment*]]: [[#sfw][]]

--------------

#+begin_quote
  In [[https://en.wikipedia.org/wiki/Quantum_mechanics][quantum mechanics]], the *quantum eraser experiment* is an [[https://en.wikipedia.org/wiki/Interferometer][interferometer experiment]] that demonstrates several fundamental aspects of [[https://en.wikipedia.org/wiki/Quantum_mechanics][quantum mechanics]], including [[https://en.wikipedia.org/wiki/Quantum_entanglement][quantum entanglement]] and [[https://en.wikipedia.org/wiki/Complementarity_(physics)][complementarity]].

  The double-slit quantum eraser experiment described in this article has three stages:

  - First, the experimenter reproduces the interference pattern of [[https://en.wikipedia.org/wiki/Young%27s_interference_experiment][Young's]] [[https://en.wikipedia.org/wiki/Double-slit_experiment][double-slit experiment]] by shining photons at the double-slit interferometer and checking for an interference pattern at the detection screen.

  - Next, the experimenter marks through which slit each photon went, without disturbing its wavefunction, and demonstrates that thereafter the interference pattern is destroyed. This stage indicates that it is the existence of the "which-path" information that causes the destruction of the interference pattern.

  - Third, the "which-path" information is "erased," whereupon the interference pattern is recovered. (Rather than removing or reversing any changes introduced into the photon or its path, these experiments typically produce another change that *obscures* the markings earlier produced.)

  A key result is that it does not matter whether the erasure procedure is done before or after the detection of the photons.

  Quantum erasure technology can be used to increase the [[https://en.wikipedia.org/wiki/Optical_resolution][resolution]] of advanced [[https://en.wikipedia.org/wiki/Microscopes][microscopes]].

  * 
    :PROPERTIES:
    :CUSTOM_ID: section-3
    :END:
  [[https://i.imgur.com/TeQOzP8.png][*Image*]] [[https://commons.wikimedia.org/wiki/File:WalbornEtAl_D-S_eraser_no_POL1svg.svg][^{i}]]
#+end_quote

--------------

^{Relevant:} [[https://en.wikipedia.org/wiki/Delayed_choice_quantum_eraser][^{Delayed} ^{choice} ^{quantum} ^{eraser}]] ^{|} [[https://en.wikipedia.org/wiki/Index_of_physics_articles_(Q)][^{Index} ^{of} ^{physics} ^{articles} ^{(Q)}]] ^{|} [[https://en.wikipedia.org/wiki/Mach%E2%80%93Zehnder_interferometer][^{Mach--Zehnder} ^{interferometer}]] ^{|} [[https://en.wikipedia.org/wiki/The_Fabric_of_the_Cosmos][^{The} ^{Fabric} ^{of} ^{the} ^{Cosmos}]]

^{Parent} ^{commenter} ^{can} [[/message/compose?to=autowikibot&subject=AutoWikibot%20NSFW%20toggle&message=%2Btoggle-nsfw+cs8cepq][^{toggle} ^{NSFW}]] ^{or[[#or][]]} [[/message/compose?to=autowikibot&subject=AutoWikibot%20Deletion&message=%2Bdelete+cs8cepq][^{delete}]]^{.} ^{Will} ^{also} ^{delete} ^{on} ^{comment} ^{score} ^{of} ^{-1} ^{or} ^{less.} ^{|} [[/r/autowikibot/wiki/index][^{FAQs}]] ^{|} [[/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/][^{Mods}]] ^{|} [[/r/autowikibot/comments/1ux484/ask_wikibot/][^{Call} ^{Me}]]
:PROPERTIES:
:Author: autowikibot
:Score: 1
:DateUnix: 1434463154.0
:DateShort: 2015-Jun-16
:END:


************ I'm talking about macro effects. That's part of my point, in fact. The parts making up the whole can alter in small ways, and do, and that doesn't change the functioning of the system to any great degree. But it has to be a physical part of that system to have any identity relationship with that system. Otherwise, it is simply a copy and should be identified as such.

That's why proponents of information as identity are so enamored with quantum effects. It is the only way they see to get around the identity problem of murdering the original in destructive copy scenarios. And it falls flat when it actually comes to real world examples, still requiring things like perfect informational awareness. The bane of rational models everywhere.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434494628.0
:DateShort: 2015-Jun-17
:END:

************* Macro effects cannot have properties that are irreducible to micro effects.

If particles cannot have history, people cannot have history in a physical sense. People can have history in an /abstract/ sense, but abstract history can cross the upload barrier just fine. Not because uploading can be done perfectly if you track quantum information, but because the existence of certain quantum effects demonstrates that it is physically impossible that particle history has relevance at all.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1434495452.0
:DateShort: 2015-Jun-17
:END:

************** u/TimeLoopedPowerGamer:
#+begin_quote
  Macro effects cannot have properties that are irreducible to micro effects.
#+end_quote

Irrelevant. You were trying to argue the other way, which is scientifically proven not to work. Some macro level quantum effects can be measured, but that has nothing to do with this wild over reaching. Quantum hand-waving and logical misapprehensions won't save your bad epistemology.

#+begin_quote
  If particles cannot have history, people cannot have history in a physical sense.
#+end_quote

This does not follow. Please return to reality, where macro-level causes have macro-level effects than can be measured, studied, and predicted.

#+begin_quote
  the existence of certain quantum effects demonstrates that it is physically impossible that particle history has relevance at all.
#+end_quote

Prove this outrageous claim exists in any way on a macro level. If this is what your opinion hinges on, I think I see the problem. A bad case of quantum magical thinking.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434507334.0
:DateShort: 2015-Jun-17
:END:

*************** It feels like we're talking past each other.

I'm not saying that quantum physics proves that continuity with an upload is definitely the case. That'd be quantum woo.

I'm saying quantum physics proves that at the lowest level, the physical level of individual particles, physics is not capable of giving you what it seemed you wanted - a way to care about the "actual history" of a "specific particle" - because quantum physics very particularly does not track any such thing as a "specific particle" /at all/.

For you to care, with your abstract, high-level brain, about some property of a particle, that property of the particle has to exist in the first place. It has to be in the territory before your map can meaningfully refer to it. When we talk of the history of an item, we're talking about a very complex amalgamation of physical, social and legal properties, which has a tendency of breaking down when faced with unusual situations, such as the oft-cited Ship of Theseus. What we are /not/ talking about, however, is any sort of particle-wise identity that stretches across time, because we /cannot/ be talking /about/ that, because it /does not exist at all/. There is no "about" to be talked about in the territory.

Now, physics obviously conserves certain quantities, so that for certain timeframes it can look like particles have permanence. But to derive from that that particles have /identity/ is a vast step too far, which is not supported by any evidence and in fact contradicted by quantum physics. Seriously, have you even [[http://lesswrong.com/lw/r5/the_quantum_physics_sequence/][read the Sequences]]? ;)

So, since particles do not have this property in a physical sense, anything built from particles can also not have this property /in a physical sense/. You have to be abstracting over something else, such as some notion of permanence or uniqueness or legal ownership (as in the Ship of Theseus) and wrapping up all the problems under the simplifying notion of "the same". But [[http://www.joelonsoftware.com/articles/LeakyAbstractions.html][all abstractions are leaky]], and you have to keep in mind that you're considering something that's based on Usefulness more than Truth. And, personally speaking, considering an upload continuous with your flesh-self is hella useful. :)

Now, you can, if you like, /construct/, in your head, a notion of an electron as a distinct particle with identity. And you can watch this distinct particle fly towards a silver mirror, and get simultaneously reflected and not reflected, and simultaneously fly through two slits in a small metal plate, and then this one particle which is in two positions somehow ends up in a single position which is somewhere that neither of the two positions the particle was just in should have allowed it to end up, and by this point you should have smoke coming out your ears and smelling faintly of ozone. Which should amply demonstrate to you that the initial assumption of the distinct particle was unworkable, and remind us all that while we can construct whatever we like in our heads, reality is not in fact obliged to care. Reality (apparently) runs on quantum physics, not on imagination, and /quantum physics is local/, however inconvenient that is for our maps.

And so it follows, since you cannot care about specific particles in reality, not because of any logical argument but because "specific particles" happens to be a thing that does not /exist/ in reality, you can at best care about configurations of particles. Which happens to be something that sounds significantly more amenable to uploading than specific particles, so that's convenient.

In a world where a particle's causal history sometimes had physical effects, would I think differently? I think I'd still want to have the same /preference/ - that an upload is as valuable as a fleshbody for continuation, but it would be much harder for me to argue this point, even to myself. It's plausible and convenient to count people with the same configuration as selves, but we happen to live in the rhetorically /really convenient/ world where it's actually /physically incoherent/ to do otherwise.
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1434527310.0
:DateShort: 2015-Jun-17
:END:

**************** I wrote something really long in reply. It was boring. I deleted it, saved everyone a bunch of time. You're all welcome.

Here:

#+begin_quote
  For you to care, with your abstract, high-level brain, about some property of a particle, that property of the particle has to exist in the first place.
#+end_quote

No.

#+begin_quote
  ... But to derive from that that particles have identity is a vast step too far, which is not supported by any evidence and in fact contradicted by quantum physics.
#+end_quote

Don't.

#+begin_quote
  ...

  So, since particles do not have this property in a physical sense, anything built from particles can also not have this property in a physical sense.
#+end_quote

Stop, please.

This entire thread of logic is incorrect. You worked that from both sides at once, making a strawman of my argument in the process. When I mentioned bits of matter, I wasn't referring to fundamental particles, I was referring to the human brain as a complete physical process.

I agree, no particle has this property of identity, but collections of matter in space-time do. That is what is not copied in a suicide-clone teleporter. That is why destroying /your/ brain, the only thing scientifically proven to produce a human mind, in this way kills /you/.

There is just a brand new mind in a new brain. It doesn't inherit the identity of the old brain simply because it is copied from the same information. The new brain has a different space-time history and thus identity from the original collection of matter, which continues to have its own space-time identity. The new brain thus contains a new, differently-identifiable mind.

Also, your reasoning is flawed and your use of the quantum nature of particles is incorrect. Especially that last step.

#+begin_quote
  And, personally speaking, considering an upload continuous with your flesh-self is hella useful. :)
#+end_quote

This is your failure. You prefer this view of the nature of the physical aspects of the human mind, one not supported /at all/ by science, because it pleases you. You nurture a poorly constructed ontology to make yourself feel better, and present it as a textdump because you need to spend a lot of time talking around its weak points.

Bad rational thinker. No cookie. Now please stop vomiting pop-science quantum theory onto my inbox. Go and q-sin no more.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434537722.0
:DateShort: 2015-Jun-17
:END:

***************** u/FeepingCreature:
#+begin_quote
  The new brain has a different space-time history
#+end_quote

*THERE IS NO SUCH THING*.

Show me ONE.

ONE.

Law of physics that changes its behavior based on "space-time history".

Getting a bit pissed at being called a crackpot here.

It is literally impossible that your brain cares about /actual/ "space-time history"; because that doesn't exist in the territory, it is impossible for your map to be entangled (*NOT IN THE QUANTUM SENSE, DON'T EVEN START*) with it! At best it cares about an abstraction of space-time history.

It's like caring about whether God wants you to eat pork. Since there /is/ no God; it is factually impossible for Him to want anything! Your belief is not about an actual thing that actually exists!

( Upvoted anyway, just for "It was boring. I deleted it". )

PS: Look, let's short-circuit this. Do you have a friend who you trust and who's a physicist, and can you give me their phone number/skype id? I think it'd be easier to convince them, and then they can convince you.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1434539183.0
:DateShort: 2015-Jun-17
:END:

****************** If you want to discard all of physical reality, I think there is little to discuss. Nothing is real because quantum, nothing matters because quantum. It isn't a coherent argument. If existence is an illusion, then why can't you point to a neuroscience paper that says this clearly for the human brain?

You know what outs crackpots with wild personal theories? Long rambles and no citations. Yours is the extraordinary claim. Back it up some.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 0
:DateUnix: 1434581245.0
:DateShort: 2015-Jun-18
:END:


********* u/deleted:
#+begin_quote
  I swear, if I have to live on the run from a bunch of serial teleport murderers in a dark sci-fi future, I'm blaming you idiots.
#+end_quote

OHMYGOD, now you know how I feel /all the time!/ Admittedly, in my version it's a cyberpunk ultracapitalist dark-scifi-future, because for some strange reason a lot of tech/scifi people are still allergic to egalitarianism even when the rest of the world is /finally/ moving on from trying to make life imitate /Snow Crash/.

Frankly if I'm 90-something years old and dying, and there's no /other/ mode of life extension available, then leaving behind a "copy" to live on after me as a kind of clone-child is pretty ok. Especially if I get to design the successor first.

Still not as preferable as getting a proper scientific literature explaining what continuity of consciousness /really does mean/ and being able to answer the questions with definitive experimental evidence so that I can obtain some way of staying alive /as me/, but alas, such is scifi philosophizing.
:PROPERTIES:
:Score: 1
:DateUnix: 1434471596.0
:DateShort: 2015-Jun-16
:END:

********** u/TimeLoopedPowerGamer:
#+begin_quote
  Admittedly, in my version it's a cyberpunk ultracapitalist dark-scifi-future, because for some strange reason a lot of tech/scifi people are still allergic to egalitarianism even when the rest of the world is finally moving on from trying to make life imitate Snow Crash.
#+end_quote

You're telling me you're not excited to see every single citizen become an entrepreneur contractor in a bright new hyper-competitive marketplace where an increasing quantity of resources are spent shoring up personal defenses against other actors' "fair and competitive practices"? You don't think the Randian threat of the poor rising up against excessive abuse will continue to be a logical keystone to social policies when mindcontrol, purpose-grown, and artificial/uploaded lifeforms can literally be reprogrammed not to demand so-called natural and civil rights?

What a pessimist.

--------------

As for the rest, I largely agree. I think a unsurprisingly small number of strong proponents of information-as-self would actually walk into the /24-Hour FedEx/Suicide Kinkos/ were it to appear tomorrow. And even those holding my position would do so as a seemingly truly inescapable last result. Such is the nature of internet techno-philosowanking.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 0
:DateUnix: 1434494406.0
:DateShort: 2015-Jun-17
:END:

*********** u/deleted:
#+begin_quote
  I think a unsurprisingly small number of strong proponents of information-as-self would actually walk into the 24-Hour FedEx/Suicide Kinkos were it to appear tomorrow.
#+end_quote

That depends entirely on how it is marketed. A significant number of people have /already/ expressed a desire to perform exactly the act you describe /right now/, when no such thing is actually possible. In fact, some of them have expressed the desire to do it /yesterday/. You might claim that they're saying something in Far Mode they won't support in Near Mode, but again, Near Mode can be subjected to various forms of persuasion, most of which are not even threats and a good many of which are pleasant enticements.

To steal from an LW post, sure, you can say that the simulated tropical paradise is /false/, that it will happen to /some other person/ in a morally important way, and that it's all really just suicide, but as long as it's /marketed/ as "Tropical paradise! Sea breeze! Lemonade! Sunshine!", there will be significant buy-in. And of course, by your own hypothetical, the remnant-successors of the victims won't actually be able to tell it didn't work. Those remnant-successors won't be the same person, but they /will/ remember the original person, and they /will/ be enjoying the beach.

Of course, this is what makes the whole thing troublesome: how would you be able to test whether some concept of personal identity is preserved /without/ resorting to philosophical intuition and argument, and /without/ killing yourself just to see if you wind up dead?

To which I would say: I'd be cautious of trying it /without/ such a test, but I'd be suspicious of someone who refused to accept a test that showed it did work. The thing to do with a question is turn it into hypotheses, and the thing to do with hypotheses is to test them.
:PROPERTIES:
:Score: 2
:DateUnix: 1434496637.0
:DateShort: 2015-Jun-17
:END:


*********** u/deleted:
#+begin_quote
  You're telling me you're not excited to see every single citizen become an entrepreneur contractor in a bright new hyper-competitive marketplace where an increasing quantity of resources are spent shoring up personal defenses against other actors' "fair and competitive practices"? ...

  What a pessimist.
#+end_quote

Yeah. I'm such a pessimist that I can't help but look forward only to a communist era in which compassionate, intelligent AI Minds replace the blunt, brutal machinations of the state (including the often even blunter and more brutal machinations of dumb computers), and people associate freely based on common interests and solidarity, and things like fun and love are considered the really important things in life.

I have an extremely dark worldview.
:PROPERTIES:
:Score: 2
:DateUnix: 1434497329.0
:DateShort: 2015-Jun-17
:END:

************ I'll make sure to stock up on dark trenchcoats and eyepatches.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434504880.0
:DateShort: 2015-Jun-17
:END:


******* Can't we be okay with suicide, but not okay with murder?

For humans running faulty software, it's a good idea to make a rule that says "Never murder other people, even when it seems to produce a net benefit."

I don't think that the same rule is necessary for "Never kill yourself, even when it seems to produce a net benefit." /Barring cases of psychological disorder/, we already have a pretty hard psychological injunction against that, and so (again, barring cases of extreme depression, &c) if you judge self-sacrifice to be a worthwhile action, then by all means go for it.

If I were presented with sufficient evidence that offing myself would promote my terminal value more effectively than my continued existence, then I might examine and re-examine it really, really hard, because that'd be a very surprising piece of information, but after I had done sufficient work in confirming it, then yeah, I would off myself.

EDIT: Fixed a typo.
:PROPERTIES:
:Author: callmebrotherg
:Score: 2
:DateUnix: 1434431701.0
:DateShort: 2015-Jun-16
:END:

******** u/deleted:
#+begin_quote
  I don't think that the same rule is necessary for "Never kill yourself, even when it seems to produce a net benefit." Barring cases of psychological disorder, we already have a pretty hard psychological injunction against that, and so (again, barring cases of extreme depression, &c) if you judge self-sacrifice to be a worthwhile action, then by all means go for it.
#+end_quote

CASES OF EXTREME DEPRESSION ARE NOT THAT RARE. MAKE THIS RULE OR ELSE.
:PROPERTIES:
:Score: 2
:DateUnix: 1434471882.0
:DateShort: 2015-Jun-16
:END:

********* Wouldn't that, say, be a rule against being a firefighter, since you would then be accepting a certain chance of death for the sake of some cause which you think is more important?

The difference between being a firefighter and jumping off a cliff is in the probability that you will die at any particular point in time, so of course you'd want a substantially higher payout from a 100% chance of death than from a much smaller chance of death. You're still putting your life up on the table though.

(I am heartened by your ALLCAPS, though)
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1434474488.0
:DateShort: 2015-Jun-16
:END:

********** Ok, there's a difference between self-sacrifice for the sake of /another person who's concretely there and needs you/, and just killing yourself out of some abstract conceptualization that the world is better off without you. A firefighter or other emergency worker also functions as part of a team whose training, equipment, and policies deliberately /minimize/ the chance of Heroic Sacrifices as a part of minimizing total casualties -- that's what professionalism /is/ for emergency workers. (Actually, "get stuff right ahead of time - with training, equipment, and policy - so that heroism becomes /unnecessary/" is a fairly good definition of professionalism overall.)

The former happens reasonably often. The latter is almost always a symptom of depression, to such an extent that it's worth enforcing a Corrupted Meat Injunction on taking the latter seriously.
:PROPERTIES:
:Score: 2
:DateUnix: 1434474804.0
:DateShort: 2015-Jun-16
:END:

*********** I apologize for my lack of clarity. I don't foresee myself justifying my death because the world seems to be abstractly better without me.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1434513426.0
:DateShort: 2015-Jun-17
:END:


******** u/TimeLoopedPowerGamer:
#+begin_quote
  Can't we be okay with suicide, but not okay with murder?
#+end_quote

Absolutely not. Never, ever. It is a right everyone ought to have in a free society, even permanently insane people (or, sadly, especially), but it isn't rational. Ever.

You can never have enough information to prove your decision is correct given the result of self-termination. Such goals are pretty much definitionally not rational and indicative of a major cognitive error.

#+begin_quote
  if you judge self-sacrifice to be a worthwhile action, then by all means go for it.
#+end_quote

If this happens, you are almost certainly necessarily already under such incredible psychological duress that you can't be considered sane. Either pain, the threat of such, or a fate you for some reason believe is worse than death is occurring.

Sure, maybe you want to give your life as a sacrifice to aid others, but that is a maximum danger last resort; pulling the lever to cause the train to hit you and not ten other people. Suicide is not a calm and collected "because my mind baby will be so awesome" situation for a sane and rational person.

#+begin_quote
  because that'd be a very surprising piece of information, but after had done sufficient work in confirming it, then yeah, I would off myself.
#+end_quote

How is this possible? How could you be sure your mind and informational situation were this sound? If you are surprised enough to suddenly consider this, you're already doing something wrong. You were surprised because you didn't have a sufficiently informed or detailed map of reality. Your very ability to form maps is likely compromised, and the terrain is probably not, in fact, full of snakes, but harmless bunnies.

Not suiciding because of sudden revelations, even ones long considered after that realization, should be a basic no-brainer rational pre-decision. The numbers to make this rational in the real world would require such tortured, selfishly self-destructive goals as to make the entire thing a complete mess from the get-go.

This isn't a social competition to see who can say the most self-sacrificing thing to better attract a mate. This is a rational discussion between people who aren't friends and won't ever bone. Don't drag in those showy signaling affects, please. I'm fucking tired of that. Please consider what you're saying carefully and logically. And get off my lawn, intellectually speaking.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434435423.0
:DateShort: 2015-Jun-16
:END:

********* Heh. First of all, thanks for your coherent and reasonable response. I don't encounter that nearly often enough.

I think that the biggest issue is that we have differing value systems. You care pretty heavily about your personal stream of consciousness (or so it seems-- I apologize if I've misunderstood), whereas I... don't. People in general, /including/ myself, only have instrumental value in my system.

If I were trying to signal then I'd be doing a pretty bad job, because my value system isn't very pretty. It could justify, for example, creating a world with all of the horrible things that we are so familiar with here. Happiness is as instrumental as sadness, and at the end of the day the only reason that happiness generally wins out is too many dolors, or a lack of sufficient hedons, or a bad ratio of dolors to hedons, can break people, and broken people aren't very useful.

None of that is likely to win me boning points, even if I cared about those to begin with.

#+begin_quote
  Such goals are pretty much definitionally not rational
#+end_quote

Isn't rationality dependent on what your goals are to begin with?

#+begin_quote
  Suicide is not a calm and collected "because my mind baby will be so awesome" situation for a sane and rational person.
#+end_quote

I'm not so sure. Granted, if the situation didn't require a death on this end of the line, then I'd probably be cool with there being one of me on this end of the line and one of me on the other. If teleportation did require it, though, then I won't mind much or at all (I admit that it'll be probably weird the first few times).

But if, for some reason, teleportation requires that there be only one of me at the end, then I don't really care that much about the continuity.

#+begin_quote
  How is this possible? How could you be sure your mind and informational situation were this sound?
#+end_quote

Killing myself for the purpose of donating my organs to particular people whose combined usefulness is, I believe, greater than my own. It's an unlikely example, but I think that it's able to exist in the same universe as the classic trolley problem, and the point is to illustrate that my own existence is of finite value.

#+begin_quote
  Your very ability to form maps is likely compromised, and the terrain is probably not, in fact, full of snakes, but harmless bunnies.
#+end_quote

This is a great sentence.
:PROPERTIES:
:Author: callmebrotherg
:Score: 3
:DateUnix: 1434439029.0
:DateShort: 2015-Jun-16
:END:

********** u/TimeLoopedPowerGamer:
#+begin_quote
  I think that the biggest issue is that we have differing value systems.
#+end_quote

I value individual life and freedom. I abhor the crypto-collectivist neo-altruism I see as so common in certain circles. It is very hip in some places for modern internet futurists to compete on who can be most self-sacrificing for other people and the future. Knowing a little something about anthropology, world political history, and psychology, that sort of thing leaves a sick taste in my mouth.

If that wasn't happening intentionally or otherwise, I apologize. I thought I was seeing the presentation of regurgitated material, not original thought. As you say, perhaps weakly stated and disorganized thoughts were the only thing there. In any case, please carefully consider why you believe what you think you believe.

--------------

I'm not some Luddite, however this sounds. Personally, I want an expanding cloud of /me/ exiting the Earth's atmosphere at high speed as much as the next powergaming internet egomaniac; but not if it kills me. And I see no reason to discard the scientifically supported and generally accepted theory that the human brain is everything that creates thought, however you define consciousness or self or any other hard to nail down terms.

If I am my brain, it has to have some closely interconnected, ongoing, additive processing relationship with any new processing substrate if the original is to be destroyed as a result. Otherwise my brain, which is all that I am, will die in functional isolation from any other cognitive processes, and leave only a copy of its state behind to spawn off new (if identical) cognitive processes. Fine for everyone else in the universe, if they don't think like I do, but not good for /me/.

The idea that someone is just the pattern of their brainstate, or a functional description of their goals or actions, divorced from the physical matter running it, is not even wrong. It ignores so much it doesn't even vaguely match reality. It'll also get me killed when the tech comes around if people continue to hold these unsupported personal delusions.

That destructive uploading is the way some very promising and popular existing tech seems to be heading, things like ways of recovering cryonic suspensions, should be a warning sign that people might be seeing and believing what they want, not what is. Being biased very strongly by a desire for this technology to work and give a way out of oblivion seems a real danger to any knowledgeable modern futurist debating this topic.

--------------

#+begin_quote
  People in general, including myself, only have instrumental value in my system.
#+end_quote

I think that's sort of sad. It isn't supported in any way by reality, is pointlessly reductionist in a very flawed way, and isn't required as a theoretical framework for software intelligence to also work, or even human augmentation paths to uploaded human minds. Rather, it seems like an intellectual hairshirt, something to justify sacrificing the sense of self that admittedly helps so little in rational thought.

If this becomes a popular opinion, and tech goes on the path it seems to be, eventually a lot of people are going to die and be replaced by software murder-clones. If the artificial substrate ends up working better and faster (likely), and the copy process is perfect (or people think it is), that's a very dark potential future. One you inexplicably seem perfectly fine with. This doesn't engender much sympathy, or make for a good impression of your level of rationality.

How can you fulfill not wanting to die tomorrow rationally if you suddenly have "die for awesome murder-clone" added into the mix? That seems like an incorrectly designed goal with some really off-the-wall assumptions built in. How can you not want to die tomorrow, and have that as a repeated and continuing goal, if you don't even know /what you are/?

It seems ITT people's answer is largely to pull an Asimov and redefine themselves as merely functional states, not as the generally scientifically accepted definition of the collective and continuing process of matter contemplating itself.

--------------

#+begin_quote
  Isn't rationality dependent on what your goals are to begin with?
#+end_quote

In the weakest sense, of course. But having goals that functionally conflict with reality because of poor assumptions shows a lack of rationality.

If your goal is in support of optimal murder-uploading strategies, I think this conversation might be over.

#+begin_quote
  But if, for some reason, teleportation requires that there be only one of me at the end, then I don't really care that much about the continuity.
#+end_quote

This and other comments suggests a state of self-worth that I feel is dangerous. The talk of suicide added in is making me very uncomfortable. I'm not shutting this conversation down, but I do want to make sure I'm not doing some sort of damage here. Perhaps this is a good place to wrap the topic up. But feel free, if you wish. I'll at least read a reply, though don't be offended if I don't comment again myself.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434450935.0
:DateShort: 2015-Jun-16
:END:

*********** u/callmebrotherg:
#+begin_quote
  I'll at least read a reply, though don't be offended if I don't comment again myself.
#+end_quote

No problem.

#+begin_quote
  The talk of suicide added in is making me very uncomfortable. I
#+end_quote

I apologize for that. I would have searched for some examples that did not directly involve suicide, but expressed the same point, if I had known.

#+begin_quote
  In any case, please carefully consider why you believe what you think you believe.
#+end_quote

Essentially, I don't think that it's practical to have multiple terminal values because then you're stuck in a jam when they come into conflict (if they will never conflict in any possible scenario, then more likely they're the same value with two names). I like paintings, and I like biodiversity, and I like (certain kinds of) food. I do /not/ like any of the possible nightmare wireheading scenarios which await anyone who genuinely values hedons in particular, because I want paintings and biodiversity and delicious food much, much more than I want orgasmium.

You could try to cover these things under the umbrella value of Beauty, but "beauty" is a term that has to refer to another value before it can have any meaning. So I decided to go with complexity, which as a word in general has a lot of possible meanings and so might be most quickly defined through comparisons:

- Multicultural societies are more complex than monocultural societies.

- The Lord of the Rings is more complex than Peter Rabbit.

- You, having learned something, are more complex than you, before you learned that thing.

- A universe with 10 hedons and 10 dolors is more complex than a universe with 20 hedons and 0 dolors.

What is Beautiful is Good, and what is Complex is Beautiful.

Within certain bounds, at least. Complexity that is incomprehensible is no better than, and possibly in some ways worse than, undifferentiated simplicity. Both are noise on the airwaves. But of course I also have an interest in increasing our capacity to comprehend the complex.

In this system, hedons are no more valuable than dolors, except for the fact that we are, you could say, addicted to them, and people can be permanently damaged or at least temporarily impaired if they don't get enough hedons in the long term or get too many dolors. It would be nice to fix that, but we would obviously have to be very careful about it.

Also, perhaps unfortunately, humans are also instrumental values, since I don't think it's feasible to hold multiple terminal values. We're awesome things in my system because we are like engines of complexity, producing a whole bunch of it just by, well, /being/.

#+begin_quote
  How can you fulfill not wanting to die tomorrow rationally if you suddenly have "die for awesome murder-clone" added into the mix?
#+end_quote

I identify a person by zir personality, memories, &c &c. If my best friend's mind were to be switched with the mind of someone else, then I would continue my interactions with my-friend-in-someone-else's-body, no matter how quickly or slowly the transition happened.

(Now, it might be that physical differences between the bodies might cause a change in my friend's personality such that our relationship could no longer be maintained, but the same could be said of brain injuries and the like)

If my friend were to suffer complete amnesia, then my response would be exactly the same as if she had physically died. If my friend were uploaded to a computer then, no matter how quickly or destructively it happened, I would identify the upload as my best friend. And if she went through a teleporter that did not require the destruction of the original body, and so there were two of my friend, then I would, as that just implied, consider both of them to be my best friend. That may eventually cease to be true in the future, but that would be so even without teleporters entering the picture.

I care about (at the risk of oversimplifying) the particular brain-state or personality snapshot as it exists in that time and place. It's been argued that, strictly speaking, people are thus dying from moment to moment according to this system. That seems to be true, but from moment to moment each successive identity is similar enough that I have no problem with continuing to update the status of "best friend" to this series of identities. I am not only her friend, but hope that the identities which follow this one will be friends with the identities which follow her present identity.

In the long run it's easier to see the difference. I don't care about fifteen-year-old me and I consider him to a different person. Assuming no time shenanigans to the effect that a change to Past Me would result in a change to Present Me, I have no more concern for Past Me than I do for any other person (and perhaps a little bit less, because Past Me was an asshole and I assume people to be nice until proven otherwise).

The desire to not die tomorrow might, in my conception of identity, be better expressed as the desire to be replaced by something better than myself tomorrow, rather than nothing at all.

(To be clear, though, I don't advocate forcing people to do things, and if I were somehow given the reins to the world then you'd be left to do as you pleased, and upload or teleport or not, as you chose)

#+begin_quote
  This and other comments suggests a state of self-worth that I feel is dangerous.
#+end_quote

Possibly. I remain open to the possibility that it may be the result of a mental illness, since it /is/ rather odd and I can see how it might be easier to hold some or all of these ideas by experiencing, say, depersonalization disorder. I'd prefer to have more than assumptions, though, so for the time being I feel comfortable with remaining where I am in belief-space.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1434474182.0
:DateShort: 2015-Jun-16
:END:

************ I do hope you realize that if you endorse "maximize complexity", then you're endorsing Bostrom's "Disneyland with no children" scenario. I advise trying to have multiple terminal values that are all measurable, and which can thus be traded-off to form a utility function.
:PROPERTIES:
:Score: 3
:DateUnix: 1434476003.0
:DateShort: 2015-Jun-16
:END:

************* Consciousness is pretty darn complex, so my Disneyland needs to have kids (or self-aware entities, anyway) to be Disneyland.

If I'm misunderstanding you, I apologize.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1434476376.0
:DateShort: 2015-Jun-16
:END:

************** u/deleted:
#+begin_quote
  Consciousness is pretty darn complex, so my Disneyland needs to have kids (or self-aware entities, anyway) to be Disneyland.
#+end_quote

No, you just value consciousness. It is not the Maximally Complex Thing, so someone maximizing complexity could well discard subject experience and self-awareness as unnecessary, inefficient instruments for the achievement of complexity, instruments that are surpassed and replaced fairly quickly.
:PROPERTIES:
:Score: 2
:DateUnix: 1434478019.0
:DateShort: 2015-Jun-16
:END:

*************** Hm. I feel like minds would be, (figurative) pound for (figurative) pound, more complex than the same matter arranged in another combination. Short of being able to definitively calculate the complexity of this, that, and every other thing, though, it might be safer to tie the complexity to conscious minds and look to increase the complexity of those minds and of their experiences.
:PROPERTIES:
:Author: callmebrotherg
:Score: 1
:DateUnix: 1434513315.0
:DateShort: 2015-Jun-17
:END:

**************** That still doesn't pick put conscious, human minds. Complexity-maximizing Fnargl can get more complexity per kilogram out of nonconscious but more knowledgeable AI minds, particularly since that lets Fnargl skip the simplistic lizard-brained baggage common to almost all human minds.
:PROPERTIES:
:Score: 1
:DateUnix: 1434540522.0
:DateShort: 2015-Jun-17
:END:


*********** u/deleted:
#+begin_quote
  Personally, I want an expanding cloud of me exiting the Earth's atmosphere at high speed as much as the next powergaming internet egomaniac;
#+end_quote

Uhhhh... is that /really/ so normal among this subreddit's users? Expanding cloud? Really?
:PROPERTIES:
:Score: 1
:DateUnix: 1434475593.0
:DateShort: 2015-Jun-16
:END:

************ Only amongst the powergaming internet egomaniacs. But to the point where new internal politics have developed to attempt to make a workable system to morally regulate the resource conflicts involved in wanting maximum you to exist in the universe.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434493284.0
:DateShort: 2015-Jun-17
:END:

************* u/deleted:
#+begin_quote
  Only amongst the powergaming internet egomaniacs.
#+end_quote

Now, for those of us who mod this sub and therefore don't understand the details of clique formation at this time... who are these people?

#+begin_quote
  But to the point where new internal politics have developed to attempt to make a workable system to morally regulate the resource conflicts involved in wanting maximum you to exist in the universe.
#+end_quote

Who's been developing new internal politics to manage tiling the universe in masses of single individuals? What internal politics are these?

I'm really, /really/ confused now.
:PROPERTIES:
:Score: 1
:DateUnix: 1434495719.0
:DateShort: 2015-Jun-17
:END:

************** This is just pretty typical transhumanist dinner party fodder. Nothing happening in this sub that I'm aware of. But calculations have been done elsewhere regarding post-singularity, cross-galaxy survivor parties. RSVPing gets messy for software minds and involves statistical predictions, and it's hard to set a date without FTL.

I've still got weird I haven't even used left but I think this might be getting off topic, even with the uploading-related nature of the discussion.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434507880.0
:DateShort: 2015-Jun-17
:END:

*************** I'm not sure if I really want to attend these dinner parties, or want everyone at those parties taken out and shot.
:PROPERTIES:
:Score: 1
:DateUnix: 1434508636.0
:DateShort: 2015-Jun-17
:END:

**************** Ah, so you are familiar with the people of whom I speak.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434520451.0
:DateShort: 2015-Jun-17
:END:

***************** No? I'm actually just a communist. We say that about people who try to treat large portions of the planet/universe as their exclusive personal stuff for reasons of ego.
:PROPERTIES:
:Score: 1
:DateUnix: 1434540274.0
:DateShort: 2015-Jun-17
:END:


********** u/deleted:
#+begin_quote
  People in general, including myself, only have instrumental value in my system.
#+end_quote

Then you need to specify a terminal goal so we can talk about which of your subgoals are well-planned.
:PROPERTIES:
:Score: 1
:DateUnix: 1434475459.0
:DateShort: 2015-Jun-16
:END:


***** u/deleted:
#+begin_quote
  This is the techno-altruism trap. Accept it and the value of the individual to society becomes zero. Anything can be justified by saying the new task schedule and resource allotment will benefit from the sacrifice of the old order. It is the path to one of the darkest possible future digital hells.
#+end_quote

You're mixing the factual question of /whether/ the individual continues (and in what form they might /prefer/ to continue or not, based on their full-information, full-rationality volition) with the moral question of whether we should sacrifice individuals for some greater good that somehow doesn't contain that person.

I'm uncertain about the first question, pending additional evidence, but return a strong "FUCK NO" on the second one.
:PROPERTIES:
:Score: 1
:DateUnix: 1434472035.0
:DateShort: 2015-Jun-16
:END:

****** You can have your own preferences as to the morality of sacrificing instances of individual intelligences for efficiency's sake, but it has been shown time and again that when the individual is expected to sacrifice a certain way in a private decisions, society at large quickly makes that choice mandatory to weed out defectors attempting to play hawk games with those personal costs. Complete loss of social standing and ostracization is the typically the lightest punishment for such infractions.

Continuity of physical substrate and the issue of individual identity is linked to this issue: if it is assumed individuals are nothing but informational resources, perfect copies are going to be just as good as originals and likely much better in many ways. Any optimizing social order will have members demanding specific horrific results because of that meme.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434493835.0
:DateShort: 2015-Jun-17
:END:

******* u/deleted:
#+begin_quote
  it has been shown time and again that when the individual is expected to sacrifice a certain way in a private decisions, society at large quickly makes that choice mandatory to weed out defectors attempting to play hawk games with those personal costs. Complete loss of social standing and ostracization is the typically the lightest punishment for such infractions.
#+end_quote

If I interpret you correctly, even mandatory military service is not on a level with, "Kill yourself now to help this other person." We do not routinely ask individuals to extinguish themselves entirely under almost all circumstances.

#+begin_quote
  Continuity of physical substrate and the issue of individual identity is linked to this issue: if it is assumed individuals are nothing but informational resources, perfect copies are going to be just as good as originals and likely much better in many ways. Any optimizing social order will have members demanding specific horrific results because of that meme.
#+end_quote

What sort of social order has started out by disregarding the volition of its own members? That doesn't sound like a very good one.
:PROPERTIES:
:Score: 2
:DateUnix: 1434496159.0
:DateShort: 2015-Jun-17
:END:

******** u/TimeLoopedPowerGamer:
#+begin_quote
  We do not routinely ask individuals to extinguish themselves entirely under almost all circumstances.
#+end_quote

True. But the western world and culture is less than a hundred years old, in current form. We're just getting to the point where resources get scarce and outside threats become scary enough to drive change. Sacrifices of lesser levels are still demanded by society for children and family, for ideologies, for religion. These are mostly of time and money, the level at which most civilians face risk nowadays. Religious ideas of sacrifice still permiate, with the idea of giving up life for the greater good just under the surface of popular culture even in secular nations.

Almost all empires have had ritual self-sacrifice in non-wartime situations. Several Chinese empires, Indo-European ones, several iterations of Roman ones, Native American unions, South American federations. All demanded intense personal sacrifices to improve the state of society in various ways, from elders walking into the wilderness in times of starvation, to ritual blood sacrifices, to suicides to prevent political retaliation on entire bloodlines (which seldom actually were honored or voluntary in any way). I don't see those social pressures and traditions as having been countered by any widespread post-enlightenment cultural movements. If anything, nationalism has made it worse.

So if society doesn't see it as extinguishing a life, and only moving information, voices against this sort of thing will be shouted down or at best intentionally misunderstood and ignored. It isn't like they're killing anyone, right?

#+begin_quote
  What sort of social order has started out by disregarding the volition of its own members?
#+end_quote

Every government ever. Even "pure" democracies limited the voting right to part of their group of individuals they claimed power over, and more modern ones reduce the volition of the individual and their direct effect on government policy to make sure society runs smoothly and panics don't cause huge swings in direction.

This murder-copying would be a demand on the level of mandatory school vaccinations, from the point of view of someone looking at self as information only. Not that vaccinations aren't a good thing, obviously.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434506836.0
:DateShort: 2015-Jun-17
:END:


**** u/deleted:
#+begin_quote
  Suppose you're going to die in 60 years if you don't get brain-uploaded now, and your brain-uploaded version gets to live 60 trillion years if you do.
#+end_quote

False dichotomy. If it's only possible to get yourself replaced when you've got most of your natural lifespan left, something perverse is going on.
:PROPERTIES:
:Score: 1
:DateUnix: 1434475337.0
:DateShort: 2015-Jun-16
:END:

***** Indeed, and that's why it isn't a good idea to sign up for cryonics and commit suicide now even if you're selfish, but I don't see how that changes the point. Feel free to divide both sides by whatever amount to make it seem reasonable: 1 hour versus 100 million years is fine by me.
:PROPERTIES:
:Author: philip1201
:Score: 1
:DateUnix: 1434495391.0
:DateShort: 2015-Jun-17
:END:

****** Well to me it's not about "how many hours", it's about "how much of my /natural/ lifespan as my /original/ self have I exhausted." You're positing a model in which there is only one way to extend the lifespan of anyone like me, and it involves a discontinuity that destroys one version of me and makes another. The second one can live quite long, but if we assume that /I just am/ the first one, then I should care about /my/ lifespan /as the original/, not out of any lack of "altruism" towards the next version, but precisely because if the two are separate individuals, then they're ethically equal and the one who's already vastly better-off should accept a trade-off to help the worse-off.
:PROPERTIES:
:Score: 1
:DateUnix: 1434495920.0
:DateShort: 2015-Jun-17
:END:

******* u/philip1201:
#+begin_quote
  "how much of my /natural/ lifespan as my /original/ self have I exhausted."
#+end_quote

What does the natural state of affairs have to do with anything?

#+begin_quote
  The second one can live quite long, but if we assume that I just am the first one, then I should care about my lifespan as the original, not out of any lack of "altruism" towards the next version, but precisely because if the two are separate individuals, then they're ethically equal and the one who's already vastly better-off should accept a trade-off to help the worse-off.
#+end_quote

I agree with (my interpretation of) everything up to "ethically equal". I don't see where the second conclusion comes from. Since the two are ethically equal, second per second, there is a trillion-to-one preference for uploading, which manages to get a trillion times as many seconds out of the same amount of resources.
:PROPERTIES:
:Author: philip1201
:Score: 1
:DateUnix: 1434505071.0
:DateShort: 2015-Jun-17
:END:


** To some extent it depends on your preferences. You are 100% totally allowed to prefer universes in which the pattern-that-is-you stays in the same physical format all the time.

It's interesting that this preference is only possible for people who care about the entire course a universe takes, rather than just snapshots in time - if you only care about the state of the universe at each given time, then it no longer makes sense to talk about an "original." But humans are not known for having simple desires.

I think the "what do you do if you're left behind" question is pretty trivial - just do what you would do anyhow. Like, eat a sandwich or something, and then go talk to friends or family. I'm a little worried about duplication on consequentialist grounds - I don't expect it to be a big benefit for me (not me now, nor either of future me), and it takes up resources and makes things a bit more complicated.
:PROPERTIES:
:Author: Charlie___
:Score: 2
:DateUnix: 1434404767.0
:DateShort: 2015-Jun-16
:END:


** What experiments can falsify different theories of continuity of identity?

Oh, you think this is "pure" philosophy? No, there's a cognitive structure implementing the measure-predicate that returns the extent to which any possible rebuild, upload, teleport-output, or clone /is you/. You need to know what that predicate says, and how it can be correctly updated to deal with new ontological models in which more abstract, intuitive concepts don't appear.

DEAL WITH IT.
:PROPERTIES:
:Score: 2
:DateUnix: 1434472168.0
:DateShort: 2015-Jun-16
:END:

*** Agreed. I hate the very idea of the precautionary principle or suggesting anything similar, but the risks involved in this idea demand at least some actual science to happen first. Some actual supporting evidence instead of string-theory-esque hand-waving and nonempirical hypothesis stacking.

Current neuroscience as I understand it falls squarely in the "physical substrate and its operations is self" camp. Which makes this a suicide booth, regardless of any informational copying process, as that substrate is destroyed.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 0
:DateUnix: 1434496050.0
:DateShort: 2015-Jun-17
:END:

**** I do not reject the axiom of identity.
:PROPERTIES:
:Score: 1
:DateUnix: 1435755403.0
:DateShort: 2015-Jul-01
:END:


** Yes, as long as it's safe: I don't get destroyed, I don't end up somewhere else (like, someone steals my information and create another copy of me somewhere else without leaving any trace, making them free to do whatever they wanted to me, because no one knows I exist there).
:PROPERTIES:
:Score: 2
:DateUnix: 1434497111.0
:DateShort: 2015-Jun-17
:END:


** I'm speaking as someone who is in the medical field and has studied neurology to the point where I can discuss the most common pathways and teach medical students.

There's a HUGE difference between 99.99999999999999% you and 100% you. That 10^{-Xth} difference may be all it takes between being you and someone who is depressed, or psychotic, or in chronic pain, or anything else that we consider to be purely mental.

We can't make a machine, label it 100% perfect, and expect flawless continuity.

Most likely, the first people who are teleported will have a stroke because you can't build organic structures that have grown with time the same way you'd program a 3D printer to try and replicate it, layer by layer.

Even if you get the physical structure correct, with all of the neurons in the correct locations, how are you going to replicate the exact same electrochemical gradient and internal packaging of neurotransmitters? The wrong number of sodium-potassium pumps or calcium channels will render dendrites ineffective and others will be reinforced instead, altering the delicate patterns of thought-stands we have woven.

Even if we get that right, how will we "know" that our "knowing" is true? Devastating neurological disorders can be accompanied with the brain's inability to notice that anything is wrong. Any attempts to convince them otherwise just increases agitation and fixation on the wrong idea. Examples: read about hemineglect in stroke patients, delusions for psychotic patients, confabulation for Wernicke-Korsakoff syndrome in alcoholics. [[https://en.m.wikipedia.org/wiki/Hemispatial_neglect]] [[http://en.wikipedia.org/wiki/Delusion]] [[http://en.wikipedia.org/wiki/Wernicke%E2%80%93Korsakoff_syndrome]]
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 6
:DateUnix: 1434414828.0
:DateShort: 2015-Jun-16
:END:

*** u/FeepingCreature:
#+begin_quote
  99.99999999999999%
#+end_quote

Do you actually mean that or did you just pull a number from a hat?

I'd be willing to bet most people who are 99.99999999999999% me are indistinguishable.

#+begin_quote
  how are you going to replicate the exact same electrochemical gradient and internal packaging of neurotransmitters?
#+end_quote

Really carefully. Come on, it's a thought experiment.

#+begin_quote
  Devastating neurological disorders can be accompanied with the brain's inability to notice that anything is wrong.
#+end_quote

It's harder to conceive of a devastating neurological disorder that is impossible to recognize from the /outside/. Which is amusing, certainly, considering philosophy's focus on the vaunted "internal experience".
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1434418220.0
:DateShort: 2015-Jun-16
:END:

**** We could say that our nearest simian ancestor is 99.99999999% identical to us (or pick any relative who shares your DNA who isn't actually you.) My point is that there's an illusion of precision that I wouldn't risk. That 0.000000001% difference as it turns out, IS ME.

Just because I'm thinking about a thought experiment's flaw in defining it "perfectly" and "100%" doesn't mean I'm not willing to engage in it.

I mean, it'd be fun to teleport, but if the mechanism involved a computer scanning me and them disintegrating a version of me, I wouldn't do it. No matter how small the error bars may be, it's not worth it... in a mundane scenario. If the Earth were exploding and it's the only method of escape and everyone is using it and it's been observed by me to be safe, then I won't be an old die-hard about the risks.

[edit:] after some more consideration, I realized that we are disputing definitions. [[http://lesswrong.com/lw/np/disputing_definitions/]] I'm pushing back against the simplistic nature of the thought experiment and breaking it down into some of the practical components that CS/EE majors may not realize about how our wetware differs from the circuitry of certainty. You're asking if I had an exact clone of myself, would that be me. Indisputably, yes. I recognize how our fundamental building blocks of mass and energy are just orderly patterns that "magically" could be duplicated and teleported and I would be both versions of me in two different places. I do study the pathology of people for a living, so forgive me if I just expect such a device no matter how well-intentioned, to go horribly wrong in real life. (But who knows, we used to think that humans couldn't live in space or fly too high in the sky because science!danger! And that must be a bias of mine, to assume risks where they may not exist.)
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 6
:DateUnix: 1434425011.0
:DateShort: 2015-Jun-16
:END:

***** Is the 99.99999999% figure being applied to the entire physical system (including bones, blood, muscles, fat, digestive system, etc) or to the brain (the only bit I personally care about for the purposes of this discussion?) Because, unless I'm mistaken, the brain contains 100 billion neurons and 99.99999999% of 100 billion is 10. I can't find the numbers but I'd be willing to bet that's fewer than is lost in the average concussion. That being the case, I'd be far more comfortable with an upload of six sigma fidelity than a baseball bat to the head.
:PROPERTIES:
:Author: gryfft
:Score: 3
:DateUnix: 1434469630.0
:DateShort: 2015-Jun-16
:END:


***** u/deleted:
#+begin_quote
  I do study the pathology of people for a living, so forgive me if I just expect such a device no matter how well-intentioned, to go horribly wrong in real life. (But who knows, we used to think that humans couldn't live in space or fly too high in the sky because science!danger! And that must be a bias of mine, to assume risks where they may not exist.)
#+end_quote

Essentially, before operating a teleporter, we should do mice experiments or something to figure out what the risks are, and where we can put the safety rails to minimize them.
:PROPERTIES:
:Score: 3
:DateUnix: 1434475021.0
:DateShort: 2015-Jun-16
:END:


***** u/IWantUsToMerge:
#+begin_quote
  Nearest simian ancestor 99.99999999%
#+end_quote

I don't know about our ancestors but if you're talking about bonobo the number is 98.7%, which means you'd be off by a factor of 1.3 billion. That is very very wrong. 0.000000001% /actually ain't shit/ on the scales of the genome or the brain. Although I imagine you could do a fair bit of damage if you were able to change 6 specific codons in the human genome, 6 random ones? Really? You think that would do something?
:PROPERTIES:
:Author: IWantUsToMerge
:Score: 3
:DateUnix: 1434581417.0
:DateShort: 2015-Jun-18
:END:

****** I made up a number to make a point and maybe I should've been more precise in keeping it on at least a micro or nano level rather than a pico/femto level of precision. If we consider "6 changes" to the genome in each cell in our body as well as disruption/displacement of our microtubules, microfilaments, DNA strands... as well as other connective tissues... then yes, six random changes per cell in my body amount to significant amounts of changes.

If we equate it with milliREM, I would tolerate a transporter machine that equates to levels similar to background radiation or an airplane trip around the world. Much more than that would expose me to too much stochastic risk.

[[http://www.epa.gov/radiation/understand/health_effects.html]]
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 3
:DateUnix: 1434858303.0
:DateShort: 2015-Jun-21
:END:


*** How different are we from our 10 year old version? Probably more than that.
:PROPERTIES:
:Score: 3
:DateUnix: 1434495917.0
:DateShort: 2015-Jun-17
:END:


** James Patrick Kelly, /Think Like a Dinosaur/.

Greg Egan, just about anything before the Clockwork Rocket series.

John Barnes, /A Million Open Doors/ and sequels.

Vernor Vinge, /Just Peace/, where you don't destroy the original.

There's another novel involving a duplicative teleportation with a title that's "something star" by blish or aldiss or hoyle or one of those guys that I can't recall.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1434391784.0
:DateShort: 2015-Jun-15
:END:

*** Greg Egan wrote a lot of good stuff exploring what it means to be you. Things like the consequences to cloning your consciousness and then deleting the clone (what if you end up as the clone), what happens when your digital consciousness is being copied (it goes through states where it is partially copied), or what happens when you are replaced by a computer brain that thinks exactly like you (or maybe not quite exactly).
:PROPERTIES:
:Author: ben_sphynx
:Score: 1
:DateUnix: 1434392496.0
:DateShort: 2015-Jun-15
:END:

**** Black Mirror has an episode [[#s][that does this beautifully too.]]

[[#s][Speaking of which,]]
:PROPERTIES:
:Author: notmy2ndopinion
:Score: 1
:DateUnix: 1434415532.0
:DateShort: 2015-Jun-16
:END:

***** That episode of Black Mirror is my favorite episode of anything ever. Dear lord. So much technoterror. /shudders away from computer/
:PROPERTIES:
:Score: 1
:DateUnix: 1434417356.0
:DateShort: 2015-Jun-16
:END:


** I think that the book "The Golden Age" handled this relatively well. And lightly bridges a concept I have been thinking about. In the book each person has a prime with seconds, thirds etcetera which are copies of the prime. Seconds are effectively you and hold a trusted position but do not technically share your property rights. Although you are expected/required to support your clones.

My belief is that the concept of person that we use today is a special case due to the nature of human physiology. Continuity of consciousness is considered important because humans see it as the de facto standard when in fact we really only have the appearance of continuity as it is.

I think a hydra is a good representation of what the timelines of consciousness in a transhumance are like or something like git if you are familiar with version control. Where there are multiple conscious branches of each person which can merge back together and believe that merging back together is a normal behavior. If you wanted to visit another solar system you could send a radio message of you mind to Alpha Centauri, have that "clone" of yourself adventure there and when he got bored he would "push" the changes that occurred in his mind back to the original head. Then the clone in alpha centauri could chose to shut himself down or continue on in alpha centauri while pushing updates back home periodically. On Earth you would send a radio message to alpha centauri and several years later get a message back with your adventures. The original mind would then look over the memories and evaluate the evolution in the clone's belief system and keep whatever it wanted.

My answer is that both the original and the clone are "me". A person is not an unbroken stream of consciousness but a collection of memories, choices, and beliefs colored by its choice of operating system or means of embodiment.

I have been thinking on this for a while, and thoughts on it would be useful to expand my thinking.
:PROPERTIES:
:Author: hangleader
:Score: 2
:DateUnix: 1434429499.0
:DateShort: 2015-Jun-16
:END:


** Yes, I probably use it (once it's been established as safe). I'd also be fine if they scanned my brain onto a computer. I'm not sure what you mean by "the consciousness left behind".
:PROPERTIES:
:Author: protagnostic
:Score: 1
:DateUnix: 1434441621.0
:DateShort: 2015-Jun-16
:END:

*** u/MugaSofer:
#+begin_quote
  I'm not sure what you mean by "the consciousness left behind".
#+end_quote

If you're scanned into a computer, but not /destructively/ scanned - the original is still intact.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1434469316.0
:DateShort: 2015-Jun-16
:END:


** Honestly, I'd probably be a little leery of it until people had used it with no clear problems, then would avoid thinking about the issue. It'd get easier as the visceral convenience will eventually trump the abstract fear.
:PROPERTIES:
:Author: ancientcampus
:Score: 1
:DateUnix: 1434740790.0
:DateShort: 2015-Jun-19
:END:


** u/deleted:
#+begin_quote
  Do you use this teleporter?
#+end_quote

If it is cheap to use and I have need to travel. I don't often fly on a plane either.

#+begin_quote
  What if they scanned your brain onto a computer?
#+end_quote

I am now willing to pay all my resources to have a copy preserved.

#+begin_quote
  What do you do if you're the consciousness left behind?
#+end_quote

Cooperate with myself to my benefit as a single entity with twice the agency and moral weight of others who have not traveled. Work more jobs to be able to continue the process. Pool my resources to live more efficiently.
:PROPERTIES:
:Score: 1
:DateUnix: 1435755223.0
:DateShort: 2015-Jul-01
:END:


** I would not use the teleporter at all.\\
As far as I know consciousness arises from the electrochemical processes in the brain. You exist in your brain. If your brain's structure is uploaded to a computer your consciousness won't magically transfer over. There's no magical soul that's tied to the data encoded in your brain.

Although I suppose I might do it if I'm on my deathbed and there's no way tonaviod death. Why not have a nice doppelganger who can survive you?
:PROPERTIES:
:Author: okaycat
:Score: 0
:DateUnix: 1434406083.0
:DateShort: 2015-Jun-16
:END:

*** Well, that's the whole point, that there's no magical soul. If there's no magical soul, then you must be the processes in your brain. If you are the processes in your brain, then a computer that perfectly simulates those processes would have identical properties and therefore also be you.
:PROPERTIES:
:Author: SublimeMachine
:Score: 4
:DateUnix: 1434412116.0
:DateShort: 2015-Jun-16
:END:

**** u/TimeLoopedPowerGamer:
#+begin_quote
  a computer that perfectly simulates those processes would have identical properties and therefore also be you.
#+end_quote

Only if you define /you/ as "that which functions the same as you" and ignore the casual history of the matter making up your brain. Which would be a really inaccurate and reductionist thing to do when discussing or modeling a real-world system.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 0
:DateUnix: 1434420137.0
:DateShort: 2015-Jun-16
:END:

***** Well obviously we would arrange the simulation to match the same initial state as the processes in your brain were at the time of upload/transfer. For example if we were copying a computer, we wouldn't just make an identical micro-chip, we would start it up running the same processes with the same memory state (otherwise I wouldn't consider it a perfect copy).
:PROPERTIES:
:Author: SublimeMachine
:Score: 5
:DateUnix: 1434421106.0
:DateShort: 2015-Jun-16
:END:

****** And if it destroyed the original, it is still killing /that person/. That the universe gets a copy, even a perfect copy, is not rationally defensible without exceptional circumstances, or the deepest and most selfishly twisted world views as to the value of an individual.

You're arguing the wrong points. I am a very strong transhumanist. I believe in software minds, uploading, and AI-as-functional-consciousness. I also believe in the value of individual consciousnesses.
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434421443.0
:DateShort: 2015-Jun-16
:END:

******* I guess different viewpoints on what constitutes an individual consciousness. I don't see "continuity" as being a meaningful metric. Every moment the current me ceases to exist and is replaced by a future me, but since we are nearly identical, I'm alright with that, and don't consider it "dying". Its just like if you (carefully) froze your brain and then thawed it. You'd still be alive.
:PROPERTIES:
:Author: SublimeMachine
:Score: 6
:DateUnix: 1434422286.0
:DateShort: 2015-Jun-16
:END:


*** u/deleted:
#+begin_quote
  There's no magical soul that's tied to the data encoded in your brain.
#+end_quote

There's no magical soul that's tied to the specific matter of your brain.

Consciousness emerges from the way your brain works. It doesn't matter if it emerges from your brain, or from somewhere else where the way your brain works is being replicated.
:PROPERTIES:
:Score: 2
:DateUnix: 1434496287.0
:DateShort: 2015-Jun-17
:END:


*** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1434434324.0
:DateShort: 2015-Jun-16
:END:

**** Because I love living. I have a personal subjective experience created by my btain, that is who I am. It doesn't matter if my brain is being simulated by a computer somewhere, an identical copy of my consciousness will be created.

I suppose for everyone else that won't matter; they won't see any difference. But on the inside there is a difference. My subjective experience would end when I'm disingetrated or whatever. I'll die. What's the point of immortality when you'd don't even get to experience it?
:PROPERTIES:
:Author: okaycat
:Score: 1
:DateUnix: 1434467001.0
:DateShort: 2015-Jun-16
:END:

***** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1434470506.0
:DateShort: 2015-Jun-16
:END:

****** Don't know about them, but I'd have to be forcibly restrained from doing nothing but use it in that scenario.

[[https://www.youtube.com/watch?v=mV2075vV_Dw][Me, me, me.]]
:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 0
:DateUnix: 1434495475.0
:DateShort: 2015-Jun-17
:END:


** If the universe is infinitely large, I'm guessing there's probably infinite 100% identical copies of me out there, although they'd almost certainly be very VERY far away. Does that mean I'm okay with dying as long as one of those copies survives?

HELL NO.

Am I okay with a form of teleportation in which a computer scans me, utterly annihilates me, then sends the 100% identical-to-me scan of me somewhere else to be replicated back into real life?

HELL NO, for exactly the same reason.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 0
:DateUnix: 1434494314.0
:DateShort: 2015-Jun-17
:END:
