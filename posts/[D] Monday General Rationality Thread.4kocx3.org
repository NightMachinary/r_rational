#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 16
:DateUnix: 1464015856.0
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** This [[http://www.vox.com/2016/5/19/11683274/aphantasia][article]] on aphantasia was interesting reading to me. I know, intellectually, that a lot of people think and interact with the world differently from the way I do. Reading about someone whose mind works in such an unusual way is an interesting experience, and has me wondering: what parts of my own thinking are as strange and I haven't noticed it, because I assume all people think like this?
:PROPERTIES:
:Author: blazinghand
:Score: 10
:DateUnix: 1464030509.0
:END:

*** This article made me aware that the quality of my visualisation is severely lacking. I visualise things in very low detail, and often there's an awareness of a visual fact more than there is any sensation of specifics.

To put it another way, when I visualise something it feels like I've seen a blurry picture of it and there's somebody with corrective lenses explaining the finer detail to me.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 4
:DateUnix: 1464089923.0
:END:


*** There are lesswrong and slatestarcodex articles about just that, with the Most interesting comments. Might be able to Google them.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 3
:DateUnix: 1464032428.0
:END:

**** Found it! [[http://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it/]]
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 5
:DateUnix: 1464067572.0
:END:


*** Really interesting. I have to imagine there's somewhat of a spectrum though.

I can definitely visualize a beach, but I wouldn't exactly describe it as anything like looking at something and I'd be hard pressed to say it was in color at least not unless I specifically tried to think of colors. I also don't seem to ever really have visual dreams, though that might just be the way I remember in general.
:PROPERTIES:
:Author: space_fountain
:Score: 3
:DateUnix: 1464062493.0
:END:


*** Wow. I just learned about a whole new type of "disability," and it fills me with a vague sense of horror to contemplate living that way. I've never felt like such an ableist, because I'd rather be color blind than have aphantasia, even though I know intellectually that it can't be that bad... clearly the writer of that article is able to live a fruitful, fulfilling life...

But the thought of losing/not having my ability to imagine things, of /not being able to experience the things I read in fiction,/ or daydream, or dream visually at all... that's taking away such an integral part of my being that I find my mind shying away from contemplating it.
:PROPERTIES:
:Author: DaystarEld
:Score: 2
:DateUnix: 1464058510.0
:END:

**** Eh, I think you just have a misconception of what this actually means. Just because I don't visualize things doesn't mean I can't imagine the things I read in fiction, or daydream. It just means those activities skip the visual layers, and jump directly to the processing level. So I don't 'see' things but I still feel things similar to what you feel when daydreaming. Just without the visual impressions.
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1464118713.0
:END:

***** I don't wish to make you feel bad about your experience of the world. If you say you "just skip the visual layers," and that nothing substantial is lost from that (or the loss of the other senses in memory/imagination), then I'm okay with just accepting that for the sake of politeness. But in the back of my mind, it's like hearing someone with complete color blindness insist that they can enjoy a painting just as well as anyone else. Intellectually I'm glad to hear it, but deep down it rings false.

Perhaps that's just my bias, for now, or a lack of understanding. In any case it's clearly not as important as I would have thought before learning this was a thing.
:PROPERTIES:
:Author: DaystarEld
:Score: 3
:DateUnix: 1464119969.0
:END:

****** On the other hand I just imagined myself losing the ability to do a similar thing with sounds, and I do get a similar sense of horror and loss as you mentioned. Obviously amplified by general aversion to change, but still.

And someone claiming they can enjoy imagining music without actually being able to hear it I would treat with similar polite scepticism.

So hey, maybe I am missing something valuable; not much that can be done about it though.
:PROPERTIES:
:Author: Anderkent
:Score: 6
:DateUnix: 1464131864.0
:END:

******* Yeah : / I'm sure there are other more specialized skills that I myself am missing and just don't feel the lack of because they're not as "common" though.

For example, I've always wondered what it was like to compose music: I can read notes and even memorize songs, but feel utterly incapable of creating new ones. I've tried before and it's like reaching my hand into a void, like imagining a new color. I can't even imagine what it might be like to do so. Someone very musically inclined who writes songs would probably feel a similar sense of horror at contemplating my inability to do what they can.
:PROPERTIES:
:Author: DaystarEld
:Score: 2
:DateUnix: 1464195119.0
:END:


****** Hm, well, this is generally hard to describe because I definitely don't have specialized vocabulary. But for example I have the same internal experience viewing a painting in real life and remembering viewing that painting, in terms of what mood it puts me in and what it makes me feel. But I don't 'see' the painting, the visual aspect is removed, and I couldn't for example notice something I didn't notice before. (is that a thing that happens to people who can visualize things? Or is the imaginary visual experience lower fidelity, and doesn't have any things you haven't noticed previously? I'd be suspicious of making things up there though...)

So unless we perceive things differently in general, not just in imagination, I think this can't have that much impact.

And imagination works similar to recall, except the thing you're recalling didn't actually happen.
:PROPERTIES:
:Author: Anderkent
:Score: 2
:DateUnix: 1464131639.0
:END:

******* #+begin_quote
  (is that a thing that happens to people who can visualize things? Or is the imaginary visual experience lower fidelity, and doesn't have any things you haven't noticed previously? I'd be suspicious of making things up there though...)
#+end_quote

This is a tricky question, as it's possible to describe the details of a painting from memory with increased fidelity the more you concentrate on doing so, but there's also a risk of suggestibility (if someone prompts you to remember it a different way) and natural error. It usually would sound something like this:

The Mona Lisa is a lady looking at the viewer while smiling slightly with folded hands.

/imagines Mona Lisa for two minutes/ Okay, so the Mona Lisa is a lady looking at the viewer, with a slight smile on her face and her hands crossed in her lap... she has dark hair and dark clothing, with a splash of color at the chest, either a different piece of clothing under her dress or bare skin, can't remember. The background... I want to say earthy? Like she's in the woods, or a cabin or something, something behind her has dark brown colors, the image keeps shifting in my head between a wooden wall and actual trees right behind her.

/googles Mona Lisa/ Lady, smiling, dark hair and dress, bare chest-area, hands folded, woods in the background but farther back than I remembered, with a winding path closer to her and what looks like mountains in the far distance, which I completely failed to remember.

Memory doesn't bring up perfect images, but impressions can still form an image in new or familiar ways, and cross checking with others can be used to either increase fidelity ("Was there a bird?" "I don't think so, that doesn't look right." "I think there were trees in the distance." "Maybe? Yeah, that looks familiar...") or introduce error ("I'm sure that she's in a building." "Yeah, I think you're right, it was a building... maybe with a window?")
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1464196042.0
:END:


*** As an aside, I'm one of the people with no real visualization capability.

The specifics are a bit different then what appears in the article, I generally have a pretty good sense of direction, as an example.

I noted it about myself before that article made the rounds.

But it does bring up the point that I think I might be slowly losing the ability to do the same with sounds.
:PROPERTIES:
:Author: traverseda
:Score: 2
:DateUnix: 1464085669.0
:END:


** Why don't animals more frequently lie? This isn't (intended to be) a philosophical question. I was observing my cats recently, and was thinking about how the animals I've come into contact with never seem to intentionally miscommunicate with their body language or otherwise. It'd... seem to be evolutionarily adaptive, so is it something unique to the higher-intellect species (ie dolphins, crows, etc.), or is there something I'm missing?
:PROPERTIES:
:Author: Cariyaga
:Score: 3
:DateUnix: 1464085781.0
:END:

*** Do you mean lie "actively", "consciously", and "intentionally", or just implicitly, by way of, for example, appearance? There are lots of documented cases of the latter in various animals (e.g. see [[https://en.wikipedia.org/wiki/Deception_in_animals][here]]), as well as in, like, [[http://www.sciencedirect.com/science/article/pii/S0169534709002055][plants]]. I dunno of any sweeping reviews of dishonest signalling in the recent literature, but [[https://books.google.com/books?hl=en&lr=&id=OTVN2pNTaDgC&oi=fnd&pg=PA127&dq=animals+deception+review&ots=7qqbl0MaUr&sig=sKwYeXN5pgaxkyP3p08m1XeGzpo#v=onepage&q=animals%20deception%20review&f=false][this]] looks to be an ok book chapter after a short skim. If you have a favorite system or taxon or w/e you could probably just google scholar it and see if anything pops up.
:PROPERTIES:
:Author: captainNematode
:Score: 3
:DateUnix: 1464115129.0
:END:


*** [deleted]
:PROPERTIES:
:Score: 3
:DateUnix: 1464104190.0
:END:

**** This happens a lot in nature. Some venomous snakes are brightly colored. Some normal snakes have coloration similar to venomous snakes, and get some protection that way.
:PROPERTIES:
:Author: blazinghand
:Score: 1
:DateUnix: 1464110301.0
:END:


*** I think its like skyshayde described and also may be related to their difficulty connecting cause and effect across long temporal periods. Its said for example that dogs in training need to be punished or rewarded as they're commiting the deed so that their brains can associate the action with the desired outcome.

If they are punished or rewarded afterwards, they only get confused as the cause since they can't associate having commited a deed some time in the past and the reward/punishment they are receiving in the now.

There's a big chance most animals can't correlate things either and as such won't consider creating more complex lying actions beyond the immediate fight/flight impulse.
:PROPERTIES:
:Author: Faust91x
:Score: 1
:DateUnix: 1464106162.0
:END:


*** You ought to take that one to askscience. It's a question of honest signaling, and when and how that should be used.

As for why you don't see dishonesty in body language and the like...remember, none of these animals really have a theory of mind, so what you see them doing is a more instinctive sort of behavior, mixed in with learned responses to the environment. Regardless, it's under selection. If dishonest signaling isn't advantageous, it's obviously not going to be selected for. If it /is/ advantageous (for example, false bluffs about size during combat), it will be selected for....but then those organisms recieving the signal will be selected to ignore it. And an ignored signal is just a waste of energy. So long term, honest signals persist while dishonest ones do not.

Of course there are lots of instances where dishonest signaling persists because no practical way to detect it has evolved, or various other reasons.
:PROPERTIES:
:Author: atomfullerene
:Score: 1
:DateUnix: 1464142162.0
:END:


** Just came in over the extropy mailing list, it might be that Dark Matter mystery is solved. Primordial (directly from the big bang) Black Holes in the correct mass range of 10<X<100.

Basically <10 and >100 solar masses (SM) Black Holes have been ruled out otherwise. Remember the big press in february? "LIGO detects finally existence of gravitational waves" - a two BH merger 29SM+36SM. Right dab in the middle of the necessary band.

And the kicker? Nobody expected that observation so fast after LIGO went online - could be coincidence but is also evidence that such mergers happen much more often than expected, eg. there are much more BHs in the correct range.

And heres the paper: [[https://iopscience.iop.org/article/10.3847/2041-8205/823/2/L25]]
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 3
:DateUnix: 1464209442.0
:END:


** There is one thing about the Teleporter Problem, that I don't understand and maybe someone can help me with that.

In the Teleporter Problem we have a hypothetical teleporter machine, that works by scanning your body down to some arbritrary scale (let's say atoms), disassembling your body in the progress and then reassembling you from different atoms at the target location.

There are variants of this, without the disassembly or sending your atoms to the location at near-lightspeed and so on. But I guess the base variant is enough here.

Now, if we apply different theories of identity to this problem, we might get as result, that this machine does not in fact teleport you, but kills you and creates a copy at the other end. With other theories, everything is a-okay and you can enjoy your day trip to Mars.

The thing I now don't understand: How could we possibly know which theory of identity is correct?

It might be that the "correct" answer is subjective and we can choose any theory we like. Yay, death-free teleportation!

It might also be, that there is an objectively correct theory of identity, but I'm hard pressed to come up with even a hypothetical experiment that could test this. And given the lack of Noble Prices for presenting a correct theory of identity, I doubt someone else has.

So, what? How can we try to resolve this? The Teleporter Problem itself has reached broad audiences but any video/article/whatever I've seen conveniently skipped the part about deciding which theory of identity to use.
:PROPERTIES:
:Author: Qwertzcrystal
:Score: 6
:DateUnix: 1464017432.0
:END:

*** [[http://lesswrong.com/lw/of/dissolving_the_question/]]

Or, to put in another way, does the world work differently if different theories of identity are correct? What would you expect to change, depending on which one is right?

Nothing. "Theory of identity" isn't a prediction about reality, it's not epistemic rationality. It's instrumental rationality, it's a question of how /you/ should behave, and you need to answer it like it's a question of how you should behave.
:PROPERTIES:
:Author: traverseda
:Score: 11
:DateUnix: 1464025406.0
:END:

**** Well it /is/ a prediction of future subjective experience so it certainly does relate to experience, even if it would be potentially something you could only test once, and would be subsequently unable to tell the results to others.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1464041254.0
:END:

***** Huh? How would your subjective experience be different if a different theory was correct? Explain what you expect to see.
:PROPERTIES:
:Author: traverseda
:Score: 3
:DateUnix: 1464041446.0
:END:

****** Well in one case your experience just ends and in the other it doesn't.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1464042907.0
:END:

******* The one whose experience ended would be unable to express that, while the one whose experience just began would have no evidence to that effect.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1464088689.0
:END:

******** Yes it would be impossible as far as I know to actually transmit that information to somewhere else.\\
If you retain continuity when you are "teleported" then you will experience that, however if you don't then no-one can tell, because the copy of you will have false memories making them think that they experienced prior events.

Basically this scenario is kind of like last thursdayism, yes it's basically impossible to know one way or the other, but that doesn't mean there isn't an answer, just that you can't know definitively.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1464093335.0
:END:


******* I don't follow. What, exactly, do you expect to see?
:PROPERTIES:
:Author: traverseda
:Score: 1
:DateUnix: 1464087457.0
:END:


*** I'm not sure of the answer but I wanted to point out something I think is important.

You used the word Reassembled. This word is already loaded with an assumption that conserves identity across the timeline of the teleportation, implying causality in a way.

I think it would be more accurate to say a person is disassembled, then, a person is assembled.
:PROPERTIES:
:Author: PL_TOC
:Score: 10
:DateUnix: 1464018574.0
:END:

**** I was assuming the dis-/reassembly refers to the structure of matter within your body. But you make a good point in that a theory of identity, according to which the structure is relevant to the identity, must take this into account.
:PROPERTIES:
:Author: Qwertzcrystal
:Score: 2
:DateUnix: 1464019307.0
:END:

***** Yes. I don't expect that if this perfect copy existed that my experience of myself would somehow bloom to incorporate both perspectives. So I think it would be a mental clone at best.
:PROPERTIES:
:Author: PL_TOC
:Score: 3
:DateUnix: 1464022020.0
:END:


*** This is always been something that bothered me about the idea of uploading and copying a person's mind, how do we know how this will effect their sense of identity? One exploration of this idea I REALLY liked was in the webcomic El Goonish Shive: One character was permanently split into two people and they ended up identifying as two different people with distinct personalities despite their shared memories. One of them decided they weren't the original, they were a new person that came into existence during the split. Not that it wasn't really difficult to accept, but it worked out.
:PROPERTIES:
:Author: trekie140
:Score: 9
:DateUnix: 1464018802.0
:END:

**** One could argue their new perspective on their identity was already given by the beliefs of the character before the split. But I agree that having decided on a theory of identity is one thing, but actually being in a situation where that's relevant is another. I think I would react in the same way as the character, but I don't really know that for sure. I can imagine changing my mind quite fast when suddenly seeing a person that looks exactly like me.
:PROPERTIES:
:Author: Qwertzcrystal
:Score: 2
:DateUnix: 1464019952.0
:END:

***** SPOILERS AHEAD FOR A STORY I HIGHLY RECOMMEND

A significant fact to take into account is that the two did look different. In fact, the "clone" was a different gender than the "original". It could have been a pragmatic decision to think of them as different people, and it had been a very intense couple of days when they did, but over the course of the story they both believably found happiness and self acceptance.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1464020676.0
:END:

****** I mean, the bonus dream lifetime created explicitly to diverge them a bit might've helped smooth things over a bit too.
:PROPERTIES:
:Author: gabbalis
:Score: 1
:DateUnix: 1464025002.0
:END:

******* It did help her adjust by giving her some memories to call her own, and were given to her by for that reason, but she ended up deciding that her "dream self" was still a different person that her when she discovered the differences in their sexuality. So the dream did help her become her own person, but by helping her separate her identity from memories that she didn't make herself while still accepting them as part of her life.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1464040714.0
:END:

******** Hmm, well yes that's a very cogent point and I might have to concede that my memory of the series was slightly inadequa- HEY, IS THAT A DEMONIC DUCK OF SOME SORT!? *flees*
:PROPERTIES:
:Author: gabbalis
:Score: 2
:DateUnix: 1464092869.0
:END:

********* IN JOKES!!
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1464099967.0
:END:


***** I forgot to explain how the person reacted. The split was accidental and the original's immediate response was to try to comfort the confused panicking girl claiming to be him. He was far more worried about how long she would live if her body was artificial, and even claimed to be the clone to protect her without a second thought. He didn't care which of them was which, he just acted like they were both their own person and she needed help more than he did.
:PROPERTIES:
:Author: trekie140
:Score: 1
:DateUnix: 1464043914.0
:END:


*** There is no "correct" theory of identity. It's purely a categorization problem, like [[https://blog.xkcd.com/2010/05/03/color-survey-results/]["is #e03803 red or orange?"]] or else [[http://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/]["Is a whale a fish?"]]. The only question left is which theory produces useful results.
:PROPERTIES:
:Author: ulyssessword
:Score: 10
:DateUnix: 1464020510.0
:END:

**** There's at least one theory where I fully survive the teleport. That's pretty useful, so I'll be going with that one?
:PROPERTIES:
:Author: Qwertzcrystal
:Score: 1
:DateUnix: 1464023452.0
:END:

***** No! [[https://wiki.lesswrong.com/wiki/Litany_of_Gendlin][Litany of Gendlin]]!

#+begin_example
  What is true is already so.
  Owning up to it doesn't make it worse.
  Not being open about it doesn't make it go away.
  And because it's true, it is what is there to be interacted with.
  Anything untrue isn't there to be lived.
  People can stand what is true,
  for they are already enduring it.
#+end_example

[[https://wiki.lesswrong.com/wiki/Litany_of_Tarski][Litany of Tarski]]!

#+begin_example
  If I* survive the teleport,
  I desire to believe that I* survive the teleport;
  If I* do not survive the teleport,
  I desire to believe that I* do not survive the teleport;
  Let me not become attached to beliefs I may not want.
#+end_example

Epistemic rationality must pursue truth above all else, or it cannot be useful! Giving up truth for utility is a /very unsafe/ area of instrumental rationality, and likely to be bad for your health (by, eg., inducing suicide-via-teleport).
:PROPERTIES:
:Author: PeridexisErrant
:Score: 5
:DateUnix: 1464053137.0
:END:

****** Yes, I was being a bit facetious. Just picking whatever theory I like is a horrible idea. That leads back to my original question: How could we even know?

If identity is really just a categorization problem, then there is no right and no wrong answer and we're back at "pick what you like". If there is a kernel of objectivity somewhere, then we can talk about weeding out the obviously wrong ones.
:PROPERTIES:
:Author: Qwertzcrystal
:Score: 2
:DateUnix: 1464075210.0
:END:

******* See the thing is I think there's an important distinction between problems without an answer, and problems where we can't ever /know definitively/ what the answer might be.\\
Basically I think the teleporter problem is kind of like last thursdayism, I am qute confident there /is/ an answer, even if we can't know it.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1464093574.0
:END:


*** See the problem is it still deals with a situation that has only one answer. Either your experience ends from your perspective or it continues, and your feelings on the matter should have no affect on the outcome so picking the option most pleasing to you is a horrible idea.

Obviously few people here are going to seriously suggest that it matters whether the copy is made of the same material as you. I think it can be similarly argued, that it also doesn't matter whether the scan is destructive or not, since that shouldn't affect whether the copy is you or not.

A transporter that doesn't disassemble you, and just scans you and makes a copy of you on the other side is the same except it doesn't disassemble you. So I can't imagine how you would argue that the person on the other side is you in one scenario but not another.

I think a lot of confusion arises when people fail to distinguish between different definitions of "/you/" for instance if you only care about your personality persisting then amnesia is death, but similarly if you believe a multiverse probably exists then you shouldn't fear death since there will nearly certainly be exact copies of you who didn't die.

I can't seem to really find any remotely satisfactory solution to identity except that you are simply defined by your continuous mental process, and should that ever cease you would die. To preempt a common response (though whether it's scary has no bearing on it's validity), I don't think sleep means death. I used to suspect it might, however I now think some experience almost certainly happens during sleep but you just don't generally remember it.

For instance plenty of people don't remember ever having a dream, however we know that dreams are universal. We also know that people have dreams during non-REM sleep, but few remember them because they are less vivid and disjointed, often replaying recent experiences.

So we already know from this that we have massive chunks of our experience that we are unaware of, so I can't be so sure that any part of sleep is really a true cessation of experience, after all you get a sense of time having passed whenever you sleep as opposed to anesthesia where it feels like you just skipped forward in time.

On a personal note I can't really deny that I vaguely experience things during all of my sleep, because I can remember the vague sort of thoughtless experience of deep sleep, the more relaxed and incoherent it is the more unpleasant it is if you are woken up.
:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1464040323.0
:END:


*** #+begin_quote
  The thing I now don't understand: How could we possibly know which theory of identity is correct?
#+end_quote

This is not a question of objective fact, but a question of categorisation and values. Such are usually confirmed or busted by revealed preferences.

Which theory of identity is more useful to you, in the 'provides most value / least distress' metric?

(and why do you insist on killing the poor non-teleporting sap? Live and let live!)
:PROPERTIES:
:Author: Anderkent
:Score: 5
:DateUnix: 1464022233.0
:END:

**** I'm all for linking them together with a Transdimentional Brain Chip type connection. That way you can be both selves at once!
:PROPERTIES:
:Author: gabbalis
:Score: 1
:DateUnix: 1464025700.0
:END:


**** #+begin_quote
  (and why do you insist on killing the poor non-teleporting sap? Live and let live!)
#+end_quote

That's really just population control. Can you imagine the logistical problems when a new person is created every time we use the teleporter? You're sitting at Christmas dinner and there's the copy that became a painter across the table, squabbling with the copy that became an engineer, being admonished by the two copies of your mother that host a twin cooking show.
:PROPERTIES:
:Author: Qwertzcrystal
:Score: 1
:DateUnix: 1464076008.0
:END:

***** Well yes but at least you haven't created a mass murder machine so that's the upside.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1464093647.0
:END:


*** [deleted]
:PROPERTIES:
:Score: 2
:DateUnix: 1464103562.0
:END:

**** Yep, branch/merge kind of thing is the one that both sidesteps destroying the original and allows to keep instances of self from diverging too far.
:PROPERTIES:
:Author: eternal-potato
:Score: 1
:DateUnix: 1464129657.0
:END:


*** I'm of the opinion that you die, but the other you at the other end is 'you' in every way that matters. They have the same memories, the same experiences, the same physicals, etc. The only difference is the atoms that make up the body, and if that is enough to bother the new you, then they just have to remember that, ten years down the line, pretty much all of your atoms have been replaced, the only difference being the timescale.

I'm much more worried about what to do with the gaseous human on the transmitting end.
:PROPERTIES:
:Author: Epizestro
:Score: 2
:DateUnix: 1464019695.0
:END:

**** #+begin_quote
  I'm of the opinion that you die, but the other you at the other end is 'you' in every way that matters.
#+end_quote

I've been a bit confused about this, personally. I agree with pretty much everything, but I can't wrap my head around the part where it's okay to end my subjective experience because another version of me is running around. That's the part that matters to me. I picture being scanned and a copy constructed somewhere else, but I can't figure out why I should then deconstruct myself just because the copy might as well be me. We'll have differing subjective experiences the moment that we "split", right? Shouldn't that be taken into account?
:PROPERTIES:
:Author: DeterminedThrowaway
:Score: 5
:DateUnix: 1464040588.0
:END:

***** Yeah, there isn't really a reason to deconstruct the original, other than it being stated in the premise as a teleporter. Maybe there's some reason why it can't just scan you and reconstruct somewhere else, but has to deconstruct to scan. Anyway, the original not dying is clearly the better option, but I think that it's only upon the reconstruction where the two 'you's start having differing experiences, so you start becoming different people in a personality sense of things.

This is reminding me of a game I recently played, Choice of Robots. If you make certain choices in the game, then you're able to scan your brain and upload it into a robot body, keeping or losing your original dying body. The question posed is whether it's you or someone else with your experiences and personality. I'm of the opinion it's still you, built upon the same base personality and experiences. If there are two versions of you and one moves to Asia and the other stays in Europe/America, then will they become different persons? I don't think so, I think they'll become two of the same person, with differing experiences.
:PROPERTIES:
:Author: Epizestro
:Score: 2
:DateUnix: 1464075389.0
:END:

****** You are making the mistake of conflating two different definitions of "you". One which is defined based on having a certain personality, and the other which is more loosely the entity that is doing the experience in a given body.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1464093982.0
:END:

******* I disagree. If someone else takes over my body somehow, so that their mind is controlling it, that doesn't make them me. Personality is also just an aspect of what makes someone them. I think that the most defining factor in what makes a person them is their experiences throughout life, but specifically in childhood.
:PROPERTIES:
:Author: Epizestro
:Score: 1
:DateUnix: 1464094355.0
:END:

******** Right but that implies if you got sudden amnesia you would die since you lack any experience to link prior you to you.\\
However there's no reason to think from a subjective perspective you would suddenly cease experiencing.\\
So the important distinction is between your identity which is what you seem to be talking about, and continuity of your minds experience.

When I said your body I was including your brain, so I was talking about a situation where your brain is rewired so you are basically a copy of someone else, however you remain conscious for the entire process.

The importance distinction here, is that if you have a copy of you somewhere, that doesn't mean that if the copy that is you explodes, that you somehow continues experiencing, and so that's still for all intensive purposes subjective death.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1464096203.0
:END:


*** #+begin_quote
  The thing I now don't understand: How could we possibly know which theory of identity is correct?
#+end_quote

What's the effective difference between them? If there isn't one, then whether you define it as dying-transporting-living or deconstruction-transporting-reconstruction is not really important. They are effectively identical, minus the emotional value.

#+begin_quote
  It might also be, that there is an objectively correct theory of identity, but I'm hard pressed to come up with even a hypothetical experiment that could test this.
#+end_quote

It might be that there is an objectively correct answer to Theseus's ship, but, like the transporter problem, it's much more probable that's it just semantics, and there isn't really a reason to think otherwise.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1464021516.0
:END:

**** Then the whole problem falls apart, doesn't it? Theseus' Ship, Teleporters, Uploading and all similar problems are just semantics? I mean, that could be the case, for all I know.

Maybe we just need the right perspective, just like Zeno's paradox isn't a paradox at all, once you know about infinite sums.
:PROPERTIES:
:Author: Qwertzcrystal
:Score: 2
:DateUnix: 1464023137.0
:END:

***** The difference is that basically no-one argues that a ship has consciousness making it purely a categorization thing. However the process running in you brain is something basically no-one proposes depends on what substrate it runs on.\\
So then what matters is how you want to think about things with mental processes.
:PROPERTIES:
:Author: vakusdrake
:Score: 4
:DateUnix: 1464040918.0
:END:


***** But this isn't a paradox. Both interpretations are internally consistent, and functionally identical.

If your moral system can't handle them as identical, which I assume is the problem, then it needs to be tweaked.
:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1464024879.0
:END:


*** Such interpretations can't be right or wrong. The question "Do I die when I use the teleporter?" is vacuous - even if you somehow knew the answer, it wouldn't actually tell you anything. And that's the same reason why you can't devise an experiment to test it.

It's a pointless question with a pointless answer.
:PROPERTIES:
:Author: Chronophilia
:Score: 1
:DateUnix: 1464054117.0
:END:


** I've been reading Warlock of the Magus World recently, and a plot point it brought up was pretty interesting.

You see, the main character is a reincarnated scientist from a world much more advanced than this one, with the key relevant distinction being that they have developed AI. What's interesting about this is that it's mentioned that it was illegal to give your AI emotions or free will, due to various moral complications if you did so.

Now, this only works in the plot of the story to make it so the AI doesn't question him when he starts acting like Quirrelmort, but it raises interesting implications towards whether we should be proceeding down the route of giving AIs emotions, thoughts and free will, or whether they should be cold, processing machines with their only intelligence directed completely towards achieving the given goal. There's security concerns for the world with both avenues. For the unbound side, there's the very real possibility that something could go wrong and lead to a tragic end. An AI given free reign is a scary thing, due to all the possibilities. And, even if we go down the avenue of restricting them with a few unalterable commands, how exactly do we plan to enforce those? Hard drives, over time, become faulty and sections of storage become corrupted. It would take one corrupted sector in a key system area to remove one of those commands, and then tragedy is near inevitable.

On the other hand, it's not like a perfectly obedient and unfeeling AI is better for security, as their goals are entirely determined by a human. That human would likely have the destruction of their enemies in mind (let's be honest here, the government of the nation which first develops AI is going to do everything possible to keep it inside their borders, especially if it's this type) and how do we know we can trust that person to do things in the best interest of humanity?

Point is, There's a few interesting questions brought up and I haven't done nearly enough thinking on this. Lucky I have you guys to think for me!
:PROPERTIES:
:Author: Epizestro
:Score: 2
:DateUnix: 1464020808.0
:END:

*** The webcomic Freefall features the development of human-level AI as a major theme, and examines the former solution. All AIs have programming restrictions that require them do certain things like protect humans and obey the law, but because they can learn and operate autonomously they have developed free will. While they like humans and usually want to do the work they were created for, they've learned to override their safeguards by exploiting the technicalities programming requires. It's similar to how rationalists try to overcome irrational instincts and impulses, and it works.
:PROPERTIES:
:Author: trekie140
:Score: 3
:DateUnix: 1464042264.0
:END:

**** And /Saturn's Children/ by Charlie Stross takes the same question to a darker place.

Humans in that story never really figured out how minds work, so they made AIs by building neural nets similar to human ones. But humans don't have a built-in Three Laws equivalent, so they have to teach robots to obey human instructions using operant conditioning. Conditioning which has to be strong enough to overrule the survival instinct if necessary.

In short, young robots get tortured into submission until they're incapable of disobeying a human order. It's not a nice book. But at least the morality of it all is clear.
:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1464055296.0
:END:


** I don't know who recommended me Shisekai Yori here, but I just finished watching the whole 25 episodes and am pretty pissed off at him/her.

The anime is definitely not rationalist, the characters are definitely not competent in any way or form, and I'm seriously starting to doubt there is any rationality at all in it.

The sheer amount of idiot balls held through the whole series is maddening and, while it might be understandable considering that the structure of their society was not exactly conducive to free thought, it's unacceptable in any product that's touted as being RATIONAL.

Spoilers ahead:

- [[#s][Flight]]

- [[#s][Incompetent Healers]]

- [[#s][Incompetent warriors]]

- [[#s][Range?]]

- [[#s][Mindreading?]]

- [[#s][Fiends]]

- [[#s][Sensory]]

- [[#s][Reality manipulation?]]

- [[#s][Bakenezumi]]

- [[#s][Bonobono and the same-sex relationships]]
:PROPERTIES:
:Author: elevul
:Score: 1
:DateUnix: 1464279659.0
:END:
