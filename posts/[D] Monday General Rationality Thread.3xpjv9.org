#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 27
:DateUnix: 1450710299.0
:DateShort: 2015-Dec-21
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** Does anybody know why Spacebattles and Sufficient Velocity hate the Rationality meme-system? I haven't been able to get an answer out of any of them other than "Yudkowsky's navel-gazing cultish nonsense", much less a reasoned dissenting argument that'd I'd be able to update on. Did Methods of Rationality kill all their pets or something?
:PROPERTIES:
:Author: Vebeltast
:Score: 17
:DateUnix: 1450740919.0
:DateShort: 2015-Dec-22
:END:

*** Rationality in general has a PR problem. People hear about it and based on whatever past experiences, dismiss it right away. Individual tenets of rationality, or even the whole hog, are accepted by people if you don't introduce them as rationality. You can put lipstick on this pig.

Of my friends, some hate the rationalism, and the one who hates rationalism the most is also the one who uses it the most. It's just a name / branding issue really. Stuff like the ideas in [[http://lesswrong.com/lw/f1/beware_trivial_inconveniences/][Beware Trivial Inconveniences]] or [[http://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/][The Toxoplasma of Rage]] or whatever rationalist article, if presented without rationalism mentioned, are usually really popular. I can just take the idea, present it myself, and people will like it. It's hard to give them follow-up reading though.

It's just a bad brand. I can't speak about SB and SV specifically, but that's just what I've observed.
:PROPERTIES:
:Author: blazinghand
:Score: 17
:DateUnix: 1450749599.0
:DateShort: 2015-Dec-22
:END:

**** I remember talking to some people on LessWrong a few years ago about /why/ the brand was a bad one and getting some combination of denial ("It's not a bad brand!"), obstinate refusal to see this as a legitimate problem ("It's a bad brand because we say things that are true!"), or placing blame on others ("It's the haters!"). It just convinced me that I wasn't likely to have a productive conversation on the matter. Same with the "cult" stuff, which is closely related.
:PROPERTIES:
:Author: alexanderwales
:Score: 13
:DateUnix: 1450759003.0
:DateShort: 2015-Dec-22
:END:

***** My own pet peeve on that score: why is "the Sequences" usually (or often) capitalized?

For purposes of comparison, Christians like to capitalize "Old Testament" and "New Testament," "the Koran" is capitalized, etc.

*It's not a big deal*, and I suppose most people don't pay much attention to details like that -- but I've always found it a little creepy.
:PROPERTIES:
:Score: 12
:DateUnix: 1450760105.0
:DateShort: 2015-Dec-22
:END:

****** I've observed a couple people as they read through the sequences. I think that it's capitalized like the other books because it has comparable power. If it's new to you, you can understand, and you it buy into it, you can build most of your personal philosophy around it. It's about as creepy as, say, an Asian person reading a Bible translation around the age of 20 and suddenly becoming a furious Christian.
:PROPERTIES:
:Author: Vebeltast
:Score: 7
:DateUnix: 1450761576.0
:DateShort: 2015-Dec-22
:END:

******* If it was all placed in the context of traditional academic statistics and philosophy, it would seem a fair bit more commonsensical but a fair bit less Deeply Profound.

Ironically, the thing I like most about this subculture is that we value the commonsensical and the natural /over/ the Deeply Profound.

It was Eliezer who said they're 85% non-original material, even though they don't cite much.

We need way better introductory books.
:PROPERTIES:
:Score: 8
:DateUnix: 1450796908.0
:DateShort: 2015-Dec-22
:END:

******** u/Vebeltast:
#+begin_quote
  Ironically, the thing I like most about this subculture is that we value the commonsensical and the natural over the Deeply Profound.
#+end_quote

See, that's the interesting thing that I've just realized: a lot of what we see as being common-sense is, to people who haven't seen it before, Deeply Profound. Like, the bit about death being bad: We see it as blindingly obvious. Death is simply a /bad thing/ and I can't see anything in the laws of physics that demands it, and so I would prefer the universe where nobody has to die. But if you say all that to someone who hasn't thought about it, you end up Deeply Wise!

Agreed on needing better introductory material. EY's single biggest achievement is that he put all of this in a single place and organized it so a single person can assemble it for themselves. It's an important achievement, but it can be duplicated more easily now that it's been done once. We just have to get it put in more places.
:PROPERTIES:
:Author: Vebeltast
:Score: 4
:DateUnix: 1450821680.0
:DateShort: 2015-Dec-23
:END:

********* u/deleted:
#+begin_quote
  See, that's the interesting thing that I've just realized: a lot of what we see as being common-sense is, to people who haven't seen it before, Deeply Profound. Like, the bit about death being bad: We see it as blindingly obvious. Death is simply a bad thing and I can't see anything in the laws of physics that demands it, and so I would prefer the universe where nobody has to die. But if you say all that to someone who hasn't thought about it, you end up Deeply Wise!
#+end_quote

Oh right. As a group, [[http://lesswrong.com/lw/2pv/intellectual_hipsters_and_metacontrarianism/][we're split into the five-year-old children and the meta-contrarians]]. Oy.

(Although the Second Law of Thermodynamics /does/ seem to demand a heat-death of the universe /eventually/. That's just not relevant to our timescales right now, unless you're searching for [[http://meaningness.com/preview-eternalism-and-nihilism][Eternal Deep Truths]] to solidify a worldview.)
:PROPERTIES:
:Score: 2
:DateUnix: 1450821839.0
:DateShort: 2015-Dec-23
:END:


****** I don't know what "the Sequences" are, but doesn't the "the" imply it is a name, making it a proper noun? Therefore always capitalized.
:PROPERTIES:
:Author: Vicioustiger
:Score: 2
:DateUnix: 1450762429.0
:DateShort: 2015-Dec-22
:END:

******* [[https://wiki.lesswrong.com/wiki/Sequences]]

I'm not sure whether it is a name (the webpage I linked to above is titled "Sequences" but has both EY's collection of posts that are usually referred to this way with a capital S, as well as other sequences by different lesswrong-affiliated authors).

Regardless of whether it is a name, I still find it a little creepy to see someone told to "read the Sequences."

Just something off about that. Although: I may be the one off here, I suppose creepiness is in the eye of the beholder.
:PROPERTIES:
:Score: 4
:DateUnix: 1450763024.0
:DateShort: 2015-Dec-22
:END:

******** I can understand that worry and comparison after reading some of the other comments here. The word cult has been used at least 5 times just discussing it, and when the entire point is to come to rational conclusion then anything with a cult connotation would seem off-putting.
:PROPERTIES:
:Author: Vicioustiger
:Score: 3
:DateUnix: 1450764089.0
:DateShort: 2015-Dec-22
:END:


****** I always assumed it was because Yudkowsky was planning on turning them into a book or something. [[https://en.wikipedia.org/wiki/Prolegomena_to_Any_Future_Metaphysics][Prolegomena to Any Future Metaphysics]] is capitalized because it's a title, even if it's a purely descriptive title.
:PROPERTIES:
:Author: blazinghand
:Score: 2
:DateUnix: 1450811788.0
:DateShort: 2015-Dec-22
:END:

******* You may be correct (and I believe he did turn them into [[http://www.amazon.com/Rationality-From-Zombies-Eliezer-Yudkowsky-ebook/dp/B00ULP6EW2][a book]]). Still, even so, "read the Sequences" sounds exponentially more creepy than "read Plato's Republic," no?
:PROPERTIES:
:Score: 3
:DateUnix: 1450812858.0
:DateShort: 2015-Dec-22
:END:

******** But in all seriousness, do read Plato's Republic. With footnotes.
:PROPERTIES:
:Author: Rhamni
:Score: 3
:DateUnix: 1450814092.0
:DateShort: 2015-Dec-22
:END:


******** I haven't actually run into anyone who's told me either of those things in response to a query so I can't say in context. Comparing "Read Yudkowsky's /The Sequences/" vs "Read Plato's /The Republic/", the latter sounds better, but this to me again boils down to a branding issue. If I wrote a book called /Modern Cognitive Science and You: Seventeen Easy Steps to Success/, even if it contained the same content, you'd have a real different experience recommending it to people. Same if a famous cognitive scientist wrote it and gave it a more professional title.

I'm sure it's not helped by rationalists suggesting it in a strange way, either. People in general don't know how to sell things. I doubt rationalists are an exception.
:PROPERTIES:
:Author: blazinghand
:Score: 3
:DateUnix: 1450814140.0
:DateShort: 2015-Dec-22
:END:

********* Wait, how is that that people don't know how to sell things? I though people being social creatures and being capable of empathy implied that they had some capacity for manipulation. This is really trivial but looks like I had it wrong the whole time. Thank you for pointing this out.
:PROPERTIES:
:Author: aintso
:Score: 1
:DateUnix: 1450914599.0
:DateShort: 2015-Dec-24
:END:

********** Selling things is hard. In order to be capable of making a sale, you need to be able to compete with millions of other, better sales agents out there. If you're not able to do that (which mostly people can't, not with just standard social manipulation and empathy) then you're not able to make a sale, and thus don't actually know how to sell things.

I think sales is a really common thing to get Dunning-Krugered on, since I've seen a lot of really inept people trying to sell things (including rationality).
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1450934366.0
:DateShort: 2015-Dec-24
:END:


******** u/deleted:
#+begin_quote
  Still, even so, "read the Sequences" sounds exponentially more creepy than "read Plato's Republic," no?
#+end_quote

I think that depends on whether you know the actual content of Plato's /Republic/.
:PROPERTIES:
:Score: 2
:DateUnix: 1450825344.0
:DateShort: 2015-Dec-23
:END:

********* I mean, the eugenics stuff isn't even well run. A yearly rigged lottery? You don't think people will end up having sex outside of that?

In all seriousness, he was a very thoughtful, intelligent man who lived in a society that thought slavery was ok and became the cultural capitol of 'Greece' by using money raised as tribute. Fortunately the main message is not that you should agree with him on every point. It's that you should collaborate with others, analyse arguments thoroughly, discard the ones that don't hold up, even if they come from him, and keep searching honestly for the truth.
:PROPERTIES:
:Author: Rhamni
:Score: 2
:DateUnix: 1450829024.0
:DateShort: 2015-Dec-23
:END:

********** u/deleted:
#+begin_quote
  It's that you should collaborate with others, analyse arguments thoroughly, discard the ones that don't hold up, even if they come from him, and keep searching honestly for the truth.
#+end_quote

And also that slave-taking is fine, virtue-ethics is a thing, all objects are mere projections of perfect Forms that live in a Heaven of Ideas, etc.

Frankly, I'm not willing to let any one thinker or group of thinkers claim ownership over basic critical thinking, in the same way that they don't get to "own" physics.
:PROPERTIES:
:Score: 2
:DateUnix: 1450830887.0
:DateShort: 2015-Dec-23
:END:

*********** Fair enough, although I will point out that in Plato's imagined Republic, there are no slaves. There is a caste system, but all the material wealth stays at the bottom, while political power comes with forced asceticism and gender egalitarianism. Children are assigned caste independently of their parents, depending on how well they do in school (although the eugenics program suggests he expects most apples to fall near the tree). It's clearly far from a society I or others of today would endorse, but while the realm of the forms and all that jazz is plainly silly, the critical thinking was presented in a way that helped me become more interested in philosophy. Obviously Plato does not 'own' critical thinking, but he's an early master of it.

He is not in any way mandatory reading, but he was an excellent starting point for me, and I still enjoy reading a dialogue every now and then.
:PROPERTIES:
:Author: Rhamni
:Score: 2
:DateUnix: 1450834762.0
:DateShort: 2015-Dec-23
:END:


**** The problem with talking about "rationalism" like this is that you seem to be conflating multiple ideas. It's like, rationalism is about making smart choices, which noone can argue against, and then oh by the way, you're supposed to believe in evil AIs going FOOM and donate to Yudkowski now.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 9
:DateUnix: 1450754266.0
:DateShort: 2015-Dec-22
:END:

***** Good point! That's what I'm talking about.

There's definitely some terminology problems here. "Rationalism" as it is used refers to a bunch of different ideas, some of which people like, and some of which people do not. This is exactly why, when you want to talk the things you want to share, you don't call it rationalism.

In a similar vein, when I try to introduce other things (like socialism or libertarianism or whatever charged idea there is) I don't call them by name. Names and labels hurt people's ability to be good about this kind of thing.
:PROPERTIES:
:Author: blazinghand
:Score: 12
:DateUnix: 1450756307.0
:DateShort: 2015-Dec-22
:END:


***** Also, "rationalism" means Descartes and "rational" has a tendency to be used as "Think what I tell you to!"
:PROPERTIES:
:Score: 3
:DateUnix: 1450797115.0
:DateShort: 2015-Dec-22
:END:


*** I'd guess because they have had a number of obnoxious posters trying to encourage people to read less wrong or support the singularity institute. Probably also bits of appealing to Yudkowsky as an authoritative source despite his lacking of credentials the opponent would find meaningful.

There was/is a personality cult around Elizier because of the halo effect and people creating an image of him as a person based only on his high rated and carefully crafted posts.

While lesswrong has a number of transhumanist memes, the ones that are only really associated with lesswrong tend to be the weird and implausible ones like the A.I foom theory, worry over existential risk, roko's basilisk, cryonics and overuse of the word rationalist as an adjective.

The community does resemble a religion in a number of respects. The meetups, solstice celebrations, the insular community, the weird beliefs and most importantly the appeals for money from the machine intelligence research institute (formerly known as the singularity institute)

People do stereotype members of the rationalist community. They are wrong to write the community off as a whole but they do have a point. The people that do deliberately advertise that they belong in this community tend to be the obnoxious posters I mentioned earlier.
:PROPERTIES:
:Author: MrCogmor
:Score: 10
:DateUnix: 1450745846.0
:DateShort: 2015-Dec-22
:END:


*** [deleted]
:PROPERTIES:
:Score: 4
:DateUnix: 1450788947.0
:DateShort: 2015-Dec-22
:END:

**** ...Last time I saw them, maybe a month ago? Tim Josling was in the process of doing that when I poked the Less Wrong slack chat to see if it was interesting, testing how hard it was for random people on the subway to pick up some of the simpler ideas against bias. Maybe we're just hanging out with different rationalists?
:PROPERTIES:
:Author: Vebeltast
:Score: 7
:DateUnix: 1450795905.0
:DateShort: 2015-Dec-22
:END:


**** Last week my roommate was very pissed off at how hard it would be to run an independent study on bacopa monnieri, compared to quick and dirty trials we can run on other drugs.

Yesterday I tried to make coloured-flame candles and used basic science throughout. I used basic science to figure out seam strength when bonding two pieces of mylar space blanket together just last week.

We constantly use science to binary search though our 3D printers' problem space.

Science is not what lesswrong brings to the table though. It's impossible to do any kind of engineering job without at least a basic adherence to the scientific method.

A lot of the rationality techniques that I value most aren't just basic science though. When I did a CFAR workshop that was something that kept coming up, the cost of information and dealing with uncertainty.

As an individual, you don't have the time or resources to test your questions against reality.

Take, as an example, the question of what career to take, or which job offer to take. The scientific method won't help you here.

People conflait lesswrong style rationality with science because +they+we talk about science a lot. But science is only one tool in the toolkit, and although it's often useful in my day to day life, it's only useful when your claims are testable.

The practical explanations of cognitive biases, cached thoughts, etc are really what make it a useful toolkit.
:PROPERTIES:
:Author: traverseda
:Score: 3
:DateUnix: 1450818438.0
:DateShort: 2015-Dec-23
:END:

***** Why did you need to bond two space blankets?
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 1
:DateUnix: 1450829572.0
:DateShort: 2015-Dec-23
:END:

****** I want to build a space blanket tent, like the double walled inflatables they use near the arctic.

I like a lot of the libertarian ideas around start up cities and seasteading, but using conventional construction the startup costs are just too high. I'd like inexpensive open source infrastructure to be a thing.

This is an early test in patterning mylar to make a sort of bubble-wrap type surface. About soft-ball sized bubbles.

One of my other goals, well more of an aesthetic then a goal, is self-sufficiency and ultra portability. This potentially tackles that pretty well as well.

I also have most of a yurt. It's a lot less portable then my ideal, and probably unreasonably expensive to heat then the mylar if it works well.
:PROPERTIES:
:Author: traverseda
:Score: 2
:DateUnix: 1450837162.0
:DateShort: 2015-Dec-23
:END:

******* I think inexpensive land construction is pretty much a solved problem now that house-printers are out, so you can focus on solving "portable shelter"!

Sounds nice.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 1
:DateUnix: 1450838946.0
:DateShort: 2015-Dec-23
:END:

******** Are house printers actually cheaper than building a house the normal way?
:PROPERTIES:
:Author: ayrvin
:Score: 2
:DateUnix: 1450916391.0
:DateShort: 2015-Dec-24
:END:

********* not yet. Technology not yet fully developed, low hanging fruit not yet picked, no economy of scale etc. However, unlike 3D printing complex objects, where economy of scale and build quality really favour a centralized approach (despite what 3d printing enthusiasts say) house printing does not have the same issues.

However, seeing that labor costs make about 50% of house projects the advantages should be obvious. Naturally not all or even most of that is going to be masonry, (maybe 20?) but you should be able to have reduced build times as well. House owners and investors should like hugely reduced build times.

The construction savings are most evident if the houses you build dont need a lot of afterwork finish, eg. basic houses, shelter for the very poor or refugees. Without tons of plumbing, internal wiring, a basement etc. This is where we expect to see first widespread deployment, and indeed that is where we see the stories currently breaking taking place.

TDLR: Extremely basic houses, yes. Modern amenietes(?), developed world houses, not so much, and not until a while.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 1
:DateUnix: 1450918003.0
:DateShort: 2015-Dec-24
:END:


******** House printers have a bunch of problems. Cement isn't that good of a material, for one thing. Another is that they have serious problems with overhangs.
:PROPERTIES:
:Author: traverseda
:Score: 1
:DateUnix: 1450892662.0
:DateShort: 2015-Dec-23
:END:


*** What is the rationality meme-system?
:PROPERTIES:
:Score: 6
:DateUnix: 1450741438.0
:DateShort: 2015-Dec-22
:END:

**** The memplex of Xrisk,AI-Xrisk, effective altruism, human biases,bayesian calculation, and evopsych that originated on lesswrong.com. Eg. Harry Potter James Evans Verres style thinking.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 16
:DateUnix: 1450743721.0
:DateShort: 2015-Dec-22
:END:


**** Basically anything related to Rationality as used here, utilitarianism, friendly AI, existential risk, etcetera. I've seen hostile reactions to the mere mention of biases, black swans, or recursive self improvement.I can't figure out why, either, because the reaction is never explained past "it's nonsense".
:PROPERTIES:
:Author: Vebeltast
:Score: 9
:DateUnix: 1450744753.0
:DateShort: 2015-Dec-22
:END:

***** Considering how much sufficient velocity and space battles pushes for starships that make very little sense, I'm pretty sure the "it's Nonsense" thing is not the real answer. But they as a total group might now know themselves.

Incidentally I've started crossposting a story that is threaded with a whole bunch of rationality stuff to their forums and responses are pretty positive so I think it's likely that their actually more concerned with the 'dressings' of rationality being distasteful and not the actual core ideas.
:PROPERTIES:
:Author: Nighzmarquls
:Score: 12
:DateUnix: 1450746927.0
:DateShort: 2015-Dec-22
:END:

****** [deleted]
:PROPERTIES:
:Score: 8
:DateUnix: 1450754084.0
:DateShort: 2015-Dec-22
:END:

******* While Eliezer's first post yelling at Roko was a very unfortunate, and the least calm thing I've ever seen him write, I think it's painfully clear that it's a non-issue E. and all of Less Wrong have no interest in talking about, but others like to bring it up again and again because it sounds silly, especially if you haven't read anything about all the stuff you need to read about for the idea to make sense.

I have tried to give Less Wrong a chance a few times, but it doesn't capture my attention. I read a few sequences every now and then, I like this sub, and... well, that's about it.
:PROPERTIES:
:Author: Rhamni
:Score: 3
:DateUnix: 1450786161.0
:DateShort: 2015-Dec-22
:END:


******* u/Vebeltast:
#+begin_quote
  It was reading about Yudkowsky, MIRI, and the reaction to Roko's basilisk.
#+end_quote

I guess that's sort of what I'm wondering about. Spacebattles et al. seem to be completely on board with 95% of the individual ideas if presented on their own - all you really have to do is rephrase them and post them in isolation - but if you mention that you got it from LW it's suddenly rejected. As if the argument's validity is somehow dependent on who came up with it first. "They're rejecting what they see as a cult" might explain that, though.
:PROPERTIES:
:Author: Vebeltast
:Score: 2
:DateUnix: 1450795556.0
:DateShort: 2015-Dec-22
:END:

******** People don't like to be fed "hooks" where you start with seemingly commonsensical ideas and end up with radical, implausible-sounding stuff, at least not as Author Tract-y stories. If you think your logic is airtight, you'll usually just talk to scholars. If you want to write a story, you keep the weird-logic in the background and let people work things out on their own.

People have very little reason to shift their fundamental, semi-metaphysical beliefs about the world just because someone's preaching at them. In fact, it's rude and gets people mad.
:PROPERTIES:
:Score: 3
:DateUnix: 1450799187.0
:DateShort: 2015-Dec-22
:END:


***** "Black swans" are indeed a load of bullshit. If your model (eg: Black-Scholes Equation) puts an extraordinarily low probability on an event (eg: demand-starved, debt-driven financial crisis) that other models (eg: conventional Keynesianism) called practically inevitable, and which has happened before (Great Depression), /it's just a bad model./
:PROPERTIES:
:Score: 3
:DateUnix: 1450797321.0
:DateShort: 2015-Dec-22
:END:

****** ...Yes? AFAIK "black swans" are just another manifestation of optimism bias. If you can deal with your optimism bias directly by saying "my model is probably not as right as I think it is and I should prepare fallbacks for if it turns out to be an awful model", great! If not - and this is probably the case more often than not - here's another tool you can use to formalize (and therefore regress-toward-mean the success rate of) the process of removing optimism bias.
:PROPERTIES:
:Author: Vebeltast
:Score: 1
:DateUnix: 1450822146.0
:DateShort: 2015-Dec-23
:END:

******* See, I was under the impression that Nicholas Taleb had introduced this weirdo idea of "Black Swan" events /not/ as flaws in your model, but instead as /innately unpredictable events/ which no reasonable model could hope to capture, but which nevertheless occur frequently enough that we all need to make "antifragile" policies for responding to them.
:PROPERTIES:
:Score: 1
:DateUnix: 1450822311.0
:DateShort: 2015-Dec-23
:END:

******** Hmm, that might be the disagreement. To me, "there exist innately unpredictable events" means "there is a theoretical cap on the accuracy of any model". Which, maybe? Map-territory distinctions and computational complexity theory do suggest to me that there are systems which can produce events that couldn't have been predicted by any model simpler than the system itself.
:PROPERTIES:
:Author: Vebeltast
:Score: 2
:DateUnix: 1450822845.0
:DateShort: 2015-Dec-23
:END:


*** Well I can't speak for them, but I can say why I don't like it.

At its worse, the community seems more like a cult than a group of people interested in overcoming biases and well thought out fiction.

For example, Friendly AI/Singularity stuff is just Rapture without the Jesus, AI-X Risk is Caveman Scifi for the modern age, Roko's Basilisk is Pascal's Wager with the serial numbers filed off (though at least noone takes that seriously) etc.

For all its focus on being rational, there's a lot of outlandish ideas passed around without any critical thinking.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 3
:DateUnix: 1450753719.0
:DateShort: 2015-Dec-22
:END:

**** And this is why our cult leader's most under-appreciated saying is, "Beware things that are fun to argue about."
:PROPERTIES:
:Score: 6
:DateUnix: 1450799291.0
:DateShort: 2015-Dec-22
:END:


**** u/Vebeltast:
#+begin_quote
  any critical thinking
#+end_quote

Perhaps the critical thinking is there you just haven't seen it being done? For example, it sounds like you're conflating [[http://www.yudkowsky.net/singularity/schools/][at least two of the different versions of the singularity]]. I mean, a recursive self-improvement explosion is clearly a thing that could actually happen - we could do it ourselves pretty trivially if we didn't have all these hangups about medical research with psychedelics or if we dumped a spacex-sized pile of money into brain-computer interfaces - and the risk of unfriendly AI is obvious enough that /Hollywood/ has been making movies about it since the 60's, though as always [[http://www.fimfiction.net/story/62074/1/friendship-is-optimal/prologue-equestria-online][the real deal would be much more subtle and horrifying]]. I'll give you the initial response to the Basilisk, though; it's a non-issue now that people have realized that it's a wager and deployed the general-purpose wager countermeasure, but the flawed memetic form is still floating around causing problems.

I can see how it would be extremely cultish if viewed from the outside, though. It's a large, obviously coherent system of beliefs, with a consistent core and an unusual but relevant and deep-sounding response for many situations, and that gives it the [[http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/][seemings and feelings of deepness]] that you usually only see in religions. And then it comes down to whether your first impression suggests "Bible" or "Dianetics".

Probably explains why 95% of it is well-received if delivered on its own. Without the rest of the large mass giving it unusual coherence and consistency, it seems like just an awesome idea rather than a cult. Which would kind of explain the success I've had directing unsuspecting people to just the sequences, since by the time they've gotten to critical mass they've bought into most of what they've read.
:PROPERTIES:
:Author: Vebeltast
:Score: 2
:DateUnix: 1450795277.0
:DateShort: 2015-Dec-22
:END:

***** I suppose this is a side tangent, but I'm fairly skeptical about the scope for recursive self improvement.

First off, it's hard to make an argument that doesn't already apply to human history. Education will make people smarter, and then they figure out better method of education and so on. Technology makes people more effective and then they invent better technology, etc. Humans have been improving themselves for centuries, and the pace of technological advance has obviously increased, but there's no sign of a /hyperbolic/ takeoff, and I don't think there ever will be.

The other issue is that it flies in the face of all evidence and theory. Theoretical Computer Science gives us a lot of examples where there are hard limits on self improving processes. But FOOM advocates just ignore that and assume that all the problems that matter in real life are actually easy ones where complexity arguments don't apply, somehow.

Sometimes they get sloppy and ignore complexity entirely. If your story about FOOM AI involves it solving NP Hard problems, you should probably rethink your ideas, not the other way around. And yes, I know that P != NP isn't technically proven, but noone seriously doubts it, and if you want to be pedantic, you could substitute something like the Halting Problem, which people often implicitly assume AIs can solve.

There's also this weird obsession with simulations, without any real consideration of the complexity involved. My favorite was the story about a computer that could simulate the entire universe, /including/ /iteself/ with perfect accuracy in faster than real time. But pretty much any time simulations comes up, there's a lot of wooly thinking.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 6
:DateUnix: 1450799332.0
:DateShort: 2015-Dec-22
:END:

****** I don't really know anything about these questions, but my first (and perhaps very naive) reaction to this: isn't the mere possibility that the takeoff could be very fast and the computational problems tractable something to be worried about?

For example, if you were 95% confident that one of your objections here would hold in real life, that still leaves a 5% chance of potential disaster.
:PROPERTIES:
:Score: 3
:DateUnix: 1450812691.0
:DateShort: 2015-Dec-22
:END:

******* In /Superintelligence/ Bostrom argues that medium or fast takeoff is more likely than slow takeoff, a sentiment which is echoed by a fair number of people on LessWrong. There was a recent article by Scott Alexander that said he thinks we live in a world where the jump from infrahuman to superhuman is going to be very fast.

If the argument were "fast takeoff is unlikely but given the risks involved it's still something that we should take seriously" it would be a lot more palatable (though then it might read like Pascal's mugging). Unfortunately, I think there's also a tendency within the LessWrong crowd to first argue that FOOM AI is possible and then treat it as though it's probable, which doesn't do them any favors, especially given the lack of rigor applied to the question of probability.
:PROPERTIES:
:Author: alexanderwales
:Score: 6
:DateUnix: 1450813330.0
:DateShort: 2015-Dec-22
:END:

******** u/deleted:
#+begin_quote
  There was a recent article by Scott Alexander that said he thinks we live in a world where the jump from infrahuman to superhuman is going to be very fast.
#+end_quote

He's entirely wrong about that. Even Eliezer and Bostrom's arguments rely on the AI starting out /human-level/ intelligent, that is, capable of doing the computer-programming tasks necessary to improve itself usefully. A jump from "cow" to "superhuman" is so implausible I'd buy "someone deliberately upgraded it" first.
:PROPERTIES:
:Score: 1
:DateUnix: 1450830423.0
:DateShort: 2015-Dec-23
:END:


******* There are a lot of other unlikely but possible disasters to worry about though. What if runaway climate change triggers a feedback loop which makes the earth uninhabitable? What if a new killer disease emerges? What if an asteroid hits the earth?
:PROPERTIES:
:Author: Uncaffeinated
:Score: 1
:DateUnix: 1450833736.0
:DateShort: 2015-Dec-23
:END:

******** We should worry about all of these!

I can't speak for the lesswrong people who are into AI risk research, but I imagine they would say that there are already a lot of people thinking about climate change; NASA is launching a mission to [[https://www.nasa.gov/content/what-is-nasa-s-asteroid-redirect-mission][redirect an asteroid]]; but comparatively fewer people are seriously thinking about AI risk.
:PROPERTIES:
:Score: 1
:DateUnix: 1450835416.0
:DateShort: 2015-Dec-23
:END:


****** u/Vebeltast:
#+begin_quote
  but there's no sign of a hyperbolic takeoff, and I don't think there ever will be.
#+end_quote

My understanding is that you don't really need hyperbolic takeoff, or even a move move up the computational complexity hierarchy, to get hard a disastrously hard takeoff. All you really need is to move your intelligence off electrochemical computational platforms and onto semiconductors, which gives you something like a factor of 1e8 speedup. Then you accidentally raise a single hyper-fast serial killer without having an equally performant police department in place, and creating that equally performant police department is chicken-and-egg problem.
:PROPERTIES:
:Author: Vebeltast
:Score: 2
:DateUnix: 1450823729.0
:DateShort: 2015-Dec-23
:END:

******* There isn't really any reason to believe in a hard takeoff at all. AI is a large and extremely active field, so there aren't any low hanging fruit. Noone's going to come up with a 1000x improvement overnight.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 1
:DateUnix: 1450833958.0
:DateShort: 2015-Dec-23
:END:


****** Oh, /lovely/, I've always hoped someone would raise the /realistic/ objections!

#+begin_quote
  The other issue is that it flies in the face of all evidence and theory. Theoretical Computer Science gives us a lot of examples where there are hard limits on self improving processes. But FOOM advocates just ignore that and assume that all the problems that matter in real life are actually easy ones where complexity arguments don't apply, somehow.
#+end_quote

I think this is a problem of communication between the theoretical computer scientists (huh, do I count as that?), and the computer-science undergrads, and the general public.

As I recall about NP-completeness for instance, there are many NP-complete problems in which, if an Oracle of some sort gives you 1/3 of the solution, the rest is poly-time computable from that third. Many NP-complete or NP-hard problems can be /approximately/ answered in tractable time. "Best" answers to many questions are intractable, but merely "good" answers are actually pretty easy.

(For example, the non-convex optimization involved in modern deep learning is NP, but /as it turns out/, most local minima in deep learning loss functions tend to be very near each-other, so we don't actually care which one we get, and stochastic gradient descent unto a local minimum is basically linear-time in the number of samples we learn from.)

The thing is, if you just read through the above, you now know more about computational trade-offs than average, because for some reason we tend not to tell undergrads about those "approximation" thingies.

This is /important/, since [[http://danroy.org/papers/FreRoyTen-Turing.pdf][thinking is quite probably conditional simulation]] and [[http://web.mit.edu/pbatt/www/publications/HamrBattTene11CogSci33.pdf][coarse stochastic approximations to true theories can still yield very useful results]].

We then get the nasty question of: well, what if your "AI" has a good theory of how to trade-off resources like time and memory for empirical accuracy and precision of its models? Perhaps a [[http://rspa.royalsocietypublishing.org/content/469/2153/20120683][theory of decision-making with information-processing costs]], cast in terms of the physics that apply to living minds?

In those cases, you certainly can't get some nigh-magical FOOM. But you very likely /can/ get something that is considerably more worrisome because it requires actual expertise to understand and can't be explained neatly to laypeople. Long story short, we often only /care/ about aspects of a problem which /can/ be answered tractably, and we /definitely/ care about tractability when it's a choice between losing a /little/ precision versus /gajillions of years/ of compute-time, and we should assume that halfway-reasonable AIs can carry about the same consideration of trade-offs as us.

#+begin_quote
  if you want to be pedantic, you could substitute something like the Halting Problem, which people often implicitly assume AIs can solve.
#+end_quote

[[http://www.ics.uci.edu/%7Erickl/publications/1996-icml.pdf][The halting problem is actually PAC-learnable]], though very difficult.

#+begin_quote
  There's also this weird obsession with simulations, without any real consideration of the complexity involved. My favorite was the story about a computer that could simulate the entire universe, including iteself with perfect accuracy in faster than real time. But pretty much any time simulations comes up, there's a lot of wooly thinking.
#+end_quote

Yeah, that's based on Omohundro's "Basic AI Drives" paper, which, at least on the front of, "AIs will want to replace X with a simulation of X", isn't very good. If your AI cares about X in the first place, and X already exists, then it's almost definitely cheaper to obtain information about X by /actually observing it/ than by trying to find principles that allow you to cheaply simulate it with high accuracy (for instance, many sophisticated chemical processes).

So that one's actually wooly thinking and not just lies-to-laypeople.
:PROPERTIES:
:Score: 1
:DateUnix: 1450830239.0
:DateShort: 2015-Dec-23
:END:

******* The fact that it is PAC learnable is more of a mathematical curiosity than anything, since all it's really saying is that given a distribution of terminating programs, you can estimate a time bound below which most of them will terminate.

Re approximation: There are some problems where approximation is useful and some where it isn't. Generally, any problem inspired directly by the real world (routing your trucks, optimizing manufacturing processes, etc.) is a problem where approximations are useful. By contrast, more abstract problems, such as anything from cryptography tend to require an exact solution, where approximations are useless.

There also seems to be a conservation of hardness thing. A randomly generated SAT instance is usually easy, but if you take a hard problem, say factorization, and convert it into a SAT, the resulting SAT instance is also intractable. There aren't any free lunches.

To the extent that "increasing intelligence", whatever that means, increases the ability to solve hard problems, then increasing intelligence is at least as hard as every problem which it enables a solution of. Complexity results just don't allow loopholes like that. (You can still do stuff like increase clock speed, since that's just engineering, but you'll quickly run into physical limits there)
:PROPERTIES:
:Author: Uncaffeinated
:Score: 1
:DateUnix: 1450833561.0
:DateShort: 2015-Dec-23
:END:

******** u/deleted:
#+begin_quote
  Re approximation: There are some problems where approximation is useful and some where it isn't. Generally, any problem inspired directly by the real world (routing your trucks, optimizing manufacturing processes, etc.) is a problem where approximations are useful. By contrast, more abstract problems, such as anything from cryptography tend to require an exact solution, where approximations are useless.

  There also seems to be a conservation of hardness thing. A randomly generated SAT instance is usually easy, but if you take a hard problem, say factorization, and convert it into a SAT, the resulting SAT instance is also intractable. There aren't any free lunches.
#+end_quote

Well yes, of course.

#+begin_quote
  To the extent that "increasing intelligence", whatever that means, increases the ability to solve hard problems, then increasing intelligence is at least as hard as every problem which it enables a solution of. Complexity results just don't allow loopholes like that.
#+end_quote

I do agree. I just also think that most problems related to the physical world, the ones that decide whether or not intelligence has real-world uses in killing all humans, are mostly problems were increasingly good characterizations (eg: acquiring better scientific theories) and approximations (possibly through specialized methods like building custom ASICs) can be helpful.

If we put this in pseudo-military terms, I don't expect a "war" against a UFAI to be "insta-win" for the AI "because FOOM", but I expect that humanity (lacking its own thoroughly Friendly and operator-controlled AIs) will start about even but suffer a steadily growing disadvantage.

#+begin_quote
  (You can still do stuff like increase clock speed, since that's just engineering, but you'll quickly run into physical limits there)
#+end_quote

When you're worried about the /relative/ capability of a dangerous agent to /gain advantage/ over other agents, "just engineering" is all the enemy needs. A real-life UFAI doesn't need any access to Platonic truths or computational superpowers to do very real damage, nor does a real-life operator-controlled AI or FAI need any such things to do its own, more helpful, job competently.
:PROPERTIES:
:Score: 1
:DateUnix: 1450835425.0
:DateShort: 2015-Dec-23
:END:

********* But if you don't have hard takeoff, you're unlikely to have just one AI that's relevant. You'll have multiple AIs that are about equal, or maybe the others aren't quite as good.

But if say, Google has a slightly better AI than Apple, that doesn't mean they win everything.
:PROPERTIES:
:Author: Uncaffeinated
:Score: 1
:DateUnix: 1450838107.0
:DateShort: 2015-Dec-23
:END:

********** Yes, that sounds about right to me. But then you get into Darwinian or Marxian pressures from ecological competition/cooperation between AIs, which generally go towards simpler goals -- unless the AIs are properly under human control, in which case they should be able to stably cooperate for their operators' interests.
:PROPERTIES:
:Score: 1
:DateUnix: 1450839384.0
:DateShort: 2015-Dec-23
:END:


***** Out of curiosity, what is the "general-purpose wager countermeasure?"
:PROPERTIES:
:Score: 1
:DateUnix: 1450812759.0
:DateShort: 2015-Dec-22
:END:

****** The wager depends on hypothetical existence of a thing Y which reinforces you significantly for a belief X, using a huge weight on the reinforcement to balance out the minuscule probability of its existence. The counterargument is to construct an equally-likely hypothetical Y' that reinforces belief not-X in the same way.

This was originally constructed in response to Pascal's wager, the reification being "Yes, but if I believe in God and I'm wrong then Azathoth or Thor will smite me".
:PROPERTIES:
:Author: Vebeltast
:Score: 2
:DateUnix: 1450820050.0
:DateShort: 2015-Dec-23
:END:


****** Precommitment?
:PROPERTIES:
:Author: traverseda
:Score: 1
:DateUnix: 1450818908.0
:DateShort: 2015-Dec-23
:END:


*** We /do/ tend to act like a bunch of cultish autists. Also, SpaceBattles is all about Truly Ridiculous Lulz: lecturing them about Rationality or Science is like lecturing Ashmodai on Torah. Who are we to tell them to Stop Having Fun?
:PROPERTIES:
:Score: 2
:DateUnix: 1450796748.0
:DateShort: 2015-Dec-22
:END:

**** u/Vebeltast:
#+begin_quote
  lecturing them about Rationality or Science is like lecturing Ashmodai on Torah.
#+end_quote

See, that's the thing- they respond with hostility even if there's no lecture, even in incredibly not-lulzy-fun threads. Like, you go into the debate subforum and check out the thread about life extending drugs, and you find out that they've spent three entire pages piling on a guy that said he wanted everybody to live forever. It's just weird.
:PROPERTIES:
:Author: Vebeltast
:Score: 1
:DateUnix: 1450822506.0
:DateShort: 2015-Dec-23
:END:

***** Shit, really? And here I just thought spacebattles was all about stuff like /Shinji and Warhammer 40k/.
:PROPERTIES:
:Score: 2
:DateUnix: 1450823407.0
:DateShort: 2015-Dec-23
:END:

****** Nah, there are /damn/ serious threads on there. Like, [[https://forums.spacebattles.com/threads/the-last-angel.244209/][The Last Angel]] is one of the single best portrayals I've ever found of AI in literature - it even [[#s][minor background spoiler]] - but I know from watching usernames that, if I posted any link back to an LW blog post, the author would be the only person in the thread that didn't join the lynch mob.

(apologies if I missed a sarcasm there... )
:PROPERTIES:
:Author: Vebeltast
:Score: 2
:DateUnix: 1450824227.0
:DateShort: 2015-Dec-23
:END:

******* I did not know, at all, that Spacebattles considered anything to be Serious Business.
:PROPERTIES:
:Score: 2
:DateUnix: 1450824678.0
:DateShort: 2015-Dec-23
:END:


******* And now I'm actually reading /The Last Angel/ and finding it horrifically boring, basically a bland attempt at +HFY+ STANDARD SPACEBATTLES FARE - BATTLES IN SPACE - with overdramatized, clearly pulp-scifi-inspired navel-gazing attempts at Deep Meaning that even invoke "souls" and "sins" for spaceship AIs.

Oy gevalt. The damned thing thinks it has a soul and kills because it /hates/. And this is realistic?
:PROPERTIES:
:Score: 1
:DateUnix: 1450825073.0
:DateShort: 2015-Dec-23
:END:

******** Maybe my perception of it is different because I didn't even care that it's focused on humans. All I really saw was that it listed off about five or six civs that hit various different AI extinction risk failure modes - there's a CelestAI, a paperclipper, one that's unexplained so far but that I suspect is going to turn out to be a Whispering Earring, etc - and the one group that pulled it off because they did an upload and kept it as close to normal psychology as possible (even preventing forking) rather than attempting to design a mind from scratch. Which causes problems and /seems/ cheesy, but I wouldn't be surprised is actually relatively realistic. Also doesn't present AI as magical or "has already read the second half of the book" or "beep boop kill all humans".

I'll give you that it's overdramatic and cheesy in places. I don't feel like the AI is one of them, nor that its navel-gazing is that far past what we'd do in that situation.
:PROPERTIES:
:Author: Vebeltast
:Score: 1
:DateUnix: 1450825939.0
:DateShort: 2015-Dec-23
:END:

********* Or maybe I just skimmed it really shallowly.
:PROPERTIES:
:Score: 1
:DateUnix: 1450827719.0
:DateShort: 2015-Dec-23
:END:

********** ...I was wondering if the "'hate' engraved on each nanoangstrom" thing had really come up that early in the story, yeah.

Also, I don't often recommend this, but I'd suggest possibly going through the "who replied" list and reading the author's comments in the thread. It's possible that some of the things I'm remembering were extracted from author comments and wouldn't appear in the story proper unless he's done an edit pass. Which is a bit of a technical failure in the construction of the story, yes.
:PROPERTIES:
:Author: Vebeltast
:Score: 1
:DateUnix: 1450828939.0
:DateShort: 2015-Dec-23
:END:

*********** u/deleted:
#+begin_quote
  ...I was wondering if the "'hate' engraved on each nanoangstrom" thing had really come up that early in the story, yeah.
#+end_quote

I read the first forum page, and then skipped to the 143rd page.
:PROPERTIES:
:Score: 1
:DateUnix: 1450830920.0
:DateShort: 2015-Dec-23
:END:


** I watched the movie Pan (2015) today. I enjoyed it, but it has one of the worst messages and most irrational villains I have ever seen. [[#s][Rant]]
:PROPERTIES:
:Author: Rhamni
:Score: 14
:DateUnix: 1450737250.0
:DateShort: 2015-Dec-22
:END:

*** Thats what you get for watching big hollywood productions!

Its fun to dream of a time where all the common media are suffused with the rationality-memeplex. Childrens books about planning fallacies and sunk cost fallacies!
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 18
:DateUnix: 1450737690.0
:DateShort: 2015-Dec-22
:END:


** What's a good way to go about giving to charity? The way I see it, here are two parts to the question: how you choose a cause/organisation to support, and how you go about actually supporting it.

For the first, one obvious answer among this group would be some form of effective altruism, and just leave it at that. That leaves me with the question of what to do about groups that I'm personally involved in, or else are relevant only to the local area.

I don't have a good or simple answer for the second half, other than to give money (the currency of caring.) Beyond that, do you guve a lump sum once a year? Wait for some matching opportunity? Automatic monthly ones? Also, volunteering seems like a good idea for some things, mostly akrasia and community building.
:PROPERTIES:
:Author: ulyssessword
:Score: 13
:DateUnix: 1450713147.0
:DateShort: 2015-Dec-21
:END:

*** I'm not a very charitable person, but there are a couple of causes/organizations I support. I mostly donate to feel good about myself, but also out of some basic duty to donate some nonzero amount of money. I donate about 0.5% of my gross income. Whenever I think about the money I donate, I feel proud of myself. It's also great to talk about. In terms of value for the money, donating to charity is a great way to make yourself feel good.

I donate a small amount of money to [[https://wikimediafoundation.org/wiki/Ways_to_Give][Wikipedia]] every year. I do this because I think Wikipedia is great, and I get a lot of use out of it. Wikipedia needs (I think) about 3 dollars per year per user to operate, so I donate 10 dollars a year and feel pretty good about helping out one of the most useful tools at the level of "I'm doing my part, at least, and covering for a couple less fortunate people".

I also donate a medium amount of money to [[https://donate.doctorswithoutborders.org/onetime.cfm][Doctors Without Borders]], who do good. Givewell doesn't find them transparent enough to be a good idea to fully evaluate (compared to AMF or other charities) but gives them a [[http://www.givewell.org/international/charities/doctors-without-borders][positive review]]. Doctors Without Borders is often involved in crisis areas, and also helps provide medicine and medical care throughout the world in underdeveloped communities. It's nice to donate to Doctors Without Borders and feel good about myself.

My last donation is a political one, so you can stop here if you want. I donate a moderate amount of money to the American Civil Liberties Union (which I will not link, since it's political). The ACLU is an organization that defends the rights and liberties of Americans in court and by pushing legislature. Traditionally, they advocate for freedom of speech and religion, defending for example anti-war protestors. They also fought on behalf of the Japanese-American internees during WW2, and more recently for the rights of students, homosexuals, and the poor. They're also aggressively against the PATRIOT act, a set of laws that vastly increases the powers of the state and restricts civil rights in order to fight terrorism. I feel like the ACLU is one of the few big organizations fighting to keep America great and free.
:PROPERTIES:
:Author: blazinghand
:Score: 5
:DateUnix: 1450723491.0
:DateShort: 2015-Dec-21
:END:


*** Ask your group if they need money or labor more right now.
:PROPERTIES:
:Score: 3
:DateUnix: 1450799761.0
:DateShort: 2015-Dec-22
:END:


*** Perhaps you can publically commit money to the first matching opportunity that arises for a given rate and recipient. (Which is effectively matching with the reciprocal rate.) Although all these zero-sum moves in a game of charity are kind of silly, and I don't know how to mathematically tell apart "matching" and "taking hostages" and "proposing trade" and "blackmail".
:PROPERTIES:
:Author: Gurkenglas
:Score: 5
:DateUnix: 1450720798.0
:DateShort: 2015-Dec-21
:END:


*** I personally donate 10 percent of my income to patron free fiction on the internet. The stuff I want there to be more of like The Mother of Learning and tales from my D & D campaign. I feel they are less likely to quit half way if they are making money of it.
:PROPERTIES:
:Author: thefreegod
:Score: 1
:DateUnix: 1451384311.0
:DateShort: 2015-Dec-29
:END:


** [[https://www.reddit.com/r/spacex/comments/3xs21y/spacex_on_twitter_the_falcon_9_first_stage/][SpaceX just landed the first stage of a rocket.]] Which is pretty cool, since that means you don't have to build a new one every single time you go into space. It's not gonna make space travel cheap, but it's going to bring the price down quite a lot.
:PROPERTIES:
:Author: Rhamni
:Score: 10
:DateUnix: 1450751122.0
:DateShort: 2015-Dec-22
:END:

*** A Falcon 9 launch currently costs $61 million (according to wikipedia). According to reading on [[/r/SpaceX]], the first stage makes up 75% of the cost. So yes, a rocket launch can now be down to $15 million. ULA, SpaceX's main competitor, costs the US government $380 million per launch.

To be fair, the cost for the government from SpaceX is $130 million instead of $61 million because of regulations and stuff, so a fair comparison would be $15 million versus $175 million. It's a /bit/ of a difference.
:PROPERTIES:
:Author: gbear605
:Score: 8
:DateUnix: 1450751946.0
:DateShort: 2015-Dec-22
:END:

**** Does anyone know of the science behind speed of adoption of innovations vs costsavings? Eg. is there a law or relationship between magnitude of cost savings vs adoption rate?

I remember from my chemistry years that industrial replacement cycles are on the order of >50 years; 2007 some of the oldest, most inefficient methods for H² or SO⁴ production were on their last factories, which surprised me. I'd have expected the huge costsavings trough increased effiency to have much faster market adoption.

Related to Falcon 9, I'd expect x10 in cost reduction to lead to 90% adoption in 2 build cycles.
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 1
:DateUnix: 1450830031.0
:DateShort: 2015-Dec-23
:END:

***** Most interesting, is that SpaceX's goal is to be sending up a Falcon 9 every two weeks. At that pace, no other company can near keeping up unless they also go to the reusable model. Because of this, the rate of adoption will be quite high - ignoring the fact that NASA and other regulatory bodies will slow it down. We'll see what happens.

The major question right now is how much refurbishment will need to be done to the first stage after landing it.
:PROPERTIES:
:Author: gbear605
:Score: 3
:DateUnix: 1450841602.0
:DateShort: 2015-Dec-23
:END:


***** I'm not aware of any studies, but the obvious relationship would be based on the required capital investments and rate of return. Effectively ROI. For things like chemical factories I assume regulatory costs of opening a new facility slow the process down by lowering ROI

As older facilities breakdown eventually it becomes worth it to upgrade.

But I suspect if you asked this on marginal revolution you'd get a better answer. Isn't it tabbarok's law -- there are always more studies than you think.
:PROPERTIES:
:Author: TaoGaming
:Score: 1
:DateUnix: 1450884600.0
:DateShort: 2015-Dec-23
:END:


** I just read a very interesting abstract of recent research [[http://www.eurekalert.org/pub_releases/2015-12/ionr-psl122115.php][here.]]

I consider my understanding of the topic amateur but this seems like potentially a really big deal for understanding brain structure.
:PROPERTIES:
:Author: Nighzmarquls
:Score: 4
:DateUnix: 1450737271.0
:DateShort: 2015-Dec-22
:END:


** Is there a word for "something like backlash, but without anything to backlash /against/"?

The two examples that bring this to my mind are the actress playing Hermione Granger on stage being black, and Canadian Defense Minister Harjit Sajjan being Sihk and East Indian (I'm sure that there are more, and non-race ones as well.). I haven't seen any criticism of them, but I've seen a lot of people loudly proclaiming that they're not only okay with it, but fully supporting it as well.
:PROPERTIES:
:Author: ulyssessword
:Score: 4
:DateUnix: 1450754761.0
:DateShort: 2015-Dec-22
:END:

*** forlash?
:PROPERTIES:
:Author: Seeworthy121
:Score: 9
:DateUnix: 1450756413.0
:DateShort: 2015-Dec-22
:END:


*** Sometimes I see the-thing-being-complained-about arise, in such a case... as backlash-to-the-backlash, and then it's used as evidence that the backlash was well-founded to begin with.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 2
:DateUnix: 1450760085.0
:DateShort: 2015-Dec-22
:END:


*** u/eaglejarl:
#+begin_quote
  The two examples that bring this to my mind are the actress playing Hermione Granger on stage being black,
#+end_quote

It took me a full minute to realize that you meant Hermione was being played by a black actress. I thought you meant that Emma Watson was performing in blackface
:PROPERTIES:
:Author: eaglejarl
:Score: 2
:DateUnix: 1450830215.0
:DateShort: 2015-Dec-23
:END:


*** Signalling, showing solidarity, rallying, that sort of thing?
:PROPERTIES:
:Author: Charlie___
:Score: 1
:DateUnix: 1451071767.0
:DateShort: 2015-Dec-25
:END:


*** Don't forget the black Stormtrooper in the new Star Wars film! Although there as well, I haven't seen anyone being racist. Just a few people going "Aren't the stormtroopers all clones of that one guy?" and then someone saying "The clone thing didn't work so now they just kidnap babies" and the 'racists' going "Oh, ok."
:PROPERTIES:
:Author: Rhamni
:Score: 1
:DateUnix: 1450786405.0
:DateShort: 2015-Dec-22
:END:


** [[http://strongfemaleprotagonist.com/issue-6/page-5-6/]] cross post to [[/r/getmotivated]]
:PROPERTIES:
:Author: rugurhfurghhrughh
:Score: 4
:DateUnix: 1450811505.0
:DateShort: 2015-Dec-22
:END:


** So hey, if you ever wanted to build a rational dystopia, China has got a few tips for you. [[https://www.youtube.com/watch?v=lHcTKWiZ8sI&feature=youtu.be][Imagine Facebook, if the opinions and financial situation of your facebook friends made it easier or harder for you to get jobs, loans, etc.]]
:PROPERTIES:
:Author: Rhamni
:Score: 4
:DateUnix: 1450827507.0
:DateShort: 2015-Dec-23
:END:

*** Its been done before! [[http://www.smbc-comics.com/?id=2286]]
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 3
:DateUnix: 1450830213.0
:DateShort: 2015-Dec-23
:END:

**** That's neat, although the China thing appears to be actually happening.
:PROPERTIES:
:Author: Rhamni
:Score: 3
:DateUnix: 1450830601.0
:DateShort: 2015-Dec-23
:END:


** I take it all back now. Merry Christmas.
:PROPERTIES:
:Author: Gigapode
:Score: 1
:DateUnix: 1450913119.0
:DateShort: 2015-Dec-24
:END:


** Check out this paper about quantitative style features of more and less successful literary works:

[[http://aclweb.org/anthology/D/D13/D13-1181.pdf]]

I am most surprised by the fact that more successful books are found to use more prepositions.
:PROPERTIES:
:Author: rhaps0dy4
:Score: 1
:DateUnix: 1450998264.0
:DateShort: 2015-Dec-25
:END:
