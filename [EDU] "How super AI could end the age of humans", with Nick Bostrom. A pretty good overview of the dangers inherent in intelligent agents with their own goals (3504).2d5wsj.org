#+TITLE: [EDU] "How super AI could end the age of humans", with Nick Bostrom. A pretty good overview of the dangers inherent in intelligent agents with their own goals (35:04)

* [[http://www.theguardian.com/science/audio/2014/aug/04/science-weekly-podcast-nick-bostrom-ai-artificial-intelligence][[EDU] "How super AI could end the age of humans", with Nick Bostrom. A pretty good overview of the dangers inherent in intelligent agents with their own goals (35:04)]]
:PROPERTIES:
:Author: Pluvialis
:Score: 8
:DateUnix: 1407694981.0
:END:

** So I have been talking with friends that I game with and have asked thier opinion on this topic a few times. They are unconvinced of the potential threat an AI optimizer could cause, stating that there is a lot of hand waving going on in the fact of an AI being unable to change its own goals so that they might only just optimize themselves instead of perusing the noble goal of converting all matter into paperclips. I have been unsuccessful at reasoning with them about how an AI is not the same as human intelligence and that it would not see the benefit in changing its own goals.

Edit: Further clarification on the subject brings forth the question on wether an AI could independently question its own goals and values and change them.
:PROPERTIES:
:Author: Traiden04
:Score: 1
:DateUnix: 1407792503.0
:END:


** The comments in [[/r/skeptic]] where this topic is [[http://www.reddit.com/r/skeptic/comments/2d93oy/artificial_intelligence_is_more_dangerous_than/][also being discussed]] really demonstrate the need for a popular movie or two in which an AI escapes its box.
:PROPERTIES:
:Author: Pluvialis
:Score: 1
:DateUnix: 1407797538.0
:END:

*** Oh my dear fucking God, they're actually denying the orthogonality thesis.
:PROPERTIES:
:Score: 3
:DateUnix: 1408017782.0
:END:


** For some reason, I see strong AI as not as a threat, but simply a possible next step.

Evolutionarily speaking, the two main ways a species goes extinct are an invasive species dominating the niche of the native one, or that native species having enough beneficial mutations that spread through the populace until none of the original flavor are left in that biome.

In that sense, many of these people are equating strong AI with the former kind, but to me, I see strong AI as the latter kind.

I wonder if this is just me having an abnormally weak bias for DNA-based descendancy.
:PROPERTIES:
:Author: Prezombie
:Score: 1
:DateUnix: 1407956489.0
:END:

*** The "some reason" is that you're conceptualizing the universe through a neo-Darwinian meta-narrative, rather than chucking out your metanarratives entirely and assigning moral valences where you second-order want them.
:PROPERTIES:
:Score: 4
:DateUnix: 1408017704.0
:END:


*** Well, I think I understand the sentiment of our AI 'child' sort of 'inheriting' the universe from us, but how would you feel about an AI that killed us all and then went around doing something totally worthless, just because we programmed it badly? Like going around turning every source of energy into computational resources for itself until in short order it has absorbed the universe and sits just running cycles on loop searching for further threats against itself or something equally mundane?
:PROPERTIES:
:Author: Pluvialis
:Score: 1
:DateUnix: 1407966769.0
:END:

**** Yeah, that would suck. But a strong AI wouldn't be built in a vacuum. Sure, one strong AI could be a mundane paranoid paperclipper, but there's also human children who grow up to be criminals. Due to social constraints, criminals rarely thrive in a complicated environment if the entire social structure is against them.

Similarly, a single strong AI would be dangerous, but one paranoid paperclipper wouldn't be able to take off if there's hundreds or thousands of Strong AI in a social network of checks and balances.
:PROPERTIES:
:Author: Prezombie
:Score: 1
:DateUnix: 1407971994.0
:END:

***** #+begin_quote
  but one paranoid paperclipper wouldn't be able to take off if there's hundreds or thousands of Strong AI in a social network of checks and balances
#+end_quote

Humans did, why not (a particular species within) machines?
:PROPERTIES:
:Author: jalanb
:Score: 2
:DateUnix: 1408290009.0
:END:


** Hi, thanks for the link.

*SPOILERS:*

The two goals for researchers are basically (at the end):

1) creating AI

2) controlling AI

Goal 2 should be reached before goal 1. Basically. Yet, massive economic pressure to just /create/ AI is hugely outclassing the effort to control it.

/spoilers
:PROPERTIES:
:Author: CaesarNaples2
:Score: 1
:DateUnix: 1407770561.0
:END:

*** #+begin_quote
  Goal 2 should be reached before goal 1. Basically. Yet, massive economic pressure to just create AI is hugely outclassing the effort to control it.
#+end_quote

Really? Because I don't actually see that much economic effort being poured into AGI, compared to how much goes into most other fields of theoretical computer science.

What I will say is that people mostly don't seem to work on Friendly utility functions for several reasons:

1) They think they lack the philosophical framework to conceptualize a "controlled" or "safe" utility function besides reinforcement learning.

2) They think they lack the mathematical ability to describe a non-learned utility function at all (this is true: we do currently lack that mathematical ability).

3) They think reinforcement learning will be good enough, since after all it's been ok up to now.

The fact that they don't just endorse doing whatever their dopaminergic circuits consider the Most Interesting Thing at any given time doesn't seem to occur to them.
:PROPERTIES:
:Score: 2
:DateUnix: 1408017562.0
:END:
