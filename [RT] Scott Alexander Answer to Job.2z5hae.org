#+TITLE: [RT] Scott Alexander: Answer to Job

* [[http://slatestarcodex.com/2015/03/15/answer-to-job/][[RT] Scott Alexander: Answer to Job]]
:PROPERTIES:
:Score: 66
:DateUnix: 1426447861.0
:END:

** I'm curious why this got zero (other) comments. Most other posts this highly rated would have /something/ after 11 hours.
:PROPERTIES:
:Author: ulyssessword
:Score: 3
:DateUnix: 1426489097.0
:END:

*** Could be a lack of [[/r/rational][r/rational]] specific things to talk about? slatestarcodex has a pretty active comments section, people could be taking the discussion there.
:PROPERTIES:
:Author: blazinghand
:Score: 8
:DateUnix: 1426491504.0
:END:


*** I'm guessing most people are discussing it on the site rather than here.
:PROPERTIES:
:Author: demontreal
:Score: 4
:DateUnix: 1426490603.0
:END:


** What if Job's suffering, while certainly sucking hard for Job, allowed his story to spread, and the results of that caused more people to obey God and go to heaven?

What if, when Job died and went to heaven himself, he learned of this, and in fact agreed it was right for him to suffer as he did, for the lessons his story would bring to so many for so long?

Any criticism of morality in the Bible that doesn't take an eternal afterlife into account is not worth very much. Eliezer Yudkowsky argued that it was OF COURSE better to torture a man for 50 years than have 10^^^3 people get a speck of dust in their eyes. So he at least should be totally onboard with what happened to Job... right?
:PROPERTIES:
:Author: robobreasts
:Score: 3
:DateUnix: 1426522539.0
:END:

*** Sure. But this is not a separate theodicy, because a solution to popularizing a certain religion that involves torturing Job is still imperfect. So it still needs its own justification, like "you're in the multiverse, and this is better than nothing," or "Just deal with it, God is a [[http://en.wikipedia.org/wiki/Satisficing][satisficer]] not an optimizer."
:PROPERTIES:
:Author: Charlie___
:Score: 3
:DateUnix: 1426528897.0
:END:


*** I am pretty sure [[http://biblehub.com/esv/job/42.htm][that's canon]]. Doesn't mention the afterlife specifically, but the point of Job is definitely to demonstrate How One Ought To Faith.

Like the incidents where God tells people to kill their kids: it's not that it doesn't suck to kill your kid, it's just that mere life and death is outweighed by mumble mumble trust in God mumble uhm.
:PROPERTIES:
:Author: notentirelyrandom
:Score: 1
:DateUnix: 1426544937.0
:END:

**** When does God tell someone to kill their kid other than Avraham, in which it wasn't supposed to be real?
:PROPERTIES:
:Author: itisike
:Score: 2
:DateUnix: 1426550154.0
:END:

***** [[https://www.biblegateway.com/passage/?search=Judges+11][Jephthah.]]

God didn't directly say "you must kill your daughter," but Jephthah promised that if he won this battle he'd sacrifice whatever (literally translated "whoever") came out of his house to meet him when he returned home. I'm not really sure what he was /expecting/ to happen. His daughter volunteers to go through with it, but asks for two months to weep about the fact that she'll never have sex. (I promise I'm not making this up! She really did pick a number out of thin air without making it a big one! And pick that one specific thing to weep about. But whatever.) Afterward, he kills her.

It's portrayed as being tragic but necessary. And Jephthah does get mentioned later in God's canonical nonexhausive [[http://biblehub.com/esv/hebrews/11.htm][list]] of Most Faithliest Heroes.

His daughter doesn't.
:PROPERTIES:
:Author: notentirelyrandom
:Score: 3
:DateUnix: 1426553175.0
:END:

****** Can't source this right now, but the traditional interpretation of that is that he was wrong, and wasn't supposed to kill his daughter. There's also one view that he didn't really kill her but just sent her away iirc.
:PROPERTIES:
:Author: itisike
:Score: 2
:DateUnix: 1426553926.0
:END:

******* While God does hate human sacrifice, we do know he hates disobedience more. (c.f. Isaac, holding that the correct answer is not "Strewth, that would be immoral.") And a vow to God is Serious Business, so God's opinion would be at the very least uncertain if it weren't for the fact that Jephthah made it onto the Hebrews 11 list.

It would obviously have been better all around if he had never made the vow, but given that he did it's about as clear as Biblical anecdotes get that he had to keep it.

As for what the vow /was/, I feel like "I will sacrifice it as a burnt offering" is pretty clearly fatal.
:PROPERTIES:
:Author: notentirelyrandom
:Score: 3
:DateUnix: 1426555227.0
:END:

******** Biblical law clearly states that one cannot kill another person and a Vow is not a good enough reason.

A direct command from God is different from an obligation you took upon yourself.

There would be grounds to cancel the Vow anyway becase they didn't expect the subject to be human, even if it wasn't invalidated by not being legal.

Edit: see [[https://en.wikipedia.org/wiki/Jephthah]]
:PROPERTIES:
:Author: itisike
:Score: 2
:DateUnix: 1426555625.0
:END:


*** You need to escape your up-arrows.
:PROPERTIES:
:Score: 0
:DateUnix: 1426528632.0
:END:

**** Thanks.
:PROPERTIES:
:Author: robobreasts
:Score: 1
:DateUnix: 1426529951.0
:END:


** And this is why utilitarianism is wrong.
:PROPERTIES:
:Score: -6
:DateUnix: 1426496778.0
:END:

*** If you reject God's arguments here, it still only rejects 'total utilitarianism', not 'average utilitarianism'.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 12
:DateUnix: 1426496854.0
:END:

**** Fair enough. And I think average utilitarianism is closer to being correct. However, it /still/ includes the problem that it authorizes you to allow or create quite a lot of suffering so long as the /mean utility across everyone/ goes up. There are quite a lot of ways to do this that are trivially immoral, such as many "rich get richer" situations in real life (where the immorality arises from the fact that mean utility goes up at the present time, but power dynamics are created that allow some people to directly inflict suffering on others for their own interest).
:PROPERTIES:
:Score: 7
:DateUnix: 1426497246.0
:END:

***** #+begin_quote
  However, it still includes the problem that it authorizes you to allow or create quite a lot of suffering so long as the mean utility across everyone goes up.
#+end_quote

I don't understand this objection. Don't those scenarios mean something like 'going to war against Nazi Germany', which yes creates suffering in order to prevent more suffering?

What is /your/ position? Does it tolerate the creation of no suffering whatsoever?
:PROPERTIES:
:Author: ArisKatsaris
:Score: 8
:DateUnix: 1426524046.0
:END:

****** Easiest to answer in reverse.

#+begin_quote
  What is your position? Does it tolerate the creation of no suffering whatsoever?
#+end_quote

What Scott Alexander once called "post-hoc consequentialism": "There is a pie and two people. We each take half the pie, and don't have to calculate anything about unboundedly large numbers of imaginary people for complicated mathematical reasons."

I find it hard to state a complete algorithm or anything like that, since I don't know enough about cognition to give anything like a psychologically or normatively realistic algorithm that can actually be implemented by a real agent.

#+begin_quote
  I don't understand this objection. Don't those scenarios mean something like 'going to war against Nazi Germany', which yes creates suffering in order to prevent more suffering?
#+end_quote

I meant more like, creates suffering because it sets up power relations and otherwise systematically neglects certain actual individuals in pursuit of utility. To give a nice counterexample, average utilitarianism says that if I spend all my efforts on creating very happy people, mean utility goes up, and I didn't have to do /anything/ about the starving villagers Over There.

You can outlaw creating people /and/ suppose that we only deal with psychologically realistic humans in order to make pathological counterexamples or repugnant conclusions of utilitarianism go away, but those are such flagrantly ad-hoc moves that the best we can say is that average-utilitarianism is an admissible heuristic for a good moral code in those (unrealistic: birth and death are regular events) situations, rather than that average utilitarianism is normatively correct in all situations.
:PROPERTIES:
:Score: 2
:DateUnix: 1426528566.0
:END:


***** Chart pain as an exponential function? A thousand dust specs doesn't equal one stubbed toe, because greater concentrations of pain are worth substantially more.

Not certain /exactly/ how to deal with utility monsters, without having a clear understanding of how one could exist, or how to define sentients, with sentient meaning "something who's preferences I'm going to take into account".
:PROPERTIES:
:Author: traverseda
:Score: 5
:DateUnix: 1426509304.0
:END:

****** While I haven't actually done enough reading to have any answer to this stuff, I do think that by the time you've twisted utilitarianism far enough to get the right answers most of the time, you've gone so far from its original shape you might as well just not be using anything called "utilitarianism".
:PROPERTIES:
:Score: 3
:DateUnix: 1426511039.0
:END:


***** I never understood why people accept the premises that lead to repugnant conclusion so readily. Surely, the trivial solution is to accept that the life is worth having only if there is at least a minimum amount of happiness in it. Then you just specify whatever minimal level of happiness you consider sane and you are done.
:PROPERTIES:
:Author: AugSphere
:Score: 3
:DateUnix: 1426515735.0
:END:

****** #+begin_quote
  Surely, the trivial solution is to accept that the life is worth having only if there is at least a minimum amount of happiness in it. Then you just specify whatever minimal level of happiness you consider sane and you are done.
#+end_quote

Well yeah and that leads to the repugnant conclusion where rather than increase the level of happiness of people everywhere, you create more people until everyone is at the minimal level of happiness that makes life still barely worth living.
:PROPERTIES:
:Author: Anderkent
:Score: 2
:DateUnix: 1426520362.0
:END:

******* Why would you set the minimal level so low then, if people living at that level appals you? The utilitarian reasoning is not the problem.

- set a target level of happiness
- adjust population, while maintaining a uniform distribution of happiness over agents until you reach your goal.

As your society develops, adjust the target happiness accordingly.
:PROPERTIES:
:Author: AugSphere
:Score: 2
:DateUnix: 1426522136.0
:END:

******** Because the 'minimal level' isn't a question of an arbitrary boundary, it's a fact. At what level of happiness do people prefer to die rather than live? You'll find that level is pretty damn low. Saying the minimum is higher than that is effectively telling those people "hey your life isn't worth living, even though you think it is, you should die".

It's not any particular person living at that level that's intuitively appalling; it's the moving of everyone down to the minimally satisfying level in order to create more people.
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1426522832.0
:END:

********* I think that this dislike of repugnant conclusion is reflected in your utility function. It's reflected in pretty much everyone function, so if you did a proper calculation of expected utility CEV-style over existing minds (if that is even possible), you'd find that the actual preferred minimal level of happiness would turn out to be rather higher than the bare survival minimum.

Since this kind of calculation is obviously beyond us, the best we can do is target some reasonable level of happiness and adjust population accordingly.
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1426525537.0
:END:

********** #+begin_quote
  I think that this dislike of repugnant conclusion is reflected in your utility function. It's reflected in pretty much everyone function, so if you actually did a proper calculation of expected utility CEV-style over existing minds (if that is even possible), you'd find that the actual preferred minimal level of happiness would turn out to be rather higher than the bare survival minimum.
#+end_quote

That would be a very weird coincidence, if the two cancelled each other rather than one dominating the other.
:PROPERTIES:
:Author: Anderkent
:Score: 1
:DateUnix: 1426525696.0
:END:

*********** I weakly expect the existence and nature of human happiness set-points would lead to the outcome I predict, but it's not a sure thing by any means. In cases when I'm not sure what a proper utilitarian calculation would give me, I prefer to fall back on some sane course of action.

In any case, naive utilitarianism is a pretty flimsy theory in practice, since the calculations needed to actually use it are intractable, so arguing about some hypothetical paradoxes in it is a waste of time.
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1426527688.0
:END:


****** #+begin_quote
  I never understood why people accept the premises that lead to repugnant conclusion so readily.
#+end_quote

I think utilitarianism has a lot of truthiness to it?
:PROPERTIES:
:Score: 0
:DateUnix: 1426517548.0
:END:

******* You can still be a utilitarian if you accept that not all existence is preferable to non-existence.
:PROPERTIES:
:Author: AugSphere
:Score: 1
:DateUnix: 1426522203.0
:END:

******** The basic problem is that a lot of the time, utilitarianism seems to work pretty well. Some ethical and moral systems or whatever are obviously false or can never be used, or contradict themselves in obvious ways. Utilitarianism only seems to do this in extreme or unlikely examples.

You can spend a lot of time thinking about Utilitarianism and have it make sense, only for it to appear to fail spectacularly later. For example, the idea of having some sort of progressive income tax fits well with utilitarianism. The first $50,000 you earn give you a lot more joy than $50,001 through $100,000, so we tax your higher earnings more and give it to people who make less than $25,000, thereby increase total happiness. To a Utilitarian, it makes sense. It also gives us some info about how much we can tax. Pretty great! There's tons of examples like this. We get plenty of chances to basically think Utilitarianism is correct in everyday life.

This makes it much more jarring when we talk about Utilitarianism and find problems. If someone told me that there were flaws in catholicism, I would not be surprised. I don't use catholicism in my every day life. If someone told me that the basic laws of motion weren't correct, this would be much more jarring. I use those all the time. There are tons of contrived examples where Utilitarianism does not make sense at all. Since Utilitarianism is useful so much of the time, this is surprising to many and we start adding patches to try to make it work.

I find this acceptable. People don't go into Utilitarianism planning to divert a trolley to hit one guy instead of 5, but to find a framework that will let them determine the best tax rates or structure of society. I suspect it's tough to make a moral system that works in both normal circumstances and "you're God and literally creating universes" circumstances. Utilitarianism works well in the former and poorly in the latter.
:PROPERTIES:
:Author: blazinghand
:Score: 4
:DateUnix: 1426528556.0
:END:

********* Proper utilitarianism requires some tough computation and nigh-omniscient knowledge. What we do in practice is take the crudest possible approximation and use that, because it still works better than alternatives. And then we are dismayed when the crudest approximation of a theory returns strange results in some extreme cases. It's almost comical.
:PROPERTIES:
:Author: AugSphere
:Score: 3
:DateUnix: 1426529198.0
:END:


********* #+begin_quote
  I suspect it's tough to make a moral system that works in both normal circumstances and "you're God and literally creating universes" circumstances.
#+end_quote

I think that what we need here is a notion of bounded rationality for ethics: the degree to which we need our "morality algorithm" to be completely correct (in the sense that we would derive a clear contradiction if it was any /other/ way) scales with the amount of causal influence our agent's decisions actually have.
:PROPERTIES:
:Score: 1
:DateUnix: 1426528953.0
:END:


**** 'Average utilitarianism' seems silly to me. Is world with 5 super-super-happy people better than one with 5 previous people + 5 ordinarily super-happy people?
:PROPERTIES:
:Author: satanistgoblin
:Score: 1
:DateUnix: 1426627877.0
:END:

***** Yes, I believe that a multiverse where 100% of the people are super-super-happy is better than one where 50% are super-super-happy and 50% are only super-happy.

Your example of 10 people leads you astray because it makes your mind think of finite /rooms/, not of the whole multiverse across eternities and infinities. In your example, a room with 10 people (5 of them super-super-happy, and 5 of them only super-happy) would increase the average happiness of the /universe/ more than than a room of 5 super-super-happy people would, and so as a room is preferable.

But when talking multiverses, percentages may be a better way to evaluate whether one possible multiverse is better than another -- indicating how likely a mind is to find its preferences satisfied and to what extent.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 2
:DateUnix: 1426629534.0
:END:

****** So, if those happy people (who like to live) drop dead instantly the multiverse is improved?
:PROPERTIES:
:Author: satanistgoblin
:Score: 1
:DateUnix: 1426631778.0
:END:

******* If they like to live and want to keep living, their deaths would be events that thwarted their preferences, so no, the average utility of the multiverse would decrease.

5 people whose preferences are satisfied and 5 people whose preference fail utterly to be satisfied is worse (on average) than 10 people whose preferences are satisfied in different degrees.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 1
:DateUnix: 1426633003.0
:END:

******** But they are dead now and do not have preferences any-more, so why do you still take them into account?
:PROPERTIES:
:Author: satanistgoblin
:Score: 1
:DateUnix: 1426633503.0
:END:

********* As before, the answer is that I'm averaging across the /whole/ of the universe. This includes both the past and the future.

What you're doing when you're asking me whether the multiverse is improved by such-and-such an event, is asking which of two possible /timelines/ is preferable. Killing an existing person is /not/ morally equivalent to the person not having existed in the first place. (Murder is not equivalent to contraception/celebacy).
:PROPERTIES:
:Author: ArisKatsaris
:Score: 1
:DateUnix: 1426636924.0
:END:


** Fuck. That. Pun.
:PROPERTIES:
:Score: 0
:DateUnix: 1426754091.0
:END:
