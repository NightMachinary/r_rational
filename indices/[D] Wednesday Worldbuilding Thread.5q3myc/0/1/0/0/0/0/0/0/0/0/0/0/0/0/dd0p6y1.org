:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1485616185.0
:DateShort: 2017-Jan-28
:END:

#+begin_quote
  You say my arguments are 'insane' which I find quite arrogant for someone who hasn't refuted any of them. Certainly for someone who has said himself that he didn't really care if the human species continued to exist.
#+end_quote

I was saying that getting people to create an AI that they /know/ will wipe out humanity would require you get them to buy into some insane ideas, because it's literally suicidal. Remember it's not actually granting /them/ happiness because it's much more efficient to just do it for itself. See it's not even a matter of wireheading because there's no question that they aren't the one's actually getting the benefit here.

#+begin_quote
  In my scenario, all the existing humans get uploaded onto the servers and then 'simplified'. So they all gain from it. Also, since the robots will create new digital 'beings' far faster than humans can possibly reproduce, the 'simplified' humanity will reproduce extremely rapidly. The only difference is that they aren't really humans anymore. They don't have concepts such as autonomy, freedom, self-consciousness,... but these concepts don't even apply to them. So the only thing you need to say is: "If you follow me, you will have much, much, more happiness and we will spread much more happiness to the entire galaxy. You won't have all the things you like now, but you won't care since you won't like them anymore."
#+end_quote

Remember my original point was that the goal of maximizing happiness would /not/ lead to wireheading humans, because it's much more effective to kill all humans and just maximize your own happiness which saves resources you would have to waste uploading (albeit crudely) humans.\\
Though I got sidetracked on how extremely uncommon and terrifying to most people your non-problem with wireheading is.