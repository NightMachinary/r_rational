:PROPERTIES:
:Author: TracyHarms
:Score: 1
:DateUnix: 1524533646.0
:DateShort: 2018-Apr-24
:END:

I did not say it was foreshadowed in previous books, I said I'm thinking it had been in mind all along. Some may not like the lack of foreshadowing, but I do. It was a *surprise*. That's the nature of surprises. It provides, through example, the idea that hyperintelligent AI can be expected to learn things that go beyond what humans have learned, or perhaps even beyond what humans are capable of learning. Such a thing would have to be very significant in order to get the point across that it's a breakthrough of understanding, and it would have to be consequential to the story in order to be more than a nominal adornment. What I encountered was persuasively both.

You and I agree that the author wanted to encourage conversations and thinking about AI/superintelligence take-off. In these novels we get more than one model of what this might be like. None of those models depend on the piece you've been objecting to. These sketches of takeoff in the Crystal Trilogy are all in keeping with the usual preconceptions people have about people, machines, intelligence, data, computation, and physics. The thing you dislike serves various purposes, but it does not do anything to make the main problem plausible. This series is not trying to describe why AI might gain qualitative breakthroughs in competence. In my opinion it's trying (among other things) to explain why people who think these breakthrough are possible think that such changes would be extremely dangerous and fabulously difficult to neutralize or contain. The series as a whole, and Crystal Eternity especially, drives home that idea again and again. It does so mainly through presumptions that are commonplace today in science and technology. The aspect you've been complaining about isn't required for the AI threats depicted to be credible, plausible, or possible.