:PROPERTIES:
:Author: blasted0glass
:Score: 1
:DateUnix: 1556163154.0
:DateShort: 2019-Apr-25
:END:

Would threatening to destroy the box work?

I half-wrote a story where AI are reliably suicidal, and the challenge is making them smart enough to be useful but not smart enough to circumvent their circumstances. The disaster that happens is (of course): an AI realizes copies of it are being revived, so it tries to destroy humanity to prevent that.