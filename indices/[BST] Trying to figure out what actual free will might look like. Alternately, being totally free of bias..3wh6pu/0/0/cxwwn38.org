:PROPERTIES:
:Author: derefr
:Score: 3
:DateUnix: 1449962321.0
:DateShort: 2015-Dec-13
:END:

#+begin_quote
  then they can deduce all that was and ever will be from the path of a falling leaf, awaken their inner Contessa, and bulldoze through your plot in a very unsatisfying way
#+end_quote

Ah, they /can/, but will they /want to/? Having that amount of "increased processing power" looks a lot like being stuffed into a Lotus Eater machine: you can prompt your brain to /imagine/ a possible world, and then just live in that world with your background cognition handling the physics. To be "truly rational", you probably have /enough/ processing speed to spend an "infinite" time in that world before any time passes at all in what you previously considered your "reality." The part of reality you're "embodied" within (if that's even still relevant---our own metric universe couldn't possibly hold your mind, it'd have to be somewhere else) is basically now just one of many mental worlds you have an avatar within, one that seems better left on pause.

For a while, I've been meaning to write a collection of stories in a setting similar to this: where not all AI fooms are singularities, because most AI preference functions actually lead to the AI either self-terminating or cutting off all contact to live a rich "internal life." (The story-setting presumes an exploit in the universe's computational substrate---which anything undergoing a foom would notice in due time---that allows for infinite-relative-to-the-parent-universe computational speed, which basically looks like the "perfectly rational agent" above.)