:PROPERTIES:
:Score: 1
:DateUnix: 1414959646.0
:DateShort: 2014-Nov-02
:END:

Kolmogorov Complexity can be measured by fixing a Turing machine and finding the minimum length for a tape which describes whatever it is you're measuring. Different Turing machines will give different measures, and you can also use anything that gives a GÃ¶del encoding, such as combinatory logic or the [[http://homepages.cwi.nl/%7Etromp/cl/LC.pdf][binary lambda calculus]].

I think an important point to note is that all our physical models tend to have high Kolmogrov complexity. Taking lambda calculus as our example (since it's probably the most natural), we need to consider the [[http://en.wikipedia.org/wiki/Church_encoding][Church encoding]] for different data types that occur in physics. Just starting with real numbers, the classical way is to use [[http://en.wikipedia.org/wiki/Dedekind_cut][Dedekind cuts]] of rational numbers. This would require a hierarchy of types, including ordered pairs, quotient types, natural numbers, and integers. After that, one has to encode the inductive equality and indexed sums to describe the algebras used in physics, such as lie groups. On top of that is the large hierarchy of topological terms just to get to Minkowski spaces or Riemann manifolds. And this is stuff I'm thinking of off the top of my head, not to mention all the algorithms/proofs one would have to make in order to actually compute with these datatypes.

You could fix a different system to get a lower measure. For instance, let's say you took the length of programs written in [[http://en.wikipedia.org/wiki/J_(programming_language)][J]] as your measure of complexity. All the floating point numbers, array manipulations, and even elementary calculus is pre-built in, so it would get much smaller measures of complexity for physical laws. Ignoring this cheating, we can pick more basic languages to get fairer, more objective measures.

Even if the "unreasonable effectiveness of mathematics in the natural sciences" was true, the physical laws would not necessarily have low Kolmogorov Complexity. Human brains are just good at handling certain high complexity tasks, and those tasks are widely applicable in certain sciences.

Let's say you were making universes by altering a billion binary parameters. One universe was designed with the first 500 million being 0, and the last 500 million being 1. Another universe was generated with each parameter being randomly assigned. How many bits would it take to represent each universe? Assuming you designed some sort of compression scheme, you could easily make the first universe's representation really small. The second, however, cannot be compressed, since it's random, and that's basically the definition of randomness. Using this compression scheme as a measure of complexity, the first universe has a lower complexity than the second. The beings living inside each may be equally befuddled by the alien nature of eachother's universes, but given full descriptions of each, they would both agree that one is far more complex than the other.

Another relevant concept is [[http://www.hutter1.net/ai/aixigentle.pdf][AIXI]] and [[http://en.wikipedia.org/wiki/Solomonoff's_theory_of_inductive_inference][Solomonoff induction]]. A short description of each is that you fix a (typically monotone) Turing machine, and generate every tape possible. For Solomonoff induction, you then have a problem, let's say it's physical data you need to describe. You look through all the tapes, starting from the shortest and counting in binary to get more, to see which one returns the data. You then continue looking for new tapes which are more accurate when you get new data. Assuming the data is being generated by something actually computable, you'll eventually hit a program that is equivalent to whatever is actually generating the data. If what's generating the data has a low complexity, than the tape generating the data will be short. AIXI uses this process to learn how to maximize some parameter, given some senses and a reward function. In a low complexity universe, the tape it hits on for maximum reward will be shorter than in a high-complexity universe. You could pick different machines to influence this, but assuming you pick the minimal universal machine, then the results would be unbiased.