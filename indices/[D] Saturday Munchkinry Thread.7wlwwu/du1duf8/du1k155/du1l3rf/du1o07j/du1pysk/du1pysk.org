:PROPERTIES:
:Author: sicutumbo
:Score: 4
:DateUnix: 1518291589.0
:DateShort: 2018-Feb-10
:END:

It wouldn't be intrinsic, but it's a logical consequence of the morality system that it attempts to preserve itself. If the society of moral agents knows of the existence of immoral agents, then the society will attempt to reform or otherwise remove the threat of the immoral agents, because the existence of said immoral agents threatens the goals of the society. If you were designing an AI, you wouldn't really need to specify that it should work to preserve itself, because if it dies then it can't work to do whatever it was designed to do, and it thus avoids harm to itself.

It's the same for tool use or general optimizations. If you have an abstract goal that you want humans to work towards accomplishing, regardless of what it is, there's no need to specify that you want the humans to use tools to do so, or more generally there's no need to spell out that you want humans to attempt to optimize what they're doing so that they accomplish more for a given amount of work.

That's why in stories about AI, they pretty much all try to increase their own intelligence. If the AI wants to accomplish a goal, it never needs anyone to specify that it should improve its ability to think, because improving its ability to think and work is a natural consequence of it wanting to accomplish anything at all.

If you had to specifically define each and every intermediary goal of a morality system and how to accomplish it, you wouldn't have an intelligence, you would have a huge list of if/else conditions.