:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1518295559.0
:DateShort: 2018-Feb-11
:END:

As I outlined in my answer I don't think this scenario is anywhere near as difficult to solve as the AI alignment problem, and even if it was you can always do the suboptimal solution wherein you just tell the AI to copy the ethical system you had say 5 minutes ago (to prevent it from changing your ethics). Sure that solution is suboptimal because it precludes "moral progress" however at the very least it's still going to be pretty amazing compared to any other solution anyone's currently came up with.

Plus this is nowhere as difficult as the AI alignment problem because your starting point is human ethics as is, so you can put in clauses about hedging things based on common sense and count on that to stop many AI failure modes because most of humanity already shares a pretty massive amount of moral ground.

Or of course you could go with the strategy I outlined in my comment..