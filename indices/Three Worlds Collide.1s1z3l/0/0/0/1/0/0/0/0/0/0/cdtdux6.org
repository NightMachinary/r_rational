:PROPERTIES:
:Score: 2
:DateUnix: 1386172568.0
:DateShort: 2013-Dec-04
:END:

#+begin_quote
  Coherent Extrapolated Volition is a hard problem.
#+end_quote

Nastier than that (I've been meaning to type out a LessWrong post on this issue). There's a number of big issues with CEV itself:

- When you get down to it, CEV is a very /simple and intuitive/ meta-ethical specification on its face. "Your CEV is all things we can do to you and your environment such that you'll approve of the plan prospectively /and/ retrospectively, with this approval property holding under enactment of many successive plans."

- But what about in the middle? I mean, is it ethical to torture you for 300 years if it will, for instance, prevent the heat-death of the universe? You might approve before and after, but every "before" and every "after" are, at some point, the present. If we approve of our lives only when we're not actually living them, we've done something wrong there, too.

- CEV is only a /declarative/ description of what we meta-want. There's nothing written there about how to actually compute/deduce even /one/ pre/post-approved plan, even though we humans can think of some very easy /conservative improvements/ to our lives (things like: "nobody ever goes hungry, goes thirsty, gets horrible diseases, or dies, ever again" are generally considered pretty reasonable).

- The issue of what it means for the CEV's beneficiary to "approve" packs a metric fuckton of hidden complexity, and we haven't even considered the issue of whether the beneficiary is one human, a group of humans with possible speciation into diverse groups, a group of humans under enforced unity, or something else entirely. Hell, for AI purposes, would you even consider a single human being as a unitary agent, or is it more appropriate to model real humans' values as collections of disparate and interacting agents?

- That second bit about approval holding under successive plans packs a metric fuckton of hidden complexity. Successive CEV plans for 1 year of life should not add up, after 10 years, to something your original self would disapprove of, if the original had the knowledge available at the end of 10 years -- how do we enforce something like that given the limited foresight of the real world?