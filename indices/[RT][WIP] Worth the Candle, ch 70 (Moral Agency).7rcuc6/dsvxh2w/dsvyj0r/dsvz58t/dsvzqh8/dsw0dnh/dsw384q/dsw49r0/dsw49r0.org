:PROPERTIES:
:Author: SoylentRox
:Score: 1
:DateUnix: 1516316090.0
:DateShort: 2018-Jan-19
:END:

#+begin_quote
  The simulation needs to be not unbiased, but heavy biased instead. Last time i checked, there is no such thing as "objective good" or plain "good" in the terms of universal constant.
#+end_quote

You have "terminal values". That's what your goals are, what your intentions are to accomplish overall. You cannot guarantee your goals are accomplished as well as you hope within the laws of physics of this universe, but you can at least compute the action that has the highest probability of success, limited by the data, algorithm architecture, computing power, and memory you have available.

So if your terminal values are "have at least as many people from the generation of your creators live functional, reasonably happy lives for as long as possible," then a cure for aging, even if it does cost you some deaths (you "pay" a cost per death, a bigger cost if it your fault) is worth it.

So no, morality isn't an absolute. But we humans do have a rough idea of what we would rather have, and once we decide on that, given a set of data there is only one optimal course of action that maximizes the expected gain towards our terminal values.

I am aware that our language doesn't have the ability to describe such values, nor can it fit in a few lines of programming code or math. At least, not complex values like "happiness" and "living".

I figure we'll find a reasonable way to describe those things eventually, but first we need to stick to values that are simple. A robot in a test cell, where it's terminal values are "get these red balls into this output hole as fast as possible, with points off for damage to your actuators or impacts"

Or, the terminal values are actually "+x reward/ball". "-y reward for time passing. -z reward for impacts * energy_impact^{2"} . Choose max(reward)