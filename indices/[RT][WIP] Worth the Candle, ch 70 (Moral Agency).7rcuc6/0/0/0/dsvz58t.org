:PROPERTIES:
:Author: SoylentRox
:Score: 2
:DateUnix: 1516311006.0
:DateShort: 2018-Jan-19
:END:

But then if the "Evil" hero gets better outcomes, as measured by a set of terminal values that both the Good and Evil heroes share, doesn't that make the Evil hero more "Good" than the good hero?

That is, ultimately what /matters/ is the expected outcome of a given set of actions. Measured against what you care about - whatever that is - the /best/ choice to make in our reality is that one that results in the best /expected/ outcome. (it may be wrong but you update your model with each actual outcome and always choose the best from what you know about)

"Good" means "I'm not willing to do certain actions, whether or not they result in a better outcome". Which is evil.

If I increase the probability of 1 million people dying from old age because I'm not willing to kill 100 people as test subjects for a treatment for old age, I'm evil.