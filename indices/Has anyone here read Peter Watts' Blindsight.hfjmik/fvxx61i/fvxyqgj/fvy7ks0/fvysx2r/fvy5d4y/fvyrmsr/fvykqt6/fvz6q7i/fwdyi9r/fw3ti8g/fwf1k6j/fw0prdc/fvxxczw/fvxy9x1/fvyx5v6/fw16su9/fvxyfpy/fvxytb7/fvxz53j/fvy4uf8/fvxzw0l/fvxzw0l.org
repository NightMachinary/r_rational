:PROPERTIES:
:Author: Transcendent_One
:Score: 2
:DateUnix: 1593083965.0
:DateShort: 2020-Jun-25
:END:

A possible method, for example, would be ascribing "goals" to non-self-aware agents, based on what do they tend to do, how does their behaviour change in various conditions, etc., and evaluating their performance with relation to these "goals". Of course, this is just a projection, it could be potentially faulty; in a universe where self-aware lifeforms went extinct and non-self-aware ones dominated, the self-aware ones could argue about self-awareness being necessary for the "win" to count...oh wait, they couldn't.