:PROPERTIES:
:Score: 4
:DateUnix: 1516056314.0
:DateShort: 2018-Jan-16
:END:

So, they've announced [[https://www.lesserwrong.com/posts/4WbNGQMvuFtY3So7s/announcement-ai-alignment-prize-winners-and-next-round][the winners of the first round of the AI Alignment Prize]]. [[https://medium.com/@pwgen/friendly-ai-through-ontology-autogeneration-5d375bf85922][One of them in particular]] caught my eye as requiring snark in the form of a Facebook tag-group thingy, but I didn't want to snark too nastily in a real-name format, in case the person didn't Mean Anything By This.

#+begin_quote
  Imagine a company called Autogenerated Fantasy Worlds Inc. They develop software that ingests books/movies/video games related to some fantasy world (Star Wars, Harry Potter, or even traditional gaming worlds like Legend of Zelda) and automatically generates a virtual reality MMORPG corresponding to that world. Instead of handcrafting dozens of digital words, the company only needs to refine a single ontology autogeneration software package. It doesn't need to be perfect to begin with, as long as it saves labor over handcrafting. As players send in bug reports regarding inaccurate aspects of a world, the company's profit incentive becomes aligned with solving the ontology autogeneration problem at maximum fidelity. NPC-related bug reports would be especially useful, since they would provide information about your ability to model a character's values. Given the profit potential of this project, it might be possible to attract VC investment and use little philanthropist money.
#+end_quote

@sounds like your business plan would destroy the world if successful but ok

Keep in mind, [[https://improbable.io][last year]] I had to sit through a presentation where someone basically proposed to build the Matrix, then protested that they didn't mean it in the /Matrix/ or /Black Mirror/ way, so I'm getting less surprised to see This Will Inevitably Go Wrong scifi cliches as serious ideas.