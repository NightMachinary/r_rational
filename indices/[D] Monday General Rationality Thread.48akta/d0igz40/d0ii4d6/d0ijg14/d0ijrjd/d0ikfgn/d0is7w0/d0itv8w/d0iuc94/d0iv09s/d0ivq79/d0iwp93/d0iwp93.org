:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1456801155.0
:DateShort: 2016-Mar-01
:END:

#+begin_quote
  I actually thought people were talking about a mix of conventional hedonic utilitarianism (pure-strain Peter Singer EA-types) and conventional preference utilitarianism (most everyone else).
#+end_quote

I don't believe it's necessary to be a hedonic utilitarian to be an EA at all. I just want to make it clear that when I say I'm infected by EA, I'm not talking about hedonic utilitarianism or Peter Singer in particular in any capacity. I'm talking about scope-sensitized empathy and effectiveness evaluation and distribution of interventions.

#+begin_quote
  Doesn't using it as a relativist framework require some way to normalize preferences across individuals so they have the same numerical scales for the same subjective strength of preference?
#+end_quote

Naturally. I don't believe there is a single singularly compelling normalization schema, however. Markets are a fair try but don't actually exist and depend on resources as intermediaries. Normalization is done when comparing utilities, but as there is no universal reference frame, the normalization is itself relative.

I could handwave some mathematical formalism where two people's utility functions contain terms for the other's utility, and eventually some convergence might be reached, but I can't guarantee convergence and I doubt there aren't pathological examples in reality where two empathetic beings literally cannot decide. Pie distribution comes to mind as a fairly familiar model.

#+begin_quote
  If you ask, "Do our moral judgements pick out real (although possibly local) properties of the world?"
#+end_quote

I'm not entirely sure what that means. Do you mean that there are things that will objectively make us (in the instant) happy or sad, or harmed or helped?

I also have an issue here pertaining to existentialism and self-actualization. I think you should be free to choose your preferences by System 2, and to modify yourself so that your System 1 reacts to reality accordingly. (That's another problem with using the standard mathematical formalism to talk about utility, our utility functions mutate.)

#+begin_quote
  it becomes impossible to have a disagreement over moral facts
#+end_quote

Well, I don't think so. I think that moral "facts" don't exist insofar as they are always relative to some preference system, but they are facts when considering the reference frame. I also think that we can have useful conversations about relative preferences by talking about people in classes, and trading values against each other. For my Ethics final, I made an argument that preference relativism can be used to describe society as constituents collaborating with a preference system generalized over them all, and that trade with society is generally good because the constituents are more social than not, comparative advantage and specialization makes sociality a positive-sum game, and that this in effect can counteract the individual loss of utility for each person where they differ by raising the utility where they share. I can't talk more right now, or even edit, so I'll leave it at that rather muddled run-on sentence.