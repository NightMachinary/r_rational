:PROPERTIES:
:Score: 2
:DateUnix: 1408017562.0
:DateShort: 2014-Aug-14
:END:

#+begin_quote
  Goal 2 should be reached before goal 1. Basically. Yet, massive economic pressure to just create AI is hugely outclassing the effort to control it.
#+end_quote

Really? Because I don't actually see that much economic effort being poured into AGI, compared to how much goes into most other fields of theoretical computer science.

What I will say is that people mostly don't seem to work on Friendly utility functions for several reasons:

1) They think they lack the philosophical framework to conceptualize a "controlled" or "safe" utility function besides reinforcement learning.

2) They think they lack the mathematical ability to describe a non-learned utility function at all (this is true: we do currently lack that mathematical ability).

3) They think reinforcement learning will be good enough, since after all it's been ok up to now.

The fact that they don't just endorse doing whatever their dopaminergic circuits consider the Most Interesting Thing at any given time doesn't seem to occur to them.