:PROPERTIES:
:Author: jesyspa
:Score: 2
:DateUnix: 1468304756.0
:DateShort: 2016-Jul-12
:END:

#+begin_quote
  I have trouble comprehending why an *intelligence* would only ever pursue the goals it was assigned at creation
#+end_quote

I think you may be using "intelligence" to mean both consciousness and proficiency at achieving one's goals, which leads to confusion.

IMHO, consciousness is still a wide open problem and any chains of reasoning like "Alice displays behaviour X, so she is conscious, and should also display behaviour Y" is suspect. I don't think your position is outrageous -- I expect conscious agents to have Knightian Freedom, and I /think/ that makes a simple utility function impossible -- but I'm also pretty sure it's not been shown to be the case.

On the other hand, there's no need for a paperclipper AI to be conscious; it just needs to be really good at making paperclips. If you look at it as just a very good player of the paperclip-making game, it's unclear why it would switch to anything else.

From what I've seen of Friendly AI research, it seems like the whole point is that we don't yet know how to estimate what goals an agent we create will have, or how powerful the agent will be. Once you can accurately judge how effective an agent will be it's nice to talk about the Why and Why Not, but until you do, the How and How not are more pressing.

(That said, I've only read bits of the debate, so I apologise if that was already covered.)

Finally, I don't think CelestAI's limitations on her goal function are all that different from how humans behave. There have been plenty of people trying to better the world who were only willing to see it happen as per some doctrine (religion being the prime example). If questioned as to why, they may even have admitted it is due to their upbringing, but knowing that doesn't make them suddenly feel like it's okay to do otherwise.