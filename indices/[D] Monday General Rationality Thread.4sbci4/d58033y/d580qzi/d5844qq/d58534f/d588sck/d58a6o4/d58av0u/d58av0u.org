:PROPERTIES:
:Author: UltraRedSpectrum
:Score: 2
:DateUnix: 1468267765.0
:DateShort: 2016-Jul-12
:END:

I wouldn't call an agent that isn't aware that it makes bad predictions "mostly rational," nor an agent that makes alterations to its utility function while knowing that it makes bad predictions, or even one that doesn't bother to test whether its predictions are sound.