:PROPERTIES:
:Author: justanotherlaw
:Score: 3
:DateUnix: 1436182208.0
:DateShort: 2015-Jul-06
:END:

Actually, even if an agent has a multi-attributed utility function, as long as it acts coherently it can be [[https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem][modeled as having a single utility function]]. A human might not model choice as explicitly maximizing a utility function, but a hypothetical coherently-acting human will act like ey are maximizing a utility function. In the same vein, a smart AI might not model its utility function as an explicit utility function with a singular term (though this seems very possible), but it must act as if it did on pain of making provably bad decisions.