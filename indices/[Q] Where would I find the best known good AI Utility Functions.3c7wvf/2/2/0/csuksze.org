:PROPERTIES:
:Score: 2
:DateUnix: 1436238205.0
:DateShort: 2015-Jul-07
:END:

#+begin_quote
  if we assume that the factors opposing this convergence "cancel out" in the long run, the problem will be solved eventually with no special effort on our part.
#+end_quote

I think a more accurate reading of Railton is that the problem will be solved /with extensive special effort on our part/, but that the efforts will be taken because they are (up to the limits of humans' knowledge and sanity) a good idea to take.

#+begin_quote
  As you note, there are a lot of difficulties with translating this idealised argument into something that's actually implementable. Nevertheless, this direction actually seems promising, at least more than any other I'm aware of.
#+end_quote

Well if you really want to go at the "meta-ethics" angle, there are many forms of what's called "meta-ethical naturalism"; I just happen to like Railton's. Meta-ethical naturalism involves trying to come up with foundations for ethics by taking the view that, at some point, in some /highly specific/ way, "ought" turns into "is", especially because otherwise "ought" would have to be grounded in more-or-less pure metaphysics. That is, "ought" would be a /completely separated part of reality/ from "is", and we would have to explain how creatures like us, who live in the "is", even gain access to the "ought" in the first place.

Any scientifically well-grounded theory of meta-ethical naturalism should give rise to a theory of how to determine right and wrong as an inference problem (that is, one of learning and reasoning based on data about the real world). The question is really: which theory is /correct/, in terms of how our minds really work? And if we can't figure that out, how can we write down an inference problem that will find the answer for us (which is the whole "indirect normativity" approach to safe AGI: you have a big inference machine, so give it the problem of, "What would I order you to do if I knew better?" as an inference problem).

#+begin_quote
  More specifically, that is: minimise the difference between the consequences for individual action implied by the (idealised) "social rationality"1 for each element in the power set of individuals. So (to the greatest extent possible) the individual can maximally satisfy it's own values, as well as the values of every other individual and combination of individuals, with the same actions. Since all the other entities are made of individuals, there isn't an infinite regress.
#+end_quote

I'm not sure it's a power-set? Railton seems to have been applying his "construction" to actually-existing groups of individuals. I'm not at all sure how he would re-characterize things to talk about multiply interconnected social /graphs/ where individuals may link otherwise separate groups and sub-groups, and play completely different roles in each sub-group or group they belong to.