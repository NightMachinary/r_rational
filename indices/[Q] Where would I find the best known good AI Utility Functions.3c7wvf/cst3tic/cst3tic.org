:PROPERTIES:
:Author: MugaSofer
:Score: 8
:DateUnix: 1436127647.0
:DateShort: 2015-Jul-06
:END:

There are no known good utility functions.

Eliezer, who founded the Machine Intelligence Research Institute IIRC, wrote up [[http://intelligence.org/files/CEV.pdf][this]]; but that's more of a /definition/ of "a good utility function" than an /example/, and it's not considered that important for practical purposes.

I believe most research in the area these days is more into keeping goals /stable/, dealing with self-modification, that sort of thing. With a vague eye toward some way of putting in [[http://lesswrong.com/lw/v1/ethical_injunctions/][safeguards]] that an AI wouldn't /want/ to work around; ways of having the AI shut down a plan if you don't like it that don't result in the AI just not telling you it's plans, that sort of thing.

If you want to see the /papers/, a lot of them deal with "tiling agents", and I can't understand a word of them beyond that point.