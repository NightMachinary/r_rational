:PROPERTIES:
:Score: 6
:DateUnix: 1436155745.0
:DateShort: 2015-Jul-06
:END:

I actually had a really, really, /really/ long post on this almost entirely typed up... and then a keyboard slip made my browser go "Back". FFFFFFFFFFFFFFFFUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU

The easy TL;DR is, "A computational implementation of Railton's /Moral Realism/", but that's not necessarily supported by everyone, because it's only one level less hand-wavy than trying to directly specify a code of ethics in English words, and only two levels less hand-wavy than "Pick something nice-sounding, throw it at a Magic Genie, and hope things come out well".

So here's a completely bullshit theory that's /sorta/ based on reading some stuff Nate ([[/u/So8res]] on LW) wrote, and /sorta/ based on my own studies, and /actually/ needs /several metric fucktons/ more evidence behind it before anyone should put serious stock in it. But it's a start?

We think that the human mind learns big, recursive hierarchies of models of the world in order to function. As in, you'd be surprised how many "layers" of models are needed to just understand and apply basic arithmetic; you are a /ludicrously/ deep, well-trained neural net, by the standards of our current science of neural networks. Each of these models will have some variables about it called "features". Some of these models, call them concepts since Nate's writing agrees with my reading there, are "feature-governed" concepts, like "Santa Claus" (a /specific combination/ of appearance, sound, and texture are necessary to make an object be Santa Claus). Others are more sophisticated: "causal role" concepts defined by what the object thus classified causes to happen. Even seemingly simple things like "uncle" can be role-governed, in the sense that while yes, an object does have to be a /male human/ to be your uncle, it also has to be /your parent's brother/; "uncleness" is a /relationship/ involving three different objects. These concepts are harder to learn, but human minds do tend to form them once we've got sufficiently much training data and a vocabulary of feature-governed concepts to bootstrap relationships between.

Well, according to the Nate article I remember reading, the human mind learns not only perceptual features and causal structure, but /evaluative/ features -- things about the modelled pieces of reality that can be rewarding or punishing for the human agent. The mind then does not make choices in order to maximize expected reward as a causal function of the world-state - which would be computationally intractable since the mind rarely knows the real causal structure of the world in sufficient detail to maximize reward - but instead to maximize expected reward as a function of the evaluative features, which thus function as variables-correlated-with-reward.

Of course, in actuality, there may be multiple "reward" variables, and we distinctly care not only about the evaluative features, but about the process for learning them. We've managed to vaguely hypothesize such a thing as "human preferences", but not yet to talk about "Under full information and full reflective rationality", let alone talk about such utilities for massive numbers of people in ways that include the relationships between those people and thus capture /socially rational/ evaluation (ie: the relationships between members of an in-group and each-other, the relationships between members of an in-group and the abstracted group identity or goals as a whole, in-groups versus out-groups, etc.). Or we can bullshit ourselves and say, yeah, socially rational evaluation is just evaluative features of the human mind's models of social relationships, but that again involves knowing roughly how the human mind is modelling the world, especially as applied to a special category where special-purpose social cognition kicks in instead of all-purpose causal modelling. But the bullshit theory would be ever-so-elegant if it were true. Alas: I would need a lot more reading and some actual professional would probably have to run a bunch of experiments, with well-developed alternative hypotheses and decades of cognitive-psychology literature under their belt, to treat this seriously.

So how do we address "full information"? Well, that's just a matter of "porting" the human evaluative features from the human world-models (and person-models) over to the AI's (presumably more accurate) world-models, and then making damn sure the AI's models are genuinely more accurate. That "porting", though, is a massive ball of unsolved transfer-learning problems. The upside is that this kind of transfer learning (translating features and parameter values from one model to another, when both models capture the same objective phenomenon at different levels) is a can of worms you more-or-less /have to/ open in order to get a software learner that can think about its environment in a reductionistic, scientific way when necessary, while remaining tractable for its own scale of environment when not necessary. At least, to my knowledge it is. There's also no general-case guarantee, short of requiring the evaluative-feature functions over model-states (settings for the model's non-observable variables) to be cheaply invertible, that we /can/ translate back from evaluative features to model-states to other-model-states to "port" features from one model to another. But it's at least a stab at "full information".

Ok, now how to talk about "full rationality"? That's a biiiiiiig can of worms, and not just because "rationality" is a normatively-loaded term in common discourse. We can say "reflective equilibrium", the state of having resolved all conflicts among one's merely factual beliefs, and that reduces it one step, but it only addresses factual beliefs. Presumably, if we have some way to port evaluative features while maintaining their relationship to the original reward levels, we'll have some way to "do the arithmetic" on conflicts between evaluative features (since some will be negative, representing "neg-reward" or "punishment", since AFAIK, the brain has "good stuff" and "bad stuff" in separate neurotransmitters and quantities, rather than on one real-number line like economists enjoy pretending). But then we need some way to talk about the causal roles and valuable causal relationships /between/ the environment and the human agent. A naive value-learning algorithm might accidentally learn, "Find a causal path that maximizes the human's reward signal and does that", and this will be something like wireheading or some other drugging. A slightly less naive one might learn, "Find the causal paths that maximize the human's learned evaluative features, and do that", and if this doesn't take the causal relationship between the human and the environment into account, it will be a Lotus Eater Machine. You need some way to learn evaluative features /of the two-way relationship between the human and the environment/, so that you can separately capture preference-concepts like "freedom" (from having some external force optimize the human's choices in ways they didn't deliberately cause) or "meaningfulness" (in the sense of having causal access to as much of reality as possible, rather than being "trapped", knowingly or unknowingly, or simply being unable to affect one's environment) or "a bit of $YOUR_FAVORITE_WEAK_DRUG is actually ok sometimes" (because sure it /slightly/ wireheads the human, but even its long-term effects are balanced out by their remaining desires for other things).

Once you've figured out a remotely sane way to talk about that human-environment causal relation, and in fact about the past-present-future human causal relation, such that you can evaluate prospects like "Should I take up an addictive but pleasurable drug?" or "Should I rewrite my basic emotional cognition to have a higher baseline happiness?", /then/ you can /maybe/ even /begin/ to talk about, "Human preferences under full information and full [reflective] rationality", and attempt to build a proper "Do What I Mean" agent whose judgements you'll be able to /trust/.

And then there are doubtless endless weaknesses I haven't managed to think of, because it's late and I'm tired. Oh, and this whole thing is probably built on utter sand, since I haven't read /nearly/ as many papers on evaluative cognition as I'd like. Such papers /do/ exist, though, including papers on the basics of "morality" as a kind of social cognition. So it's not as if the material isn't out there for the professionals to go through and use.

*TL;DR: Ask [[/u/xamueljones]], as he actually studies human cognition and thus might actually know something where the rest of us are just building castles in the clouds for the fun of speculation.*