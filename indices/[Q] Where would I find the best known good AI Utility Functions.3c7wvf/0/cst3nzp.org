:PROPERTIES:
:Author: Empiricist_or_not
:Score: 8
:DateUnix: 1436127339.0
:DateShort: 2015-Jul-06
:END:

I'm not all the way up to speed on this academically, but I think we are a ways of from there yet. I've got a way to go in my own professional work before I can start looking in this direction. I would recommend Yudkwski's interesting, but not peer reviewed, and a bit fluffy papers on the topic as good entry points:

General intelligence and seed AI (which I can't find a copy of ATM, so will not be re-posting mine)

[[https://intelligence.org/files/CFAI.pdf][Creating Friendly AI 1.0: The Analysis and Design of Benevolent Goal Architectures]]

In more general terms you've got some gross conceptual errors from fiction if you think a utility function will be in human readable terms. How will the AI define "human" after all? How will it define "life"? Or lets just get to the scary one, how will it define "good?"

To use a politically spidery hand grenade, if an AI has some value that collaborates to promoting the survival of each individual within it's power to affect,(without driving it mad), what will it's actions be in the domains of abortion, capital punishment, suicide, hospice care, and do not resuscitate orders? All of which will depend on it's conceptual definitions, and humanity has some interesting disagreements on these edge cases as it is.