:PROPERTIES:
:Author: zarraha
:Score: 4
:DateUnix: 1473745689.0
:DateShort: 2016-Sep-13
:END:

As a utilitarian and game theorist, I believe that most if not all problems people have with utility is that they fail to define it sufficiently robustly. Utility isn't just how much money you have, or material goods, it's happiness, or self-fulfillment, or whatever end emotional state you want to have. It's stuff you want.

A kind and charitable person might give away all of their life savings and go help poor people in Africa. And for them this is a rational thing to do if they value helping people. If they are happy being poor while helping people and knowing that they're making the world a better place, then we can say that the act of helping others is a positive value to them. Every person has their own unique utility function.

A rudimentary and easy adjustment is to define altruism as a coefficient such that you can add a percentage of someone else's utility to the altruistic persons. So if John has an altruism value of 0.1, then whenever James gains 10 points, John will gain 1 point as a direct result just from seeing James being happy. And if James loses 10 points John will lose 1 point, and so on.

Thus we can attempt to define morality by setting some amount of altruism as "appropriate" and saying actions which would be rational to someone with more altruism than that amount are "good" and actions which would not be rational to someone with that much altruism are "evil". Or something like that. You'd probably need to make the system more complicated to avoid munchkinry, and it still might not be the /best/ model, but it's not terrible.