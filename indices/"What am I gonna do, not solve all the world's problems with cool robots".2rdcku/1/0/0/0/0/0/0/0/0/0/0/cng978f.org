:PROPERTIES:
:Author: psychothumbs
:Score: 1
:DateUnix: 1420561412.0
:DateShort: 2015-Jan-06
:END:

What's the logistical problem? Worst case scenario, the AI's intelligence should be able to expand proportionally to how much processing power it can get. It's more difficult to imagine how this /wouldn't/ lead to an intelligence explosion. Obviously I can't predict what action some super-intelligent entity would take, but my example, nanobots, certainly seems like something that could make an AI incredibly powerful very fast. We already have the ability to manipulate matter on the nanoscale, it wouldn't take long to go from what we have to effective machines if you knew exactly what you were doing, which a sufficiently intelligent AI likely would.