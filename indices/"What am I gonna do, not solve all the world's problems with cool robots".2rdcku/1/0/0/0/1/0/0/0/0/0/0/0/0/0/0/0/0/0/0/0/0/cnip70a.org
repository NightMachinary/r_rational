:PROPERTIES:
:Score: 1
:DateUnix: 1420747924.0
:DateShort: 2015-Jan-08
:END:

#+begin_quote
  Inductive reasoning works, but it is limited by its inputs and the processing power available (including inter-processor latency, which is a significant problem in supercomputing).
#+end_quote

Yes, of course AI is limited by sample complexity and computational complexity.

#+begin_quote
  It is simply that the assumption that an AI will somehow transcend human ability in the dramatic ways hypothesised requires more rigor than has been presented thus far.
#+end_quote

It's not so much that AI will "transcend human ability". It's that simply reasoning faster and more accurately /given the same sample-complexity as a human/ (that is, with the same efficiency as a smart human of being able to generalize from data) /is already dangerous/. And that even reasoning slower than a human, but with a lower sample-complexity, /is still dangerous/.

Besides which, you also have to contend with the fact that humans don't actually /use/ most of our intellectual capacity most of the time. We have other stuff to be getting on with, and intellect costs calories that we often couldn't afford in our ancestral/evolutionary environment. An AI can use /all/ its calories for running /just/ the cognitive infrastructure it /actually needs/, as opposed to our comparatively wasting energy on heaps and heaps of evolutionary baggage like appendices and limbic systems.

So yeah. It's less that AIs will "transcend human limitations" (we actually don't have that many, since we're inductive reasoners), then that they'll actually have some advantages over us from the start.