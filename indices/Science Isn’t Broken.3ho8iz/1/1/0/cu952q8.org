:PROPERTIES:
:Author: whywhisperwhy
:Score: 2
:DateUnix: 1440044063.0
:DateShort: 2015-Aug-20
:END:

I guess that's exactly what I'm worried about. I think for most studies I end up having serious doubts about their methodology / data analysis. How are you supposed to trust any results when like you said, variables / accurate data are hard to acquire and even then there could be some real-life factor that invalidates the conclusion (leaving aside intentional manipulation or unconscious p-hacking).

Edit: A point of this article was that although there are many pitfalls, ultimately science still corrects itself and moves forward. But while it leaves little doubt that with some work you can spot problems with an answer, it doesn't make me confident that even a seemingly good answer might not have problems with it.

Nitpicking, but as regards to assertion that you can't tell if two referees are giving more cards than others, I thought that sort of thing is specifically what ANOVA tests are for (measuring whether there's variation between different populations of data, such as each referee's card history), among others? Similar to how statisticians can tell if certain teachers are manipulating standardized test results?