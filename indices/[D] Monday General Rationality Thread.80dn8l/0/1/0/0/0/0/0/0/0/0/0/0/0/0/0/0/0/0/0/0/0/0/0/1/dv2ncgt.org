:PROPERTIES:
:Author: MrCogmor
:Score: 1
:DateUnix: 1520003636.0
:DateShort: 2018-Mar-02
:END:

No it is saying that the A.I box experiment is not a accurate simulation of an super-intelligence because it is involves two humans. Elizier has hidden what actually went on in the experiment because he believes the results would be disputed and they would be. Humans cannot create a false argument that is irrefutable to humans because the person making the false argument is human and not convinced by their own argument. If he actually actually released the information there would be hordes of people pointing out the stupid mistakes on the part of his opponent. I doubt he used purely rational argument (see [[http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/][here]]) and convincing a gatekeeper to let you out of a box is not the problem we are discussing. Emotional manipulation can get you to take an action on impulse but it generally takes time or a receptive subject to change longstanding beliefs and even when it works you can get people that 'Believe in belief' without actually believing. You might be able to convince people that 1+1 is not 2 with a whole 1984esque apparatus but not through just rhetoric.

Edit: expanded on last sentence.