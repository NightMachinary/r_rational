:PROPERTIES:
:Score: 1
:DateUnix: 1505228878.0
:DateShort: 2017-Sep-12
:END:

#+begin_quote
  the problem of reasoning about complex systems
#+end_quote

Wargh. What do we mean by "complex systems"? As in complex-systems theory? Something else?

#+begin_quote
  Statements of the form "If A then B" for human-recognizable categories A and B will typically be useless, because by the nature of complexity, we can't get enough bits of shannon information about such propositions for them to be practically useful. Moreover, sometimes when it seems like this sort of reasoning is trustworthy, it isn't.
#+end_quote

Certainly. Verbalized sentences don't really pin down sensory observables very precisely, and we should try not to use them as if they do. Conceptual uncertainty is an important part of clear thinking: /accounting/ for the fact that words map to mental models only noisily, that mental models still generate sensorimotor uncertainty and error, and that when choosing actions we need to weight mental models up and down by how much /sensorimotor/ uncertainty and error they produce, /not/ by their verbal neatness.

This is why I'll tend to get in loud, vehement arguments with philosophy-types about methods: moving concepts around according to the rules of logic doesn't get rid of the inherent uncertainty and error /about the concepts themselves/.

#+begin_quote
  More generally, Taleb argues: many people make the mistake of trading short-term bounded risks for long-term existential risks. Quite often, preventing short-term disasters just sweeps problems under the rug until they all collapse at once. For example, bailing out big banks instead of letting them fail just maintains the status quo and ensures that there will be another market crash from corrupt practices. Polluting the atmosphere to generate electricity in the short-term has long-term environmental consequences. Using plasmid insertion to create super-crops that solve hunger in the short term could lead to an ecological disaster in the long term (hence the GMO issue from last time).
#+end_quote

Yep yep! One nasty bias in our decision-making, possibly even in /optimal/ decision-making, is choosing to control the events we can control /most precisely/, while siphoning risks into the inherently noisier part of the possible-worlds distribution, hoping that noise will save us. Well, the noise is in the map, not the territory, so /actually/ we probably need to marginalize out precision-of-control parameters to make good decisions.

#+begin_quote
  Talebs says: "Hey you guys. Stop naively applying modus ponens and bell curves to complex systems. Instead, here's a bunch of mathematical tools that work better: fractal geometry, renormalization, dynamic time-series analysis, nonlinear differential equations, fat-tailed analysis, convex exposure analysis, ergodic markov chains with absorbing states. It's a lot of math, I know, but you don't need to do math to do well, just listen to the wisdom of the ancients; practices that have survived since ancient times probably don't have existential risk. If you want to go against the wisdom of the ancients, then you'd better be damn careful how you do it, and in that case you'd better have a good grasp on the math."
#+end_quote

I really like that he actually proposes math. That's a very good thing.

I'm generally careful about the Wisdom of the Ancients, because the Ancients are dead. The thing about them is, one of the longest-running, most-repeating narratives about Ancient Civilizations is that they had some fatal flaw and destroyed themselves.

Which may render their advice counterproductive.

#+begin_quote
  Regarding survivability: it's not that surviving is Taleb's terminal goal so much as it's a prerequisite for all goals. If you don't survive, you can't do the utilitarian goal-maximization that you want to do. Therefore, maximizing your long-term survival chances should always be your first worry. You can never eliminate all risk, but you can choose which kind of risk you want to deal with. Fat-tailed risk (like non-value-aligned artificial intelligence!) virtually guarantees that everyone will die, it's just a matter of when. Thin-tailed risk (like specialized or friendly AI) is survivable long term.
#+end_quote

Sounds pretty intuitive, actually, but it also contradicts the principle above of marginalizing out the precision parameters that control whether tails are fat or thin.

#+begin_quote
  So that's Taleb's general position, and I think a lot can be learned from it. That's why I recommend reading his books even if you don't agree with him. In the places where he is wrong, he is wrong in an interesting and non-obvious way.
#+end_quote

Got a book you can recommend?

#+begin_quote
  An overview of these things would make a great State Star Codex article, for example, if Scott Alexander decided to investigate.
#+end_quote

You can suggest it in an open thread.

#+begin_quote
  This is why I wanted Eliezer Yudkowsky to weigh in last time.
#+end_quote

His reddit name is his real name, no spaces or underscores. You can just tag him and see if he responds.