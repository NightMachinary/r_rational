:PROPERTIES:
:Author: LieGroupE8
:Score: 2
:DateUnix: 1505223695.0
:DateShort: 2017-Sep-12
:END:

I'm going to respond to all your posts here, in one place. Just to tie things together, I'll tag the other people who responded to me (thanks): [[/u/eaturbrainz]] [[/u/696e6372656469626c65]] [[/u/gbear605]]

So here's my secret, ulterior motive for bringing up Taleb over and over: Taleb has intellectual tools that I covet for the rationalist community. We may not agree with everything he says and does, we may have different goals than he does, but if there are useful analytical tools that we could be using but aren't, we should greedily pluck them from wherever we can find them.

Logic and Bayes' theorem are great and all, but as Taleb would point out, the formal structures navigated by those tools are not sufficient for a certain class of problems, namely, the problem of reasoning about complex systems. Of course, logic /constructs/ the tools needed, because it constructs all of mathematics - but the direct application of modus ponens might not work out so well. Statements of the form "If A then B" for human-recognizable categories A and B will typically be useless, because by the nature of complexity, we can't get enough bits of shannon information about such propositions for them to be practically useful. Moreover, sometimes when it /seems/ like this sort of reasoning is trustworthy, it isn't.

For example, here's a mistake of reasoning that a starry-eyed young utilitarian might fall into:

1) If something is bad, then we should stop it from happening as much as possible

2) Wildfires are bad because they destroy property and hurt people and animals

3) Therefore, we should stop as many wildfires as possible

You might be thinking, "What's wrong with that?" But consider this: preventing small wildfires creates a buildup of dry brush and greatly increases the chance later on of a massive, even-worse wildfire. Thus it is better to accept the damages of small wildfires right away to prevent things from being worse in the long-term.

More generally, Taleb argues: many people make the mistake of trading short-term bounded risks for long-term existential risks. Quite often, preventing short-term disasters just sweeps problems under the rug until they all collapse at once. For example, bailing out big banks instead of letting them fail just maintains the status quo and ensures that there will be another market crash from corrupt practices. Polluting the atmosphere to generate electricity in the short-term has long-term environmental consequences. Using plasmid insertion to create super-crops that solve hunger in the short term could lead to an ecological disaster in the long term (hence the GMO issue from last time).

Talebs says: "Hey you guys. Stop naively applying modus ponens and bell curves to complex systems. Instead, here's a bunch of mathematical tools that work better: fractal geometry, renormalization, dynamic time-series analysis, nonlinear differential equations, fat-tailed analysis, convex exposure analysis, ergodic markov chains with absorbing states. It's a lot of math, I know, but you don't need to do math to do well, just listen to the wisdom of the ancients; practices that have survived since ancient times probably don't have existential risk. If you want to go against the wisdom of the ancients, then you'd better be damn careful how you do it, and in that case you'd better have a good grasp on the math."

Regarding survivability: it's not that surviving is Taleb's terminal goal so much as it's a prerequisite for all goals. If you don't survive, you can't do the utilitarian goal-maximization that you want to do. Therefore, maximizing your long-term survival chances should always be your first worry. You can never eliminate all risk, but you can choose which kind of risk you want to deal with. Fat-tailed risk (like non-value-aligned artificial intelligence!) virtually guarantees that everyone will die, it's just a matter of when. Thin-tailed risk (like specialized or friendly AI) is survivable long term.

So that's Taleb's general position, and I think a lot can be learned from it. That's why I recommend reading his books even if you don't agree with him. In the places where he is wrong, he is wrong in an /interesting and non-obvious way/.

P.S. I feel like these ideas will not have their maximum impact here on a weekly [[/r/rational]] thread. Suggestions of where to put them instead are welcome. An overview of these things would make a great State Star Codex article, for example, if Scott Alexander decided to investigate. This is why I wanted Eliezer Yudkowsky to weigh in last time. Part of my confusion is /why isn't the rationalist community talking about these important issues and techniques? Does the community have good reasons for disagreement, or are they just unaware?/