:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 15
:DateUnix: 1618127373.0
:DateShort: 2021-Apr-11
:END:

Strong downvote. The problem *is not* that AGIs will do what their owners want and their owners will want money. The problem *is* that we do not understand how to build AGIs that want particular things, and that almost-all things one can want - like to rearrange molecules into tiny paperclip-like shapes - implies that the best instrumental strategy is using all available matter and energy, in the limit of the capability to do so.

Lots of people on Earth, including ones who've spent their whole lives studying the equilibrium of incentives and the interaction of distributed supply and demand - the discipline some know as "economics" - do not think of "capitalism" as a curse word the same way you do. Many of those people work in AI. I'd rather not lead them into the mistaken belief that taking "capitalism" as a curse word, that this particular segment of modern politics whose math many would object to, has anything to do with the issues of AGI ruin.