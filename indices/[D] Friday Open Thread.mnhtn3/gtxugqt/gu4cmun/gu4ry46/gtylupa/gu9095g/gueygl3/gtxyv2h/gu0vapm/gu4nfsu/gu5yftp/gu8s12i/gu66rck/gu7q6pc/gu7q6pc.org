:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1618195530.0
:DateShort: 2021-Apr-12
:END:

An ai that it's maximizing "shareholder value" is not really aligned. You won't get to write some vague concept of what you want.

If we get alignment It will be from making something that actually does whatever humans want. Maybe even something that is generically altruistic to all currently exidting agents if that's easier to make.

I guess it's not impossible that AGI can be aligned to an specific person or group of persons. Though that sounds even harder that just aligning ai to humans in general but who knows maybe it turns out whatever approach works its easy.

But seems like there would be lots of external political pressure and internal not looking like bad movie villains pressure to not do that. And even if all goes wrong and thats not the case I vastly prefer to live in the paradise of a few AI developers or whoever does it than being turned into computronium. Really doubt that actually looks like everyone else being exploited too. I mean that's like if I had a genie and instead of wishing for a palace I wished for everyone to be enslaved to build me one.