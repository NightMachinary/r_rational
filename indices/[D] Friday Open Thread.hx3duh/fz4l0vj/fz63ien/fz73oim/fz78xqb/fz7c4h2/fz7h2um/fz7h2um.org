:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1595693837.0
:DateShort: 2020-Jul-25
:END:

It really depends on the specifics. GPT-3 is terrible at rhymes, with no improvement over GPT-2, and my prediction is that GPT-4 won't show any improvement if the primary difference is more parameters, a better corpus, or anything like that. You /could/ build a framework around the text output, and select the most-probable word that actually /does/ rhyme when you want it to, and thus force GPT-3 into doing end rhymes, which is one of the things I mean by creating frameworks. (Gwern's GPT-3 page [[https://www.gwern.net/GPT-3#rhyming][has a long section on attempting to get GPT-3 to rhyme]], including a lot of tricks that attempt to induce it.)

However, if you generate twenty, thirty, or a hundred different endings to a line, you /might/ happen upon an actual rhyme, and if you cherry-pick those, then you can make it look like GPT-3 can rhyme, which it cannot. It might know that a noun is called for, or even a noun of a specific length, but it's blind rhymes.

Similarly, there are cases where GPT-3 will provide a completion that falls within certain bounds, but fails to correctly pick from among the short list of candidates. As an example, it might give completions that are (correctly) an emotion, but not be able to select the /correct/ emotion (this is just an example, GPT-3 does kind of okay at very basic emotions). Or it might understand that a name should go in that place, but be no better than chance at supplying /which/ name. This is a case in which cherry-picking can make it look much more capable than it actually is, and where increasing the parameters, the fine-tuning, etc. might not actually help it.

Generally speaking, I've noticed it having quite a few issues with applying adjectives that are correct for the noun they're applied to, but wrong in the context of the scene or what's been previously established about that object (places where the whole text is in the context window, so that's not the issue). Similarly, it doesn't handle negations well (e.g. if you say that someone is not greedy, the model latches onto 'greedy' and disregards the 'not', similarly, it will sometimes include its own negations that don't make sense). These are other places where I'm skeptical that increasing the parameters will be very helpful, since they were definitely problems in GPT-2 as well. They're /also/ places where cherry-picking can erase the problems.