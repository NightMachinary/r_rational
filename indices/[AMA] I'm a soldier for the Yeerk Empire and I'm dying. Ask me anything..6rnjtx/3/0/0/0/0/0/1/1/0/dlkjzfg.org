:PROPERTIES:
:Score: 2
:DateUnix: 1502650593.0
:DateShort: 2017-Aug-13
:END:

Yo, infodump!

So the first thing is to maybe read [[http://philosophyofbrains.com/2015/12/14/surfing-uncertainty-prediction-action-and-the-embodied-mind.aspx][the introductory overview]] to predictive processing. We can go from there. You might also have seen some SSC posts about "Bayesian brain" stuff: this is that stuff.

The important thing for Yeerks is that actions are created when top-down predictions are more precise than bottom-up prediction-errors; in Bayesian terms, when the prior is more informative than the likelihood. In this model, everything from the highest layers of the neocortex to the spinal cord does its little "Bayesian handshake", and then samples a downwards prediction of its own. The only difference is that when the spinal cord does the "handshake", it has a second way to shake hands: it can quash the bottom-up error signal, the one saying "That muscle isn't /there!/", by just /moving/ the muscle.

This means that the balance between action and body-placement sense is found in those top-down and bottom-up precision signals. When the bottom-up precision is greater, you sense where your muscles /are/; when the top-down precision is greater, you /move/ the muscles.

There's an interesting detail here: what humans consciously perceive is the top-down predictions, not the bottom-up errors. So when you sense something, the signal actually travels upwards to compute a new prediction, it all handshakes once all upward errors have become negligible, and then you perceive the new prediction that moves downwards again. Maybe Yeerks work that way, maybe they don't.

This is all important for how you infest a brain. Let's say you sit wrapped around the highest layers of cortex, and you start learning how those cells are working from your "skin", which of course has a layer of your own neural tissue. Assume relative electrochemical compatibility between the two kinds of tissue. Eventually, you're going to learn to interpret the activities of the neurons below you, and you're going to "sense" the waves of predictions and errors that constitute the waves of perception and action.

What can you do, as the Yeerk? Well, you can peek and poke signals to the neurons you can touch. Now, sending your own signals down purely exteroceptive perception pathways will just cloud your own vision: you /need/ percept signals to perceive the world from inside the skull. But you can do something useful: you can send your own signals down proprioceptive and motor pathways, /including/ precision signals. In fact, you can probably modulate the precision signals so that the "predictions" (motor commands encoded /as/ predictions) /you/ send down the motor pathways have /more/ precision than those that the rest of the infested brain is trying to generate. So the motor pathway ends up disregarding the rest of its own brain, and listening to /you/ instead.

Since you're already peeking at the perceptive and memory pathways, you can integrate the information. You perceive as the human perceives, remember as they remember, and can act as /you/ please.

Of course, there's going to be some dangers here. Emotion is modeled (in this whole theoretical framework) as interoceptive ("from the guts", inside-the-body) perception (perceiving "how am I" in the bodily sense). Strong emotion will come from highly precise interoceptive prediction errors. Likewise, "willpower" could be modeled as the executive functions of the brain volitionally tuning up the precision on its own motor signals, but spending energy on the precise computations needed to do so (since tuning up precision means taking away autonomy from the lower layers, resulting in more time spent on Bayesian handshakes at the higher layers). So extraordinarily strong emotion or willpower /could/ override the Yeerk's control signals to break through, as we once saw with Marco's mom.

There'll also be the trouble for the Yeerk of learning how to "pilot" the autonomic motor systems well enough that they don't start firing large, precise errors all over the place, causing the rest of the Controlled brain to pitch a fit. In fact, the Yeerk will have to spend some time learning how to control the motor systems and understand the perception systems for each new species it infests, as an individual. Yeerk pools will probably need have some training hosts available to teach newly spawned or arrived Yeerks how to pilot the local bodies.

Aaaaaand that's the end of it. I have to go buy groceries. In real life there would be numerous incompatibilities of biology, neural tissue, and neurocomputational algorithms, but for hard scifi purposes, we can more-or-less render Yeerks and other brain parasites neuroscientifically and cognitive scientifically plausible /enough/.

For extra twist-the-screws horror, the Yeerk can sometimes sadistically note to the host that each layer of cortex is basically a Yeerk-esque controller to the layer below it. That's why Yeerks can /be/ so plausible: the hierarchies of the brain are constructed around each layer up piloting the one below it through both perception and action, ending at the highest executive functions and the muscle reflex arcs in the spinal cord.