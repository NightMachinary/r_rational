:PROPERTIES:
:Author: jcolechanged
:Score: 3
:DateUnix: 1495879199.0
:DateShort: 2017-May-27
:END:

You're right. He was real at some point. I'm not contesting that.

What I'm trying to show is that its possible to have clever workarounds that mitigate new qualia production and that theoretically there might be other abstractions that could further mitigate them. Or remove them.

For example, if you know all the details of an environment and you also have the mind state of someone, can you derive what they've done recently? Can that be used to postcog part of a simulation? Can you iteratively postcog with that? Does the postcog produce qualia? If you can't derive from that well enough to be useful, how is it that some people can have perfect recall well enough to be useful? If you derive what happens successfully, but you did it without needing to run the qualia producing actual simulation... then you just expanded the number of things that can happen in simulation without requiring qualia to be a thing.

I feel like there is enough that we don't know, that ruling out the idea that its possible to implement things without qualia is naive. Especially when every example of an AI that I actually do know how to make doesn't seem to have a mechanism that allows for consciousness and whatever that mechanism that allows for consciousness is, I don't know it.

Meanwhile, the claim that its not possible to find evidence of the simulation being real or not real? It isn't true. It's looking at the simulation only, which is equivalent for practical purposes, rather then the wider world- which doesn't have that restriction and can cache or do a whole host of other things which are going to be investigable.

Interestingly, practically speaking? Treat them as real anyway. Even if they aren't, if they are equivalent, they aren't going to take kindly to being treated badly. No point in being cruel, especially when reciprocity is a thing.