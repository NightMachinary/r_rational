:PROPERTIES:
:Author: alexanderwales
:Score: 6
:DateUnix: 1450813330.0
:DateShort: 2015-Dec-22
:END:

In /Superintelligence/ Bostrom argues that medium or fast takeoff is more likely than slow takeoff, a sentiment which is echoed by a fair number of people on LessWrong. There was a recent article by Scott Alexander that said he thinks we live in a world where the jump from infrahuman to superhuman is going to be very fast.

If the argument were "fast takeoff is unlikely but given the risks involved it's still something that we should take seriously" it would be a lot more palatable (though then it might read like Pascal's mugging). Unfortunately, I think there's also a tendency within the LessWrong crowd to first argue that FOOM AI is possible and then treat it as though it's probable, which doesn't do them any favors, especially given the lack of rigor applied to the question of probability.