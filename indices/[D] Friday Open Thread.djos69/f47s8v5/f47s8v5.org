:PROPERTIES:
:Author: mateon1
:Score: 1
:DateUnix: 1571427200.0
:DateShort: 2019-Oct-18
:END:

Over the last day-and-a-half, I've been doing a lot of thinking and self-reflection. (WARNING: Post may be rather rambly, and may be more of a direct thought dump.)

Today, I decided to write down the things I thought about to better help me organize and refine my thoughts. I wrote down a large list of points in a notepad app, and I don't think I really came to any particularly interesting conclusions, but, of the more interesting things, I explored the space of potential core values (which should apply to several kinds of agents, including humans, aliens and AI).

I have a vague feeling that the simplest single core value that leads to desirable behavior is based on "preserving (useful) information" (but that needs a bit of refinement and clarification). Archiving a lot of information ultimately requires compression, and compression requires world modeling of some sort. Modeling agents - including the self - is also useful (for archival, search, etc.). A lot of values that seem desirable are compatible with preserving information (Preserving species, lengthening lives: irreversible information loss - like death - is the WORST thing that can happen from this perspective). Another potential core value is "preserving potential", or "ensuring potential is developed", but that is extremely vague and nebulous.

An interesting thought I came across, is that evolved life is "lazy", and instinctively avoids work to preserve energy. Reinforcement learning AI notably DOES NOT do this, unless it was trained in an environment heavily constrained for time or energy! That means that as long as the fitness function is "stuff gets done", and not "stuff gets done efficiently", then RL AI will get stuck on suboptimal - but working - solutions.

A really surprising observation is that "torture" is actually extremely nebulous. *SUB QUESTION:* What do people actually think about torture? The concept of is inherently bad. Why? (or why not?) What exactly IS torture? Is it pain? /(No.)/ Negative reinforcement? /(Maybe?)/ Negative reinforcement with extra conditions (not being able to control the source of negative reinforcement)? /(Maybe?)/\\
Is it the damage in body and mind that traditional torture causes?\\
What if torture did not result in any damage - physical, mental, emotional, and social - would it still be bad? /(I don't think so, but would that even be torture?)/\\
In any case, what about AI/agents that do not have a concept of pain/negative reinforcement. Can they even be tortured in principle?

Even as I'm writing this comment I'm updating my notes, but I've noticed that at 80 bullet points I'm starting to have trouble navigating, writing, and reading those notes - but I can't really think of a way to condense down the text in any meaningful manner.

Things that I have not even finished elaborating on:

- Social values
- How exactly do world modeling, compression, information, memory, and consciousness correlate?
- Other "reasonable" values (that is, any values that you can see an agent reasonably have).
- Physics, on a fundamental level, seems not to allow information to be lost. Sadly, the information is usually irretrievable after thermodynamics/black holes/etc. happens to it.

  - What about situations like kugelblitz black holes? What if you create a kugelblitz around a planet with sentient life, that can remain self-sustaining for billions of years. How does relativity interact with this? What happens to the information inside?
  - In practice, the information, AND the potential of things inside, is lost, so those two goals are set against doing this. What about other moral values? Is it moral to do this? Does it morally matter what happens inside /after/ the kugelblitz is made?

- A bunch of concepts that seem nebulous (or just fuzzy). Can they be clarified? If not, why do we have a concept like that? (see other nebulous concepts like free will.)
- I haven't fully elaborated on why some counter-intuitive things can be potentially good for us, depending on how the universe works. (consider various simulations, with various purposes: research, entertainment, etc. For example, causing a false-vacuum collapse in a distant galaxy, whose future light-cone will never cross ours may increase our chances of survival. In most interpretations, this is bad, or just irrelevant, though).

  - It's interesting to consider what each core value leads us to do in these kinds of situations. Maximizing the chance of survival of ANY sapience makes us /not/ bomb the distant galaxy, preserving information deems it irrelevant since the future light-cones will never cross.

My notes are a bit too big to paste them into a reddit comment in their entirety, so here's a link to a paste with the notes I made as I was riding my train of thought: [[https://paste.sh/jiiZqQV-#ydwtC_0O5ZyWOqM0iDnd-BFP][LINK]]

I'd appreciate it if people poked holes and mentioned interesting things I missed in those notes. I think this topic is really fascinating to explore and that I barely made a dent in what there is to consider.