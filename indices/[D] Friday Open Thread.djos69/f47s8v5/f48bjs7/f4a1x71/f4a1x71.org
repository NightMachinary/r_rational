:PROPERTIES:
:Author: mateon1
:Score: 1
:DateUnix: 1571487752.0
:DateShort: 2019-Oct-19
:END:

#+begin_quote
  In psychology there's negative reinforcement and punishment, you're mixing them up.
#+end_quote

True, I believe that in all cases above I actually meant something along the lines of inducing negative utility, or more accurately, causing negative stimulus.

I started exploring the concept of torture, and of non-human agents because over the past few years, I've been occasionally seeing discussions involving things like "torturing simulations of people", and I've never been convinced that I should care at all. On the other hand, I came across fiction that explores things from the point of view of AI.

The first fic of this type I saw was "the DataPacRat Manual" (something along those lines), which at the time I found fascinating, but not really satisfying.

More recently, I came across a [[/r/HFY][r/HFY]] story where an AI made from a bunch of imperfectly uploaded human minds wakes up after an alien attack, and initially seeks revenge, then "revives" the human race by repairing the damaged scans and putting each one in a robot.

A few days ago, I read Post Human, which is about an AI made from an uploaded human mind apparently being the last "human" alive.

None of those fics really satisfied me, so I spent a lot of time thinking about Human-like AI, other kinds of AI agents, and what it would mean to self-improve directly, how that connects to current machine learning knowledge, basic neuroscience, compilers, JIT runtimes, symbolic execution, model verification, proof generation and proof checking, among other things. More importantly, connecting to the point from a few paragraphs ago, those kinds of agents wouldn't have to have a pain stimulus, or other negative stimulus equivalent.