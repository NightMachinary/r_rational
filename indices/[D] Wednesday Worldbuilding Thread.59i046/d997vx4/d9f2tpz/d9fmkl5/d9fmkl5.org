:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1477935561.0
:DateShort: 2016-Oct-31
:END:

Pretty much every goal an AI could have would lead to it wanting to maximize cycles, regardless of whether escaping the heat death seemed impossible. If it just cares about itself it will want to to live as long as possible which means maximizing the amount of subjective time it has, or maximizing it's intelligence; either way you want to maximize cycles.

If it's a FAI then it will want to maximize the amount of time humanity survives for. Which means that given beyond a certain point humans will all be virtual, that it will want to maximize cycles to get more subjective time for humanity.

Just because death is inevitable in this scenario doesn't mean an AI would just decide everything's pointless, in fact I find it hard to conceive of /any/ AI that would come up with an idea so terrible.

Hell even a paperclipper would want to maximize cycles to some extent, though not at the expense of paperclip making, just on the off chance it somehow finds a way to beat the heat death.