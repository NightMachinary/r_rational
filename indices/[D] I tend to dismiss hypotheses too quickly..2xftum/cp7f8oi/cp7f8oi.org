:PROPERTIES:
:Author: derefr
:Score: 2
:DateUnix: 1425753183.0
:DateShort: 2015-Mar-07
:END:

If I pick my tendency to do this apart /in this specific instance/, it's mostly a desire to assume that "they'd have thought of that, and everyone knows they'd have thought of that, so why would they go with the most (or second-most, or any-most) obvious solution?"

The answer for me, that turns out to generalize /outside/ of this specific instance, is giving more weight to "obvious solutions", and less weight to "solutions that assume some active intelligence stopped the obvious solution from working."

Once you get used to this---and realize that other people don't default to thinking this way any more than you did---it also makes clear why some social engineering methods are so effective. Dress up as a security guard? Nobody would do that; it's /too obvious a solution!/

...which is true, in a self-fulfilling way; people don't tend to guard against it precisely because most would-be criminals discard it as too obvious, and assume it would be the first thing their opponent would think to guard against.•

Basically, this is a game theory problem, crossed with a game design problem. Assuming an intelligent adversary means assuming [[http://www.sirlin.net/ptw-book/7-spies-of-the-mind][yomi layers]] apply to your problem, because you're likely to be trying to outthink them just as they're likely to be trying to outthink you. Systems like stock markets also act like this: there is an economic incentive for the system to evolve away from anybody's ability to predict it.

But for anything that /isn't/ intelligent---and most things that happen in the universe are the result of dumb non-learning processes---you have to discard your impulse to think in terms of yomi layers, and actually look for the most obvious solution. Unless it has been fiddled with by some adversarial intelligence, the most obvious solution is also likely to be the solution that relies on the fewest things to go right, the easiest to implement, etc.

There's a reason your lizard brain bubbles the "obvious" solution up as a suggestion to your mammalian brain first (which feels like something being "obvious" from the inside.) The parts of your brain that evolved when nature itself was the adversary think in terms of obvious solutions. Your higher brain functions try to predict other animals, or even other intelligent beings, and thus quash these suggestions; it's a useful flinch for social games, but it's not a useful flinch for generally-optimal strategy. *Do not try to out-think nature, for it is not trying to out-think you.*

• (Also, security systems are /usually/ built based on previously-attempted attacks, not imagined attack scenarios, and something that is "too obvious" may have just never been attempted.)

--------------

And I guess this also gets to the core of rational story-writing, doesn't it? In a rational (not rationalist) story, the universe acts like ours: it's a predictable, non-learning, "most obvious strategy dominates" system. The winner is the first person to have a sufficient idea, not the best idea.

Part of Alicorn's /Effulgence/ described a D&D campaign world: it was a horrendous and scary place to the Bells, precisely because there really /was/ an intelligent adversary (the DM) at work, making the Most Obvious Solutions fail to work. I think this was meant to be taken as an indictment of a lot of other works, though; almost any work, to the degree that it is non-rational, has some force of "karma" that rewards deontologically-pleasing strategies when they come up against villains doing the Most Obvious Thing.