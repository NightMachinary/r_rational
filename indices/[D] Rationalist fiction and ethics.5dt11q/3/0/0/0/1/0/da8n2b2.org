:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1479677384.0
:DateShort: 2016-Nov-21
:END:

The general gist of your post seems accurate enough, but there are a couple of (relatively small) factual errors. The most important such error is that a billion billion Earths isn't /nearly/ enough to balance out the Kolmogorov complexity of the hypothesis in question; you'd need something on the order of a googolplex or so before the utility of paying the $100 /actually/ starts to outweigh the improbability of a statement like "I'm a god".

Note also that there /is/ in fact a (proposed) solution to the problem: Robin Hanson has suggested that a [[http://lesswrong.com/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/][leverage penalty]] be applied to situations in which you find yourself in a unique position to affect large numbers of other individuals. Personally, I'm not entirely convinced that this solution is, in fact, a satisfactory answer to the problem (as opposed to a post-hoc patch), but the fact remains that it /has/ been proposed, and therefore you're not quite correct when you say that "the standard lesswrong decision theory... fails completely".

Finally, I'm not sure why you think Pascal's Mugging is simply a rephrasing of Roko's Basilisk. The two are, in my mind, two different thought experiments that ask two distinct questions about two separate concepts, and it doesn't seem at all obvious that they ultimately reduce to the same thing. I'm less inclined to call this a "factual error" than I was with the other two things I mentioned, but if there was any particular line of reasoning that lead you to this conclusion then I'm not seeing it.