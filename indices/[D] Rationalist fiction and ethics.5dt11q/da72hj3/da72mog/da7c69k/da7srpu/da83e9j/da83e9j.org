:PROPERTIES:
:Author: traverseda
:Score: 2
:DateUnix: 1479644556.0
:DateShort: 2016-Nov-20
:END:

Sure. Imagine someone comes up to you and says "I'm a god. If you don't give me $100 right now, I'll create a billion-billion new earths, and torture everyone to death on them"

Now, if you're running preference utilitarianism and bayesian probability, the obvious answer is to say "okay". There's very little chance that they're telling the truth, but the result if they are is /so/ bad, that it balances out a $100. Is it not bad enough to balance out $100? They can just add a few more order of magnitude.

In this case the bayesian decision theory so many of us use day-to-day fails completely.

And this is why the standard lesswrong decision theory is unsuited for use in an AI or the like. When you're weird niche decision theory and common sense are both telling you very different things, go with common sense. Also, rationalists should win and adopting what the decision theory tells us to would very quickly lead to us losing.

It's worth noting that pascals mugging is actually "roko's basilisk" told differently. A solution to one is a solution to the other. It's been carefully reformatted not to cause anyone any distress. So the next time someone goes off about lesswrong banning roko's basilisk, keep in mind that the question is still open, just with less distressing connotations.