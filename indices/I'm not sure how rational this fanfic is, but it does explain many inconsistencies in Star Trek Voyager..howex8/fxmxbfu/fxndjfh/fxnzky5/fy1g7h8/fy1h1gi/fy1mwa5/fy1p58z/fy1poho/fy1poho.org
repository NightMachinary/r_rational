:PROPERTIES:
:Author: derefr
:Score: 2
:DateUnix: 1594742301.0
:DateShort: 2020-Jul-14
:END:

#+begin_quote
  Machine learning systems are not AIs and not going to grow into AIs
#+end_quote

Who said they were? I'm basing what I'm talking about on the particulars of the technobabble used to describe and differentially-diagnose the problems going on with the Doctor's program (and those of other hologram AIs) in various episodes. Technobabble /is/ canon, and you have to assume they chose the words to communicate the things that those words usually communicate in the domains they're from (in this case, ML); otherwise you can't know anything at all.

But my point was more generally-applicable: brains learn using vast training data. ML models learn using vast training data. The Doctor became self-aware /seemingly/ by exposure to vast training data. It doesn't matter /how/ any of these systems do it; just that they /do/ it.

We can see, at least in the ML-model case, that 1000x-ing the training data, without doing anything to the program, can qualitatively change the way the program interacts---in the case of GPT-2 vs. GPT-3, it attains the capacity for meta-learning (i.e. it can model a skill within /the weights/ of the trained model, by being primed with examples of the use of that skill.)

We also know that /depriving/ a human brain of training data, /also/ makes for qualitative changes in capacities---for example, humans raised without language never develop an understanding of syntax (and seemingly can never learn it later on), and so become only capable of constructing of the same kinds of direct, simple sentences that chimpanzees can construct in ASL.

So, from these two examples, I don't feel like it's a stretch to guess that it's the increase in the level of training data, that resulted in the qualitative change in the capacities of these holograms.