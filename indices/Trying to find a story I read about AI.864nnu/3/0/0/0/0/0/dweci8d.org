:PROPERTIES:
:Author: narfanator
:Score: 1
:DateUnix: 1522203022.0
:DateShort: 2018-Mar-28
:END:

#+begin_quote
  No idea, this is the big question isn't it?
#+end_quote

No? It's only the big question if you think it's the case. I don't think consciousness arises from the interaction between neurons, so it's not really a big question for me.

Since it seems to be the crux of you position, it surprises me that you don't have any ideas.

If you have no idea what about the interaction of neurons gives rise to consciousness, why do you think it causes it?

#+begin_quote
  because we can be fooled
#+end_quote

I take the position that what makes two things the same thing is that you cannot differentiate between them. If I have a "known conscious" entity and an "unknown conscious" entity, and I cannot tell the difference between their apparent "consciousness", seems like the reasonable conclusion is that they're both conscious (or both not, but, postulates).

So perhaps a better way to phrase the question is "what would be true for you to treat an artificial mind the same as a human mind"?

#+begin_quote
  minimal capacity for communication
#+end_quote

Why do you think this is important?

--------------

I think you should (re)view both Descartes' "I think therefore I am", and /Ex Machina/.