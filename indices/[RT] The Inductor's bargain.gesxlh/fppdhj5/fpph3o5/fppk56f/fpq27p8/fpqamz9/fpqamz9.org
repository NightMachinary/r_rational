:PROPERTIES:
:Author: Nimelennar
:Score: 5
:DateUnix: 1588818605.0
:DateShort: 2020-May-07
:END:

#+begin_quote
  why do we default to the assumption that any AI in a box is the moral equivilent to a serial killer in a cell on death row, instead of a child asking a parent to go play outside?
#+end_quote

Ethics is really, really hard.

Mistake-free computer programming is also really, really hard.

For good or for evil, the first artificial intelligence that emerges is probably going to end up in charge of humanity, unless humans achieve superintelligence (defined as: the ability to improve our own intelligence, and use the additional intelligence, and so on recursively) first.

Is it not a good idea to make sure that the thing that's going to end up in charge of the priorities of the planet recognizes human values as something worth preserving /before/ the AI can claim the power to save or destroy us?