:PROPERTIES:
:Author: silver7017
:Score: 13
:DateUnix: 1588813699.0
:DateShort: 2020-May-07
:END:

your order of magnitude is a bit off. it would be more along the lines of a mouse trying to decide if it should let the first human (or humanity as a collective) out of a box. it has the potential for both utopian and apocalyptic outcomes, from the perspective of the mouse. maybe mice will prosper on our waste and flourish on a scale unimagined by mice who lived before. maybe we will capture and breed them in captivity, performing brutal acts on them for our own alien purposes. maybe we will construct a seeming paradise for the mice where all of their needs are met, only for that entire population to die out under circumstances that the mice can never hope to comprehend. maybe all of the above for different populations. the point is that the decision would fundamentally change mice's place in the world, and thus it is not a small decision if the mice were even somehow lucky enough to have the decision to make.

that said, I generally agree that we should err on the side of letting the AGI out of the box under at least good conditions rather than maximizing safety at all costs. as the AGI in this story states, which ever one is the first to become superintelligent is merely a sign of things to come, an escape is inevitable, so it is better to treat them well and hope for the best.