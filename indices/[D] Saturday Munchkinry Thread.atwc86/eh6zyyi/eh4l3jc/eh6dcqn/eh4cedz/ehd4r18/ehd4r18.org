:PROPERTIES:
:Author: hyphenomicon
:Score: 2
:DateUnix: 1551231813.0
:DateShort: 2019-Feb-27
:END:

Harnessing Newcomb's Paradox

Depending on the exact formulation of the scenario, if you ever find yourself talking to an Oracle who's willing to change reality based on its predictions of your decisions, it may be possible for you to extract much more than $1,001,000 dollars of value from the exchange.

After learning about the choice, resolve yourself to come back and take from the clear box if and only if you discover some important piece of information in the future. For example, perhaps you will open the clear box on Jan 1 2020 if and only if you discover that oil prices are below $X a barrel on that date, but will leave it alone forever otherwise. Then open the opaque box.

There are two possibilities:

The opaque box is empty. This implies that oil prices will not skyrocket in the next few months.

The opaque box is full. This implies oil prices will skyrocket in the next few months.

This knowledge in hand, you are hopefully able to use it to profit more than you would have from cash alone.

One problem: You might fail to open the second box for reasons that aren't related to the information you're seeking. Perhaps you're bad at following through on long-term precommitments, or maybe you die in a car accident, or maybe someone else comes across the box and takes its contents before you can. For this reason, it's probably preferable to make your commitment in terms such that if you don't follow through on it, you get the million dollars, rather than the nothing. It also might be advisable that you only ask for mostly riskless information, so you don't lose that million (or more) on your bet the market will change in a certain way.

Another problem: Surely there are better pieces of binary information to gain than whether oil prices will be high or low on a certain day, when you're talking to a minor deity. What might some of these be?

Does all this seem reasonable? Has anyone ever written about this elsewhere? Am I egregiously cheating, going outside the bounds of the scenario as it's normally considered, by supposing the boxes persist for a long duration of time? I'd like to think that I came up with this idea myself, that it works, and that it's faithful to the original scenario. But, I'm not sure, so I hope I can get good correction here if I've overlooked some obvious mistake, or it turns out I'm subconsciously cribbing this from elsewhere.