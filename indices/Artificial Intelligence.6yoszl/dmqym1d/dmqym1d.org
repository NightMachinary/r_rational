:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 18
:DateUnix: 1504906453.0
:DateShort: 2017-Sep-09
:END:

I'll mention for the sake of reminder that the prediction of most possible superintelligences wiping out humanity as a side effect is not based on the supposition that they act for anthropomorphic reasons. "The AI does not hate you, nor does it love you, and you are made of atoms it can use for something else." We can also take a strong guess about some aspects of how sufficiently advanced cognition look from the outside, regardless of internal implementation, because of coherence theorems (see second link below). Which would indeed count as "radically different from humans" but that doesn't mean everything about the prediction is up for grabs.

In general, a very wide space of mind designs with a default measure over that space, doesn't correspond to lots of interesting variance in what /you/ consider a wide range of interesting-to-you outcomes; the vast majority of the design space just kills you and then tiles space with something of low cosmopolitan value.

- [[https://arbital.com/p/orthogonality/]]
- [[https://arbital.com/p/expected_utility_formalism/?l=7hh]]
- [[https://arbital.com/p/instrumental_convergence/]]
- [[https://arbital.com/p/convergent_strategies/]]
- [[https://arbital.com/p/value_cosmopolitan/]]