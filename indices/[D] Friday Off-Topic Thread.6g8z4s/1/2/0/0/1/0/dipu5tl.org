:PROPERTIES:
:Author: Nuero3187
:Score: 1
:DateUnix: 1497104155.0
:DateShort: 2017-Jun-10
:END:

#+begin_quote
  Because an AGI is likely to enter an intelligence explosion soon after its creation, and since a superintelligent entity would, by defintion, be smarter than humanity, it would be able to simply think of a way to circumvent all of our protections and countermeasures if it so wished --- outsmart us. Becauese utility functions are hard, and we will most likely mess up when writing our first.
#+end_quote

Ok. Because we have already found out about these problems, wouldn't we set up safeguards against them? Why would we give the AGI infinite resources? Wouldn't we limit them and see how they react to the resources they have, and if they deplete to much in an effort to achieve their goal, would we not try to fix that and try again? They're not going to hook up an untested AGI and give it real power without knowing how its going to go about accomplishing its task.