:PROPERTIES:
:Author: Noumero
:Score: 1
:DateUnix: 1497105353.0
:DateShort: 2017-Jun-10
:END:

The problem is, we cannot by definition know what power an AGI would be able to acquire given what resources.

We're putting AGI in a computer physically isolated from the Internet and let it talk only to one person, it uses its superintelligence to manipulate that person into letting it out. We doesn't allow it to talk to anyone, it figures out some weird electromagnetism exploit and transmit itself to a nearby computer with Internet access using it.

#+begin_quote
  Wouldn't we limit them and see how they react to the resources they have, and if they deplete to much in an effort to achieve their goal, would we not try to fix that and try again?
#+end_quote

This works, but only in a [[https://wiki.lesswrong.com/wiki/AI_takeoff][soft takeoff]] scenario. Hard takeoff sees it taking over the world before we can stop it.