:PROPERTIES:
:Author: Noumero
:Score: 2
:DateUnix: 1497096593.0
:DateShort: 2017-Jun-10
:END:

#+begin_quote
  Honestly? Yeah. I mainly think that because what's the alternative? Nothing? It could just be me but I'd prefer existing over not. Another hypothetical. Someone is deprived of any and all sensations for 100 years. Do you think they would welcome pain if it was what they first felt after years of deprivation?
#+end_quote

Hmm. Well, here we disagree fundamentally, apparently: I would prefer not-existing to existing in pain.

Being sensory deprivated /is/ a form of suffeing, so that doesn't change anything. I personally would prefer Hell to Sheol, even.

#+begin_quote
  I think its far more likely people who are that impulsive and idiotic would be removed from power. If not by the people than by other people in power who don't want the end of the world.
#+end_quote

Optimistic view.

#+begin_quote
  Why would the protections fail? Why would the AI try to destroy humanity at all?
#+end_quote

Because an AGI is likely to enter an [[https://en.wikipedia.org/wiki/Intelligence_explosion][intelligence explosion]] soon after its creation, and since a superintelligent entity would, by defintion, be smarter than humanity, it would be able to simply think of a way to circumvent all of our protections and countermeasures if it so wished --- outsmart us.

Becauese utility functions are hard, and we will most likely [[https://en.wikipedia.org/wiki/Instrumental_convergence][mess up]] when writing our first.