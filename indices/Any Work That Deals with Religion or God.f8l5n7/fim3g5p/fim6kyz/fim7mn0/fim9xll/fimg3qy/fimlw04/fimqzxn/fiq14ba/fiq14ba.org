:PROPERTIES:
:Author: zorianteron
:Score: 4
:DateUnix: 1582631363.0
:DateShort: 2020-Feb-25
:END:

If you imagine someone suffering, is that mental model suffering?

If you imagined/calculated/simulated each of their atoms in exact detail, that would presumably be morally equivalent (in this universe where dreamed universes have moral weight) to someone in 'real life' suffering in the same way.

If you decrease the resolution of the simulation, fill in the blanks with low-level guesses, how does personhood correlate to simulation fidelity? Assume (!) a simple vision in your head (if you have visual imagination) of someone suffering has negligible moral weight, you're not causing an agent significant suffering by doing that, and a full simulation is as bad as a 'real-life' equivalent's suffering. What does the curve look like?