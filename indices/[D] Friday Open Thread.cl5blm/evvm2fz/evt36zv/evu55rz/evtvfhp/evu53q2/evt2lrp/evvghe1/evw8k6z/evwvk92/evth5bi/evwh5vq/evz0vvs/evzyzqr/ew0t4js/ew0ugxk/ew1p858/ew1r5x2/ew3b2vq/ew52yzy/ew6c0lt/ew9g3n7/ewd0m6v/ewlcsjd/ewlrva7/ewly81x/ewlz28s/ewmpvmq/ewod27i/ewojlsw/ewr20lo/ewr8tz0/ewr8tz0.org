:PROPERTIES:
:Author: kcu51
:Score: 1
:DateUnix: 1565698671.0
:DateShort: 2019-Aug-13
:END:

#+begin_quote
  mind-transformations
#+end_quote

I'm not convinced that that's a better term; it sounds like "transforming" a mind into a different mind. (And it's longer.) But I'll switch to it provisionally.

#+begin_quote
  As long as every transformation in between follows the rules, all of my possible divergent selves are me
#+end_quote

That seems different from saying that "you" are exclusively a single, particular one of them. But it looks as though we basically agree.

Going back to the point, though, does every possible mind-transformation /not/ have a successor somewhere in an infinitely varied meta-reality? What more is necessary for it to count as continuing your experience of consciousness; and why wouldn't a transformation that met that requirement /also/ exist?

And, if you don't mind a tangent: If you were about to be given a personality-altering drug, would you be no more concerned with what would happen to "you" afterward than for a stranger?

#+begin_quote
  "Mistake"? Knowing what you need doesn't mean it has to care. Since we're talking about a multiverse containing all possible programs, I'm confident that "stuff that both knows and cares about your wellbeing" is a much smaller target than "stuff that knows about your wellbeing".
#+end_quote

/You/ called them "mistakes". Why would any substantial fraction of the programs that don't care about you extract and reinstantiate you in the first place? Isn't that just another kind of Boltzmann brain; unrelated processes coincidentally happening to very briefly implement you?

(Note that curiosity and hostility would be forms of "caring" in this case, as they'd still motivate the program to get your implementation right. Their relative measure comes down to the good versus evil question.)

#+begin_quote
  Sorry. I meant for that to be an obviously farcical toy example; I didn't realize until now that it could be interpretted as an uncharitable strawman of your argument here. But, yeah, now it's obvious how it could be seen that way, so that's on me.
#+end_quote

Thanks for understanding, and sorry for jumping to conclusions.

#+begin_quote
  That said, you do seem to have a habit of phrasing things in ways that appear to imply higher confidence than what's appropriate. Most relevantly, with Occam's razor. The simplest explanation should be your best guess, sure. But in the real world, we've discovered previously undetected effects basically every time we've ever looked close at anything. If all you've got is the razor and no direct evidence, your guess shouldn't be so strong that "rationality requires you to employ" it.
#+end_quote

When faced with a decision that requires distinguishing between hypotheses, rationality requires you to employ your best guess regardless of how weak it is. (Unless you want to talk about differences in expected utility. I'd call it more of a "bet" than a "belief" in that case, but that might be splitting hairs.)