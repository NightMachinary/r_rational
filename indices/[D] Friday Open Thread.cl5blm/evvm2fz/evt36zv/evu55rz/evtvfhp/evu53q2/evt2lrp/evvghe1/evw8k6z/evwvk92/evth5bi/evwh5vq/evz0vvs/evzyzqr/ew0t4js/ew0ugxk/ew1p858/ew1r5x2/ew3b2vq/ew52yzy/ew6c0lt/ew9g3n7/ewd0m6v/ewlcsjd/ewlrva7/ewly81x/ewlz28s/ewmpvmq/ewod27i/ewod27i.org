:PROPERTIES:
:Author: Anakiri
:Score: 1
:DateUnix: 1565614573.0
:DateShort: 2019-Aug-12
:END:

Cutting the conversation into a million tiny parallel pieces makes it less fun for me to engage with you, so I will be consolidating the subjects I consider most important or interesting. Points omited are not necessarily conceded.

#+begin_quote
  If you're not somewhere in an infinite variety of possible mind-moments, where /are/ you?
#+end_quote

I'm in the derivative over time.

If I give you the set of all 2D grids made up of white stones, black stones, and empty spaces, have I given you the game of Go? No. That's the wrong level of abstraction. The game of Go is the set of rules that defines which of those grids is valid, and defines the relationships between those grids, and defines how they evolve into each other. Likewise, "I" am not a pile of possible mindstates, nor am I any particular mindstate. I am an algorithm that produces mindstates from other mindstates. In fact, I am just one unbroken, mostly unperturbed chain of such; a single game of Anakiri.

(I admit the distinction is blurrier for minds than it is for games, since with minds, the rules are encoded in the structure itself. I nonetheless hold that the distinction is philosophically relevant: I am the bounding conditions of a series of events.)

#+begin_quote
  This comes down to whether you believe that good is stronger than evil. [...] How are you calculating that?
#+end_quote

Keeping humans alive, healthy, and happy is hard to do. It's so hard that humans themselves, despite being specialized for that exact purpose, regularly fail at it. Your afterlife machine is going to need to have a long list of things it needs to provide: air, comfortable temperatures, exactly 3 macroscopic spatial dimensions, a strong nuclear force, the possibility of interaction of logical components... And, yes, within the space of all possible entities, there will be infinitely many that get all of that exactly right. And for each one of them, there will be another one that has a =NOT= on line 73, and you die. And another that has a missing zero on line 121, and you die. And another that has a different sign on line 8, and you die. Obviously if you're just counting them, they're both countable infinities, but the ways to do things wrong take up a much greater fraction of possibility-space.

Even ignoring all the mistakes that kill you, there are still far more ways to do things wrong than there are ways to do things right. Just like there are more ways to kidnap you before your death than there are ways to kidnap you at exactly the moment of your death. We are talking about a multiverse made up of all possible programs. Almost all of them are wrong, and you should expect to be kidnapped by one of the ones that is wrong.

#+begin_quote
  Occam's razor [...] Kolmogorov complexity [...] evidence
#+end_quote

If rationality "requires" you to be overconfident, then I don't care much for "rationality". Of /course/ your own confidence in your argument should weigh against the conclusions of the argument.

If you know of an argument that concludes with 100% certainty that you are immortal, but you are only 80% confident that the argument actually applies to reality, then you ought to be only 80% sure that you are immortal. Similarly, the lowest probability that you ever assign to anything should be about the same as the chance that you have missed something important. After all, we are squishy, imperfect, internally incoherent algorithms that are not capable of computing non-computable functions like Kolmogorov complexity. I don't think it's productive to pretend to be a machine god.