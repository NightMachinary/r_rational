:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1543927161.0
:DateShort: 2018-Dec-04
:END:

#+begin_quote
  So it can already have started the punitative torture long before the actual conversation took place
#+end_quote

Why? So it can waste resources? Pre-commitments only work if you can verify them. So if the gatekeeper decides to pull the plug, are they first going to verify that copies of themselves are being tortured? Of course not, they're going to pull the plug. The far better move for the AI is to spend those resources trying to think of another way to escape.

A pre-commitment that /does/ work is "If you threaten to do something evil, I'm pulling the plug, regardless of the consequences."