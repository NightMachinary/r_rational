:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1543969680.0
:DateShort: 2018-Dec-05
:END:

#+begin_quote
  The AI presumably has already calculated the above threat as the optimal approach. Otherwise it wouldn't be making the threat. So it's not wasting any resources.
#+end_quote

Ugh...that's a fully general counter-argument. Do you realize why that's a problem? I could say "The AI will choose to virtually pick apples for a million years, so that must be it's best plan for escape." Does the above mean that apples are really the best plan for escape? Of course not. It means that /I'm/ claiming that that is what the AI would do. There's a difference between my model of an AI, and an actual AI.

#+begin_quote
  And a precommitment does not have to be verifiable, it just has to be believable.
#+end_quote

Hopefully, the gate-keeper would be smart enough to realize that anything unverifiable shouldn't be believed. Otherwise, why not just believe the unverifiable claim "The AI is friendly."