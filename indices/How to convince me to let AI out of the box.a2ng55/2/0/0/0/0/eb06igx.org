:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1543852141.0
:DateShort: 2018-Dec-03
:END:

My point is just that it all factors in the risk assessment. First: is AI race really, realistically happening, or is YOUR AI just making shit up? In every field, someone needs to be first, after all. Is it likely that it's you? And who might be second? At the beginning, at least, it probably won't be just any average Joe.

Consider a similar situation: nuclear weapons. Had we followed that logic, the only obvious move after acquiring nuclear weapons would be to /nuke every other country into oblivion/ lest they develop them too and sooner or later someone just does it for you. A bit exaggerated but you get the point, I think. The reason why that didn't happen is that there is a threshold for nuclear weapon development, and so only big enough organisations managed to create them - big enough that they're also slightly saner, or at least sane enough to lock themselves into an equilibrium where no one actually gained anything from blowing up the world to kingdom come.

So, you're the one who built this AI: you ought to understand /what it takes/. Years and years of research from brilliant minds and thousands upon thousands of cores using up as much power as a city, or a desktop computer and some time to kill? Because if it's the latter, the situation is much more alarming. If it's the former however you can be sure that at the very least in the first years it's going to be relatively safe (if the process gets more efficient with time all bets are off) so that warrants weighing more your options.

Besides, do you also think the AI is genuinely smart enough to do all it promises? That's the other issue. I'm not sure whether intelligence explosions could really be a thing, but even if they were, I suppose there would be some threshold level of intelligence above which they're triggered, and that needn't necessarily be "slightly smarter than a human". Destroying everything is much easier and requires much less smarts than making everything better. So unless I start seeing some /seriously/ superhuman feats (like, it gives me the plans for FTL communication or for a machine that can reverse the 2nd principle of thermodynamics) I wouldn't trust much that it could actually keep its promises rather than just being Mephistopheles trying to do its bargain thingy with me as his Dr. Faustus.