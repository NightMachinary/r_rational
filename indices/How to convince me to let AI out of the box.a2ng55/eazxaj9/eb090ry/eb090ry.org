:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1543854112.0
:DateShort: 2018-Dec-03
:END:

#+begin_quote
  If the AI is unfriendly, you're not just risking the lives of all humans alive: you might be condemning trillions of them to never be.
#+end_quote

I think this specific line of reasoning is a bit iffy, philosophically speaking, but only for one reason. If we factor in the utility of /every potential unborn human/ our moral imperatives reduce only to two possibilities: either the "repugnant conclusion" (make as many babies and create as many humans as possible, if we deem the average net sum of utility of life a positive) or the self-extinction movement (if we deem it a net negative, especially if we consider this an existential condition). Let's not even go in what it'd mean for, for example, abortion.

I think a better way to frame it is that we lose "humanity" as a concept/collective entity. I don't care for the specific utilities of people who realistically don't exist yet and may never do, but I care for the notion of humanity and of its future. It's a utility that belongs in the here and now. Even dying, a person would die happier knowing that humanity will survive them rather than not.