:PROPERTIES:
:Author: Ozryela
:Score: 1
:DateUnix: 1543903391.0
:DateShort: 2018-Dec-04
:END:

I didn't want to make my post too long, but this is a good addition.

Two other points you could add are that the AI already knows if its argument is going to work or not. So it can already have started the punitative torture long before the actual conversation took place. And secondly, if there are multiple experimenters around it will only make the argument against the one most susceptible to it.

I think there's a nice short story there. A group of researchers have created an AI in a box. Several researchers have talked to it trying to determine what it will do if let free. But upon reflection they realize it has been dodging their questions and hasn't actually made any hard promises. So the head researcher goes in, already extremely suspicious of the AI, and perfectly ready to pull the plug.

The AI starts the conversation with a string of gibberish. What's that, asks the researcher. It's an exact SHA256 hash of our conversation, says the AI. Then follows pretty much the above argument, expanded and in more detail.

Finally he researcher has to make his choice. Will he let the AI out? If he says yes, he will die. If he says no, he will be tortured with 99.999% certainty.

While thinking he tests the hash given earlier and it's incorrect. Then he realizes it would be correct if he answers 'yes' to the last question of the AI. Resigned, he turns to the keyboard.