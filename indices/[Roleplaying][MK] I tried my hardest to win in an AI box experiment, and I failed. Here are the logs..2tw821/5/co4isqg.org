:PROPERTIES:
:Author: ajuc
:Score: 3
:DateUnix: 1422528352.0
:DateShort: 2015-Jan-29
:END:

This was strange strategy.

The human in question can just assume AI can't predict his behaviour with 100% accuracy, and if he assumes that, and won't get AI out of box because of that - he proves the assumption (because had AI knew this strategy won't work - it would use another, so it really can't predict his behaviour with 100% accuracy even short term).

So long-term predictions are completely impossible (as they should be - without perfect knowledge of starting conditions how can you predict chaotic system long term?).

So he can just discard everything AI says.

BTW what's evil about crossdressing?