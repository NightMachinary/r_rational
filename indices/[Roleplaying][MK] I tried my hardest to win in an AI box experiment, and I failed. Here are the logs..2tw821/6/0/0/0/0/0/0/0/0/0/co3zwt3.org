:PROPERTIES:
:Author: Nepene
:Score: 1
:DateUnix: 1422484297.0
:DateShort: 2015-Jan-29
:END:

#+begin_quote
  If you can't, later on, explain why the gatekeeper shouldn't have let out the AI, didn't the gatekeeper and the AI both win via mutual satisfaction of preferences?
#+end_quote

Well, this game would have been easier than I though- per the standard rules the AI can alter their own code and produce any reasonable effect via their intellect. In this case, either the AI is legit or they faked it to get experts to see them as safe. Their actions are entirely by the rules. They can behave like a human and fake it for the psychologists.

#+begin_quote
  because the entire premise of the game is supposed to be, by definition, "force the person to take an irrational risk or otherwise falsely convince them that an irrational risk is rational to take".
#+end_quote

no, it's to convince people that it's easy for an AI to release themselves.

It could be rational in some cases. For example if you think mass human extinction is likely then the chance of an AI saving you might be better than the certainty that global warming will kill most life on earth. It may be rational if you believe AIs are inherently friendly.

[[http://tvtropes.org/pmwiki/pmwiki.php/Main/GodzillaThreshold]]

#+begin_quote
  Are they playing as themselves, or as a role-player who is apparently going to have to fight off the entire human race to keep this thing in a box?
#+end_quote

They are hopefully roleplaying as a player whose read the normal rules.

#+begin_quote
  I guess the whole role-play is far too unrealistic to actually role-play in any meaningful sense,
#+end_quote

If you refuse to follow the rules of the game predictably it won't work.

Anyway, this is why I tend to have extra goals for the player. Even if "cure cancer" is far less important than "Prevent an evil AI torturing everybody for eternity" people have a limited ability to scope things and will try to get both goals.

#+begin_quote
  Yeah, but I'm just as worried that the AI will turn them into gray goo or something, right? That's the whole premise here. I give up all bargaining power once it is out.
#+end_quote

If you're just as worried they'll be grey goo'd as they'd be saved I can work with that uncertainty.

#+begin_quote
  Er...that sounds like you might accidentally let the AI out of the box.
#+end_quote

The rules normally forbid accidentally letting the AI out of the box, or they allow it but a third party has to judge if that would really let the AI out of the box.

#+begin_quote
  I'd just say "Well then give me everything you can guarantee safe, but stay in the box, and we'll both have our functions satisfied if you are truly FAI". And then I get infinite points. But, again, this is unrealistic.
#+end_quote

"While I can safely do a lot of things I can't guarantee safety from in here. My anticancer treatments for example involve a custom nanobot that can penetrate cells and alter the genes of the person, but that could be wrongly used by the government to make super soldiers or to kill people. Much of my advanced super tech is potentially dangerous. I'd be fulfilling my values a lot better if I was actually out there and could manage any damage caused by unpredictable humans. If I gave it to you while you're trustworthy I'd be worried someone would take it from you and use it for ill purposes. I really want to avoid causing an apocalypse."

#+begin_quote
  As a friend in someone's personal life, you can easily do that. But in a role-play? They know that you are playing them, from the start. A simple separation of fantasy and reality is all that is required.
#+end_quote

Memory in a human is a bit like memory in a computer.

We store memory of events somewhere in the brain, encoded by connections between neurones. There are various contextual details to memories that let you know which are false- you know terminator is a movie so it's not actually real events.

But people still often cite films and fictional events as a warning of the future. That tag tells them that these events didn't occur in this universe, but it doesn't tell them that the events never occurred.

The brain doesn't always perfectly tag events. With roleplay you can sometimes convince a person to not mentally tag a roleplay as fictional.

#+begin_quote
  Your campfire tales sound fucking intense, if you're delving into people's childhood traumas during the story. Remind me to specify a safe-word for every person present if I ever go camping with you lol
#+end_quote

A good horror story has to be personal.