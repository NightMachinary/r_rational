:PROPERTIES:
:Author: EthanCC
:Score: 25
:DateUnix: 1587433874.0
:DateShort: 2020-Apr-21
:END:

So, that's basically the non-identity problem, which is one of the big open issues in moral philosophy. The question is how we make moral choices about decisions that would lead to different people existing, since all else being the same someone would rather exist than not. So that would mean, logically, a choice that makes a lot of unhappy people is better than one that makes a few happy people, and so on. This can lead to some uncomfortable conclusions, and that's not even going into how were you to program that morality into an AI it would 'dismantle' humanity to make a large number of the least resource-intensive brain that counts as a person under its programming. A lot of work has been done to try to figure out a satisfactory solution to the non-identity problem, so it's hardly the open and shut case you're making it out to be.

On the level of moral justice, depriving someone of life is generally considered an inherently unjust act. Morality often involves different types of justices and benevolences in conflict with each other, with some injustices being allowed to allow some greater justice/benevolence/outcome, so it's not like there's no morally justifiable case where you might kill. But to say murder isn't unjust is absurd.