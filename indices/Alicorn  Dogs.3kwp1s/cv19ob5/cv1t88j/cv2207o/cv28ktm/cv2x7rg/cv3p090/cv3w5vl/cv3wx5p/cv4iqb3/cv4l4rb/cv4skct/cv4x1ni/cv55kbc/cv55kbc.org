:PROPERTIES:
:Author: philip1201
:Score: 1
:DateUnix: 1442525208.0
:DateShort: 2015-Sep-18
:END:

> The Year of Bayes, 314

> Using axioms rather than priors

I can't help it if you misinterpret my usage of CEV in accordance to the one and only commonly used definition of the phrase. Calling your misinterpretation axiomatically correct for yourself and then insulting me for using the common definition doesn't exactly help resolve the conflict either.

Stripping "CEV" out of my claim, I am saying that we don't need actual dogs, especially not sapient dogs, to determine what dogs would want to do, by the time we have the power to uplift species (=> we can design sapient brain architecture which preserves simpler species' values => we can make a friendly (?) artificial superintelligence). If any part of their morality seems like a better idea than what we had come up with, we can adopt it without actually needing to listen to them because we can simulate them or find more simplified algorithms for searching morality space.

If the result of all that processing is that the world is best off with all species merging into one, then we should do that, but for now, with my limited knowledge and Occam's razor and limited imagination, I'll just say what I can say: not bloody likely.