:PROPERTIES:
:Author: Oh_Hi_Mark_
:Score: 1
:DateUnix: 1500923917.0
:DateShort: 2017-Jul-24
:END:

Exactly, and a virtuous person would strive not to stab others in the neck. Stabbing me in the neck isn't treating people as ends in themselves. Stabbing me in the neck violates ideals of justice and fairness. Stabbing me in the neck violates my natural rights. Stabbing me in the neck is against God's will, who has perfect knowledge of morality.

What stabbing me in the neck emphatically does not do is add one universe to the pile of universes where I (or someone very like me) gets stabbed in the neck. That pile is static. You can't constrain the probability of people getting stabbed in the neck by any choice you make, so you cannot therefore claim consequential responsibility for my stabbing. By stabbing me you have revealed about your character that you are a stabby sort of guy, but you did not produce the consequence of more people getting stabbed than would otherwise have gotten stabbed.

#+begin_quote
  How, exactly, are infinite copies of "me" and infinite copies of "you" relevant to the "me" and the "you" who are currently communicating with each other?
#+end_quote

Since my neck has not yet been stabbed, I assume we are talking about future permutations of this universe, in which case all stabbed and unstabbed branches of me are "me", though they may not be each other. All stabbing and unstabbing versions of you likewise.

#+begin_quote
  you can't collect all of their suffering into a collection plate and offer it up as 1,000 units of suffering because suffering is an experience and no one experienced 1,000 units of suffering in that scenario.
#+end_quote

If this is the case, then I would say that your units are improperly calibrated. If you mean that literally no amount of small harms can equal a large one, though, I would tell you that this idea is not popular in circles of moral philosophy because it is so messy as to be useless.

Where is the line between a large harm and a small one? Can large harms be added together to outweigh even larger harms? How many instances of 9000 units of suffering must be aggregated to be worth preventing by one instance of just over 9000 units of suffering? Should we purely disregard the small harms that result from our actions, no matter how far reaching or long-lasting those small harms may be?

Or is there an infinite series of harms, where the larger individual harm always outweighs any number of incrementally smaller harms? Is painlessly killing all life in the universe preferable to painfully killing one person?

More importantly, what are we meant to do with this information? The logical conclusion would seem to be that we should locate the harm of greatest magnitude, duration, and fecundity in the universe and by any means stop it, then work our way down the list. That might make some kind of intuitive sense, but it simply isn't a morality that can be practically applied to human life.