:PROPERTIES:
:Author: space_fountain
:Score: 2
:DateUnix: 1483894395.0
:DateShort: 2017-Jan-08
:END:

This is an interesting problem. It actually gave me a thought as to how some of humans less rational stances might come about. Basically I think what you'd want to do is give the AI a strong preference for non action. Others are giving good suggestions in regards to hacks essentially to gain more time, but the fundamental problem is that you can never be sure of all the ramifications. So the right course of action is to give up at least partially. Take no action unless you can be sure with greater than 99% certainty that 90% of sentient entities would want the action taken if they were aware of the possible ramifications.