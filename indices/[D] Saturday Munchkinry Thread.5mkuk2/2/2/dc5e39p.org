:PROPERTIES:
:Author: vakusdrake
:Score: 7
:DateUnix: 1483858399.0
:DateShort: 2017-Jan-08
:END:

Given I only have 12 hours (unless technoninja1's plan works) the only thing that seems like it makes sense is to find a method that forces the AI to most of the work figuring out the details itself. Since even the most well thought out moral utility functions like CEV have significant problems, or rely on assumptions about human moral nature, of which I am not willing to count on.

What I think will work best is simply asking the AI to use a hardcoded copy of your current moral system. This isn't subject to the AI worrying about corruption, nor is it divine command theory. Plus it wouldn't make sense /not/ for it to work, after all if it thinks you are this reliable moral arbiter, then using a hardcoded version of your current ethics seems like it ought to be the optimal solution from it's perspective. Since it isn't subject to you accidentally making a moral system that is untenable and contradictory and it will probably correspond best to whatever aspect of "you" that it thinks is morally reliable anyway.