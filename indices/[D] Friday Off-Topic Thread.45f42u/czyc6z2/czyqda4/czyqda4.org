:PROPERTIES:
:Author: tvcgrid
:Score: 2
:DateUnix: 1455391850.0
:DateShort: 2016-Feb-13
:END:

You know, if the end goal of the protagonist is to forecast well, perhaps there are more effective solutions than a prediction market. Tetlock covers one compelling alternative in Superforecasting.

The book's about this team that (strongly, consistently, with growing margins over time) won a 4-year long, rigorous forecasting tournament. Anyway, along the way, they ran many RCTs testing various hypotheses like 'do teams of forecasters perform better than individual forecasters', and 'does a specially curated team of forecasters beat out prediction markets'. Well, turns out such a team did outperform a prediction market. It was constructed by taking the top performers based on results of a pretty rigorous metric (Brier score, which measures forecasting accuracy) from a large sample and putting them together in a team. It raises important questions about the efficacy of prediction markets when this approach so handily beat out alternatives. Maybe the prediction market under test wasn't fully incorporating real costs/incentives, but I imagine some more RCTs can uncover that.

I'm still uncertain but it did downshift my belief that prediction markets, if constructed well, are optimal at forecasting accuracy. I'm more like 60:40 in favor of this 'superforecasting team' technique, but just haven't read enough to gain more confidence.