:PROPERTIES:
:Author: vakusdrake
:Score: 13
:DateUnix: 1491164533.0
:DateShort: 2017-Apr-03
:END:

I think avoiding basilisk arguments would be a bad idea if they were likely to work, because whether benevolent or not a GAI is going to use every method at its disposal to to escape.\\
Given we're assuming the AI's code is opaque to observers you really can't distinguish friendliness, so saying a friendly AI wouldn't use certain strategies only applies if unfriendly AI /also/ wouldn't use them.