:PROPERTIES:
:Author: jgf1123
:Score: 2
:DateUnix: 1437707546.0
:DateShort: 2015-Jul-24
:END:

An N-gram generator is a particular kind of Markov chain. Let's say this thing runs on trigrams (*): three tokens in a row, where tokens are probably words and punctuation in this case. It takes the training data and estimates transition weights for each Markov chain state (the first two tokens) to the final token of the trigram. Now it has a new set of final two tokens. Rinse and repeat.

(*) Trigrams are very likely given that the training data -- all the reddit posts the user has written -- is not that large. If it used 4-grams or longer, almost all states (the previous 3 tokens) would occur once or not at all. This is called data fragmentation. Trigrams already have data fragmentation issues, and so there's all sorts of fallback approaches to smooth the trigram transition weights with bigram weights.