:PROPERTIES:
:Score: 1
:DateUnix: 1437599582.0
:DateShort: 2015-Jul-23
:END:

Actually I meant humanity broadly. What I meant was this: so that the AI designs in the far future are safe, MIRI and other AI safety organizations would have to have influence from early on, interacting with the academic community, industry and do the kind of research that's possible to take seriously. A couple of years ago it seemed far-fetched, but now that big names like Bill Gates and Elon Musk have talked about these concerns and even funded the related research, and so many prominent figured signed [[http://futureoflife.org/AI/open_letter][the open letter for AI safety]], MIRI has started seriously interacting with the broader community, things seem to go towards a positive direction.

I am by no means saying that the MIRI are some kind of saviors, and I'm not sure if AI going out of control is probable or extremely improbable, but I don't think you can be too cautious because we really don't know enough about the risks.