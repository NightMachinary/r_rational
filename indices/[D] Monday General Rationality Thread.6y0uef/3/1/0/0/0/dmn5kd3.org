:PROPERTIES:
:Score: 4
:DateUnix: 1504713450.0
:DateShort: 2017-Sep-06
:END:

I mean, the upside is that machine learning methods /really are/ good at capturing complex, seemingly black-box statistical judgements like "Is this a cat?". The downside is that black-box machine learning methods don't capture /any/ of the causal structure that /makes/ a cat into a cat.

So the downside is that if they build AI in /any/ of these ways, it will be /wildly/ unfriendly and require /very/ close supervision to not fuck all our shit up. The upside is that, since it "experiences" the world as nothing but clusters in a high-dimensional vector space, it probably won't have a good-enough real-world understanding to non-solipsistically make paperclips, let alone the self-understanding to improve itself.

So we get really bad, stupid, nigh-malicious AIs that just can't turn superintelligent. Oh joy.

Combine many of these bad ideas about safety with [[https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/][good ideas about cognition]], though, and we're very potentially /completely fucked/.