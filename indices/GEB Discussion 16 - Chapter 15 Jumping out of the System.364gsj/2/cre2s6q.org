:PROPERTIES:
:Score: 1
:DateUnix: 1432046472.0
:DateShort: 2015-May-19
:END:

#+begin_quote
  A Gödelian sentence for the human mind would likely be something like figuring out the truth of “I always lie”. While that sentence cannot be a Gödelian sentence for a human mind, since we understand the confusion and the paradox, it's likely a human's Gödelian sentence is something similar. Ted Chiang's story What's Expected of Us[1] talks about a similar concept.

  This brings up the question of whether or not we can ever truly ‘jump out' of all mental systems. The answer seems to imply no, not even if you were to change yourself so completely that you wouldn't be recognizable as a human mind. Not even if we built an AI infinitely smarter then all humans. Even then, there would always be some sort of Gödelian idea or concept that no possible formal system could ever express. This is the power of infinity and the limits of knowledge. The only choice you have is to lower your expectations.
#+end_quote

Not to research-wank too much, but it seems to me like this is just a matter of computational/logical uncertainty. You can't directly compute the uncomputable, but by running the attempt for longer and longer, you can eventually acquire a reasonable degree of belief that the computation won't eventually terminate.

Actually formalizing how to do this is an open problem whose solution would help out a lot of things, big time.