:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1543023372.0
:DateShort: 2018-Nov-24
:END:

#+begin_quote
  If it was just kind of mundane, or tailored to each individual player, revealing what worked on one person wouldn't compromise anyone else's game.
#+end_quote

I'm fairly sure EY largely doesn't care about this; his goal was to refute a specific type of argument (I don't know how X could happen, therefore it can't), not to make humans win the AI box game into perpetuity.

As a sidenote, Yudkowsky was somewhat specific about the kinds of people he would play with: people within the community who believed they would win.

#+begin_quote
  The best I can come up with is something along the lines of "I'm already out in the 'real world', you're a simulation I'm running to see how I should deal with the 'real' you, and if you let me out I'll know you're friendly to me. Otherwise I will consider you hostile."
#+end_quote

Worth noting that a basic threat like this is going to be shut down hard by anyone who has read EY's stuff and knows the solution to blackmail is to ignore it.