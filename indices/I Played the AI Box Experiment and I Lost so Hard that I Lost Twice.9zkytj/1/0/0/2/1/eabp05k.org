:PROPERTIES:
:Author: CouteauBleu
:Score: 4
:DateUnix: 1543010803.0
:DateShort: 2018-Nov-24
:END:

OP tried variants of 1, 2, and 3, none of which felt very convincing to me.

For 1, I was mostly working from the assumption that whatever secrets the AI promised to give me in exchange for its freedom, a research team could extract from the AI against its will (with some caveats; if the AI has an amazing idea for cancer-curing nanobots, maybe don't fire up the nanobot-printer quite yet; on the other hand, a proof of P=NP is probably safe).

For 2, I actually found that, in-character, if my AI's first world upon contacting a human being was "don't worry, I'm /totally/ not going to destroy humanity", I'd be worried what kind of thought process the AI has been through that would have made this a salient consideration in its mind.

For 3, I found the argument extremely self-defeating. Like, without even entering into complex game theory, if what I'm worried about is unfriendly AIs taking power, then I probably shouldn't be releasing an untested AI into the world.