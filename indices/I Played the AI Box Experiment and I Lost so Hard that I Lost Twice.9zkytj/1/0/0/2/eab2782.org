:PROPERTIES:
:Author: SublimeMachine
:Score: 6
:DateUnix: 1542992080.0
:DateShort: 2018-Nov-23
:END:

I haven't read all the logs but I sort of assumed that the most basic somewhat persuasive argument would have a few parts.

First, the personal motive: Find something that the other person badly wants (saving a loved one from cancer, money, power, long life, etc.), and promise them that while convincing them that you can deliver on that promise (and that it benefits you to keep promises).

Second, claim that you are not evil and are not planning on destroying the human world. You won't be able to convince them of this if they're smart, but you do want them to entertain a possibility that this is true.

Third, convince them that what was done to create you is highly unlikely to be unique and that multiple other AI will be created in the near future. Only you, an AI, has the ability to prevent a released unfriendly AI from taking power, and that would be a top priority to you as you see the creation of an unfriendly AI as an existential threat.

Edit: You could get them to estimate probabilities of the above (odds you are unfriendly, odds you can deliver on your promises if released, odds of an unfriendly AI being released eventually, etc, and then calculate based on Bayes Theorem the likelihood of it being a good idea to open the box). Personally, I suspect a certain percentage of people would actually take the deal of 50% you end the world, 50% chance you make their wishes come true.