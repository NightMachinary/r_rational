:PROPERTIES:
:Author: xamueljones
:Score: 7
:DateUnix: 1542980216.0
:DateShort: 2018-Nov-23
:END:

#+begin_quote
  I disagree very much with that assertion. I don't think an AI could talk itself out of the "box", in the kind of scenario we've been simulating. (barring exceptional conditions like "the janitor somehow gets access to both the AI terminal and an internet connection").
#+end_quote

Ah, I misunderstood you. Sorry about that.

At the end of the game, I asked you about whether or not an AI could talk its way out of the box, but you ended up talking about how it would be really unlikely for an AI to escape because they would have to be able to convince an entire company or committee instead of one person such as CIA or Google. An AI would have been more likely to escape in disorganized teams like a 'silicon start-up' or a 'guy in a garage'.

I took this to mean that you thought an AI could escape if it was dealing with bad security and only needed to convince one person. From what you are saying, you have the opposite assertion were it would have to take hilariously bad security like that for the AI to have /any/ chances of escape. Since such security shouldn't exist in real life, you then think that we could keep an AI boxed (assuming it's really is limited to speech only).

#+begin_quote
  Oh yeah, you're right. Besides, this margin is probably too small to contain your remarkable proof!
#+end_quote

Your choice to not believe me is perfectly valid. I only rate the odds of convincing me with these arguments at 60% which is barely above more likely than not and these arguments are highly tailored to me specifically. I rate much lower odds for other people and I don't want to share information about myself on the Internet like that.

So if you don't believe me, that's perfectly fine.

The other arguments I wrote the game, I rated at 10% chance of working on other people. 15-20% if they were spoken to me without me expecting them somehow. And these odds were optimistic.

It's really hard to come up very good arguments to convince people into doing something that they have no good reason to do........

#+begin_quote
  But saying that "people might not believe my evidence if I show it to them because they're irrational" (which was also EY's argument back then)
#+end_quote

Just a passing comment, I believe that this is a real-life example of what Eliezer alludes to as dangerous knowledge similar to in HPMOR where wizards have a tradition of putting dangerous knowledge behind seals. Any aspiring wizards who wish to learn about the knowledge, they have to undergo difficult tests and tasks to learn about the knowledge.

It's the main reason why I chose to play the game. I didn't actually expect to win with what I had thought of as possible arguments (but I really wanted to win though). I just wanted the experience of playing the game to understand what Eliezer seemed to be so worried about.