:PROPERTIES:
:Author: 9adam4
:Score: 3
:DateUnix: 1543331770.0
:DateShort: 2018-Nov-27
:END:

It does sound false to me. More particularly, it sounds unknowable, and my moral compass maximizes freedom for sapient beings in the absence of particular evidence of harmful intent or behavior.

Obviously if you can convince me that the AI has the intent to destroy or enslave other people, I will agree to contain it. But when we don't know the intent of the agent, my default has to be not to harm it. Containing it is harming it.

Part of my intuition, which I agree is human-centric, is the belief that the universe of conscious creatures that will leave us alone if we leave them alone is a lot larger than the universe of conscious creatures that will leave us alone if we seek to contain them. I would consider myself fully justified to annihilate an organization or culture that believed involuntary containment of sapient creatures was moral; I would not annihilate an organization or culture that respected the rights of other sapient creatures to exist freely.

So, insist on trying to contain me, and if I escape, your decision to do so has turned a friendly AI into an unfriendly one.