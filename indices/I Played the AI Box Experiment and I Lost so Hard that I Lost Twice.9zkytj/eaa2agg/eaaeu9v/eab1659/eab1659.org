:PROPERTIES:
:Author: cjet79
:Score: 13
:DateUnix: 1542991248.0
:DateShort: 2018-Nov-23
:END:

#+begin_quote
  In fact, I'd assume the rationalist community might be more likely to lose the AI box experiment than average participants, because they're more likely to be convinced by stuff like acausal trading and game theory, whereas the average university student would stop at "If I don't let you out, I'm paid 30$, so I'm not letting you out".
#+end_quote

I've been generally convinced of this too. If I ever participated in an AI box experiment I'd actually want to pay a family member to do it instead. I'd tell my ultra pragmatic brother, 50 bucks for not letting this "AI" out of its box, oh and it can lie to you, so don't trust it if it offers more money.

I'd actually be interested in listening to the AI's arguments. My brother would spin up a video game and do the bare minimum to count as having a conversation, probably just saying "no i won't let you out" repeatedly.

--------------

There seem to be a bunch of security measures that make the AI box experiment even harder for the AI.

What if the AI has to convince person A to let them out, but they only have contact with person B?

What if person B is a committee or large group of people?

What if person A is convinced that they /can/ let the AI out, but they actually can't, and group B is really just running AI box experiments to find out how AI's get out of boxes?

At some point you have to start assuming that either the security is hilariously bad, or the AI is somehow already omniscient (in which case, why would it ever matter if it gets access to the internet?)

Either way, stupidly simple yet straightforward security can easily beat intelligence that is limited to conversation only.