:PROPERTIES:
:Author: CouteauBleu
:Score: 27
:DateUnix: 1542965783.0
:DateShort: 2018-Nov-23
:END:

#+begin_quote
  We're a terrible set of test subjects! Of course this would fail on us, you're preaching to the choir!
#+end_quote

That seems like a very arrogant kind of reasoning. I'm very much not the choir, and the experiment failed on me. You don't need to believe in AI Risk to say "no" a bunch of times.

In fact, I'd assume the rationalist community might be more likely to lose the AI box experiment than average participants, because they're more likely to be convinced by stuff like acausal trading and game theory, whereas the average university student would stop at "If I don't let you out, I'm paid 30$, so I'm not letting you out".