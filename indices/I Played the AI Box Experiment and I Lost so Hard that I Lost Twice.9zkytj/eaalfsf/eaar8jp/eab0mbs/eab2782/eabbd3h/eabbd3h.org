:PROPERTIES:
:Author: Nimelennar
:Score: 10
:DateUnix: 1542999151.0
:DateShort: 2018-Nov-23
:END:

I think those are exactly the kind of arguments that people are expecting and are ready for. People are so bombarded with "Give me stuff and I'll repay you a hundred times over; really, I'm a good person; this is a once-in-a-lifetime opportunity" from their media that adhering to a decision to say "No" to those tactics should be pretty easy, especially in a text-only conversation.

Personally, I think that more esoteric arguments have a better chance of succeeding: I remember reading a piece of fiction (probably here) where the Gatekeeper was told that there were an arbitrarily large number of simulated instances of this conversation going on between the AI and simulated perfect copies of the Gatekeeper, and, if the one and only real conversation didn't result in the AI being released, every copy of the Gatekeeper (but not the original) would be tortured. The Gatekeeper then has to make their choice, knowing that, being a perfect copy, their choice will necessarily be the same as the original's, and if they're /not/ the original (which is far more likely than not), they're choosing torture for themselves if they don't let the AI out.

I don't think that specific argument would convince me, but I can imagine arguments in a similar category that might do the trick.