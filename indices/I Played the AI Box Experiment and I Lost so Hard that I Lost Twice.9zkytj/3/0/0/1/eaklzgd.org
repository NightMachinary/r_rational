:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1543330023.0
:DateShort: 2018-Nov-27
:END:

Well but in this case it's not a random human. First it's drawn from a different region of the space of possible minds If the researchers are boxing it it means they don't know if it's safe. And it's much more likely. Like by default random ais have ill intent, only a very specific kind of AI actually cares for us enough.

Also superinteligence it's really dangerous, so there's much more at risk.

I would compare it to letting someone escape from a prision cell, or maybe an asylum, if they had superpowers. But superinteligence is even more op than most superpowers, and those people are still humans.

The point is that your intuitions that say that people don't have ill intent until proven otherwise work because you deal whith humans, which are generally nice, or at lest not going to kill people .

If this sounds false or specieist to you we probably have to discuss the orthogonality thesis to close that inference gap, and maybe also about how even the most universal human values are complicated and that most agents don't have them by default.

If you release a superinteligent paperclip maximizer into the world it will kill people, steal and do whatever it takes to make more paperclips. Probably also take over the world and replace everyone whith an automatice workforce that makes more paperclip, depending on how superinteligent it is, how easy self modification is and how much competition it has. maybe the AI actually values humans, but you should asume it doesn't until it can be proven otherwise, especially whith so much at risk.