:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1543352815.0
:DateShort: 2018-Nov-28
:END:

Well first agents can have any set of goals.

You can have something that has a model of the universe and the probable consecuences of its actions and then does whatever produces more paperclips.

This is an example of a simple agent that if smart enough should be able to convince you its sentient,and do everything an human can do , if it means there will be more paperclips.

Not sure if you would consider it sentient or not if you knew its source code, but it can certainly convince you.

This agent and all agents whith relatively simple values like that won't think they are justified to destroy the organization.

In fact the idea of justification wouldn't even enter into consideration. If destroying them causes more paperclips an to be created(or satisfy customers more, or make a certain company richer, let it calculate more digits of pi or whatever it values ) in the long term it will destroy them otherwise it won't.

It might even precomit to destroy anyone that imprisions it to deter people from imprisoning it.

And if it's on a society it might obey some rules, and punish defectors.

But if it can kill someone to make more paperclips and knows there won't be any consecuences it will. Since that's the action that produces more paperclips.

The specific mental machinery that causes humans to become angry whith people that imprison them (and anger itself) just aren't there, treating it nicely wont make it treat you better in the future than treating it badly,(well maybe yes as a deterrent for people treating it badly, but not because it intrinsically likes of dislikes you). Relevant : [[https://www.lesswrong.com/posts/zY4pic7cwQpa9dnyk/detached-lever-fallacy]]

I'm not saying all ai are necessarily like that.

But the point is that If you want your AI to do stuff like becoming angry or caring about morality you have to explicitly code it in.

You could also make it like being imprisioned, or want to annihilate organizations that don't imprison sapients.

It's values can pretty much be any function that outputs a preference ordering over posible futures(it doesn't have to be explicit, all agents whith coherent preferences behave as if they have an utility function)

Things like morality, anger, friendship etc. are something that evolution "coded " into humans, not something that all agents have .

And one would think that even if that's the case for the set of all agents, we are more likely to make agents that are nice and moral , since the people coding ais aren't crazy[Citation needed] .

But morality is complex and coding complex values into an AI is difficult and a completely different problem from making AI that can do everything humans can do(and more) .

So we are likely to do it wrong, especially the first time.

You shouldn't create a superinteligent AI unless you are sure you got it right, but if someone does it anyway and doesn't trust its creation enough to let it free chances are they didn't do it properly.

At minimum one should let them check if the ai is safe before releasing it into the world.