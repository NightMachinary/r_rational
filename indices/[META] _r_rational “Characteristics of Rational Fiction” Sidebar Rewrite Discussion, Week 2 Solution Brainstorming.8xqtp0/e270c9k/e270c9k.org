:PROPERTIES:
:Author: causalchain
:Score: 4
:DateUnix: 1531321110.0
:DateShort: 2018-Jul-11
:END:

I've noticed a couple ideas from other people that haven't yet been represented, so here is my attempt to do them justice:

* Intellectual Payoff:
  :PROPERTIES:
  :CUSTOM_ID: intellectual-payoff
  :END:
The enjoyment of the climax is a major defining feature of a genre. Romance has an emotional payoff of seeing the relationship progress. Power creep stories (E.g. Xianxia or RoyalRoadl) payoff from flaunting how over powered the MC is. Rational fiction often has the payoff of watching an overwhelming problem overcome with an *intelligent* solution. (I feel like this wording needs more work)

This requirement feeds into many of the things we enjoy in rational fiction:

- *consistency* is required to feel that the world, characters and problems are real and non-trivial.
- *Explaining the rules* lets us appreciate the problem and solution in it's entirety; as opposed to just being told that the problem and solution exist.

  - A reader is more invested in a problem they can *participate in solving*.
  - The intellectual payoff of the plan is heightened by *watching the main characters' thought processes*.

- *Competent* characters are needed on the protagonist's side to create and enact the intelligent solution, and on the antagonist's side to make the challenge reasonable + difficult and make the ultimate success rewarding
- *Learning, Preparation, Munchkinism* all give off intellectual payoff.
- cheating a problem using a dues ex machina or just ignoring minor problems (such as travel times) because they distract from the plot diminish the intellectual payoff at the end (the problems feel cherry picked and contrived)
- I'm stretching it a bit here: *Worldbuilding* can be seen as "solving a world". Taking the core ideas that the author wants in their world and extrapolating out to see the consequences in their stark honesty. things like WTC's "world-skewered" as a response to people losing their memories and claiming they created the world. They give the same feeling of intellectual payoff, like the author did something smart.
- *Good Writing* aims to maximise the readers' enjoyment and investment, so it is unsurprising that we would associate it with rational fiction and vice versa; good writing makes rational fiction /feel/ more rational, since it has a greater payoff.

So as a conclusion, making a story trying to maximise intellectual payoff looks like rational fiction. If it swims like a duck and quacks like a duck, it has non-trivial bayesian evidence that it is a duck.

* A ridiculous number of tags:
  :PROPERTIES:
  :CUSTOM_ID: a-ridiculous-number-of-tags
  :END:
have a tag for every subset of rational fiction imaginable and let people pick what they think applies to their story. Not so useful for defining rational fiction, but useful for organising the sub: in particular the concern that our subreddit is overly fond of sci-fi/fantasy transhumanist webfiction, a small subset of what we would consider "rational". With tags we can say: that is what rational fiction is and this is what we enjoy on the subreddit. [[/u/Noumero][u/Noumero]] has a similar opinion, using tropes to describe what we want to see in fiction. Now if only we had bayesian tagging... [80% RST]

Edit: Added a point, reworded statements.