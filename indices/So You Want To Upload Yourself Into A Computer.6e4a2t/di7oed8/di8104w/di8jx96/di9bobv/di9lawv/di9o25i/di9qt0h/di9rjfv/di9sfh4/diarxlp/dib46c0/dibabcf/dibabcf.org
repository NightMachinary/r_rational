:PROPERTIES:
:Author: ZeroNihilist
:Score: 1
:DateUnix: 1496318167.0
:DateShort: 2017-Jun-01
:END:

#+begin_quote

  #+begin_quote
    It raises the question about why your standards are so much higher for a digital version of yourself.
  #+end_quote

  Because an artifical version of myself is a difference in kind, not in scale.
#+end_quote

But you have no idea if any version of you except the current one would even pass the test. How can you be sure the test is even valid if you aren't checking it for flaws?

#+begin_quote
  I don't care if my copy is an turing-complete entity or not.
#+end_quote

The Turing test isn't about Turing-completeness. The former is a test for if something is "human-passing" (and the KilotonDefenestrator version would be if it could pass as you), the latter is a property of computing systems that are able to perform any computation a Turing machine can perform (it's the computer science way of saying "It can run any standard program").

#+begin_quote
  I want it to have the same scope of capabilities, including emotions and illusion of consciousness, as me. Because I enjoy those things, and it would be unethical to create a sentient copy of me without those things. It would be like intentionally fathering a child with handicaps, while somehow knowing that the child doesn't want those handicaps.
#+end_quote

Right, but if you asked your upload and it said "Yeah, I'm definitely conscious.", how would you know if it wasn't? You can't even verify that any other humans are truly conscious.

#+begin_quote
  I am not interested in something that is identical to me to an outside observer. I want to be sure that my copys internal experience is also identical to my internal experience. Otherwise, from my perspective, it is a failed copy.
#+end_quote

Humans don't have that level of self-reflection even for their own memories. If you examined your own brain you would be unable to discover your internal experience.

Even if you cloned yourself down to the smallest sub-atomic detail, you wouldn't know if your internal experiences matched. What you want can't be done at the moment.

Ironically, the only way you'd be able to verify they matched is by running simulations of both brains and comparing their states to identical input profiles (and, as usual, this test would fail for any version of you that wasn't the current one).

#+begin_quote
  However, if the uploaded version of me loses the ability to have emotions, or loses the illusion of consciousness, then that is a entirely different order of difference. Comparing the two makes no sense to me.
#+end_quote

So ask it. There is as yet no way of determining if something is conscious beyond just asking it, and I'm fairly certain there never will be (though we may develop more nuanced ways of "asking", like direct brain stimulation in an fMRI, for entities that can't reply with language).

And if you do somehow determine that your upload perfectly emulates you without emotions and consciousness, what exactly are emotions and consciousness doing in you that they aren't in your upload? Would it even make sense for something to be indistinguishable from you and yet lack consciousness?

It would be like if I removed internal combustion from a car engine, yet it still ran like normal. Either the engine didn't need internal combustion to function, or I didn't really remove it.

#+begin_quote
  Blatantly exaggerated to illustrate my point, it is like you are making the argument that since I have grown with one kilo since five years ago, removing one kilo of brain matter would make no difference, and I should be ok with someone doing that to me.
#+end_quote

Well, if somebody removed a kilo of your brain matter and it didn't affect your behaviour in any way, what's the problem? Seems like they got rid of something completely useless, because if it had any use---even the tiniest scrap of usefulness---your behaviour would change.

If I had an ironclad guarantee that my behaviour would be unaffected, I'd let people mess with my brain as much as they wanted. By definition I wouldn't change as a result. Of course, it's virtually impossible for that guarantee to exist, so it's a moot point.