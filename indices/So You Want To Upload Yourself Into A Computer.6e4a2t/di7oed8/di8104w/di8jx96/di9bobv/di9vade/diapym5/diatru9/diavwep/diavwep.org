:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1496286436.0
:DateShort: 2017-Jun-01
:END:

#+begin_quote
  Also, you can't tell other people are conscious by direct evidence ("shallow" inference), just conscious inference ("deep" inference).
#+end_quote

But how would installing an artificial claustrum in me prove anything? If my behaviour doesn't return to normal, it failed. But if my behaviour does return, I can't prove it restored my consciousness, and I certainly couldn't know it myself (because if I knew I wasn't conscious, it would alter my behaviour).

We could definitely prove that my ability to function was restored or not, but we could never prove that it was restored without consciousness.

#+begin_quote
  I tend to think that, for instance, a superintelligent AI (or other sufficiently powerful Thingamy) could make a meat-puppet act conscious without its actually having a mind.
#+end_quote

I agree that the meat-puppet wouldn't be conscious, but there would still be consciousness: the super-intelligent AI.

If we're able to make a simulation that appears to be conscious, there must be consciousness somewhere. It might not be in the simulation, but where else it would be hiding I don't know.

#+begin_quote
  I mean, we already basically do it with cartoons, right? Or hell, with chatbots. We know ELIZA has no mind in there whatsoever, but people still start attributing feelings and experiences to it based on intuition when they weren't told how the program works.
#+end_quote

If colloquial opinion is the metric you want to use for consciousness, it's not going to be a very useful concept.

Any test needs to adequately separate things we know are conscious (humans) from things we know aren't (recordings of humans, chatbots, etc.). Once we know it works as desired on those categories, we can then use it on things that we aren't sure about (animals, AI, uploads, clones, etc.) to see if they can be ruled out.

The starting point would probably have to be the Turing test. It may not be optimal (especially when translation and cultural difficulties are involved), but it's significantly better.

If something passes the Turing test and all other tests we come up with, we've effectively reached the limit of our ability to separate things along that axis. Either we rule that it is conscious or we discard the concept (at least until we get a better, more testable definition).

Essentially, if something passes all the tests but we still won't accept it into a category, we're not actually basing membership on the tests (or there's an additional, unspoken test, like "You have to have a flesh-and-blood human brain.").