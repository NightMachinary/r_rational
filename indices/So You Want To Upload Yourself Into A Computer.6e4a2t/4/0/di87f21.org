:PROPERTIES:
:Author: LeifCarrotson
:Score: 6
:DateUnix: 1496155570.0
:DateShort: 2017-May-30
:END:

#+begin_quote
  "forking-and-deleting in pursuit of higher goals is perfectly fine, and is in fact a good idea."
#+end_quote

This is particularly relevant because the first upload is unlikely to be a pleasant experience. I expect that early pioneers will have sensory deprivation or saturation problems, phantom pain, sleep problems, and general corruption and degradation of the clone state. Perhaps thousands, millions, or untold billions will die for this cause.

But if it means that we can actually solve the pesky biology problem of death, that's a sacrifice I would make.

One distinction I would add, too, is that a fork is not a new consciousness out of nothing. It shares commitments and, hopefully, sentiments with the historical version.

This is obvious when it comes to duplicating an uploaded human: it's an identical collection of bits, perhaps we label one "A" and the other "1". Does it matter which was the original? No. Both remember being the original, and both remember, for example, deciding that clone "A" will work on a particular task and, on completion, self-terminate, while clone "1" will get the needed results of that task and continue surviving on their limited power budget or whatever motivated the fork.

For the initial upload, yes, one now exists in carbon chemistry and the other exists in silicon. And the upload is certainly a big event! But I don't think it's particularly different than asking if a person is or is not the same person after saying a marriage vow or graduating from a military academy, or, on a less significant scale, just being a slightly different person than they were one second ago.

Talking about "higher goals", let's put this in perspective. I would be willing to make significant sacrifices for my son - he is very important to me. If that means giving up some component of my income, social life, and personal time, well, that's just plain old parenting, which I am doing now. If it means that I need to confront a gunman in my home or run into a burning building, I would do that for him - though those are unlikely, contrived situations, I would be willing to give up my life to save his. If, in this context, it means instantiating a copy of myself, expecting that they will suffer and die, I would make that sacrifice. I understand this could read as a horror short to some worldviews, so stop reading if you're sensitive to the issue.

But I would make that sacrifice again, and be grateful for the opportunity to make that greatest sacrifice twice. And three times. And ten, a hundred, a thousand million billion times. (And yes, I would be grateful that when they got it figured out I could actually upload myself successfully.)

But consider what you would do if you blinked and opened your eyes to a text prompt stating "Hello LeifCarrotson, we hope this session 008364729 of the consciousness hosting software patches the excruciating pain and wretching nausea you reported when we attempted to improve the simulated vestibular system last time. If it does feel better, please select "Yes" and we can continue working on other bug reports and research efforts, or choose "No" and describe your pain levels and nausea. You may also select "RESET" to revert to the initial upload, if the remembered pain and suffering are too much for you to function, but be aware that some of your memory of how to use the interface and of past research efforts and symptoms will be lost. And, finally, you can select "STOP" if you do not wish to subject yourself or any future instances of yourself to more of this experience."

What would you do? How manyâ€‹ times could you push "No"? I think I could last quite a while: not a billion times in a row, for sure - no human is that fool-proof, I'd probably mis-type STOP in the interface before that, but with a few forks forking themselves it could be a staggering amount of suffering. Worth it, if you ask me.