:PROPERTIES:
:Author: ZeroNihilist
:Score: 1
:DateUnix: 1496292275.0
:DateShort: 2017-Jun-01
:END:

#+begin_quote
  Well if it's not a physical phenomenon, then what the heck is it? I mean, every time nonphysical theories have been proposed, they've gotten curbstomped by science, eventually, so Bayes already hates that first sentence of yours with an unholy passion.
#+end_quote

I'm not saying it's not physical, I'm saying it's not located in any part of the brain. It's an emergent result of the interaction of a variety of neurological processes.

A car without an engine doesn't go anywhere, but it also needs wheels, axles, gears, etc. Further, the configuration of a car isn't the only one that gives rise to linear propulsion; you can have planes, trains, bacterial flagella, etc., each with their own way of producing the same emergent effect.

Likewise, if we can identify certain requirements for consciousness we shouldn't discriminate based on anything /but/ those requirements. Doesn't matter if it's a human brain, a simulated brain, or an AI.

#+begin_quote
  Uploading, on the other hand, takes an image of the structure of this circuit, and then moves it to some substrate which runs that image through a model continuously, generating new images that correspond with greater-or-lesser accuracy to the images that one would have expected the original structure to produce given the same stimuli. In this case, you get a lot of insanely complex 3D images which all represent insanely complex circuits. But, a map is not the territory; to use an old cliche somewhat inappropriately.
#+end_quote

I agree. But does consciousness depend on that level of precision? Your brain is in a perpetual state of flux, yet you stay conscious. Even people who suffer traumatic brain damage---some losing significant functionality, like the ability to make spontaneous decisions, or read, or remember---remain conscious (or, at least, I don't see people clamouring to denounce the intellectually disabled as p-zombies).

Whatever the root cause of consciousness, it must have a truly absurd degree of tolerance to encompass all the variance of the human mind.

The same is true of the quintessential you, but with significantly more stringent requirements because it has to exclude all other humans (and, depending on your definition, some past versions of yourself).

What we need is a set of criteria, applicable to all entities, by which we can say "This is me" or "This isn't me". Importantly, it needs to fit all past and possible future versions of yourself for which you would say "This is me". Ideally, these criteria would be based on our behaviour rather than our state (otherwise we're presuming a certain structure in the answer).

Even if we only count the past year of our lives as "me", there's an awful lot of variation our criteria have to cover.

So the question then becomes this: is there any set of criteria that would include all "me" and exclude all "not me"? If there is, what would satisfy those criteria?

And once we've come up with our test, why is it that we don't apply it to ourselves? We seem perfectly satisfied with an illusion of continuous memory, despite all the gaps and inaccuracies, and any upload worth its silicon would have the same illusion.

#+begin_quote
  First, assuming that the upload is concious, is the conciousness in the image, or in the model manipulating that image?
#+end_quote

Both.

Is consciousness in the structure of your brain, or in the physical laws that allow it to function? A human brain in a universe with different physical laws is not a conscious entity, because it doesn't function properly (or at all, perhaps).

It's the function that is important, not the components.

#+begin_quote
  Second, not making the assumption of the first question, is the model even capable of conciousness. [...] We know that no extant computer architecture acts in the same way - even in general - as concious agents do. Thus, while they might be capable of experience of the zeroth order (raw sensation), it is very, very dubious that they're capable of first order sensation (sensing sensations), and nearly unthinkable that they're capable of second order sensation (first order in a strange loop).
#+end_quote

How would you differentiate between a human that experiences second order sensation and a cyborg that mimics it? What is your test?

It seems like, for a lot of people in this thread, the test is "Is their processing unit a direct analogue of a human brain?". If that's the case, we're not really testing for consciousness, we're testing for humanity and calling it consciousness.

#+begin_quote
  But if the conciousness of beings with neurons arises from a fundamental property of the universe, then is that interaction still present in silico? It's certainly present in the CPU itself, but does it extend?
#+end_quote

My position is that consciousness is functional, not physical. The pattern matters more than the pieces. If consciousness is simulated it's still consciousness, and if consciousness cannot be simulated then the functionality must rely on something that cannot be simulated.

So, if consciousness cannot be simulated, what does it rely on that we can't simulate? Is there some fundamental function of the universe that is not possible to simulate? Why haven't we run into it yet, and how do we know it's vital for consciousness?

Perhaps more importantly, what would something that cannot be simulated even look like? We wouldn't even be able to describe it. If consciousness can't be simulated, it basically means the answer is "the soul" (or a very large number of very minor souls in each individual neuron).

#+begin_quote
  But it that's the case, then, rationally speaking, if I gave you an elixir that obliterated your self-awareness for all time and left your body acting the same way as it did before that juncture, then you should be as happy to drink it as not. Because, after all, under your view, there's nothing to fear, is there?
#+end_quote

Indeed, because if I act exactly the same despite having no self-awareness, self-awareness would clearly not be an important part of my behaviour.

However, my position is that such an elixir is impossible, even if you assume the existence of infinitely powerful magic. It would be like making a square circle in Euclidean space.

In simulating self-awareness, self-awareness exists. If it does not simulate self-awareness, it cannot imitate me. If the elixir results in an imitation of me, it must be self-aware, even if that self-awareness is not what I'm used to.