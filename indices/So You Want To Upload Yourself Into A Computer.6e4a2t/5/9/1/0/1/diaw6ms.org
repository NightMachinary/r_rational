:PROPERTIES:
:Score: 1
:DateUnix: 1496286820.0
:DateShort: 2017-Jun-01
:END:

Well if it's not a physical phenomenon, then what the heck is it? I mean, every time nonphysical theories have been proposed, they've gotten curbstomped by science, eventually, so Bayes already hates that first sentence of yours with an unholy passion.

I'm uncertain if artificial neurons are capable of supporting conciousness. Either answer seems equally reasonable under my premise that conciousness arises from some fundamental property of physical matter. However, replacing brain tissue with synthetic neurons is /not/ uploading, and I really shouldn't ever have entertained that it was.

Here's the thing. Let's look at the brain as an insanely complex self-modifying analogue circuit. We replace an element of that circuit with a digital one. Repeating this process, we eventually have an insanely complex self-modifying digital circuit. Which is still, in fact, a circuit.

Uploading, on the other hand, takes an image of the structure of this circuit, and then moves it to some substrate which runs that image through a model continuously, generating new images that correspond with greater-or-lesser accuracy to the images that one would have expected the original structure to produce given the same stimuli. In this case, you get a lot of insanely complex 3D images which all represent insanely complex circuits. But, a map is not the territory; to use an old cliche somewhat inappropriately.

So there are several things to consider here.

First, assuming that the upload is concious, is the conciousness in the image, or in the model manipulating that image?

Second, not making the assumption of the first question, is the model even capable of conciousness. We know, at least for ourselves (maybe) that we're concious. By observing the structural details of ourselves, we know that such details generate conciousness, unless we have a nonphysical hypothesis, at which point we can't even have this discussion because out starting assumptions diverge too much for it to be productive. We know that no extant computer architecture acts in the same way - even in general - as concious agents do. Thus, while they might be capable of experience of the zeroth order (raw sensation), it is very, very dubious that they're capable of first order sensation (sensing sensations), and nearly unthinkable that they're capable of second order sensation (first order in a strange loop).

However, we know that computers are capable of running any computable program given sufficient time and space. Thus, they can run a person supposing that a person can be expressed as a computable process. *This is where the problem is.*

We can write really damn awesome programs that model the behaviour of neurons. But if the conciousness of beings with neurons arises from a fundamental property of the universe, then is that interaction still present in silico? It's certainly present in the CPU itself, but does it extend? Don't know. But if it doesn't, any destructive uploading process is equivalent to suicide... which you don't care about given

#+begin_quote
  Either consciousness is a necessary precondition for important things like thought, emotion, memory, etc. in which case the presence of any of those things implies consciousness, or it isn't a necessary precondition and we have no particular reason to care.
#+end_quote

I disagree with you. We can view ourselves as observed processes. We can view ourselves as the observer of internal processes. If we view ourselves as observed processes, then you're right, as long as the observed process seems to behave the same way, no problems. But it that's the case, then, rationally speaking, if I gave you an elixir that obliterated your self-awareness for all time and left your body acting the same way as it did before that juncture, then you should be as happy to drink it as not. Because, after all, under your view, there's nothing to fear, is there?

...actually, you should also feel similarly about killing yourself. Seeing as destroying your conciousness forever is experientially identical to death, the only thing that's different is that some people will be hurt a bit. but as long as it's incentivised such that the moral weight is in favour of suicide, it's allllllll good.

If you're a concious being, you can't treat yourself like you treat other people. You can't say that only the outside counts. Not unless you're willing to own the nihilistic implications. I wasn't.