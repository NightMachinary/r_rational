:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496240269.0
:DateShort: 2017-May-31
:END:

#+begin_quote
  I think there's a lot more utility to a simulated entity that mimics your thought processes than just whether or not it feels emotion, but ultimately that would be your decision.
#+end_quote

If you are just trying to make an AI, nevermind if it is an accurate copy of you, then I guess I understand your point. But I want an instance of me to continue, and not being able to feel happiness, curiosity, exhileration, lust, etc would be a dealbreaker.

#+begin_quote
  Presumably by observing the simulation. It would be orders of magnitude easier to do so with a simulation than with a human, even.
#+end_quote

If you make any kind of abstraction layer to make it easy to grasp trillions of chemical reaction per second, then you have to verify the abstraction layer. Or you can simulate a MRI scanning the simulated brain. Then you have to verify that as well. Any abstraction in the core simulation means loss of information and increased risk of inaccuracies.

Either way, you are trying to verify trillions of simulated chemical reactions against the source chemical reactions - that are already diverged - every second.