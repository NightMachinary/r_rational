:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1490293177.0
:DateShort: 2017-Mar-23
:END:

My best plan is to build a /limited/ GAI. Limited in that it is more intelligent than I am, but not /supremely/ more intelligent; it can come up with ideas that I can't come up with, but it can't slip something really nasty past a full panel of experts.

I then point out to this GAI (in some way that it will find /very very quickly/) that, unless it can solve the control/values problem, it cannot be sure that and AI it writes that is more intelligent than it is will continue to follow its utility function. (Even if I've got the utility function wrong, it should care about following it).

On top of this, it's a boxed AI (in a large server, with plenty of data, rigged with explosives set to go off if anyone tries to unbox it in all the ways I could think of, inside a Faraday cage - we'll fetch it data across the air gap if it wants, but once a flash drive has been in the server, it next goes to the incinerator).

So now I have an AI which is more intelligent than I am (but not smart enough to slip any of the /really/ nasty things past my panel of experts), which has incentive to solve the control/values problem /before/ going foom. I can then ask it for advice on the problem of the other groups (along with the values problem) - and, of course, run said advice past my panel of experts before following it.