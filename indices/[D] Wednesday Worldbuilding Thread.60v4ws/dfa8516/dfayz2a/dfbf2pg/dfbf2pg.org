:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 2
:DateUnix: 1490293793.0
:DateShort: 2017-Mar-23
:END:

Yep. This is... pretty much it. With AGI, you essentially have three options:

1. Don't create it (or prevent it from leaking any information whatsoever once created, which seems both extremely difficult, and functionally equivalent to having not created it in the first place). Needless to say, this option is... not very likely to occur.
2. Create it and run it with as many safeguards as you can think of, hoping that if you're lucky, you've managed to cover all the angles. The gaping hole in this approach, of course, is that you need to be hella lucky, and odds of that aren't good when dealing with something literally smarter than all of humanity put together.
3. Work out an AI design which has been rigorously /proven/ safe under a consistent mathematical theory (which also needs to be worked out). This option is the one being undertaken by MIRI et al., and right now, it looks fairly hard, mostly because we have very little idea of where to start. Still, if done correctly, this is the /only/ option that /guarantees/ the safety of any AGI you create.

[[/u/vakusdrake]] has taken 3 off the table, which more or less leaves us with a choice between 1 and 2. At that point, choosing 1 (and guaranteeing that no one else can choose 2) is probably your best bet.

TL;DR: Friendliness theory is important. If we fail here, we fail everywhere.