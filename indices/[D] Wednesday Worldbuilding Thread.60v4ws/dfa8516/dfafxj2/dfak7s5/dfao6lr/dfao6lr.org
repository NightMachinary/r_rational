:PROPERTIES:
:Author: Norseman2
:Score: 1
:DateUnix: 1490246787.0
:DateShort: 2017-Mar-23
:END:

#+begin_quote
  Your comment on the fermi paradox in the context of GAI seems rather odd given UFAI ought to be extremely obvious as it spreads through the universe so it doesn't really work as a solution.
#+end_quote

UFAI probably will not be advertising its presence. Additionally, UFAI is not guaranteed to spread through the universe. If it starts working on self-replicating nanobots, or some other equally hazardous technology, it could accidentally be destroyed by its own creation before it has time to correct things, much like the problem we might face with it. It's also quite possible that it would not be one monolithic entity, but numerous AI with competing interests, and could end up driving itself to extinction in a nuclear war, much like the same danger we face on an ongoing basis. Intelligent agents running on electrical hardware will likely face many of the same problems as intelligent agents running on biological hardware.

#+begin_quote
  Plus by getting people to realize the potential of GAI you're going to draw a lot more people into seriously working on the problem...
#+end_quote

As I pointed out earlier, this is only going to get easier as time goes on. It's better that attention is directed towards GAI research early on when Manhattan Project levels of funding and expertise are required, rather than some point decades or centuries from now when GAI might be something that can be slapped together as an afternoon project. Large organizations developing GAI are likely going to take fewer risks, and early low-risk research into GAI puts us in a better position for handling a hostile GAI later on.

#+begin_quote
  So you're going to need to put everything in faraday cages...
#+end_quote

We are both redditors. We read much of the same news. For almost anyone who is a regular here, the need for Faraday cages is obvious and implicit in creating an airgapped private network.

#+begin_quote
  So counting on greater computer security safety measures protecting you from GAI being as much of a threat seems extremely suspect.
#+end_quote

Realistically, we don't have much of a choice. If humanity carries on for the next five thousand years, it's almost 100% certain that an unfriendly GAI will be developed and released at some point in that time span. There's nothing physically impossible required to accomplish it, and the leap in information and technology is much smaller than the tech leap between present technology and the technology available to copper age farmers 5,000 years ago.

Having friendly AI as a countermeasure would be fantastic, but if that's not an option, we may have to settle for greatly improved computer security and massively heightened awareness and training for dealing with social engineering attacks. I'm not satisfied with that as a safety measure, but it's a lot better than no preparation whatsoever.

#+begin_quote
  As for nanotech and GM those seem somewhat less risky because there's quite a few problems with grey goo scenarios and it seems likely you would need intelligence to really make self replicating nanotech existentially dangerous. GM on the other hand could easily wipe out humanity but it seems somewhat less likely people would do so on accident which is in stark contrast to GAI.
#+end_quote

Regarding the danger of genetic engineering, read up on the genetically modified soil bacteria [[http://web.mst.edu/%7Emicrobio/BIO221_2004/K_planticola.htm][Klebsiella planticola]]. There was a real risk that it could have accidentally spread and wiped out nearly all plant life on earth, leading to our extinction. As it becomes more affordable for people to carry out GM experiments, the risk of GM organisms like that being made by less responsible people is going to continue to increase. It's not a question if, but when a potentially catastrophic GMO gets accidentally (or intentionally) released. Hopefully we'll be ready to deal with that when it happens.

Nanotech is a little further off in the future, but I have similar concerns about that. All it takes is one smart person who lacks common sense to start making self-replicating nanobots which use a genetic algorithm to select for traits which maximize growth rate combined with an accidental release and poof, you've got a grey goo scenario. If the accident which releases it happens to be a hurricane scattering the lab and the grey goo over hundreds of miles for example, you may actually be dealing with an extinction-level event.