:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1490285049.0
:DateShort: 2017-Mar-23
:END:

Ok you can set limits on lifespan but that doesn't stop it interfering with humanity in ways you don't like and it may not need very long to set its plans in motion. You can try to set limits on growth but that doesn't change the fact wiping out humanity may not be that hard with the right nanotech, virus, etc plus you require a lot of complexity to prevent it from creating subagents or other AI to expand in its place.\\
Now kill count is more plausible, but still leaves massive loopholes. For one eliminating all indirect methods of killing is hard, if you go to far then due to butterfly effects it can't take any actions because it will cause all people far enough in the future to have lives (and thus deaths) different than they would otherwise. Plus even if you somehow solve that it could very well introduce a pathogen that permanently makes all future humans have some degree of mental retardation and otherwise be mentally unfit for making any sort of advancement. Or while it's at it just stick all the humans in self sufficient life sustaining vats like wireheading without the wireheading.\\
Sure you could come up with countermeasures to those loopholes, but I could just come up with more loopholes, and even if I couldn't think of any more that says very little about whether the GAI could think of more.