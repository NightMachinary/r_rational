:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1490237211.0
:DateShort: 2017-Mar-23
:END:

Even if the loss of lives is supposed to be bound I think you may find similar issues to the example of telling a GAI to just calculate a million digits of pi, it still has considerable incentives to ensure it got it right by turning as much matter as possible into computronium.

Still assuming you solve that assuming you can successfully scare all the myriad of teams that are supposed to be extremely close to completion into stopping seems suspect. Some may very well think you did this intentionally and think you are trying to stop anyone else from gaining ultimate power or some other bad but vaguely plausible logic. Plus demonstrating for the entire world that getting GAI first means unlimited power seems like it will draw many more people into the problem, many of whom will convince themselves that /they've/ solved value alignment just because they came up with a utility function that they couldn't think of any flaws in.