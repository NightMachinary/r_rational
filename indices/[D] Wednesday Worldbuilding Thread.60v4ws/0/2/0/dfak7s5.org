:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1490239910.0
:DateShort: 2017-Mar-23
:END:

Your comment on the fermi paradox in the context of GAI seems rather odd given UFAI ought to be extremely obvious as it spreads through the universe so it doesn't really work as a solution.

I'm rather doubtful that making the threat of GAI clear would actually stop all the parties involved from working on it and you only need one. Some parties will suspect this is some sort of ploy from you and wrongly surmise you're just trying to ensure you get GAI first for yourself but fail to actually begin to properly assess risk. Plus by getting people to realize the potential of GAI you're going to draw a lot more people into seriously working on the problem, which given how many teams were independently close to achieving it (and how seemingly far away control problem solutions are) makes it far likelier someone irresponsible will develop it first.

I originally imagined this scenario to be an exercise in coming up with extremely suboptimal GAI that would nonetheless be better than wireheading/extinction, but I suppose instead it's becoming a parable on insufficient pessimism about existential risk.\\
Air gapped computers are not remotely as safe as you think even to existing technology. Programs have demonstrated an ability to produce EM transmissions merely by modulating electron flow in hardware in certain ways. So you're going to need to put everything in faraday cages, now the fact you don't seem to initially realized the risk should tip you off that there will likely always be things a GAI can exploit that you haven't thought of.

As for its actions within the network, for one you need to have people look through it's code to figure out the vulnerability which introduces AI Box style problems. Second is that you can be pretty confident that it knows it's boxed, so it may be very likely to fail to cooperate since doing so doesn't increase the likelihood of escape. It may also be very good at covering its tracks and acting stealthily so you don't necessarily know which computers in the network are breached and the more time it can get you to waste trying to look through code to fix problems the more opportunities it has to use its superhuman persuasion on somebody.\\
Plus just because you could use a exploit discovered from a GAI doesn't mean you have them all and even if you got all the one's from your GAI you don't know a smarter GAI couldn't find more yours couldn't even conceive of. So counting on greater computer security safety measures protecting you from GAI being as much of a threat seems extremely suspect.

As for nanotech and GM those seem somewhat less risky because there's quite a few problems with grey goo scenarios and it seems likely you would need intelligence to really make self replicating nanotech existentially dangerous. GM on the other hand could easily wipe out humanity but it seems somewhat less likely people would do so on accident which is in stark contrast to GAI.