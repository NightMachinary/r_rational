:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1490282877.0
:DateShort: 2017-Mar-23
:END:

Ok regarding UFAI and the fermi paradox: A SAI is at substantially less risk from existential threats than humans because it only needs to survive in some protected area with some self replicating machines. You may have GAI at war with each other, but mutually assured destruction isn't really the same level of threat for them at least initially, and one AI is very likely to have a massive advantage due to a slight head start.\\
When you only need the AI to have some nanobot reserve somewhere then mutual destruction doesn't really work, it's like if civilizations could restore themselves from a single person hidden in a bunker somewhere (and were only concerned with wiping the other out) MAD just wouldn't work. Instead both parties would adapt to be extremely resilient and able to bounce back from having most of their resources destroyed and when one got a decisive advantage it would just overpower any remaining enclaves.\\
As for UFAI wiping themselves out with nanotech that seems implausible given their superintelligence. They ought to be able to predict how the nanobots they made work, spread self replicate and the like. Something much smarter than all of humanity combined shouldn't be making stupid mistakes with potential existential risk technologies.\\
As for UFAI advertising its presence, that sort of misses the point. In order to not make its existence obvious it would have to deliberately cripple its growth and refrain from astroengineering, otherwise its spread would be obvious from the stars disappearing or being enclosed and from the infrared signatures of megastructures.

#+begin_quote
  Having friendly AI as a countermeasure would be fantastic, but if that's not an option, we may have to settle for greatly improved computer security and massively heightened awareness and training for dealing with social engineering attacks. I'm not satisfied with that as a safety measure, but it's a lot better than no preparation whatsoever.
#+end_quote

I suppose I seriously doubt those sorts of measures will do much more than serve as security theater, hoping to patch all the vulnerabilities GAI could come up with in social engineering and computer security seems nearly guaranteed to fail if it actually comes against an actual GAI. Still either way those measures will probably be taken, even if only to grant the illusion of safety to the masses.

#+begin_quote
  Regarding the danger of genetic engineering, read up on the genetically modified soil bacteria Klebsiella planticola. There was a real risk that it could have accidentally spread and wiped out nearly all plant life on earth, leading to our extinction. As it becomes more affordable for people to carry out GM experiments, the risk of GM organisms like that being made by less responsible people is going to continue to increase. It's not a question if, but when a potentially catastrophic GMO gets accidentally (or intentionally) released. Hopefully we'll be ready to deal with that when it happens.
#+end_quote

The paper doesn't really support claims as strong as what you seem to be saying, the bacteria was already given very poor containment and still didn't escape, it doesn't seem to be some super bug that spreads across the world in weeks before people can react. Secondly it seems staggeringly unlikely it would be able to kill /all/ varieties of plant, since the only actual test was on wheat.

However even with a super plague or super version of that bacteria calling it extinction level is a stretch, the same thing goes with nuclear war actually, people make vast exaggerations of it's capabilities. A extremely virulent disease or famine may kill millions or billions but western countries will have the resources to give out gas masks gloves and other protection and do extensive quarantining. With famine governments could likely turn to industrial food production like soylent that can be done entirely in controlled environments. With nuclear war the southern hemisphere would still come out of things surprisingly well (comparatively) and many of the predictions of nuclear winter were rather exaggerated, plus we don't have the same kind of volume in nukes that we used to so that actually reduces the effects even further.

As for grey goo I think you're overestimating how easy that is and underestimating its limitations. Getting nanobots that can adapt massively to construct themselves from a wide variety of components (not just that but you likely need unique machinery for deconstructing every unique type of molecule, and many won't be worth it energy wise) isn't going to be simple and they will likely have great difficulty replicating and spreading outside specially made environments. Nanobots have a lot of the issues in terms of resource gathering and molecular machinery that actual microbes have and despite clear incentive no one microbe has found a way to create runaway grey goo. Plus the nanobots need energy which at their scale pretty much limits them to the same energy sources as actual microbes and thus places another damper on runaway expansion. Given people will likely want to use nanobots under controlled conditions anyway the staggering amounts of work needed to make general purpose nanobots seems unlikely to get done pre singularity. Trying to make nanobots with a hereditary system is likewise quite difficult and given the work required it will likely be easier to use nanobots with well designed functions where misreplications won't be beneficial.