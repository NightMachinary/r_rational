:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 3
:DateUnix: 1490294139.0
:DateShort: 2017-Mar-23
:END:

#+begin_quote
  I then point out to this GAI (in some way that it will find very very quickly) that, unless it can solve the control/values problem, it cannot be sure that and AI it writes that is more intelligent than it is will continue to follow its utility function. (Even if I've got the utility function wrong, it should care about following it).
#+end_quote

Why? I mean, it's got the utility function coded into it, right? As long as it can inspect its source code, it doesn't seem hard to just find (its representation of) its utility function, and then it's pretty much set. An AGI isn't like a human, who has limited introspective ability.