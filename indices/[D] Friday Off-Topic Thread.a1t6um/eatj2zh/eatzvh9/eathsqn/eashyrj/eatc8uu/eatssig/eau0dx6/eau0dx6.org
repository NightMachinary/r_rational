:PROPERTIES:
:Author: RetardedWabbit
:Score: 1
:DateUnix: 1543636084.0
:DateShort: 2018-Dec-01
:END:

Default LD criteria of course: utilitarianism and intelligence as an ends not a means. Utilitarianism is pretty straightforward with human life/happiness vs extermination risk. Intelligence itself could be a good shot in the dark: we have an obligation to bring more/better intelligent beings into the world?