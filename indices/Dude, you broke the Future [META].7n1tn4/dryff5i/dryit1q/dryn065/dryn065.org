:PROPERTIES:
:Author: Veedrac
:Score: 20
:DateUnix: 1514658557.0
:DateShort: 2017-Dec-30
:END:

Only 10 minutes in but he's already made a bunch of unfounded claims, and clearly doesn't understand the good arguments for AI risk. It /feels/ like the rant of someone who has [[https://en.wikipedia.org/wiki/Filter_bubble][filter bubble]] that only shows him the most controversial claims, and he generalises from there.

His argument so far is basically

#+begin_quote
  If something walks like a duck and quacks like a duck, it's probably a duck. And if it looks like a religion, it's probably a religion.

  I don't see much evidence for human-like self-directed artificial intelligences coming along any time soon, and a fair bit of evidence that nobody except some freaks in cognitive science departments even want it. I mean, if we invented an AI that was like a human mind, it would do the AI equivalent of sitting on the sofa munching some popcorn and watching the Super Bowl all day. It wouldn't be much use to us.

  What we're getting instead is self-optimising tools that defy human comprehension, but are not any more like our kind of intelligence than a Boeing 737 is like a seagull. Boeing 737s and seagulls both fly. Boeing 737s don't lay eggs and shit everywhere. So I'm going to wash my hands of the singularity as a useful explanatory model of the future without further ado.
#+end_quote

I'm not sure this needs explicit debunking, but if anyone disagrees I'll be happy to do so.

E: More transcriptions from the 15m mark.

#+begin_quote
  Now, Elon Musk, who I believe you've all heard of, has an obsessive fear of one particular hazard of artificial intelligence, which he conceives of as being a piece of software which functions like a brain in a box, namely, the paperclip maximiser.

  A paperclip maximiser is a term of art for a goal-seeking AI that has a single priority, for example, maximising the number of paperclips in the universe. The paperclip maximiser is able to improve itself in pursuit of its goal, but has no ability to vary its goal, so will ultimately attempt to convert all the metallic elements in the solar system into paperclips, even if this is obviously detrimental to the well-being of the humans who set it this goal.

  Unfortunately I don't think Musk is paying enough attention. Consider his own company; Tesla isn't a paperclip maximiser, it's a battery maximiser. After all, an electric car is a battery with wheels and seats. SpaceX is an orbital payload maximiser, driving down the cost of space launches in order to encourage more sales for the service it provides.

  Solar City is a photovoltaic panel maximiser, and so on. All of three of Musk's very own slow AIs are based on a architecture designed to maximise return on shareholder investment, even if by doing so they cook the planet the shareholders have to live on, or turn the entire thing into solar panels. But hey, if you're Elon Musk that's OK: you're going to retire on Mars anyway.

  By the way, I'm ragging on Must in this talk simply because he's the current opinionated tech billionaire who thinks that disrupting a couple of industries entitles him to make headlines. If this was 2007 and my focus was slightly different, I'd be ragging on Steve Jobs, and if my target was 1997, my target would be Bill Gates. Don't take it personally Elon.
#+end_quote