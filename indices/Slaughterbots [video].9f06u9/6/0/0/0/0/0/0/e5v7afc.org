:PROPERTIES:
:Author: Veedrac
:Score: 4
:DateUnix: 1536785090.0
:DateShort: 2018-Sep-13
:END:

#+begin_quote
  There's a bunch of intermediate scenarios where you'd just deal with more realistic human-ish level AIs that have their own motives and agendas but don't necessarily outclass us.
#+end_quote

I don't think the physics of the situation allow that to have more than trivial probability. I actually expect the cross-over point into an intelligence explosion to be significantly below "par-human" intelligence, and I suspect we'll see the first approaches terminate before hitting true superintelligence, instead hitting some earlier point in the hypothesis space that still poses existential risk but isn't smart in the same self-reflectively consistent way that would allow for, say, self-oversight and especially extremes like coherent extrapolated volition.

This follows from a few basic claims and observations that are hard to explain simultaneously accurately and concisely, but here's the gist:

1. Silicon is 1,000,000,000x faster than brainstuff, which is likely a conservative measure.

   - Rough numbers: neurons/synapses are 200 Hz, transistors are 200 GHz.
   - In a single neuron firing latency, a silicon mind can send a signal 5 light milliseconds away, or 1500 km. Everything within this area is capable of acting as a single agent at human-ish latencies, so that's a good anchor for thinking about how they scale.

2. Building intelligence is not /fundamentally/ hard, which is shown in a bunch of ways; here are some.

   - Evolution managed it, despite its limitations. We'd thus expect even iterative brute-force to make progress.
   - It's a large target; we're seeing intelligent behaviours (translation, audio synthesis, image synthesis) from what amounts to iterated matrix multiplication.
   - AI is far dumber than you probably think it is; there is a remarkable ability for brute speed to compensate for the most glaring flaws in reasoning. (I normally phrase this along the lines of "AI research is not about building smart machines, but about showing that problems you thought were AI-hard are actually trivial. The process terminates when we show that the Turing test, too, is trivial.")
   - Computing is stupidly young. We've had fast computers for maybe 30 years and we've already made significant progress on AI.

3. AI is most inherently applicable to certain kinds of black-box optimization tasks; building smarter AI (more general, more competent, faster learning) is much closer to practical reality than directly applying AI to real-world problems.

   - Note, of course, that this process is iterative: if the next generation is more general, it can implement a more general set of improvements to generality. The size of this transitive closure is hard to estimate.

4. Humans occupy a /tiny/ area on the space of possible minds, and we're there largely for happenstance reasons:

   - This is the earliest point in evolutionary history that we could possibly reach civilization; there are a huge number of reasons to think we aren't near the peak of this evolutionary pathway.
   - Neurons kind'a suck, and biology makes scaling up really hard. Hard limits basically don't exist for computer-based minds.
   - Our ancestors spent a huge amount of time improving aspects of generality and efficiency before "scaling up", back when brains were smaller and scaling them cost more than it was worth. AI will be far too fresh to be "well-distributed" in this sense, so we'd expect the first real AI to be propping itself up on a small amount of hyperintelligence, not a balanced diet of moderate intelligence.

5. Societies are not robust to even niche hyperintelligence. This is kind of hard to show, but we can point to structural weaknesses (1) and systematic incompetence (2), and we can look at the impacts that computers have had (3) to get a general idea of why I'd believe this. Unfortunately I'm not sure how to argue this point concisely, and the non-concise argument is probably more than I have motivation for.

   1. [[https://www.gwern.net/Terrorism-is-not-Effective]]
   2. There are US states that use electronic voting machines that have been shown trivially vulnerable to hacking.
   3. Computers took a couple of decades to take over pretty much every aspect of science, industry, home lives, etc. These are computers programmed by normal people, almost all of which suffers from institutional incompetence.