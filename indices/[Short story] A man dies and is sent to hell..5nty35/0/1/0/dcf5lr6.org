:PROPERTIES:
:Author: owenshen24
:Score: 2
:DateUnix: 1484411969.0
:DateShort: 2017-Jan-14
:END:

Okay, you're right; trusworthiness is a hard question.

(I will attempt to try and answer this once again with the caveat that I usually only sorta know what I'm talking about. If someone who knows more can jump in and correct me on points where I'm wrong, that would be helpful.)

I think even if the AI could be dishonest, if you know that it's sufficiently powerful to run such simulations, maybe you should take the threat seriously.

But like i mentioned above, people who precommit to shutting off the AI or not giving in don't even face such situations because their simulations don't give in, meaning that the blackmail doesn't happen. (Counterfactuals are weird.)

So I think most of the discussion goes into trying to figure out how to negotiate in such situations, even if you don't anticipate experiencing them, because your anticipated response determines whether or not you even face them.

If all you know about the AI is that it's a text terminal, then I agree with you that you have less reason to believe that it's honest. Given that you know this and still find yourself in real life facing such a threat, you can maybe use some sort of principle from [[http://lesswrong.com/lw/hd3/pascals_muggle_short_version/][Pascal's Muggle]]. So you can penalize complex situations (EX: "Let me out now, or I'll torture lots of simulations!").

But I think this situation you're talking about eschews the last bit about subjective memory. Or it's not important because we've established that the blackmail is actually happening.

Anyway, Pascal's Muggle is basically about sorta clever ways to update on evidence even given very small priors. Like, maybe you think the AI simulating lots and lots of conscious minds is impossible, but then it simulates you via text terminal and you realize its simulation of you is spot-on. This updating involves some thinking along the lines of Bayesian updating + rethinking your priors.

I realize I didn't directly address your question. But your points about belief / evidence are sorta addressed in the LW post. Hope this helps!