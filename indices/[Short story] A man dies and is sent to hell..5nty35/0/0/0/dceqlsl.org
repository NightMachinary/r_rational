:PROPERTIES:
:Author: owenshen24
:Score: 29
:DateUnix: 1484374821.0
:DateShort: 2017-Jan-14
:END:

Yes, you've got the answer; I think it's the standardly accepted way to resolve that conundrum.

(Erm, I don't claim to know super well the LW-ish decision theory mechanics behind this, but here goes my attempt at trying to justify why pulling the plug is good.)

I think you have to be the certain amount of smart (not too smart and not too dumb) for these sorts of blackmail attempts to work. If you aren't smart enough to understand the threat, then understandably they won't happen because simulated versions of you won't respond to the threat, which sort of invalidates any attempt to start the thing.

If you're sorta smart, then knowing that there are countless other versions of you pondering the same thing suddenly makes it seem very likely you're also a simulation. If you then think that this is sufficiently the case, such that any other thoughts are self-defeating, the AI, devil, blackmailer, etc. wins.

If you now precommit, though, to always switching off the AI or punishing the blackmailer when this sort of situation happens, then you won't be simulated.

But it's too late! You're already being simulated and you haven't had a chance to precommit. If you're smart enough, though, you can logically reason that if you /now/ act as if you've precommitted, then the simulation /also/ won't happen because you'll be smart enough to reason this out in the moment.

So if you're smart enough to implement this sort of logical decision theory, then you also won't find yourself in such situations. So something like "sufficiently intelligent people should not find themselves in such blackmail-like scenarios" is /maybe/ the takeaway from this?