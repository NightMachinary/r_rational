:PROPERTIES:
:Author: gurenkagurenda
:Score: 6
:DateUnix: 1484388031.0
:DateShort: 2017-Jan-14
:END:

#+begin_quote
  Also consider the situation where you know that the AI, from design principles, is trustworthy.
#+end_quote

Then couldn't I just ask the AI if I were a simulated copy? Or is the AI allowed to withhold information? How do we define trustworthy?

More to the point, this particular kind of loophole-closing stipulation in thought experiments always nettles me, because it seems to encode a tremendous amount of extra information in a very vague way. Once you unpack it, it seems like it might be hiding some pretty important details.

Clearly, people are able to have interesting discussions around these thought experiments despite the handwaving, but I find it difficult to focus on finding solutions when I feel like /most/ of the interesting details may be hiding behind the elephant-shaped blur in the room.

Instead, I find myself trying to unpack the stipulation. How did I come to believe that the AI is honest? I'm probably not taking that on the authority of the designers, unless I think the designers /intended/ to build an AI that would torture a million copies of me. Perhaps I have it on the authority of some respected third party who deeply understands the inner workings of the AI. I sure wish they'd have put /that/ person in charge of interacting with the AI without releasing it. Seems like that would have been smart.

I'm also probably not going to come to this belief based merely on the fact that a number of carefully proven theorems were employed in its design. I would take the validity of those theorems on authority, but I would likely have significant doubts about the implementation. Especially in light of the fairly preposterous threat it had just made. That is, it seems to me like P(threat | honest AI) is much smaller than P(threat | dishonest AI), because bluffing is a /lot/ cheaper than carrying out massive computations, and so long as the AI believes itself credible, the payoff is the same.

So do I believe the AI is honest because I deeply understand its code? That must be some pretty deep understanding, right? The kind of understanding that might /completely/ change my strategy for handling its threats?

So just to be absolutely clear, I'm not rejecting the thought experiment outright. There are conceivable reasons that I would believe that the AI was honest. I'm just saying that if I did, that would probably mean that the version of me in the experiment has a /lot/ more information to work with than the real me sitting here now. And it seems like it would be the specific kind of information that would be useful under the circumstances.

Or maybe I just find thought experiments generally annoying at a System I level, and all of that is rationalization.