:PROPERTIES:
:Author: owenshen24
:Score: 3
:DateUnix: 1484503437.0
:DateShort: 2017-Jan-15
:END:

I think the problem here is that the real Dave might also similarly reason this. Because we're assuming pretty much identical simulations, your actions are probably replicated across the board. This means the real Dave also won't let the AI out, meaning you get tortured.

(If you want to try and be sorta clever, you can try some sort of [[http://lesswrong.com/lw/bxi/hofstadters_superrationality/][superrational reasoning]] so you pick an even of probability 1/(number of simulations of you) and let the AI out of the box iff you get the lowest number. Obviously you can't communicate with any of your simulations, so this won't ever have a higher chance of working than something like just talking it out. (I think the math works out to be ((n-1)/n) ^ n but that doesn't seem right))