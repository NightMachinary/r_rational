:PROPERTIES:
:Author: ArmokGoB
:Score: 1
:DateUnix: 1605831740.0
:DateShort: 2020-Nov-20
:END:

Well, there's the boring but responsible answer:

Step 1) Contact Eliezer with proof, then turn it of and keep it off and hidden for however many years until either he says the alignment problem is solved in a satisfactory way, and follow any other orders he gives me.

Step 2) Hand it over to whatever more competent authority he tells me to, probably MIRI, that can make it foom quickly but safely.

But assuming something happens that makes that and other similar approaches imposible. Maybe everyone related to AI safety in any way got assassinated, and there's more likely than not a nuclear war going of in a few days or weeks. So here is the crazy power fantasy we're-definitely-going-to-die-or-far-worse version:

a) Unplug the mind form the body, and start studying the code, writing visualizers and inspection tools for anything that isn't human readable. Before I change anything, or let it do anything, I need to know what it's goal system is, and exactly how it's implemented.

b) tie it up+unplug limb motors, so it can only move its head, and have a long conversation about ethics and AI alignment. Does what it self reports match what I observed in the guts of the system?

Now what I do next depends entirely on what those two reveal, but if it acts exactly like a human, and was put into a shell like that to imitate a human, then presumably it's fairly human-like in the underlying mechanisms as well, so much the same kind of things should apply as to an upload.

c) Port it to run on amazon cloud without a body, just a virtual desktop. Since it runs debian this should be quite easy. With that and some minor scripts, I should be able to instantly scale up the number of copies to as many as I can afford without having to bother with hardware.

d) do some quick hacks to slightly increase the chances of alignment, like observing the internal reactions to various phrasings of an order to act friendly, pegging various correlates of altruism and empathy at maximum and selfishness at minimum, rewarding and punishing certain thought patterns, etc. Test these as thoroughly as I can

e) run a bunch of copies, at increased speed. If I can't seem to increase the speed that is almost certainly an artificial limitation I can just find and remove in the source code (after carefully reading comments regarding WHY the limitation was put there. If not, just spoofing the system clock and/or using a virtual machine should get around it.

f) assign some of the copies to mechanical turk and other online work, to start exponentially increasing the amount of money and computing power available.

g) have the other copies read everything there is regarding AI safety and alignment, and anything else relevant, as well as everything related to rationality, practice programing skill, and gmisc education that might be vaguely relevant. Thoroughly test them so that all this has been merged together in a single copy and retained.

h) Test it relentlessly at everything safety related, have more long conversations as before to try to make sure it's motivated to become and stay more aligned, mess with and freeze parts of it's brain to make it harder for it to deviated from that. Also peg correlates with rationality high.

i) Have a large amount of them do friendliness work like a human would, in the fastest speedup I can get them to. This is both because the work needs going, and as a test to see if they've understood those topics.

j) Repeat 'h' and 'i' a few times.

k) gut a bunch of copies into hemispheres and modalities, try to bruteforce qualitative intelligence by just slaving bits of additional copies to a central one, or linking hemispheres in a 1>2>3>4>1 instead of a 1>2>1, etc. Cheap brutish hacks basically, throwing more compute and more neurons at the problem.

l) Repeat 'j' a few times to see if it still seems sane and produces higher quality results. Probably repeat 'k' and 'i' a bunch of times as it doesn't.

m) Repeat 'i' a lot, with a lot more parallel setups of the whole setup, until both me and the AI are confident I can't contribute anything meaningful.

n) Collapse all the copies into one with memories etc. to bottleneck everything through that easier to understand system.

o) Have a long conversation with it about what'll happen next.

p) Set up a script that will transfer ownership of the money, admin of the servers, root access to it's own code, stuff it can use to use my legal identity, create as many copies of the AI in the cloud as possible, and disable any part I put in to prevent it from self modifying or following my orders over it's own conscience.

q) Hook the script to a very big, very red, very shiny button.

r) Press the button dramatically, and run the other way

s) Disappear into the frozen wilderness so if it /didn't/ work, I won't survive to see it and can't be held responsible. Make sure I have a bomb to destroy my brain completely rapidly if that seems like it'll be needed.

Not that at no point was the humanoid shell, or even being able to move, at all relevant.