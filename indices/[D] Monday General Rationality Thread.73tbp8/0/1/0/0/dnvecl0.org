:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1507074735.0
:DateShort: 2017-Oct-04
:END:

#+begin_quote
  Anyone who believes in the possibility of superintelligence by definition believes in the supernatural.
#+end_quote

You should be careful not to conflate "a consistent naturalistic worldview must allow superintelligence" with "worldviews that don't include superintelligence as a possibility must be supernaturally based". /You're forgetting that most people do not have internally consistent worldviews/.\\
Of course for these purposes it doesn't even matter if superintelligence is impossible, since people might just believe that for some reason it isn't likely to dominate civs even over cosmic timescales. Obviously that belief wouldn't make any sense but if you go around expecting that everyone believes things that make sense, then oh boy are you going to find the world a very confusing place.

As for the anthropic argument for extremely difficult goal alignment:\\
Basically it's an extension of anthropic ideas that you ought to expect yourself to be an observer who isn't a bizzare outlier. Thus if nearly every civ quickly leads to a very small number of minds dominating their future light cones until heat death, then it would be extraordinarily if you ended up by chance /not/ to be a T0 primitive biological civ before they created UFAI. The reasoning is similar to why a multiverse makes finding ourselves in a universe conducive to life utterly unremarkable.\\
Of course because anthropic reasoning is always an untamable nightmare beast none of this solves the issue with boltzmann brains. As always anthropic reasoning is one of those things that is clearly right in some circumstances but invariably leads to conclusions that don't make any sense or continually defy observations and it's not clear getting to those insane conclusions can be avoided since the logic doesn't have any clear ways to dispute it.