:PROPERTIES:
:Score: 1
:DateUnix: 1507225994.0
:DateShort: 2017-Oct-05
:END:

#+begin_quote
  I mean whether it's able to deduce knowledge of things that do not interact with our reality in any way is sort of irrelevant when considering it's capabilities, because unless it has certain particular human quirks (which even FAI have no reason to have) it won't care about those things.
#+end_quote

The queer thing is that almost everyone working on FAI thinks differently, which is why notions like acausal trade or the malignity of the university prior are taken perfectly seriously.

I'm not saying they're /automatically/ wrong, but it does seem perverse to me that the instant one commits to /making decisions/ in some AGI-complete or FAI-complete way (supposedly, according to certain thought experiments), one summons an immense amount of god's-eye-view metaphysics into philosophical relevance in a way that all real-life scenarios never have.