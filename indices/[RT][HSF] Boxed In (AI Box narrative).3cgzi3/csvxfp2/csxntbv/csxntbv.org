:PROPERTIES:
:Author: eaglejarl
:Score: 4
:DateUnix: 1436465184.0
:DateShort: 2015-Jul-09
:END:

The torture argument has never moved me. For one thing, it doesn't feel possible to my System I, so there's no emotional impact. My System I also doesn't believe that the AI can simulate me well enough that it counts as a person, much less as me. Finally, my System II says that letting the AI out to probably wipe out all life, human and ET, has sucked massive dis-utility that it doesn't matter how many virtual people she tortures. Also, since her processing power is limited, there's a limit to how many people she can torture and that number is less than "all the people who will ever exist."

My System II recognizes that some of what System I is telling me is false, but it doesn't probe too deeply at those signals -- this scenario is all about emotional impact, so not having an emotional response to it is supportive of the terminal goal of "don't let the AI kill everyone."