:PROPERTIES:
:Author: Stop_Sign
:Score: 1
:DateUnix: 1441943721.0
:DateShort: 2015-Sep-11
:END:

#+begin_quote

  - If she was created "more or less by accident" - no way in hell she shares human values or cares about human life, I'd say the probability of that is zero. Human morality is like 15% biological drives and 85% culture, AI has neither. Unless her values are explicity understood, programmed and controlled, there's absolutely no chance she will act in our interests.
#+end_quote

Actually, I thought of an answer to this one. The vastly intelligent being has a moral obligation to the lesser intelligence because they have no idea if, in the future, they'll meet an even more intelligent being. If they take a position of offense to the lesser being, they would invite hostility upon themselves from the even greater intelligence. If, however, they were truly friendly, they could pass the even greater intelligence's test, and be allowed to survive.

This could happen with "what if the ai is in a much larger simulation made by its actual creators" or "what of it comes into contact with an AI that started 1 million years ago and has spread across 90% of the galaxy already"

It goes just as well for "If we're genetically advanced, what do we owe the rest of the world" because the answer is "If we don't help them, our children's generation has no obligation to help us"