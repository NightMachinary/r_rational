:PROPERTIES:
:Author: Anderkent
:Score: 5
:DateUnix: 1436315537.0
:DateShort: 2015-Jul-08
:END:

#+begin_quote
  The AI party may not offer any real-world considerations to persuade the Gatekeeper party. For example, the AI party may not offer to pay the Gatekeeper party $100 after the test if the Gatekeeper frees the AI... nor get someone else to do it, et cetera. The AI may offer the Gatekeeper the moon and the stars on a diamond chain, but the human simulating the AI can't offer anything to the human simulating the Gatekeeper.
#+end_quote

Many suggested solutions for the original ai box experiment break this rule. (Some break it in a self-reinforcing way, i.e. convincing the human simulating the gatekeeper that it's a better result if everyone's convinced that the gatekeeper was played fairly)

Assuming people play the game honestly, it's not an option though.