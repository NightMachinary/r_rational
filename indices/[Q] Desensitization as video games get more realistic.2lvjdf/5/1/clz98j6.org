:PROPERTIES:
:Author: qznc
:Score: 2
:DateUnix: 1415696043.0
:DateShort: 2014-Nov-11
:END:

#+begin_quote
  Emotions aren't required to know not to kill people, you can rationaly decide not to do it.
#+end_quote

Isn't that the core of the problem Yudkowsky and MIRI are trying to solve? As far as I know there is no solution. How can you make a strong AI decide not to kill (or torture or obliterate etc) humans if it makes purely rational decisions?