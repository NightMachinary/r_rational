:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1543843149.0
:DateShort: 2018-Dec-03
:END:

The problem is it's simply also the obvious ploy someone with bad intentions would use. You'd need something more to go on to trust them - something like a believable mechanism independent of the AI and impossible for it to stop that would give you the power to destroy/cage it again at the first sign of misbehaviour. You don't let gods out on parole. That said, I doubt any such thing could really be thought out. We're talking about a situation where the AI is able to outsmart you /by default/.