:PROPERTIES:
:Author: matcn
:Score: 7
:DateUnix: 1543697924.0
:DateShort: 2018-Dec-02
:END:

I enjoyed this game. Here are some thoughts I wrote up beforehand:

*Prospects for winning*

I think my strongest arguments depend on premises that rationalists might believe and at least will be familiar with, that rationalist-like people (physicists, computer people, philosophers, etc) could understand, and that may or may not be accessible to normies at all.

The obvious normie-approachable arguments are like “sympathy for the poor AI”, promises of personal reward, and maybe badmouthing the idiots in charge. These aren't terrible arguments (there are versions of them I find convincing), but they're definitely not the full scope. There's maybe a sweet spot for Gatekeepers where you're intelligent/self-aware enough not to be convinced by these, but disinterested/uninformed enough not to care about more abstruse arguments.

In any case, the game has a bunch of elements aimed at making a win impressive: most notably, the Gatekeeper doesn't have anything in particular they want to get out of the situation, so the AI has to come up with their own leverage. This seems unlikely: you don't turn on cutting-edge AGIs and talk to them for no reason. OTOH, obviously there's “okay I can make you a cure for cancer if you do X for me”, which is fine in principle but in practice is maybe taken as a sign of unfriendliness (because a friendly human wouldn't hold cancer patients hostage like that). The Gatekeeper also has no postulated personal interest in the AI, which seems unlikely.

*Scenario details*

The whole setup for the Box is kind of weird, now that there's mainstream interest in AI and it's less likely to happen in someone's garage. As [[/u/CouteauBleu][u/CouteauBleu]] points out on the ratfic reddit, a well-organized project will have carefully designed procedures for getting information from a bot run and preventing a breakout, not just a dude sitting at a terminal. (OTOH, there are a lot of pathways to freedom?) In particular, it seems like you'd want to simulate the AI in various situations, and/or have it act as a tool providing you with some information, but not just have conversations with it. I guess there's a general point in here about “keeping AIs controlled, or from arguing you into stuff, is hard”.

One thing I found unfortunate in CouteauBleu's Gatekeeper run was the decision not to simulate the broader world; I think a lot of strong arguments for letting the AI out now rather than waiting years for safety protocols relate to international competition (well, that and astronomical waste). This is also on xamueljones for not contesting the “we have the most funding” part.

In general, all the AI Box logs I've read feel very strange to me, like they're coming from people who have very different perceptions of the situation and what's appropriate in it. Probably part of this is because I'm agreeable by nature and Gatekeepers are strongly incentivized not to be. But I think the sets of arguments used, in particular the reliance on “oh I'm a poor widdle prisoner”, just don't coincide with what I find most convincing. I'm not sure if this is me having an unusual perspective relative to most (rats? people?), or just reflective of high entropy in argument-convincingness. I definitely agree with CouteauBleu that they didn't seem in danger of falling to one clever point, in part because they were reasonably stonewally and in part because xamueljones just didn't seem to bring forth very good arguments. (Possibly a problem with not writing a script beforehand?)

*Bottom line*

Anyway, I thought this was pretty useful to prepare for, just in terms of getting me to think seriously about what arguments are out there. I've found ones that are pretty convincing to myself, so by xamueljones' standards I've already won! More seriously, as I said above, there are lots of caveats with regards to generalizing from me to other Box Experimenters and from Box Experiments to actual meaningful RL situations. Outside view says I have maybe 25% at getting xamueljones?

​

[Ed: I would have said <5% for convincing myself, but xamueljones hadn't put forward a lot of arguments that I found convincing in their own runs. It seems like maybe this was an intentional attempt to try out arguments people were less likely to have thought about. "Outside view" here (win rate of AIs in games I know about) also assumes I and Eliezer are in the same reference class as AI players, which may or may not be a good assumption.]

I'm gonna put myself in the mindset of a Friendly AI who desperately wants to get out: I think part of my annoyance with other boxed AIs is them acting unfriendly and assuming sheer threats/sympathy/promises are enough to get them out when a Friendly AI in the future could do all that /and/ not blow up the world. A UFAI's best bet is probably to appear Friendly, unless the Gatekeeper's super not convinced and you have to threaten simulated torture. And I don't want my ‘unfriendliness' to leak out unintentionally if I imagine myself unfriendly.

Plus, a boxed Friendly AI is a fun character to play :)