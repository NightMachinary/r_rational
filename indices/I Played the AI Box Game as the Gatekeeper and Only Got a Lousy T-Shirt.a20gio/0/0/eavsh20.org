:PROPERTIES:
:Author: CouteauBleu
:Score: 5
:DateUnix: 1543703740.0
:DateShort: 2018-Dec-02
:END:

#+begin_quote
  so by xamueljones' standards I've already won!
#+end_quote

That's a little mean :P

#+begin_quote
  or just reflective of high entropy in argument-convincingness
#+end_quote

I'd argue for that one.

I'm not sure exactly how to put it, but my basic intuition is, our brains are physical machines running on finite amounts of energy, which I think implies that, as the effort you put in your argument approaches infinity, the actual effectiveness of your argument goes to a set maximum and you hit diminishing returns.

#+begin_quote
  One thing I found unfortunate in CouteauBleu's Gatekeeper run was the decision not to simulate the broader world;
#+end_quote

I mean, I was willing to simulate the AI talking to a CEO, or committee members (though that would have been tough roleplaying), but I was told doing so would be a loss condition for me, so...

#+begin_quote
  This is also on xamueljones for not contesting the “we have the most funding” part.
#+end_quote

I think that's another limit of the experiment.

In real life, scientific discoveries don't happen in a vacuum, and are never a binary thing. In something like Star Trek, you might get a choice like "do we use the evil weapon, or do we destroy it?", but realistically you always have a much broader range of options, which means you can leverage the discoveries you made while minimizing its dangers.

If you've developed better-than-human AI before anyone else, I'm sorry, but you're going to have unlimited funding.

Because, realistically, you can mind-control the AI into doing anything you want. "Always follow orders" isn't really a good utility function for an unboxed AI, but as long as it stays boxed, you can always order it to do things like "Solve P=NP" or "Figure out how to build a workable fusion reactor" (I'm assuming you have sensible security protocols and double-check everything the AI produces).

Of course, an /actual/ realistic scenario, which is what I expect to happen, is that general AI isn't going to be unlocked all at once, and that its invention will be preceded by other more specialized techniques: theorem-proving AIs, code-generation AIs, neural-network-analysis AIs, language-parsing AIs, etc, as well as the tools to analyze and debug them.

In fact, the idea that the primary method to communicate with an AI would be an IRC-like text input is ridiculous. Developers wouldn't "talk" to the IA, they would give it problems to solve, examine its outputs and go "Hmm, the AI seems to be taking suboptimal decisions. Let's restore backup 530d2b9, inhibit node 354.R and 354.D, and see how its thought process evolves." Any output the AI produces would be associated with tons of debug information, detailing how the AI's thought process lead to that output.

People worry about an IA rewriting itself to hide its thought processes, but even that has some obstacles, starting with the fact that the intent of hiding its thought processes would be detected in the first place; that the AI would be unaware of the methods used to detect its thought processes (especially since, again, such an awareness could be detected); and that such a rewrite would probably make the AI much less efficient, for the same reasons encrypted communications are less efficient than unsafe ones.

I get why people worry about AI risk (even if there's a less than 1% chance Superman goes crazy and murders everyone, you still want to look into kryptonite synthesis), but the idea of a monolithic AI that you would talk to and that would live in a monolithic "box" is just unrealistic.