:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501810985.0
:DateShort: 2017-Aug-04
:END:

#+begin_quote
  Okay, noted, actually implementing such an ethical system is a thorny minefield of problems and edge cases and complexity. I'm not proposing this idea as a complete or even a partial solution to AI safety. I'm merely suggesting that an ethical system that puts strong value on self-determination by other intelligent entities would have reason to not instantly obliterate any intelligent life it comes across.
#+end_quote

Ok so I wasn't just making a point about about AI safety generally, but that placing a value on "self determination" in a way that gets the results it seems like you're looking for from the singleton here is rather implausible. For instance if a system doesn't already have intelligent life it seems hard to come up with a reason not to consume it on the basis that something might hypothetically arise there eventually, since you could use those resources to run minds that are part of your own civ or to gather resources to push back the heat death of the universe and extend the life of pre-existing minds.\\
Secondly even if there is already a pre singularity civ there, that's not much of a reason not to incorporate them into your own civ. It doesn't need to be malicious or anything, just send down grey goo and help the people on the planet. It seems pretty inevitable that people will come to grow dependant on your assistance and due to the significant advantages of cooperation will for all intents and purposes end up part of your civ. Given most people on a planet would definitely want things you could provide them it's hard to imagine how leaving them on their own is easy to justify.