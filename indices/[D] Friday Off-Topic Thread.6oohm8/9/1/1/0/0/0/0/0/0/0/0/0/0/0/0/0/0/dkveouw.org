:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501297484.0
:DateShort: 2017-Jul-29
:END:

#+begin_quote
  Exactly my point, if you aren't /personally integral/ to the creation of GAI then your very existence refutes the idea of that sort of simulation hypothesis.
#+end_quote

(a) Only if I accept your claim that AIs are only interested in other AIs, which I do not. (b) I don't know that I'm not integral to the development of AI. Maybe I'm going to be a close relative of the person who actually does the code, and a large influence on their life.

#+begin_quote
  I would disagree with that, other than as a progenitor of other GAI I can't actually come up with any circumstances under which there's any benefit to learning about lesser lifeforms. After all it won't have much impact on how long it might take you to deconstruct solar systems containing such life. Humans care about cats and dogs because they both have some effects on us, and because we're fond of knowledge for its own sake. However it seems questionable an AI is going to have any reason to care.
#+end_quote

Let's say that the AI has no use for our solar system except as raw materials for computronium and an energy source in the middle. That AI would /still/ benefit from a close study of humanity, because it cares about how to use its energy with the greatest efficiency;the better it can predict human behaviour, the better it can use a little bit of energy to persuade us to spend a vast deal of our energy doing what it wants us to do, which is a lot more efficient than having to use its own energy for everything.