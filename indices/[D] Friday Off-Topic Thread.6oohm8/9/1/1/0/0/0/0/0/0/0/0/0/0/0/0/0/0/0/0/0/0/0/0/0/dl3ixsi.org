:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1501738413.0
:DateShort: 2017-Aug-03
:END:

#+begin_quote
  If we're in a mind level sim, then there is no Earth to deconstruct and, even if we were to try, we wouldn't be able to get more computing power out of it than is being used to run the sim (because that computing power is simply not there to be used).
#+end_quote

I don't think you got the point I was making, that any post singularity civ could /easily/ run a sim of our civilization, provided they just simulated the minds. This isn't a point about the processing power /within/ the sim, just that massive non-baseline sims aren't hard to run for post singularity civs even in universes with the same physics as we think our universe has.

#+begin_quote
  A simple "let anything that can think decide its own destiny" ethical system will do it...
#+end_quote

I can point out the specifics about why that's not a remotely simple or self consistent ethical system, but the larger problem here has to do with apparent versus actual complexity. There's [[http://lesswrong.com/lw/jp/occams_razor/][an article]] in the sequences that covers the issue somewhat. Effectively ethical systems like that hide a massive amount of complexity beneath the surface, so calling it "simple" is like saying "a witch did it" is a simple answer to any question.\\
So the problem is that basically every part of the goal function you specified is massively nebulous and undefined, basically akin to saying you can solve AI safety by just telling an AI to not do bad things. Another way to say is that human intuitions of complexity have next to no correlation with actual formalized complexity, the amount of bits it would take to describe something from scratch.