:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1500756051.0
:DateShort: 2017-Jul-23
:END:

#+begin_quote
  The paperclipper can cause there to be more paperclips on average if he gets other AIs to produce paperclips in worlds where he didn't get to be the first AI in space. If he finds an AI that behaves like PrudentBot, he can get it to produce paperclips in worlds where he didn't get to be the first AI in space by satisfying its values until diminishing returns set in.
#+end_quote

Ok so this ties into a larger point of me which is that, acausal deals like that don't work as I argued [[https://www.reddit.com/r/slatestarcodex/comments/60ry4v/repost_the_demiurges_older_brother/df9vhoa/][here]]. Effectively there's plenty types types of cooperation which would work just fine were you able to be certain of the other ai's source code and vice versa, as is the case in the article you linked. However in practice you can only actually know the other ai isn't deceiving you if you are vastly more powerful than it, in which case there's little point to cooperating with it anyway, and even if you tried it would be unable to know whether or not you have truly precommitted.

#+begin_quote
  You're not going up against another AI, you're eating a system that might have developed an AI. In order to see which, you have to simulate it until it develops an AI, then read its sourcecode to see what it'll do before simulating its universe exploration starts consuming computing ressources in earnest.
#+end_quote

And what exactly is the point of trying to figure out the possible AI that might have arisen in a system had you not consumed it supposed to be exactly?

Anyway you're making something pretty close to if not indistinguishable from the demiurges older brother argument so you should definitely read that comment I linked responding to it.