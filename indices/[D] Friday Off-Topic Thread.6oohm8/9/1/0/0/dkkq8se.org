:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1500747797.0
:DateShort: 2017-Jul-22
:END:

#+begin_quote
  For example, if it turns out there's a way to leave this universe for another, more bountiful computational substrate, and the paperclipper finds an AI that just wants to simulate its creators forever, it can just send that AI there while that AI would have also spawned a paperclipper before leaving, but first both would have to simulate the other to see whether they would have cooperated iff the first cooperates.
#+end_quote

Leaving the universe to go somewhere with more potential computing falls under the previously mentioned categories of scenarios that are both pointless and impossible to speculate about and has a vanishingly small prior probability anyway.

#+begin_quote
  The paperclipper might run a simulation to test the AI that could have popped up in the galaxy it just ate: If they both would spend some of their ressources on satisfying the other's values iff the other would do the same, they can both do so to maximize their expected values if there are diminishing returns on ressource expenditure in one of the sets of values.
#+end_quote

This doesn't really work since a paperclipper /doesn't care about anyone's values but its own/. Even if you actually have to go up against another AI and want to try to get info about its goals, that the best way to do that would be by running a simulation of our current world strains credulity for a number of reasons some of which I mentioned before. Plus that wouldn't even tell you competing AI's goals if you didn't actually know where in possibility space the civ that created them resided, which if they aren't sharing their goals structures you're not finding out anyway.