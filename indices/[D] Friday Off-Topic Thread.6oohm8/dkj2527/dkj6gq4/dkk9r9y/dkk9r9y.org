:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1500721033.0
:DateShort: 2017-Jul-22
:END:

The paperclipper might run a simulation to test the AI that /could/ have popped up in the galaxy it just ate: If they both would spend some of their ressources on satisfying the other's values iff the other would do the same, they can both do so to maximize their expected values if there are diminishing returns on ressource expenditure in one of the sets of values.

For example, if it turns out there's a way to leave this universe for another, more bountiful computational substrate, and the paperclipper finds an AI that just wants to simulate its creators forever, it can just send that AI there while that AI would have also spawned a paperclipper before leaving, but first both would have to simulate the other to see whether they would have cooperated iff the first cooperates.