:PROPERTIES:
:Author: Gurkenglas
:Score: 1
:DateUnix: 1500750604.0
:DateShort: 2017-Jul-22
:END:

You're not going up against another AI, you're eating a system that might have developed an AI. In order to see which, you have to simulate it until it develops an AI, then read its sourcecode to see what it'll do before simulating its universe exploration starts consuming computing ressources in earnest.

The paperclipper can cause there to be more paperclips on average if he gets other AIs to produce paperclips in worlds where he didn't get to be the first AI in space. If he finds an AI that behaves like [[http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/][PrudentBot]], he can get it to produce paperclips in worlds where he didn't get to be the first AI in space by satisfying its values until diminishing returns set in.

Another way to look at it is that the paperclipper doesn't know whether it's in reality and can maximize paperclips by eating everything, or a simulation, where everything it does is irrelevant except for the observations the simulator makes about its actions.