:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1501228430.0
:DateShort: 2017-Jul-28
:END:

#+begin_quote
  I don't just mean dreams though, I'm saying that all stages of sleep have something which it's like to be in them, even though the mental activity occurring isn't particular complex. There is something which it is "like" to be in even the deepest non-rem sleep. Whereas I'm not quite sure the same can necessarily be said about being under anesthesia, since from what I remember it did feel exactly like I just skipped forward in time.
#+end_quote

Yeah, I agree with you there. My mental state on waking is often very different to my mental state on sleeping, so /something/ is clearly going on in the interval, even when I don't remember any dreams.

#+begin_quote
  I meant "care" in a more general sense, in that they need some reason to care about any information they could get out of the mind for some reason.
#+end_quote

Ah, I see. But that might well be "let's see how this simulated mind reacts to torture".

#+begin_quote
  However as I argued before it seems unlikely that the best way to get good data on minds would be to simulate not only a perfect copy of the relevant minds, /but also that you would need to simulate a massive swathe of other minds in a civ, that aren't directly connected to the development of GAI./
#+end_quote

Why on earth would you need to simulate more than, say, two dozen minds? Fill the rest in with newspapers, background characters, and a few dozen semisentient AI-controlled drones, and you can make a sparsely populated world look overcrowded from the inside.

#+begin_quote
  That's because it's hard to imagine any point to running those massive sims until you have become powerful enough that you only care about other GAI, and even in that case you'd only want to run the sims to see what kinds of programing the humans would put in the AI,
#+end_quote

Then wouldn't you only be interested in simulating those who /are/ connected to the development of the AI?

Also, there's /plenty/ of other reasons to simulate minds. I can't imagine a successful GAI that stops caring about anything except other GAI, partially for the same reason as most humans haven't stopped caring about cats and dogs, and partially because humans have a dramatic impact on our environment, and while a GAI is not at severe risk from this, it would still benefit from understanding (and, if necessary, directing) that impact.

#+begin_quote
  Well as I just mentioned it's also probable that the point of the sim in the first place is likely to investigate stuff related to the creation of GAI generally. Or if the simulators have some minds sufficiently weird as to justify running the sim as basically a zoo, then they would likely just consistently roll back time once we got to GAI.
#+end_quote

From an inside-the-sim point of view, I'm not seeing any difference between "abrupt end of the sim" and "rolling back time" - I'm just as dead, even if a younger me gets a new lease on life.