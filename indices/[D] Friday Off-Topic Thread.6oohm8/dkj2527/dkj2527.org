:PROPERTIES:
:Author: B_E_H_E_M_O_T_H
:Score: 5
:DateUnix: 1500656195.0
:DateShort: 2017-Jul-21
:END:

I'm thinking a lot about the possibility that we're in a simulation; I'm sure most people here are familiar with the basic argument, but I'll reiterate anyways.

In the case that we achieve artificial intelligence and easy access to supercomputers, one of the main things we would do is simulate complex realities, to see what happens in those realities given a certain set of circumstances. We would do this a lot; there's no reason not to. Given this information, the chances that our reality is one of these simulations is very high.

The problem that I've been thinking about is one of failure states. What is a set of circumstances that could occur in a simulation that would cause someone to turn that simulation off? The one that jumps out to me the most is if that simulation suddenly started using a lot more operating power than it previously did. The main way I could imagine this happening is if that simulation also achieved artificial intelligence and started simulating realities of their own.

Given the possibility that reaching that point could cause the simulation /we're/ in to be turned off, is it worth it to consider whether we shouldn't try to create complex simulations like this at all? Is it worth it to think about failure cases so we can try to avoid our simulation no longer existing in general?

I worry about the irony of trying to work out the preferences of an omnipotent being to avoid behaviors they might not like, considering how much I've derided that idea over my years of being an atheist, but that's... kind of a different discussion.