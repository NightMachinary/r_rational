:PROPERTIES:
:Author: Esparno
:Score: -2
:DateUnix: 1442703940.0
:DateShort: 2015-Sep-20
:END:

#+begin_quote
  which is something humans don't value.
#+end_quote

Humans absolutely value not being tortured. And there are quite a few humans that seemingly respond best to threats of violence against them.

It's not a stretch to imagine a "friendly" AI thinking it's doing the right thing by using such tactics to achieve its goals if the end result is a better life for humans in the real world.

I'm pretty sure it's exactly this type of situation that Eliezer Yudkowsky has said is a primary reason for figuring out the rules for a "friendly AI".