:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1505972602.0
:DateShort: 2017-Sep-21
:END:

#+begin_quote
  For life insurance to work at all, insurance adjusters must be able to put a finite monetary value on a human life. I'm not sure what that value is, but it would make a starting point.
#+end_quote

This doesn't quite work, for multiple reasons. First off, I would be very surprised to find a life insurance company that actually cares for its customers, enough to truly give them the value of their life. It's all about making money. Rather than ethical debates on the value of human life, insurance companies typically set their prices and their payouts based on things like how many customers the insurance company has, and what the average rate of death is among their customer base, what specific pre-existing conditions their customers have, etc. It's very much an economic construct, and the economy, being an imaginary human construct, is inherently subjective. So I find it highly unlikely for the objective moral value of a life to be depending on such subjectivity.

Not to mention that insurance companies don't even agree on the same payouts. Some pay more than others, making their money by charging their customers more. Are the lives of people who pay more then worth more than the lives of people who pay less? What about the lives of people with no insurance? What if the life insurance pays in different currencies? How are you dealing with currency exchange? Is the moral value of a life dynamically changing based on the current value of the dollar? Is my life worth more if I move to another country? And what happens if someone tries to artificially change the moral value of human life by adjusting the life insurance payouts? What if it turns out life insurance companies are shams that will declare bankruptcy instead of paying up when most of their customers die in some disaster?

#+begin_quote
  Even if you can't put exact figures to it, it seems it would be usually possible to intuit which course of action has more moral value than the next.

  Alternatively, since all you really need to know is whether a given course of action has a greater moral value than another one or not, you might even be able to get away with not directly assigning an explicit value at all; as long as you can estimate an ordering between different courses of action.
#+end_quote

This does not sound like an objective morality at all, if its based on people "intuit"ing/"estimating" what the moral value of each choice is. After all, "intuit"ing/"estimating" things is by its very nature, very subjective; people disagree on what the most moral action is all the time.

At best, you can argue for the existence of a moral gray area, where things are not objectively morally right or morally wrong. But then, if objective morality exists, there should be objective boundaries on the gray area. So now you need to determine the exact boundaries of the gray area, putting you back at square one since you now have to argue why the gray area should start at 0.45124 instead of 0.45125 or 0.45123. Argh!

Alternatively, you could argue for a gradient transition between the gray area and the objective area, with no boundaries other than the extremes. But then the resulting moral system isn't really objective or useful, since it only tells you objective rules at the extreme cases and makes guesses about everything in between, and you wouldn't even be able to tell how accurate these guesses are or where you are in between because the boundaries and the gray area are poorly defined.