:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1505868058.0
:DateShort: 2017-Sep-20
:END:

I'll be honest, I don't think I really understand your post, so this reply will be mostly me guessing your intentions.

Let me explain my thought process. If objective morality exists, that should imply the existence of some (non-empty) set of rules/axioms that can be followed to achieve some objective moral "good". In particular, you should be able to follow these moral axioms in all contexts, since they are objectively right.

For example, the naive utilitarian system says "you should always maximize total utility, even at the cost of individual utility". If that is an objective moral axiom, then you should be able to obey it in all contexts to achieve some objective moral good. In other words, you can't say "oh but in this particular context the sacrifice requires me to murder someone for the greater good, so it doesn't count and I shouldn't follow the axiom". If you wish to do that, then you have to change the moral axiom to say something like "you should always maximize total utility, even at the cost of individual utility, unless it involves murder". And you have to keep adding all sorts of little nuances and exceptions to the rule until you're satisfied that it can be followed in all contexts.

With that in mind, whenever I encounter a system of morality, I test whether it is objectively right to follow this system by imagining hypothetical scenarios of agents following this system, and try to find one that leads to a dystopia of some sort. After all, if it leads to a dystopia, a state of the world that many would reject, then how is it objectively right?

I have not found a system that passes this test, so my conclusion is that there could be one, but I don't know of it.