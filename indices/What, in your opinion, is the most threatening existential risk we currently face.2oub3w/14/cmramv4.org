:PROPERTIES:
:Author: scruiser
:Score: 1
:DateUnix: 1418255615.0
:DateShort: 2014-Dec-11
:END:

You use the term "currently" I think AI will start being an existential risk around the time we start getting near human AGI (which I expect will be done with first scan/uploads of human brains). Once we are at the point, then it jumps up to a huge existential risk, but before that point its not a "current" concern. Also, another issue is the difficulty of estimating how close we are to human level AGI. So basically we could go decades without any risk of AGI, and then the first near human AGI is developed, and then the FOOM happens 'quickly'. Its still something we should plan for, and we probably want to solve many of the related problems (i.e. Friendliness) well ahead of time, but I think it is still a difficulty to estimate amount of time away and thus not current.