:PROPERTIES:
:Author: eaglejarl
:Score: 1
:DateUnix: 1418245274.0
:DateShort: 2014-Dec-11
:END:

Ah, cool. I didn't know about Teller proposition. Thanks for the pointer.

I think you're being a bit casual about AI and nanotech. An AI would be, by definition, self-modifying. Even if you /could/ program in a "certain set of ethics and desires that excluded mass death of humans" how would you ensure that those retained intact across multiple iterations of self-modification. Also, an AI does not need to have "exterminate, exterminate" as its utility function in order to be an extinction risk. Paperclippers (or the equivalent) are a far more probable threat.

Killing nanotech depends on how much of it has generated and where before people become aware and start fighting back. I don't know enough about the subject to speak authoritatively, but "point a flamethrower at it" sounds a bit too casual for my comfort.