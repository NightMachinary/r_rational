:PROPERTIES:
:Author: OffColorCommentary
:Score: 1
:DateUnix: 1418249750.0
:DateShort: 2014-Dec-11
:END:

I think rogue AI is the single biggest risk, as in most likely to actually happen and least likely for us to survive.

Climate collapse seems even more certain, but the brunt of it is rather far out, and it won't be sudden. Even though we as a species seem pretty inept at preventing it, we'll have plenty of time to adapt and deal with it, even if all the actual solutions suck.

Nuclear war is more imminent - it's more likely to happen this year than a runaway AI is - and plenty lethal, but it doesn't seem that likely to actually happen in the long run. Maybe it's more likely this year than it has been in a long time, but we got past the cold war. I think the warnings actually got through to leaders on this one.

Things I see as non-issues:

- Antibiotics failing - It will be incredibly awful for people with weak immune systems, and hospital mortality rates will skyrocket, but we won't all die from this. A really big problem we should be concerned about, but not an existential risk.

- Asteroid - Vanishingly unlikely during human timescales.

- Grey Goo - Runaway self-replicators are an especially bad lab accident, not a world-ending disaster. Nanobots need specific materials, and will slow or stop as they run out of their proper environment. Growing past a puddle will require specialized structures, which we have no reason to program in (unicellular plants didn't make it onto land). Also, despite the stock saying, the worst-case growth rate is quadratic (surface area of a sphere), not exponential.