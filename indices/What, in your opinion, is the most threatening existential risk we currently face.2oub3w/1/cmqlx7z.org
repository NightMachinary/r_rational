:PROPERTIES:
:Author: alexanderwales
:Score: 18
:DateUnix: 1418196687.0
:DateShort: 2014-Dec-10
:END:

Rogue nanotechnology, super disease (engineered or evolved), and nuclear armageddon all get my vote over AI. The thing to worry about with AI isn't /just/ that AI gets developed, it's that:

1. AI gets developed
2. It's able to recursively self-improve
3. It goes rogue
4. In a way that we can't stop

I don't even know which of those points to be most skeptical about. When people talk about being afraid of artificial intelligence being developed, it's always the nightmare scenario of an artificial intelligence so advanced that it can talk people into anything or spin up new technologies on the fly. I'm not saying that I don't take it seriously, because I do. But in terms of the actual probabilities ...

Well, look at the other scenarios. As an engineering challenge, self-replicating nanotechnology is /tough/, but I don't think it's as tough as hard AI. Same goes for engineering a bacteria or virus capable killing large swaths of humanity to the point where we can't bounce back. And nuclear armageddon only needs the wrong person (or people) to get into power.

Maybe I'm just disillusioned by having read /The Singularity is Near/ at an impressionable age, which had some dates that made the future seem a lot nearer than it really was, or maybe I've just worked on too many software projects to have much confidence in one so vastly exceeding expectations.