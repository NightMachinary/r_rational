:PROPERTIES:
:Score: 6
:DateUnix: 1418221351.0
:DateShort: 2014-Dec-10
:END:

I basically agree with your whole list, but there's one thing to note: the Apocalypse Level of the event. FHI, for instance, has a tendency to, in my personal opinion, underrate "Econ-Eco Apocalypse" merely on grounds that it would not actually /completely annihilate humanity or technological civilization/. I consider this a problematic assessment, because I think there /is/ a chance it would destroy technological civilization, setting us back to a permanent low-productivity, low-energy, nigh-Malthusian existence.

#+begin_quote
  I'm lumping all of the extinction events where one of humanity's technological children kills its creator, either by bug or malice. Terminator, Matrix, Grey Goo and Paperclip all own less of my fear space than even an asteroid impact - not only would we have to make a lot of mistakes to get there, but we're going into it with our eyes so wide open. There are already a lot of smart people thinking about how to avoid this.
#+end_quote

The big thing that reduces my worry about UFAI is that even a recursively self-improved superintelligence does have sample complexity and computational complexity bounds it /cannot/ exceed, and the /first/ AGI agents /will not be/ recursively self-improved superintelligences. They won't be /able/ to self-improve without first gathering enough data and performing enough processing on it (call it "education") to form an accurate, naturalistic model of the world that includes itself and includes the necessary understanding to code an improved agent.

/That/ phase will take time and data, lots of it, during which we humans will still have the advantage and /probably/ be able, if the makers have /bothered/ to take decent precautions in the first place (see: entire sub-field of Corrigibility, currently in its infancy), to shut the damn thing off, by force if necessary.

Which isn't to say there's no risk. It's to say that the risk is more on the order of massive radiation spills than on the "INSTA-KILL" level.