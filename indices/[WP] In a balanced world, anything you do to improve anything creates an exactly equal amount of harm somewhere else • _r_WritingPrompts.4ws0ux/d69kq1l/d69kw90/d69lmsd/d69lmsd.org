:PROPERTIES:
:Author: Jiro_T
:Score: 3
:DateUnix: 1470693839.0
:DateShort: 2016-Aug-09
:END:

What if I hire you to do something? What if I pay some money into a fund, and other people withdraw money from the fund 50 years later and use it to do something? What if I would consider something harmful if I were to know about it, but I don't? What if the "harm" or "improvement" from something won't mature for a while, such as reducing by 10% the chance that I will lose my crops the next year? What if I'm a normal human with an inconsistent discount rate for future events, what does the world use to determine how much harm balances how much improvement? What if I'm inconsistent in other ways (for instance, my beliefs logically imply that X is harm, but I don't consider X to be harm)? Exactly how far ahead does this look--if I research better weapons, does it count the harm caused by future people using the weapons in bad ways, and the improvement from future people using the weapons in good ways, and the effects on the trade routes caused by the fact that availability of weapons changes the political balance to give more power to one country which has slightly higher taxes, leading to a slight change in poverty rate that causes some guy to die one week earlier and fail to discover spaceflight? Also, how does the world deal with harm to future people and/or the prevention of the existence of future people and/or the non-identity problem?

Or to put it another way, take every single problem that everyone has ever tried to find with utilitarianism and realize that this just postulates that the world is capable of solving all of them. You might not know whether you should consider torture versus dust specs to be a fair trade, but the world knows based on your utility function.

#+begin_quote
  If multiple people planned it or were involved, then it will conform to all of those people's standards.
#+end_quote

What if it is logically impossible to conform to multiple people's standards, such that balanced harm for one is necessarily unbalanced harm for another and vice versa?