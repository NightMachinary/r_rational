:PROPERTIES:
:Author: Fresh_C
:Score: 2
:DateUnix: 1506728426.0
:DateShort: 2017-Sep-30
:END:

By motives I believe they're talking about the driving goal behind the AI. From our current understanding of how we believe AI will function (Likely an extension of neural networks) the AI will need some basic goals that it can "reward" itself for achieving and some metric by which to measure success and failure of those goals.

For modern AI systems like Google's Alpha Go, it's motives are simple. Just choose the move which it thinks will lead to best chances of winning the game of Go that it's currently playing.

A General Purpose AI like the type we're describing won't have such a simplistic goal, but it will still have to have some basic goal or more likely a combination of goals that it hopes to achieve, otherwise it wouldn't actually do anything.

In the case of what OP is describing, the goal(s) would more or less to answer the questions that we ask of it as accurately as possible.

But even a goal as simple and seemingly non-threatening as this could be detrimental to humanity if there aren't serious restrictions on how the AI carries out this task, or if the AI doesn't share our ethical values.