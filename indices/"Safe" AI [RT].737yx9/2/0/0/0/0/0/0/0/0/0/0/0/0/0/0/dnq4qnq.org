:PROPERTIES:
:Author: Fresh_C
:Score: 1
:DateUnix: 1506797108.0
:DateShort: 2017-Sep-30
:END:

A) Yes what you're describing is a system that's miles ahead of what we currently have. But basically it's self contained. Once it's built it's built. The way it functions won't be any different from the day it's created to the day it stops running.

That is not what most people are describing when they talk about a General Purpose AI.

B) It doesn't make it automatically not a threat. It just makes it much easier to anticipate any threats that might arise from it. It's a system built by humans who more or less understand its function and limitations. I'm not saying systems like these can't be dangerous, because they definitely can. But the danger isn't likely to be as broad as something that an AGI can bring to the table.

(Note: I probably shouldn't have used such dismissive language as "it doesn't pose any significant threat". Consider that hyperbole.)

Whereas a system that's self-optimizing is initially built by humans, but what it will eventually become is impossible for humans to 100% predict because it will be changing it self at a rate beyond our ability to keep up with.

It's possible that the very humans who build the first self-optimizing AI system won't be able to even follow the code of the system they built once it has been through several iterations of self-optimization.