:PROPERTIES:
:Author: Daneels_Soul
:Score: 1
:DateUnix: 1506752696.0
:DateShort: 2017-Sep-30
:END:

Sorry, but I don't think that this actually answers my question.

I am familiar with general arguments about why we should be wary of powerful AIs and will grant that any substantially superhuman AI programmed solely to optimize X (for basically any simply specified value of X) probably kills everybody.

However, I think that there's a big logical leap between "employs low level optimization procedures as a subroutine" and "is an agent whose large scale behavior is to optimize some well specified real world quantity".

I mean consider the possibility of a near linear time SAT solver. This would be hugely powerful and may well employ a lot of optimization procedures as subroutines, but its definitely not (by itself) going to turn the world into paperclips.