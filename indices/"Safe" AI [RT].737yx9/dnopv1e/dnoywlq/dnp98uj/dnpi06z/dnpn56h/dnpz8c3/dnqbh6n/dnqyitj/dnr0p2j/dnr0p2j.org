:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1506849666.0
:DateShort: 2017-Oct-01
:END:

#+begin_quote
  On the other hand, this would imply that you could have two programs with identical behaviors one of which is an AI and the other of which is merely a calculator, which seems like it is throwing functionalism out the door.
#+end_quote

It's not that surprising. Compare a postman and an email service. Both have the same functionality: they deliver letters. Yet one has intelligence while the other is just a simple algorithm.

Personally, I consider a program an AI if it has creativity, if it can do things in ways the designers did not foresee. So if you have an infinitely fast computer that just instantly brute-forces all possible values for the variables of a SAT problem until it finds one that fits, that isn't an AI, because it's not creative. It's only following a simple algorithm that the designers clearly know, just doing it much much faster. In contrast, the slow program that decides to look up methods for solving SAT problems in a library IS creative (unless the designer programmed it to do that or something, which would be weird), and so is an AI.

#+begin_quote
  I'm still not sure what you think that self improvement should be necessary for significantly dangerous AIs.
#+end_quote

Well, let's break down the analysis. An AI has to be built by something, so it is either built by another AI (self improvement), or built by humans. (If it's built by humans blindly following the instructions of an oracle AI, that counts as self-improvement.)

If it is built by humans, then it is necessarily within the realm of human understanding. That's a heavy restriction on the amount of both usefulness and danger it can pose. After all, it's technological level wouldn't be that much ahead of human technology, and we KNOW how it works since we built it and it hasn't improved itself.