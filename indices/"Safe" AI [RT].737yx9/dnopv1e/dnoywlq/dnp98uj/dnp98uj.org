:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1506737473.0
:DateShort: 2017-Sep-30
:END:

#+begin_quote
  What definition of "AI" are you using in order for this to be true?
#+end_quote

Well, since the thread talks about Friendship is Optimal, I'm considering AIs with the same level of power. I.e., AIs that rapidly self-improve to the point where they have the ability to cause extinction events in just a few years.

In general, whenever I see someone talk about AI safety, I assume they mean superintelligent AI: AI that is actually powerful. Otherwise, why worry about safety? After all, if the AI is only a bit more intelligent than humans, then it's not anymore threatening than a super villain at worst.

#+begin_quote
  it seems that the far safer way to do it is to build oracle AIs, and simply ask them questions that allow you to better design the next generation of oracle AIs.
#+end_quote

Huh. That's an interesting idea. You would need to be very careful to ask the right questions, and ask for a very robust design for the next generation that can't be screwed up to create an unfriendly AI, and ensure it doesn't try to find answers to questions by trying to increase its computational power or doing anything in the real world... but it sounds promising.