:PROPERTIES:
:Author: Fresh_C
:Score: 1
:DateUnix: 1506734406.0
:DateShort: 2017-Sep-30
:END:

That's why it's all hypothetical at this point.

We don't know what an General purpose AI (AGI) is going to look like. But the closest things to a general purpose AI that we've developed run based on the principles of optimization.

So if we're to assume that General AI uses similar techniques to neural networks and machine learning then, yes they will have to have something or a group of things that they're trying to optimize (which is basically what I meant when I said "goal").

I'm not sure that I'm actually knowledgeable enough to explain to you in a comprehensive way how a General AI would be much more dangerous than a simple machine learning program.

But the basis of the argument is that a General AI has the intelligence of a human or greater, with the speed of processing of a computer, and only the ethical restrictions that its creator had the forethought to put into it.

(Check out this article about a [[https://wiki.lesswrong.com/wiki/Paperclip_maximizer][Paperclip maximizer]] for a better explaination than the one I'm about to give).

Basically the concern is that the AGI will naturally exploit every possible tool it has at its disposal to optimize whatever result it's trying to optimize. It's as smart, or likely smarter than a human so we can't predict how its going to behave. We can't outsmart it. We can't even be sure that it hasn't outsmarted us. Once you've created it the only thing you can do is hope that you've taught it the right ethical guidelines so that it doesn't do something unethical in its pursuit of optimizing whatever it's trying to optimize. Or attempt to severely restrict the tools it has at its disposal, so that you can hopefully minimize the damage it can do.

I hope that kind of explains where I'm coming from and isn't just a massive text of rambling that doesn't answer your question...