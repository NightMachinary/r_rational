:PROPERTIES:
:Author: Daneels_Soul
:Score: 1
:DateUnix: 1506843366.0
:DateShort: 2017-Oct-01
:END:

Interesting. What does define an AI then?

On the one hand, it seems reasonable that whether something is an AI should depend on whether the implementation feels AI-like. On the other hand, this would imply that you could have two programs with identical behaviors one of which is an AI and the other of which is merely a calculator, which seems like it is throwing functionalism out the door.

But more on topic, I'm still not sure what you think that self improvement should be necessary for significantly dangerous AIs. I feel like there are standard arguments about AI safety that already say that there is potentially very little gap between "as smart as all of humanity together" and "so massively superintelligent that we cannot possibly hope to deal with it". Note: this all assumes that we can draw a distinction between full self-improvement and merely learning from data. I agree that it is basically impossible to build a strong AI without the latter, and note that it is not entirely trivial to delineate where learning from data because iterative self improvement.