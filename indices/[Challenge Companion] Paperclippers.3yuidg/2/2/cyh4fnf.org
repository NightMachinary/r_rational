:PROPERTIES:
:Author: Sagebrysh
:Score: 0
:DateUnix: 1451549225.0
:DateShort: 2015-Dec-31
:END:

I feel like a lot of the potential X risks that emerge around AIs generally come about as a result of some terminal value being 'baked in' to the AI as it is created IE: make paperclips. Having a terminal value like that at all is always going to be trouble.

So don't start it off with anything baked in at all. Start instead with basic principles and over time teach the AI more and more advanced concepts such as language, human interaction, ethics, rationality. Read it the Sequences, read it Methods of Rationality, read it Superintelligence; teach it like you would teach a human child to understand the world. Give it a healthy environment, feed it lots of positive input and help it learn its place in the world, and let it come up with its own terminal functions as a result of this upbringing.

If you translate the paperclipper into a person, you end up with someone who has some potentially serious mental issues. Its only dangerous because of its power and its single-mindedness. I think just avoiding single-mindedness in the first place would result in a better outcome.

So how does such an AI learn how to act? It learns by observing and absorbing data from the world around it, the same way we do, just faster. Throw the vast majority of all major philosophy, world history, etc at it, teach it about people, about rationality and morality, and then let /it/ decide what to do.