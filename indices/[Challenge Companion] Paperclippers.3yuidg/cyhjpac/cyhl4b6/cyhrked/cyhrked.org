:PROPERTIES:
:Score: 4
:DateUnix: 1451602491.0
:DateShort: 2016-Jan-01
:END:

#+begin_quote
  Since when was attempting to independently think things through an act of intellectual arrogance?
#+end_quote

Uhhh... it's not.

#+begin_quote
  What we're dealing with with intelligence is the opposite of the flight situation. Instead of a situation where everybody is trying to replicate a natural behavior by cargoculting it, we have a situation where everybody is trying to replicate a natural behavior but is ignoring the fact that it already exists.
#+end_quote

Yes, quite true. Most of the AI community are failures for precisely that reason.

#+begin_quote
  Why aren't there more psychologists at the forefront of AI development?
#+end_quote

Except that [[http://web.mit.edu/sjgershm/www/GershmanHorvitzTenenbaum15.pdf][cognitive scientists and cognitive psychologists /are/ at the forefront]] of the best, most viable work towards understanding how minds-in-general can actually work (which is how you would actually go about building one).

Look, the point of my post was just that trying to solve the whole "how do I /avoid/ a paperclipper" problem by first imagining a magic genie that obeys verbal commands, and then trying to give it the perfect verbal command, is /completely/ unrealistic. That basically proposes that you first solve natural-language processing, then use that to /translate/ verbal instructions /into/ VNM-conformant "utility functions".

I actually agree with you about "utility function mythology". For instance, it's more accurate to say that humans are something like value learners than to say that we have a utility function: we're reinforcement learners with (IIRC) /separate/ reward and punishment signals, /both/ of which are driven by /multiple/ sources of reinforcement via the limbic system (again, IIRC, I am not a neuroscientist and just know one on Facebook, I am not a cognitive scientist but have read their survey papers).

Further, trying to /write down/, in computer code and machine-learning models, a "utility function" like "maximize paperclips" is /really fucking hard/, actually, because real minds don't come with anything that says, "Here's what a paperclip is" built-in. But then again, the /actual point/ of the paperclipper thought-experiment /isn't/ that we expect it to be genuinely easy to build /the exact fictional thing/ called "paperclip maximizer", but instead that /unless/ we make deliberate efforts to build learning systems and full "AIs" with /exactly/ the goals we want them to have, the actual goals they seek when they gain full autonomy from their operators will be sufficiently random and arbitrary that they will, in the limit but /inevitably/, come to conflict with their operators' real interests and with human interests in general. /This would be really, really bad/ because by that point the AIs would have sufficient autonomy that we couldn't shut them off, except /possibly/ by force, and /even that/ would be damned risky.

It is believed, as part of the "community mythology", but part which I've seen no reason to disbelieve in the research literature, that what we actually need is a kind of utility-function or reward /inference/ machinery so that the resulting "AIs" approximate their operators' interests and wishes /more and more closely/ as they gain knowledge, computing power, and thus inferential ability, rather than deviating further and further away as-is the default case.