:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 0
:DateUnix: 1451525675.0
:DateShort: 2015-Dec-31
:END:

Speculation: the way to avoid making a paperclipper is not to come up with a better-defined utility function (IE, the progression of better utility functions from "maximize paperclips" to "minimize suffering" to "do what current me would be happiest about"). It's to construct an artificial mind /without/ a single utility function, one that has several disjointed basic human drives like "survive", "have an accurate model of my environment", "find and mimic beings analogous to myself", and such, and, from those drives, develops additional utility functions that it feels more strongly than its built-in utility functions - just as humans might consider their utility function to revolve around a cult that they joined, for example even though it's obviously not something that was pre-set, and might die in battle in service of that cult, even though that contradicts their built-in survival drive.