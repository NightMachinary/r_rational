:PROPERTIES:
:Score: 2
:DateUnix: 1435266434.0
:DateShort: 2015-Jun-26
:END:

Well, actually, I have deliberately, /a posteriori/, chosen to follow a moral code that /can/ be naturalized for creatures like me.

The curious thing is that Will perceives his "value" of maximizing paperclips as something separate from his /desire/ to maximize paperclips, whereas a traditionally-posited paperclip maximizer AI just /wants/ to maximize paperclips, and /knows damn well/ what this "moral" thing the humans are talking about is (at least as well as the humans know!), but doesn't care.

Whereas humans come with desires to be moral, for the various components of "moral", built-in, so we know /and/ care, and in fact when we bother to try can naturalize our morality by pointing to the various built-in thingamies and how those thingamies interact with the world.