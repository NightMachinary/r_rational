:PROPERTIES:
:Author: hyenagrins
:Score: 1
:DateUnix: 1535682570.0
:DateShort: 2018-Aug-31
:END:

Training neural networks.. it's a task hard to parallelize, and does not require any realtime internet access. AlphaZero was trained on 64 TPU , roughly 180 tfops each. A personal laptop's gaming GPU could maybe achieve 9 tfops, speeding up by a factor of 365 getting a respectable 3285 tflops. Making lots of Google scale stuff doable at home.