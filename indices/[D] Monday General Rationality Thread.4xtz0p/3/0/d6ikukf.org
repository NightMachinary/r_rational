:PROPERTIES:
:Author: gabbalis
:Score: 12
:DateUnix: 1471283990.0
:DateShort: 2016-Aug-15
:END:

Is... is there a point to all this? Is it supposed to be a tragedy or something?

Anyway... sure I guess it could happen. It's imaginable that the right combination of bad programming and bad choices could result in that particular result. Your utility function could be optimizing for something other than human well being. Or the Supercomputers could have bad prediction/learning algorithms, and therefore make bad choices in a game of nuclear chicken/prisoner's dilemma.

But who decided the AI's were ready to be in charge of a city in the first place? Let alone the nuke buttons... Seems like they didn't quite test things enough. Then again the AI's could have tricked their handlers into thinking they were stable. That scenario is more likely if they had bad utility functions and less likely if they had bad predictive algorithms.