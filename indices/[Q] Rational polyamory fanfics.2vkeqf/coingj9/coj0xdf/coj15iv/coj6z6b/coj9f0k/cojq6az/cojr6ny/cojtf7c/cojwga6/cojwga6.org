:PROPERTIES:
:Author: Nepene
:Score: 1
:DateUnix: 1423794826.0
:DateShort: 2015-Feb-13
:END:

To your edit.

#+begin_quote
  If the AI can talk you into actually addding weight to the "relationship" terminal goal, and actually removing weight from the "exclusivity" terminal goal, then your umbrella term should be actually coming out higher than it was before. If it isn't, then the AI did not, in fact, talk you into polyamory. It just forced a monogamous person to be in a polyamorous situation.
#+end_quote

My happiness term is composed of a number of things- social stress, physical contact, frequency of intense romance.

I could have a low exclusivity term and high relationship goal but still be sad because I was getting less physical contact due to time management issues, more stress because there were more points of failure in my relationship and more stressful situations to handle, less intense romance because of time management issues.

#+begin_quote
  We're both talking about characters in romance settings here, right? They're not allowed to do that. (Picture, specifically, the characters of Edward Cullen and Jacob Black in Twilight if it helps. They both have---at least, what at first appear to be---spiritual bonds to the same person. "Removing the stimuli" actually hurts them in an active, persistent sense. This particular Gordian knot was later cut by authorial fiat, but most aren't. Of course, most such bonds are more metaphors than literal fantasy plot-devices, but the same effect is achieved.)
#+end_quote

Those two would likely have an immense aversion to sharing. The better way to cut the knot is either to clone them, as happened in luminosity, or to make similar versions of her who are similarly attractive.