:PROPERTIES:
:Author: derefr
:Score: 2
:DateUnix: 1423789032.0
:DateShort: 2015-Feb-13
:END:

Note that when I say "talk into", I'm meaning a very specific thing---that the AI would be able to /give you facts that would force you to convince yourself/ that that was the better course of action. I'm not suggesting that the AI would side-channel-attack your brain with its words or anything. Just that it would make you aware of things you hadn't considered, and that, almost undoubtedly, this would be enough. (Mostly because it doesn't have to be fair about making you aware of /other/ facts that might be evidence in the opposite direction.)

#+begin_quote
  I'd likely also find that rather unpleasant and would likely become very depressed and self hating
#+end_quote

Well, you'd stop, then, wouldn't you? If you're rational yourself. I'm presuming here that you have an "umbrella term" in your utility function for your own happiness, and that you won't (continue to) do anything that turns out to satisfy one terminal goal but makes you less happy overall.

If the AI can talk you into /actually/ addding weight to the "relationship" terminal goal, and /actually/ removing weight from the "exclusivity" terminal goal, then your umbrella term should be /actually/ coming out higher than it was before. If it isn't, then the AI did not, in fact, talk you into polyamory. It just forced a monogamous person to be in a polyamorous situation.

#+begin_quote
  Many values are hardcoded, perhaps by culture, perhaps by biology, and when triggered cause aversion and disgust.
#+end_quote

The human brain is much more plastic than you give it credit for, I think. A phobia is a "hardcoded" value---and yet exposure therapy removes it. I'm suggesting here that the protagonist would basically be giving these people long term cognitive-behavioral therapy to detach them from the cultural aversions et al. stopping them from being happy with their choices, because he sees that as the surest path to global utility. (Now that I think of it, it might be more efficient, in some the cases, to---using the same therapeutic approach---remove one of their friend's desire/attraction to another of their friends. But not all.)

#+begin_quote
  the sadness can be overcome by avoiding the stimuli
#+end_quote

We're both talking about characters in romance settings here, right? They're not allowed to do that. (Picture, specifically, the characters of Edward Cullen and Jacob Black in Twilight if it helps. They both have---at least, what at first appear to be---spiritual bonds to the same person. "Removing the stimuli" actually /hurts/ them in an active, persistent sense. This particular Gordian knot was later cut by authorial fiat, but most aren't. Of course, most such bonds are more metaphors than literal fantasy plot-devices, but the same effect is achieved.)