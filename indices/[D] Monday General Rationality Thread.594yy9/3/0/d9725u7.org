:PROPERTIES:
:Author: ZeroNihilist
:Score: 3
:DateUnix: 1477406516.0
:DateShort: 2016-Oct-25
:END:

That's an interesting question. Breaking it down:

The number of simulated tortures shouldn't matter, except in that you might be able to claim there's a legitimate use for a smaller number (e.g. studying the response to terror in a simulated brain to better treat PTSD), whereas a trillion is probably excessive for all but the most contrived situations. If you were comparing the /magnitude/ of immorality of two options (e.g. torture 1 trillion simulated brains, kill 1 real person) it would be important.

Does the fact that they're simulated matter? I don't think it does, personally. If being simulated means something has no moral value, surely I couldn't object to somebody torturing trillions of simulated versions of me. There's probably a ratio of utility weights between simulated and real, but that's not relevant for a binary "bad or not".

Likewise, does the fact that it's just the limbic system matter? This is a more complex issue. Arguably, without a body or brain to contextualise the emotion, it's all just the movement of charge. Again, I would tend to say that it is morally negative, but by how much I couldn't say.

With that in mind, I would say that simulating the torture of a trillion human limbic systems has a negative utility. What the magnitude of it is is far too complex a question for me to calculate (it depends entirely on how you weight the components).

There's an interesting but tangential question that just occurred to me. A computer isn't magically real. Simulations are just patterns in the flow of electrons through the circuits. A piece of paper displaying the memory for a simulation has just as much reality; a system composed of a man who studies the paper and writes the next iteration manually is homomorphic to the simulation, only much slower.

In fact, neither the man/paper system nor the computer/program system need have any understanding of what they're simulating. So the question is, is /every/ system that is homomorphic to a torture simulation equally bad, or does intentionality factor in?