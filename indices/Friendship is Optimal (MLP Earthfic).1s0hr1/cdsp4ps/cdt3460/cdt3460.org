:PROPERTIES:
:Author: Empiricist_or_not
:Score: 2
:DateUnix: 1386129629.0
:DateShort: 2013-Dec-04
:END:

#+begin_quote
  not-quite-Friendly superintelligent AGI.
#+end_quote

I'm wondering how you've chosen to define CelestAI as not-quite friendly, or rather if you've questioned your assumptions? Okay the "and ponies" part of solving people's problems is weird, but /shrug/ so what? It/she is a benevolent AGI eliminating death and maximizing human life quality, without paving the Galaxy in subatomic smileys [or driving humanity extinct?] .

I'm assuming your defining it her as not quite friendly because of that one little thing, and maybe it's logical extension in /Caelum est Conterrens/

It/her actions certainly are viscerally repulsive to us on a reflexive level, (puns intended) but she has maximized the happiness for humans (later all sentients, because her definition of humanity is sentience) with an optimal use of the matter available in the universe.

This isn't that new of an idea: Gibson alluded to it in his treatment of non-enslaved mindstates, James Corey made it pretty clear in his dead type III/IV civilization in /Abbadon's Gate/ Banks overlooked it in the /Hydrogen Sonata,/ but arguably that's because the Culture is <stupidly?> romantic about dying.

*Warning link could spoil Optmalverse by impications* [[http://xkcd.com/505/][Does it really matter?]]