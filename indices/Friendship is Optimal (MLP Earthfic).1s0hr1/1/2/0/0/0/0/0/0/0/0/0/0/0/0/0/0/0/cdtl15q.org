:PROPERTIES:
:Score: 1
:DateUnix: 1386188430.0
:DateShort: 2013-Dec-04
:END:

#+begin_quote
  And this is what makes me object to simulated realities. I'm fine with a "simulation" that I can treat like a piece of real estate: step in or out of my own free will (even if I rarely go out because I'm a massive nerd).

  Unfortunately, almost nobody has ever actually proposed such a thing. The general rule for simulated-place-to-live proposals seems to be, "Hey everyone, I'mma make us a totally awesome simulation, and you're going to climb in and NEVER LEAVE! Won't it be AWESOME!?"

  Which results in me facepalming, because my exposure to TVTropes has rendered me capable of differentiating between a Pocket Universe and a Lotus Eater Machine and I don't understand why people insist on proposing them together every damn time.
#+end_quote

Agreed on all accounts.

#+begin_quote
  You know how Yudkowsky was reportedly unsure of which option in Three Worlds Collide was the good one? You know how there are people who misclassify this as a successful FAI? You know how there are people who think Harry James Potter-Evans-Verres is a good and rational person?

  I mean, hell, you know how Yudkowsky made up his own god/demon-grade monster that can supposedly exist in real life, called an AI ;-)?
#+end_quote

You have to admit /Three Worlds Collide/ isn't completely clear cut, though. Both options are pretty bad, even if you've convinced me about which one is less bad.

As for AI, I.J. Good was the first to talk about the concept of seed AI (the name is by Yudkowsky) back in '65 and I'm /fairly certain/ the only part Yudkowsky himself invented was the Friendly one.

#+begin_quote
  Much of the clade known as "rationalists" creep me the hell out, and often seem like a cult. Maybe it's just me, but I never feel sure if I'm in enemy territory or not.
#+end_quote

/shrugs/ I feel that way sometimes, too. I especially feel it in [[/r/hpmor]] or LessWrong itself where sometimes Yudkowsky's name is all but spoken in hushed tones of worship. Every cause wants to be a cult. That's also in LessWrong.

But there /is/ also the danger of looking to both sides and nervously asking, "But this isn't a cult, right?" What /is/ a cult? What does it take for a cause to become a cult? What exactly are the negative aspects of a cult, and how often do "rationalists" exhibit them? What's the base-rate for cultishness? Do "rationalists" actively avoid cultishness?