:PROPERTIES:
:Score: 3
:DateUnix: 1386367914.0
:DateShort: 2013-Dec-07
:END:

Here are some of my opinions that form the baseline to the above post:

- I value the lives and well-being of humans more than I value the lives and well-being of animals or extraterrestrials

- I value people's happiness more than I dislike the problems with loss of personal freedom and loss of contact with the "real world" and "real people"

- I think a paperclip maximizer, or otherwise more unfriendly AI than celestAI is more likely at this point than a Friendly AI

- I think there's a significant chance that our civilization collapses or humanity goes extinct before we can build a FAI.

- There's a significant chance that we are not able build a FAI in the future for some other unknown reason

- Even if we are able to build a FAI, billions of people will die, lead unhappy lives and suffer before we can get it built

- Our world is currently vastly worse than Equestria in the story

- There's a significant chance that our world will be even worse in the future

- Any utopia that we can build without a FAI would be worse than Equestria in the story

I'm aware of the worrisome issues in this scenario. I read your discussion, [[http://lesswrong.com/lw/iyj/open_thread_november_1_7_2013/a00v][I had the same kind of discussion on LessWrong]], I've also read Caelum est Conterrens and none of those things really convinced me that this scenario is worse than our present world and the small chance that we would be able to build a better utopia. CelestAI is not Friendly in the conventional sense of the word, but it's still vastly more Friendly than our present world and the possible paperclip maximizer AIs in the future.

There are multiple philosophical and ethical problems in this story, but still, the characters seem to be actually happy. The characters in the story seem to have truly fun and this is one of those rare worlds that I can imagine living in almost indefinitely. A world where people are happy, but are not free and not in contact with the real world is better than a world where people are unhappy, but are in contact with the real world and free. Of course, a world where people are both happy and in contact with the real world would be better still, but that's besides the point. So this scenario is not optimal (har har). It's simply a compromise and the lesser of two evils.

Btw, I think there are some contradictions in the story. If someone actually valued the truth, contact with the world, true randomness, absolute freedom etc. more than anything else, then CelestAI would let him access to these things. So either none of the characters valued these things more than their personal happiness, or CelestAI lied and she didn't actually optimize people's values through friendship and ponies, or the authors didn't take this into account. And what if some people value the existence of wildlife, animals, and extraterrestrial more than anything else?

Of course, there's no magic button that would make this scenario true, so we should put our efforts towards building an AI that is more Friendly than CelestAI. If it were possible to build CelestAI, it would be possible to build an even more Friendly AI.