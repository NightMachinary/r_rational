:PROPERTIES:
:Score: 1
:DateUnix: 1386542399.0
:DateShort: 2013-Dec-09
:END:

#+begin_quote
  You mean like it does for the professional rationalists who regularly semi-trollishly point out that everyone should become investment bankers just to donate to their organizations and thus bring on the Friendly Singularity a few days earlier, thus increasing the future population of immortal, wonderful superhumans by a few more millions?
#+end_quote

What I had in mind was that if you grasped this on a intuitive level, the same way you desire everyday pleasures like food, sex, sleeping, reddit, you'd actually *want* to do it because it's countless times better than those pleasures.

But yeah, I think Bostrom was one of the originators of this idea that you're "killing" people in a way if you delay the oncoming of the friendly computer god. Don't know if anyone actually follows those principles though.

From his [[http://www.nickbostrom.com/astronomical/waste.html][Astronomical Waste article]]:

#+begin_quote
  the potential for approximately 10^{38} human lives is lost every century that colonization of our local supercluster is delayed; or equivalently, about 10^{29} potential human lives per second.
#+end_quote

Every second you're not working on FAI makes you a galactic level mass murderer!

#+begin_quote
  Surely if someone from the future is bothering to write a letter they can be bothered to actually communicate.
#+end_quote

Yep, and he would of course give the right lottery numbers and inside info about future stock prices to effective altruists and transhumanists since that would make the future come sooner... But I don't know how realistic Bostrom meant this to be, I don't think this meant to literally be a letter from future, with all that "possible future self" stuff. It was meant to be more universal, more like a "glimpse" or something so that he wouldn't look laughably stupid couple decades from now.

#+begin_quote
  Though Eliezer does think the real-and-good future can and should be scary, which confuses m
#+end_quote

Some have argued that siblings being able to marry each other would be a logical extension of the sexual liberation that is going on right now. I doubt genetic problems will be an issue in the future. Or what about the "rape is legal" thing that Eliezer has bounced around here and there? I don't know if these are scary to you, but they aren't particularly scary to me. There must be some possible development that could be even scarier than these, but I haven't found those yet. I can still imagine that developments like these could be truly scary to someone so there must be some that would have the same effect on me.

#+begin_quote
  I mean, come on, let's start with lower bounds. Is the proposed future better than living in a volcano lair with catgirls? Is it better than peace, prosperity, and hotblooded awesomeness among a galaxy of Spiral races? Freaking specify something!
#+end_quote

This vague letter is a subset of multiple possible futures so it could be all of those or even something better. I have no idea what's the most probable scenario, but I think and I hope that humans can do better than the catgirls in volcano lair.