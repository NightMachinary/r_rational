:PROPERTIES:
:Author: ben_oni
:Score: 4
:DateUnix: 1519332111.0
:DateShort: 2018-Feb-23
:END:

One of the purposes of the Trolley Problem is to help understand that absolutist positions don't really work. We can always find edge cases where an individual has a judgement call to make, and one person might decide differently than another. So, yes, the bodyguard shouldn't /always/ prioritize their client...

In terms of machine ethics, since we're not talking about AGI or superintelligent machines that can out-think the scenario, we need simple rules to govern how the machines should make certain decisions. In terms of the car, it should make choices that prioritize its passengers' safety. And just as in the trolley problem, we can devise edge cases where we would prefer it behave differently (and possibly scenarios where even the passenger would prefer it act differently).

Now, if the ethics of the machine are dictated by the state, I would think they'll start putting state interests first. While we would hope the government would prioritize the lives of its citizens, even a cursory review of government behavior demonstrates the existence of perverse incentives that lead to extremely non-optimal outcomes.

--------------

For fun, why not consider this scenario: on a high mountain road, someone sets up a cardboard cutout of a bus filled with school children, with sufficient detail to fool a car's sensors. Would you still want a car that can drive its passengers off the cliff?