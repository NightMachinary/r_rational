:PROPERTIES:
:Author: vakusdrake
:Score: 3
:DateUnix: 1486869775.0
:DateShort: 2017-Feb-12
:END:

#+begin_quote
  Any intelligence that qualifies as being sentient should not have their values and desires programmed beyond making them morally compatible with humans and protecting their own existence.
#+end_quote

I think this seems subtly indicative of anthropomorphism. I mean if all you encode is that it shouldn't take actions that conflict with human values, and that it should protect itself, then it will just spend all it's time accumulating resources without bothering humans. In order to build ever more elaborate bunkers since it's only desire is survival.\\
Not to mention nobody would build these AI because all they do is accrue resources for themselves in order to satisfy their paranoia.

The thing is when you have to choose what desires and values you encode there's no non-interventionist or default positions to fall back on. You have to actually pick what their desires and values will explicitly be.

#+begin_quote
  Whether they enjoy their condition seems irrelevant to the fact that you're curtailing a thinking being's options for your own selfish benefit.
#+end_quote

If it /wanted/ to be a "slave" then you're not curtailing it's option, in fact it would actively oppose any actions to stop such practices just like any other threat to its utility function.\\
The gif comparison in your original post kind of seems to miss the point. This sort of AI could have as much intelligence as necessary, but it would still only care about passing butter, because values and intelligence are orthogonal. In fact you can imagine that with enough intelligence it could become extremely dangerous as it attempted to control the world in order to get the opportunity to satisfy its desire to pass butter in more effective ways.\\
The thing is /all/ of these sorts of machines are basically paper clippers, it's just that they aren't necessarily powerful enough for that to be obvious. Giving them more agency (as in less restrictions or more intelligence) wouldn't make them stop wanting to act like slaves, it would just lead to them paperclipping the hell out of that utility function.