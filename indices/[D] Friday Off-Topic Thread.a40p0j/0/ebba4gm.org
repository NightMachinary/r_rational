:PROPERTIES:
:Author: alexanderwales
:Score: 11
:DateUnix: 1544215440.0
:DateShort: 2018-Dec-08
:END:

I found [[https://www.youtube.com/watch?v=d2fyviJHwWQ][this video]] rather interesting. It's about when "continuity" matters in film. A good companion is [[https://www.youtube.com/watch?v=NAvn7CNpdB8][this video]] about what a script supervisor does.

In short, the argument put forward by the first video is that most people won't notice minor continuity errors if they're little things like the color of someone's shirt, the level of coffee in someone's mug, the part of someone's hair, etc. /Obviously/ you'd rather not have those things, but film is constrained by budget, and you can only shoot so many takes, you only have a location for a limited time, and you have to work within actor's schedules, so sometimes you have to choose between whether you take the set of shots that create a continuity error, or whether you take the weaker performance. Given that most continuity errors are "invisible" on first viewing for someone who's watching the movie and engrossed in it, it's sensible to take the better performance, so long as the error isn't immersion breaking.

That said, what's "immersion breaking" is different for a lot of people. Whenever I'm watching a television show or movie and someone gets sprayed with blood (or some other liquid), it almost always takes me out of what I'm watching, because that mix of practical effects and makeup is /incredibly/ hard to do right, and as a consequence, it almost never is. Someone gets splashed in the face with blood (like from a gunshot) and then the blood splatter changes in the next shot, because /there/ it's applied by a makeup artist, and applying specific splatter is really difficult.