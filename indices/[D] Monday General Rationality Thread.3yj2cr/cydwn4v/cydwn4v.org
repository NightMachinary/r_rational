:PROPERTIES:
:Author: tvcgrid
:Score: 17
:DateUnix: 1451320963.0
:DateShort: 2015-Dec-28
:END:

I've been reading Superforecasting by Philip Tetlock (5 chapters in) and also doing a semi-simultaneous summary of it as I read (instead of the usual margin notes or whatever). Surprisingly handy to have a small, thin notebook alongside the thing I'm reading... I ended up making all sorts of asides too.

Superforecasting is about recent findings about certain methodologies of forecasting that seem to have very good results. IARPA (apparently that's a thing now, as of '06, because of the 2002 iraq wmd fuckup it seems) conducted a tournament over 4 years to figure out if there's better ways to make forecasts. Each team had full control over how they actually produced the forecasts. Topics ranged from political to economic, domestic to international and so on, and the forecasts needed to be submitted daily (I think). The Good Judgement Project beat out everybody, even people with access to classified info, with handy margins, consistently over 4 years, and with growing effect each year. Lots of interesting details about how this was done in the book.

Turns out, a forecast is not worth much if it's not time bound and if it doesn't pass the clairvoyance test -- if you can see the future state of the world perfectly and still can't thumbs up or thumbs down if the forecast is correct or not, it's not a good forecast. There's more to it too... using actual numbers to indicate probability and so on. Lots of tidbits and details.

It's also a book I keep recommending to friends because of how broadly accessible it is. It nicely summarizes a whole bunch of common rationality topics like base rate neglect, regression to the mean, system 1 vs system 2 thinking, not understanding randomness, etc.