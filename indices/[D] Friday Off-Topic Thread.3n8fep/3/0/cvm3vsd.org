:PROPERTIES:
:Author: Transfuturist
:Score: 18
:DateUnix: 1443818995.0
:DateShort: 2015-Oct-03
:END:

White bisexual trans female, apparent male. Arizona. I was raised as a nondenominational Protestant. I was also raised as a science-lover, my biggest influence being Douglas Hofstadter.

Previously I saw Christianity as having a God-directed mission, and I was wondering why Christians didn't act consistently with moral duties laid out in the bible. At the same time, I was learning more and more science, and some moral philosophy. Evolution was never discouraged in our household, though I opposed it in middle school because it threatened my worldview. With some help from atheist writings, including Common Sense Atheism, I eventually learned what evolution was, learned what the problem of evil was, and the entire secular, empirical side of my life clicked into place and simply squashed my religious upbringing.

I have always been interested in AI research, and motivated thinking prevented me from really seeing AI safety as a problem. I dismissed the question of malice naturally, and at the same time I did not think value alignment was a problem, because AIs could approximate human value systems, and people have never tried or wanted to take over the world or anything. Continued exposure to MIRI's research, the Sequences, and Eliezer's deprecated publications (General Intelligence and Seed AI, Creating Friendly AI, Coherent Extrapolated Volition), convinced me that AI safety /was/ a serious problem, and that I had been wanting all along to build a gun without a handle.

My primary goal is to live as long as I like in a body I enjoy. This is contingent on safe AI. I was previously somewhat scornful of MIRI's research, even (especially) after I became convinced that AI safety was a problem, because all of their publications focus on incomputable agents. Then a friend reminded me that publishing research on bounded agents destroys the world. This makes me wonder if MIRI has a section of unpublished research purely for internal use. This makes me want to join MIRI.

I don't think I can join MIRI.

I am nearly twenty, and I feel old. Time is speeding up, and I can't keep the pace. I find it extremely hard to hold focus on one project for over two weeks, and my interests constantly cycle without ever progressing. Writing, AI, games, writing, AI, games. There are programmers I follow who are more capable than me, and I think one might be younger. I see the kind of people who /do/ join MIRI, Harvard, Stanford, MIT, and I wonder why I ever thought I could slack off in high school. I am constantly masculinizing, and I hate the shape of my body. I have no job. I can't drive. I'm going to community college, taking more classes than I think I can handle in order to avoid being forced to /get/ a job, and I'm wasting time not studying math and computer science this semester. I'm isolated from people I know offline, and I don't have the social skills to make new friends even if I managed to find people I thought could stand me. I'm worried that all of the tests I've taken since kindergarten were wrong, and that I can't hack it in STEM. I have had these thoughts and problems continuously, even when I was on antidepressants, and I'm worried that they're right despite my depression, and that I will never get past them. I don't want to die, but I would sacrifice my life as it is to improve the chances of eutopia for the rest of the world. I just want to be told what to do. I can't handle this social, emotional bullshit.

..Excuse me. I am very broken, I seem to be leaking.