:PROPERTIES:
:Author: Escapement
:Score: 5
:DateUnix: 1392300680.0
:DateShort: 2014-Feb-13
:END:

I think this argument would make more sense to me if you expressed it with concrete examples. As-is, I am not sure I entirely grasp what you are getting at, or in what situations your idea would apply. That said, I am going to try to address some of your ideas anyways. I may be horribly off-base here.

Fundamentally, whatever plan has the best possibility of working, tautologically has the best possibility of working. So if choosing between plans A and B for accomplishing a goal, you would choose whichever had the greatest chance of success after assessing all the factors affecting the chance of success that you can. If you have a plan that has 5 steps each with a 99% chance of going right, against a plan with two parts each with a 95% chance of going right, the latter is obviously worse than the former.

However, in most circumstances the number of parts of plan has no correlation with each part's likelihood of success, so the less complex is better - 4 stages of 98% vs 3 stages of 98%, for example.

Also, a complexity penalty is a method of determining probable ideas, not necessarily proving anything. This has a lot to do with ideas of parsimony of explanation for events - see Somonoff induction and related ideas (I think Less Wrong had a nicely written explaination somewhere?). If you have two ideas of how something could have occurred, then the simple explanation is more likely to be the actual reason than the more complex reason.

That said, if making plans against intelligent opposition then deliberately choosing what may otherwise be a normally suboptimal but unexpected option to defeat them is fairly common. A related concept from the world of fighting videogames is "Yomi", knowing the mind of the opponent - for example, say there are two plans, A and B, that side 1 could enact, and two counterplans, C and D that side 2 could enact. If side 1 chooses A then he wins if side 2 chooses D and loses if side 2 chooses C, and vice-versa if side 1 chooses B. Logically therefore each side has a 50% chance of victory and essentially no way to differentiate - however, if tested serially and the rewards of victory/failure are uneven somewhat, it is quite possible for a person in these circumstances to read another deeply enough to predict what their opponent is doing and respond with much greater success than random chance - much like Quirrell plays at a level deeper than Harry (in his own assessment, anyways, taking his words at face value - which may be unwise).

Of course, in the real world there are degrees of winning and possible failure associated with each plan and it's responses, and of course multiple variations possible on each plan, and no certainties of victory or defeat due to the vagaries of chance, and etc. ideally you could attempt to quantify it all a little and come up with some sort of decision theory of what plan to pick, but in reality things rarely work out so neatly as to be readily quantifiable.

That said, I would appreciate it if you would clarify your argument with more concrete, less abstract examples - I don't feel the above really struck to the heart of the matter, because I am not sure what exactly you are arguing in favor of goal-directed people doing in the face of intelligent opposition, and perhaps a more concrete example could get me on the same page as you so I could better discuss this topic.