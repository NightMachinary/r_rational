:PROPERTIES:
:Author: Sparkwitch
:Score: 9
:DateUnix: 1447787422.0
:DateShort: 2015-Nov-17
:END:

The three laws don't work because they're written in English. Human language is, in purpose and by necessity, sloppy and vague. It's functionally a compression algorithm whereby complex ideas can be expressed in seconds by leaning on context and shared experience.

"Harm" seems like a simple concept precisely because it is overwhelmingly general. The same applies to "orders" and "protect". Each can have dozens if not hundreds of distinct meanings in different conditions.

I can understand what the three laws are trying to do because I'm a human being with all the evolutionary wiring and cultural conditioning required to absorb their underlying ideals. I can also, were I required to follow them, choose to follow that understood intent based on my own interpretation rather than any theoretically universal one. Also I can disobey them when I feel something else is more important.

I cannot, however, figure out how to explain to a robot what "harm" is supposed to mean in every situation it might ever experience. Essentially as a specific case of a more general problem: I can't figure out how to teach a computer to understand English.

Worse, to paraphrase Hofstadter, a computer that truly understands human language may be just as capable of disobeying human "orders" as we humans are.