:PROPERTIES:
:Score: 1
:DateUnix: 1445286610.0
:DateShort: 2015-Oct-20
:END:

Fuller context:

In the probabilistic approach to cognitive science, we often observe that under tractability constraints (lack of both sample data and processing time), the mind forms very noisy but very simple and /still usefully approximately correct/ intuitive theories about various phenomena. We also know that as part of scientific reasoning, we invent theories of increasing complexity (of their deterministic causal structure) in order to increase the precision with which we can match our observable data, which we then obtain in large amounts so as to be increasingly sure of our inferences.

I want a way to quantify the sliding scale of precision and complexity from intuitive theories to precise theories, preferably by talking about the tradeoffs between Kolmogorov structural information (number of bits of deterministic structure) versus random information (number of coins flipped).

Oh hey, there's that concept. So it's actually pretty easy...