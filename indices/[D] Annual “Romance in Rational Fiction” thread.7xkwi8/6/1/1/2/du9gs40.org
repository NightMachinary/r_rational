:PROPERTIES:
:Author: Noumero
:Score: 6
:DateUnix: 1518654743.0
:DateShort: 2018-Feb-15
:END:

As per orthogonality thesis, an intelligent agent could have entirely arbitrary values, but still be rational if thon is optimizing for them rationally. Would an AI whose utility function is to maximize its proximity to a particular conscious human at all times be inherently "irrational"? I don't think so. Why would a human with a similar utility function --- or [[http://tvtropes.org/pmwiki/pmwiki.php/Main/IWantMyBelovedToBeHappy][an utility function fulfilment of which is dependant upon fulfilment of another human's utility function]] --- be irrational?