:PROPERTIES:
:Author: CreationBlues
:Score: 1
:DateUnix: 1546710089.0
:DateShort: 2019-Jan-05
:END:

Ok, let's follow your idea to it's logical conclusion. Let's imagine game developers create a Yog SogAIth, because it controls the "dream" of the game world.

The first rule (and pretty much only) of Yog SogAIth is that it is incapable of talking to human level intelligences, because a human can infer that what they're talking to is a human with an internal state per your rules. That means that any time Yog SogAIth wants to talk to someone, it has to spin up a servant. Hopefully it's servant does what it wants, because every time it starts to go off script Yog SogAIth has to destroy it and spin up a new servant mid conversation, seamlessly to everyone involved. This is actually much less bad than NPC's, because a DMNPC is allowed to have a lot more knowledge of what's behind the curtain and therefore adjust to whatever unknown unknowns the people it interacts with throws at it.

For players, Yog SoggAIth is dealing with a lot more constraints. Obviously, every NPC and NPC reaction has to be fine tuned to the current plot, quest, player group, and player the NPC interacts with. That means that the first player can have a lot of power over the NPC, which means the npc needs to get adjusted a lot, potentially multiple times per conversation. McPeasant gets replaced with McPeasant(likes red hair) gets replaced with McPeasant(likes red hair, improvisational jazz) get replaced with McPeasant(likes red hair, improvisational jazz, from fantasy florida) gets replaced with McPeasant(likes red hair, improvisatoinal jazz, from fantasy florida, has issues with authority). Remember, all of those people are distinct, and Yog SogAIth has to destroy and spin up new versions of them mid conversation, because of your requirement that there be some true version of McPeasant behind the mask.

Now, why is that story above true, why would engineers create a Yog SogAIth that is incapable of speaking directly to people? If an AI is capable of speaking directly to people, why does that act necessarily imply the creation of a fully fledged human mind inside the AI?

The misunderstanding seems to be based on what Acting pulls on. Acting is based on pulling from your own experience to put forward the impression of being someone else. A lot of human experience overlaps, so people don't have much trouble acting like someone else from their culture, but as the amount of experience that overlaps between them and their assumed persona shrinks, the person's ability to mimic that persona diminishes until it fails upon casual inspection. One of the idea's of the AI is that while it might be moderately smarter than a human, it is massively more parallel than a human and is capable of gaining more experience quicker than a human.

Your statement is that the fact that the AI is purposefully drawing on a more limited set of it's experience, operating under a restricting set of rules, causes a new person to fall out, seems suspect. An idealized McPeasant does exist, but the McPeasant presented by the AI is merely the AI's best guess at what McPeasant looks like, the AI using it's broad experience to limit itself only the the behaviors it's capable of that match McPeasant's capabilities.