:PROPERTIES:
:Author: ulyssessword
:Score: 2
:DateUnix: 1509686888.0
:DateShort: 2017-Nov-03
:END:

It's an inefficient use of computing resources.

Imagine a specific supercomputer, built of a array of 1m^{3} cubes. Each cube has enough processing capacity to simulate a 1 km cube of virtual space at 1 000 000x speed, and can communicate with its (physical) neighbors with a ping time of ~6e-9 seconds, or 1% of the speed of light.

The raw physical constraints of such a system means that at least 99.9999999999999% of physical detail is lost (which is fine, nobody cares about a specific iron atom in one dust speck), but also that the speed of long-range causality in the simulation can't be higher than 3 km/s, because neighboring realms can only communicate so quickly. Much like not simulating a specific iron atom in one dust speck, not simulating an accurate speed of light is a worthwhile tradeoff that allows for literally thousands of times as much utility as the alternatives.

At that speed, it would take over an hour to reach the other side of the Earth traveling straight through the core. It's likely that all of the useless empty space would be filled, but that's still a hard limit on travel.