:PROPERTIES:
:Author: nullmove
:Score: 1
:DateUnix: 1417019332.0
:DateShort: 2014-Nov-26
:END:

Hi, I have just read up most of this for the first time. Acausal decision theory is making sense to me on some level. Here I find EY's point sufficiently convincing though -- that the AI would have no reason to actually expand any effort in torturing people because that won't alter any past course of action, and hence won't be rational at all.

What I am wondering about is the means of carrying out the torture (the basis of the bargain I think). I have just read somewhere that the AI could loop a painful simulation of my source code. I am not familiar with this line of thinking at all. But why should I have even the tiniest bit of sympathy /right now/ towards a future entity that may well be essentially me? Not sure if I am expressing it properly, but I don't have the slightest motivation to think about the well being of something in distress that might even be an absolutely identical me. What is the reasoning that leads one to care? Btw scratching this itch is right now is more important to me than averting the wrath of a preposterous basilisk so please tell me. Or in the unlikeliest event that you deem it an infohazard then pm me, it even felt silly to write it but I find EY's original reasoning for deleting the post also convincing.