:PROPERTIES:
:Author: OffColorCommentary
:Score: 5
:DateUnix: 1416692533.0
:DateShort: 2014-Nov-23
:END:

Roko's Bassilisk is a future super-intelligent "friendly" AI. Because the single biggest moral imperative is to build a friendly AI as fast as possible, Roko's Bassilisk will brutally torture all people who:

1. Failed to do everything they can to build FAI as fast as possible.
2. Heard of this thought experiment so they are capable of being motivated by it.
3. Understands acausal decision theory.

Despite being a super-intelligent FAI, it apparently doesn't understand human psychology enough to know any of the several reasons this won't work on humans, such humans as responding poorly to threats, denying arguments if they dislike the conclusions, not inherently understanding acausal decision theory, and not being all that in control of how they allocate their effort.