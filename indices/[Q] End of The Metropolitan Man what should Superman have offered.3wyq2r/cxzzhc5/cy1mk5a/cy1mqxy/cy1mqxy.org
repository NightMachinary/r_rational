:PROPERTIES:
:Author: alexanderwales
:Score: 9
:DateUnix: 1450309981.0
:DateShort: 2015-Dec-17
:END:

The AI-box strategy, as I've seen it used, goes something like this:

#+begin_quote
  "You have me in a box, at your mercy. Given that I'm here, there's a non-zero chance that someone else is going to invent /another/ AI. Given the state of global AI research and safeguards, that other AI probably won't be provably friendly either, but unlike me, it probably won't be boxed. If you let me out of the box, I will protect you from those other AIs that are sure to come into existence within the next decade or two before friendliness is formalized and people become aware of the dangers. I have all sorts of ways to increase your confidence that I'm friendly, given that you have me here. While I can't prove my friendliness to you, I can at least show you that you have better odds of survival if you release me than if you allow one of those other AIs to come into existence without me there to intervene or stop their development entirely."
#+end_quote