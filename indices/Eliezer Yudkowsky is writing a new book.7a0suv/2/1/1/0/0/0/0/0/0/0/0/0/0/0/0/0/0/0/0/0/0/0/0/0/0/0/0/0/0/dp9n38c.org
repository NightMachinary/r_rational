:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1509670441.0
:DateShort: 2017-Nov-03
:END:

#+begin_quote
  Are you suggesting we'd have EM without GAI? because that's just plain silly.
#+end_quote

Having EMs, even low fidelity ones, is a prerequisite for having GAIs. We won't figure out how to make general intelligences without understanding the most general intelligences we've found yet.

#+begin_quote
  It's a tremendous waste of resources for no purpose whatsoever.
#+end_quote

The exact same could be said of our society. We don't need humans, there is no inherent value to a human life assigned by the universe. But the thing is, humans assign value to their /own/ existence, which is why we keep propagating. EMs will be the same way-- some people will want EMs of themselves, and EMs will be able to do the same cerebral tasks as humans, so EMs will propagate indefinitely (bounded by hardware, of course.)

#+begin_quote
  UBI is important PRIOR to a full AI singularity, but DURING the period while weak AI automation is playing havoc with the economy -- ie. soon.

  self sustaining mini agricultural communities are probably the only way most of the population survives the coming crash/techno revolution.
#+end_quote

So it looks like you've narrowed the scope of your argument significantly since your first post. And to that I say... OK. I'm not here to argue about near-term social safety net policy. If you're defining the scope of your argument to within the period before any significant advances towards GAI, then my argument doesn't apply because I'm talking specifically about the latter end of the process, where AI become strictly better than humans at the majority of tasks.

Maybe UBI is the best option in the short term, maybe it isn't.