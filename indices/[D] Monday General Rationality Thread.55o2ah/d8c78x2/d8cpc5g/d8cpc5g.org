:PROPERTIES:
:Author: zarraha
:Score: 4
:DateUnix: 1475531309.0
:DateShort: 2016-Oct-04
:END:

A rational agent seeks to maximize their own utility. Their own, not the world's. Everything you do is calculated to maximize your own happiness.

Now granted, if you aren't completely selfish then you will also value other people's happiness as well. People give to charity or do nice things for other people or try to save the world from AI, because the knowledge that they did a good deed makes them feel good inside. This can be modeled by applying an Altruism coefficient to other people, then any time their utility increases or decreases as a result of your actions, your own utility will change by the same amount multiplied by that coefficient.

So I enjoy watching movies, it makes me happy. If one hour of my time can benefit the world to make someone at least ten times as much as an hour of movie watching, then I might feel guilty about the movie and go help them. But if my hour of work would only benefit people by 2 hours of movie watching then I might not bother. The whole world might be better off if I did, but I'm not the whole world, I'm me.