:PROPERTIES:
:Author: eniteris
:Score: 2
:DateUnix: 1475613169.0
:DateShort: 2016-Oct-05
:END:

I like it. I've been brainstorming an interstellar "foe-operative" deckbuilding game, and it runs into similar problems.

*One:* I don't like fully cooperative games. Usually they end up with one person making all the moves.

That said, you still want factions and backstabbing. Especially backstabbing. Because nobody really likes being in second place, and if you're helped by an ally into first place, you have to expect a turnabout. This works fine in games with individual win conditions (Risk), but in such games there's less of an incentive to cooperate.

With a shared win condition, things get more interesting. It can't be an even win condition, otherwise the game is fully cooperative, so players are rewarded based off their contribution. But the loss condition is shared (everyone loses if things go wrong), which incentivizes players to work together. But as there is only one winner, they have to work together while working against each other.

As everyone is working toward the objective, any player who overtly opposes any player's progress will probably be teamed up against by all the other players (I haven't playtested yet, but it seems plausible). Thus, players must have hidden actions, or hidden agendas, to covertly achieve a goal perpendicular or opposite to the main goal.

*Two:* Since we're pretending to be cooperative, we need an External Threat. Otherwise there's no incentive to cooperate, as the biggest threat is other players. The External Threat has to be balanced so neither threat is overwhelming.

*Three:* Actual suggestions.

I would like to be able to both build your own AI and contribute to group projects on AI, that you can donate researched projects to. Theme-wise it could be military AI-development groups, where the government wants to race for AI, while the scientists would prefer to work together to not kill everyone.

Hidden agendas could be given out before the game (beat player x by n points), and players could have identities (military v. academic v. basement lab) which impact funding/development/research and agendas.

Risk seems really interesting, but rather than finding a 1d100 to check the risk, I think it would be more interesting to give each card a risk chance, then take all the cards that make up the AI, shuffle them upside down, then flip half? of the component cards one at a time, and if the risk exceeds a certain level, then you run into trouble. This encourages you to put multiple lower-risk components into your AI, but there should be a limitation on the maximum number of cards, or the maximum number of cards of a certain type.

Prototype testing could allow you to stop flipping cards and abort the test run. Similar to Blackjack. Do you hit one more time? or do you stand?

I'm not a fan of "Everybody Loses" (except for the External Threat). I think that failed Risk should result in a persistent global problem that makes the External Threat more difficult to take care of. Small overruns in risk (say, 101-110) could be a one-time hit to resource (loss of research, destruction of facilities). Greater overruns would cause persistent global changes (everything costs more, all players lose resources every turn), while large overruns would make it almost impossible to win (Risk-taking player loses instantly, every player loses a Component every turn). (Exactly what the penalty is could be determined by your flowchart)

An instant global loss may be the most efficient way about it, but it doesn't play well ("and the next card's a fifty. We all die. The end"). You have to inform the player "You lost because of this decision." but by pushing it back you get better player involvement ("Crap. Now there's a hostile AI that's actively trying to prevent us from developing other AIs") and also enjoyable comeback stories. But make them work for their win.