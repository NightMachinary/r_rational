:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1481058420.0
:DateShort: 2016-Dec-07
:END:

I think you are *vastly* overestimating how much of a resource cost humans would present for a singularity AI.\\
First off I think you might want to discourage biological humans from reproducing, or the superintelligence would just use super persuasiveness to convince them that biological life is a staggering waste of resources, because once everybody's digitized things become simpler and more efficient.

If a SI is using all the resources in it's future light cone, then simply by denying the few entities who wanted to reproduce at an insane rate to create offspring that would also reproduce at such a rate. You can avoid any kind of problem, because the resource cost of simulated human population expansion, can't /begin/ to keep up with the rate of resource acquisition of a singularity AI.

However if all that is wrong and FGAI ends up being vastly less powerful than this, then yeah I think you want to sterilize everybody. Only allowing a small number of extremely qualified people to reproduce, in order to match the rate of resource acquisition and the occasional suicide. I think the majority of people would even agree to this once they understood that the alternative is going to lead to staggering suffering for them and their offspring.