:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562529818.0
:DateShort: 2019-Jul-08
:END:

#+begin_quote
  I don't think the writer knew about paperclip optimizers, paperclip optimizers aren't really the most likely or scariest outcome, just the example of an AI gone terribly wrong. More likely scenarios are just god AI governed dystopias / badly calibrated moral AI gods destroying us all by accident.
#+end_quote

This is what I said.

#+begin_quote
  More likely scenarios are just god AI governed dystopias

  Do you have an argument for this? As someone sold on AI risk, this idea has never seemed like more than narrative-driven futurism than a genuine attempt at simulation to me, but I'm open to thoughts.
#+end_quote

This was your comment.

#+begin_quote
  I didn't change the topic. I've been consistent in what I was asking for the whole conversation. But I doubt this is going to be productive.
#+end_quote

Explain your question. Do you want me to give you an argument for an out of context phrase I used ? If so no thank you. If not elaborate, otherwise this is just a boring discussion.