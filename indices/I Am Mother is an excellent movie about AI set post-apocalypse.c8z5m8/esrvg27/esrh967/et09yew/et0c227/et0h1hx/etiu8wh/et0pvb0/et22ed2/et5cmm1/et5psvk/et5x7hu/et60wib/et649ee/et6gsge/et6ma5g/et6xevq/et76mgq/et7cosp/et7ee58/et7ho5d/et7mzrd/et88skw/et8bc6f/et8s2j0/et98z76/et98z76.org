:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562590202.0
:DateShort: 2019-Jul-08
:END:

#+begin_quote
  How would you even build a superintelligent AGI system that had any of these traits as their predominant failings? Like, what actually happens in your hypothesis that results in these failings?
#+end_quote

It doesn't need to be a predominant failing just one of them.. AGI isn't programmed initially to treat people independent of x variables as equals, this could lead to a biased socioeconomic structure.

SAI under the control of certain groups where it's programmed to give them advantages. Genies that only answer from X type people. AGI that isn't programmed to respect human : government / culture / preferences..

There are infinite ways things could go wrong, many of them dystopic..

Other than Genies and maybe some other examples I'm forgetting, most sufficiently powered AI would take over, either directly or indirectly.

We are talking about an alien god with near limitless power and intelligence, with a few goals it will pursue relentlessly with only a few limitations we instill on it before it's born and can't change afterwards.

If it determines some entity (country, state, group, individual) is getting in the way of it's goals there's little stopping it from removing it systematically in a way that goes around it's limitations..