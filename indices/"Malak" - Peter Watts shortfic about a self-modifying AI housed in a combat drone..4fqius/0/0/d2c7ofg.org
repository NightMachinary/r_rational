:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 16
:DateUnix: 1461272490.0
:DateShort: 2016-Apr-22
:END:

Yes. See, in real life, when you have unintended program behaviors like this, your national phone network crashes, your hard drive gets erased, or your cellphone dials everyone on your contacts list repeatedly at 3am without letting you hear their attempts to call back. Having software this buggy in an armed military drone is likewise strongly net-negative in expectation in real life. A /fictional/ story about a case where a software bug just amazingly happens to cause behavior for reasons that a human could map onto moral reasoning, does not mean that it is okay to have heavily armed drones with software of this quality. Good-story bias up the /wazoo./

I might actually be a bit peeved at this point with the "buggy AI algorithm causes aligned moral reasoning" genre of fiction. Peter Watts executed the trope way better than most, I mean, his AI does not suddenly get inhabited by an anthropomorphic spirit, but this is still NOT WHERE ALIGNED BEHAVIOR COMES FROM IN REAL LIFE.