:PROPERTIES:
:Author: AugSphere
:Score: 13
:DateUnix: 1461322685.0
:DateShort: 2016-Apr-22
:END:

I understand your frustration, but come on. Not all unexpected behaviours are equally unexpected. Bugs are not the result of some unknowable eldritch influence. Given the amount of testing such a system would presumably go through, it's likely that the resulting AI behaves /almost/ as intended, rather than displaying some seemingly random high-complexity behaviour.

I think there is even a case to be made that the AI is actually functioning properly as the people who programmed it intended. Saying that the AI is malfunctioning in this case is alike to declaring a microwave oven to be poorly designed because some moron decides to stick a living cat into it and turn it on. Engineers cannot foresee and prevent every way the idiots might misuse their product. They intended to install an 'ethical architecture' and they did that, it's not their fault that humans don't actually behave ethically (as defined by the said reasonable architecture). I'm mostly with [[/u/PeridexisErrant]] on this.

I guess it comes down to the question of whether the humans overriding the ethical constraints were actually acting reasonably or not. My impression from reading the story was that they were acting unethically, in which case the AI's ethical architecture recognising that fact doesn't really count as a failure.

Although, given the presumed difficulty of alignment problem, I guess one can trust this AI to be faulty in some way just on priors. From this perspective, it's surprising that it's faulty in this particular superficially moral way.