:PROPERTIES:
:Author: MisterCommonMarket
:Score: 3
:DateUnix: 1562324165.0
:DateShort: 2019-Jul-05
:END:

Nick Bostroms book, Superintelligence, anwsers all of your questions. There are many problems with giving AI a goal of creating 10 paperclips and then stopping their creation. It would take pages to go through them all with nuance (read the book, it is amazing) so i will focus on two.

Lets say the AI creates the 10 paperclips. If one of them was destroyed, it would not have 10 anymore. This is not acceptable because its task is to make and be in possession of 10 paperclips so the AI gets rid of all threats that could destroy a paperclip and to make sure it always has 10 paperclips, it converts all matter that is not used to protect the paperclips, in to more paperclips. If some are lost or destroyed, it still has the required number.

The second problem is a matter of probability and confidence (maybe not the right word for this?). The AI makes 10 paperclips. How sure is the AI, that it has made 10 paperclips and that it has 10 paperclips? Not 100% sure. It could have made a mistake. It's programming could be faulty. There might be a glitch. It's sensors might be mistaken. How sure are you, that the sun will rise tomorrow. Pretty sure right. But you would not give this event a 100% probability. Lets say the AI is 99.99999% sure that it made 10 paperclips. That means that it will convert all matter to more paperclips, to make sure the probability of it having 10 paperclips is as high as it could be. Even when it has converted the entire universe into paperclips it cannot be 100% certain that it has 10 paperclips. It could be hallucinating the entire experience.