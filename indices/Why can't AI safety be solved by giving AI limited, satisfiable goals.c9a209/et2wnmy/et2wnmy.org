:PROPERTIES:
:Author: IronPheasant
:Score: 1
:DateUnix: 1562424016.0
:DateShort: 2019-Jul-06
:END:

AI Safety is something a lot of people have been trying to solve for many decades now. Basically zero "good" ideas have come from it. "Good" being defined as "all of this will make it so I can trust the superintelligent genie to take care of us like we would like to be taken care of."

If you want some easy and entertaining material, Robert Miles has a series of youtube videos on the matter (a few additional ones on the computerphile channel): [[http://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/videos]]

There are so many pitfalls:

- Value alignment (Even if you can do it perfectly: /whose/ values?)
- Value drift
- Reward hacking

And so on.

#+begin_quote
  It wouldn't be incentivized to trick us into letting it out of the box because making the best use of the resources it has
#+end_quote

Hobbling a system to make it less powerful in exchange for safety is one of the things often discussed.

A system that doesn't want to and is unable to improve itself isn't going to be much of a magic wizard, however. And self-improvement opens a huge lane for drift.

(Also, gathering more processing power to run simulations in higher quantity and quality is always going to be an intrinsic instrumental goal of any General Intelligence.)