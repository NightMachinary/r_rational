:PROPERTIES:
:Author: lumenwrites
:Score: 2
:DateUnix: 1562294547.0
:DateShort: 2019-Jul-05
:END:

That's a good argument. One thing confuses me though, the alternative is trying to design the value-driven AGI on the first try? So the strategy is to try to create AGI that pursues properly aligned values, does take over the world, and then prevents other AGIs from being made?

I just thought it would be safer to try to create narrow-goal-focused AGIs and trying to keep humans in charge for as long as possible.