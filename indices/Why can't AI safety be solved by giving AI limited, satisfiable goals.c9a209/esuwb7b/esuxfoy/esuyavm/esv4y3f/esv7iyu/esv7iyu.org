:PROPERTIES:
:Author: Detsuahxe
:Score: 13
:DateUnix: 1562294969.0
:DateShort: 2019-Jul-05
:END:

You fail to understand my point. The former is a solution, the latter is a holding action. Pursuing number 2 presents an illusion of safety and control, but all it really does is (at best) delay the inevitable. It may actually hasten the inevitable unleashing of a maximizer, because it advances technology and spreads knowledge about AI without sufficient safety measures in place to prevent that knowledge from being misused. (Although, in fairness, there are no sufficient safety measures when talking about world-ending threats.) Again, it only takes one misstep when using AI to destroy the planet.

Lemme try a metaphor. Hypothetically, what if you could choose to do one of these things:

1. Give every human being on Earth a small amount of magic. This magic could make their lives easier, make them healthier and stronger, etc. It's powerful and versatile, and it even has the benefit of being stronger on defense than offense, meaning it's not very useful as a tool to harm others. However, each time a person uses their magic, there is a very small, one in a trillion chance that they'll spontaneously explode and open a portal to a hell dimension inhabited by immensely powerful, sadistic demons, each of which can open more portals.
2. Create a god. This god would be vastly powerful, easily capable of creating and destroying entire planets, with incredibly precision control. You can create this god with any properties you wish, but once it's made, it's irreversible. If you create it wrong, that will go very poorly. If you create it right, you've basically created a force of infinite good.

I know, not a very subtle metaphor. The first choice offers an illusion of safety, because you're not upsetting the status quo too much, and you're giving power to humans. But in the end, it really represents a ticking time bomb, because the power in question can't be safely used by humans, by its very nature. The second choice is scary, because it has the potential to go much more horribly, permanently wrong. But, unlike the first choice, it has a very real chance of /not/ going wrong, and being a genuine benefit. Neither choice is ideal, but only one has even a chance of not killing us all, in the long term.