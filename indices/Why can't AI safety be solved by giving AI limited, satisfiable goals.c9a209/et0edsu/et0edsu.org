:PROPERTIES:
:Author: fassina2
:Score: 1
:DateUnix: 1562377595.0
:DateShort: 2019-Jul-06
:END:

#+begin_quote
  It wouldn't be incentivized to trick us into letting it out of the box because making the best use of the resources it has and then shutting down would be the thing it ultimately wants. And then human scientist uses his own judgement to make sure there aren't any other issues with the proposed solution.
#+end_quote

We can't guarantee it won't find another way outside of the box. *For all we know thinking we need to let it outside the box might be as silly as a spider thinking it could starve humans to death by not giving us web..*

​

It thinks at the speed of light in a few minutes it could easily calculate insane ways to manipulate physics or quantum interactions free itself in the most optimal way and do whatever it likes while following it's silly goal indefinitely.

Also human error.

#+begin_quote
  As far as I understand it, the problem with AI safety is that AI needs to have values/goals to do things, and if those aren't aligned with our values, it'll just keep pursuing them indefinitely sacrificing everything in the process(converting all the matter into infinite amount of paperclips).
#+end_quote

This is an example of a really badly rogue one going awry, what would happen if a rogue AGI created itself with a silly pre programmed goal in a self learning / improvement method.

But the self learning is likely to be the main method we have available to create it. We probably won't program it ourselves. It will likely be weird algorithms. And because we are not really making it we don't have 100% control, this means we need to instill safety nets before it starts running.

The more likely bad outcome is not necessarily full human extinction but a dystopia of some sort we can't ever get out of because the AI is as close to a god as something would ever be, and changing it's goal would be impossible.

Because it would be technically make it impossible to accomplish it's goal, so it would do anything possible to avoid it, and if AI god doesn't want us to do something we just can't..

​

PS. Finite goals are just step goals, there are very few self actually contained goals. Climbing mt everest for instance might just be something somebody interpret as a required accomplishment that individual sees as necessary to his overhaul goal of living a good life..

​

Say the goal is curing cancer, it might decide to change it's definition of what cancer is, or create new cancers to cure, or delete the cure it created and any way it knows to cure it to find a new way to cure it again, it might change what it interpret what cure means, it might change it's interpretation of it's own safety paradigms or limitations..

​

PSS. We should always assume the AIs would be completely alien, just imagine how alien an intelligent arthropod would be to us. AIs would be several magnitudes more alien than that. They are immortal beings with no reproduction or any other needs, that think at light speed instead of chemical speed, with perfect memory and insane capabilities.