:PROPERTIES:
:Author: Veedrac
:Score: 33
:DateUnix: 1562301408.0
:DateShort: 2019-Jul-05
:END:

More than this, /not/ specifying the full range of things we care about means the AI is indifferent to human wellbeing. It means if the AI did think that the best way to achieve its goal involved quickly wiping out the human race, or misleading its inventors and going dark, or duplicating itself onto every accessible computer system, or making a quintillion paperclip variants for redundancy, it would have no motive not to do so. When we're talking about trivial constrained goals like ‘make me 10 paperclips' we /might/ be able to reason over the complete space of reasonable possibilities (though ‘respect the unknown unknowns' is always good advice), but as soon as we ask it to do anything intellectually sophisticated our ability to model it like that flies out the window. Generally, if your safety relies on knowing exactly what the AI is going to decide on doing before it decides it, you're adding no power but much needless risk by letting the AI do that deciding.

Further, the difference between having a goal to solve a task (what most AGI is assumed to be doing) and incidentally being optimized to solve a problem (like current ML models) is very large, since the former comes with instrumental goals like ‘become more certain' and ‘become smarter' and ‘prevent the humans from changing their mind and asking me to do something else'. Those are dangerous.