:PROPERTIES:
:Author: Detsuahxe
:Score: 4
:DateUnix: 1562297327.0
:DateShort: 2019-Jul-05
:END:

That's a valid argument. It's something worth considering. If, hypothetically, you could demonstrably prove that it's incredibly hard to make an AGI, and it will /always/ be hard, no matter how much technology progresses, that would be evidence in favor of your idea of limited goal-AGI as a serious long term strategy.

But, since at present that doesn't seem to be the case (At the very least, there is no direct evidence to support it) we basically need to take the possibility of an AGI with a bad utility function destroying the world seriously, which means trying to figure out how to make a "good" AGI, despite the risks.