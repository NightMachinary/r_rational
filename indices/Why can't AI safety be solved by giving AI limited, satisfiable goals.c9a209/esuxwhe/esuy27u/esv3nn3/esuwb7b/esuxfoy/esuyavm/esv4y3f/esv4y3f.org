:PROPERTIES:
:Author: lumenwrites
:Score: 3
:DateUnix: 1562293880.0
:DateShort: 2019-Jul-05
:END:

If we have have 2 options:

1. Try to create the aligned AGI that has good values, pursues them indefinitely, takes over the world, and steers it in the right direction making sure no evil AGIs are created in the future.

2. Try to create specific AGIs with narrow goals that solve the problems we want AGI for, but are less likely to want to want take over the world and control everything, and then try to control how these AGIs are used.

Is it safer to try to create an aligned AGI that indefinitely pursues some set of "correct" values, than to stick to creating AGIs with narrow specific goals that solve specific problems?

Both seem dangerous, but I think the 2nd one seems safer, because in the 2nd case humans remain in charge.

Although if AGI with runaway values is inevitable - then yeah, we have to figure out how to build the right one eventually anyway. But isn't it easier to first try to create a specific-goal-driven AGI instead of the indefinitely-pursuing-values AGI?