:PROPERTIES:
:Author: Detsuahxe
:Score: 26
:DateUnix: 1562289956.0
:DateShort: 2019-Jul-05
:END:

Sorta. You're solving the problem by avoiding the problem. What you're talking about is basically a script, not an intelligence. General AI is viewed as scary for a few reasons which don't apply to the limited intelligences you describe. It only takes one person failing to be safe to accidentally unleash a maximizing intelligence on the universe. Solving /hard/ problems requires a maximizer, in the sense that you want your AI to be able to self-improve so it can smart enough to solve the problem. Even if you give your general AI a "safe" goal, the same genie logic applies, where if you don't describe it well enough, it will maximize past the goal you thought you gave it. Et cetera.