:PROPERTIES:
:Author: serge_cell
:Score: 1
:DateUnix: 1562431999.0
:DateShort: 2019-Jul-06
:END:

If we assume extremely high computing power AI safety could be solved by AI. There is such thing as "inverse reinforcement learning" which infer values (goals, reward) from large records of behavior. Take slice of human history, infer values from it and give it as goals (or regularizes, or constrains) to AI. To make it more in line with "pursuit of humanity" it could be not current values but extrapolated, direction into which values evolve. Of cause it may happens that actual values towards which humanity evolve are not quite flattering, but AI couldn't be blamed for it.