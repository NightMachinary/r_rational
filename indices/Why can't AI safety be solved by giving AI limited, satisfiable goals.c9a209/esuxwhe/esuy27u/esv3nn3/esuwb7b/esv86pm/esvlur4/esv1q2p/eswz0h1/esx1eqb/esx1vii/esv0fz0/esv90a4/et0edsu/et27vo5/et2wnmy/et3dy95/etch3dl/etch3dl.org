:PROPERTIES:
:Author: OrzBrain
:Score: 1
:DateUnix: 1562684611.0
:DateShort: 2019-Jul-09
:END:

#+begin_quote
  Instead of "make more paperclips" value, can it just have "make 10 paperclips and then stop" value?
#+end_quote

So you're saying it would it care a lot about determining how many paperclips it's made, and making sure it doesn't make more (or less)? And you can't see how giving these goals to a potentially godlike power could go wrong?

Step 1: Bootstrap to superintelligence. Because that will be an instrumental goal for any other utility function you give it. Because everything's better with superintelligence.

Step 2: Make 10 paperclips and then stop.

Step 3: Convert all the rest of the matter in the universe into hypertech processors and sensors to make absolutely sure there are exactly 10 paperclips. You wouldn't want it to make a mistake, would you?

Axiom: Any given utility function or set of utility functions if fulfilled with sufficient power (intelligence) becomes a horrible absurdity.

What's that you say? You'd like to add a limit on how many resources the AI can devote to paperclips and supporting infrastructure? Sure!

Additional step: Exploit the holes in your patch to spawn a daughter AI that will convert all the rest of the matter in the universe into hypertech processors and sensors to make absolutely sure it didn't use any more than your specified resource allocation on paperclips and supporting infrastructure.