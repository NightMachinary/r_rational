:PROPERTIES:
:Author: lumenwrites
:Score: 3
:DateUnix: 1562292487.0
:DateShort: 2019-Jul-05
:END:

The way I understood it, the main reason scientists are worried about AGI is that it's likely to have it's own goals that are at odds with the goals of humanity, and it would do whatever it wants with the world regardless of what humans want. And for various reasons, any AI we design is very likely to follow that pattern.

If we can design an AI that does what we want - it will still be very dangerous, but it's a different problem. We have nuclear weapons, and it only takes one or few crazy humans to activate them and destroy the world. It is a big and scary problem, and it's very dangerous. But we don't have nukes that really want to activate themselves and are hell bent into tricking us into launching them.

Do you see the difference? I'm not saying that what I wrote would solve all the problems with AI. I'm just suggesting a utility function, that, in my opinion, wouldn't make the AI in itself, intrinsically, likely to want to get out of the box and murder us all. That's the part that I'm curious about.

If somebody fucks it up and does it wrong, yes, it would kill everybody. But if it is done right, would it work?

Designing an AI with limited goals just seems like an easier problem to solve than designing AI with values it wants to pursue indefinitely.