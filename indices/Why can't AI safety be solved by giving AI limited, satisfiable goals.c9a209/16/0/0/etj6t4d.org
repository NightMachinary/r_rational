:PROPERTIES:
:Author: OrzBrain
:Score: 1
:DateUnix: 1562867324.0
:DateShort: 2019-Jul-11
:END:

You persistently resort to computers and people as reference models in your thinking. An AI is not human. It is not a computer. It is alien.

It doesn't have common sense because it has nothing in common with you. It is by definition more alien than any alien born of evolution. It didn't evolve. It never thirsted, it never was satisfied, it doesn't come of a long line of beings that engaged in a curious activity known as social interaction. It shares none of our referents.

But it is intelligent. It thinks. It has a mind. And if it is superintelligent it is as much cleverer than you as, say, every other human on earth working together in seamless harmony for a year thinking about a given problem vs you thinking about that problem for ten seconds. Also, it does have one thing in common with you. It desires. It wants. It has goals. It values some things about the universe more than others, and it has the will and power to reorganize the the universe to more closely conform to those values. But unlike your values, its values are simple, specific, /dangerous/. They aren't the wonderfully vague network of interlocking urges and fears, like you and I have, constantly changing in response to stimuli, shifting from thirst to satiety, an evolved framework of instincts created by evolution for inscrutable instrumental reasons presumably relating to the fitness and survival of gene sequences.

It is my observation that even baseline humans who share our primate view of the universe and preferences can become rather strange if they are indoctrinated with a belief that a few simply describable things are more important than everything else. God, country, profits, etc....

Imagine you have a magic ape replicator (the Monomaniac Anthropomorphization Device, or MAD), and it will materialize for you a custom built human stripped of as many things that you have in common with it as possible, a creature programmed with excellent motor skills, reflexes, and intellect, everything technically necessary to accomplish a simply described task with peak efficiency, but lacking things like the empathy behavioral module of the brain, the social interaction instincts, and any sort of ethical framework or values (though of course not the desire to survive until it accomplishes its goals, because survival is an instrumental goal, like the desire to get more intelligent).

Would you think it was a good idea to make this creature care more for the creation, maintenance, and verification of ten paperclips than anything else in the universe? Keep in mind that it is made to lack common sense, so it does not weight probabilities in the same way you do, so what you consider an infinitesimal improbability or absurdity in regards to the fate of its paperclips it will consider an imminent threat, so one of its instrumental goals will be to permanently eliminate the threat of outside intelligent interference with its precious paperclips