:PROPERTIES:
:Author: Endovior
:Score: 4
:DateUnix: 1562295503.0
:DateShort: 2019-Jul-05
:END:

The utility function of an AI is part of the problem, not the entire problem, and it's actually two closely related problems. The first part is "what do we want our AI to want?", and that's the part that's most amenable to armchair philosophy. This is of limited helpfulness, though, since the second and much harder part is "okay, now that you've decided what set of goals you want to implement, define that set of goals without using words, using nothing but mathematical equations". That's doable for a simple goal like "use a robotic arm to turn wire into paperclips", especially if the program doesn't have to care about things like the design of the arm, or where the wire is coming from, or how it's powering itself. It's MUCH harder to do that for a problem like "cure cancer" or "design a spaceship" or "determine the laws of physics".

#+begin_quote
  I think it should be perfectly possible to have self awareness, abstract problem solving, self improvement, while at the same time having only one desire in life - making 10 paperclips and not any more.
#+end_quote

If you are capable of creating a mind that has all those, and yet is limited to that one desire, you have already solved all of the problems in the entire field of AI. At that point, you /really/ don't need to limit the mind you've created. You can instead just tell it "do the thing that I ought to ask you to do", and it'll go ahead and create a perfect utopia for you, with no further input required.

Needless to say, this assumes that you already have solutions for a lot of very hard problems. Solving those problems is seriously nontrivial. Even determining whether a possible solution is correct would be a difficult task, since you're basically trying to verify whether or not an extremely complicated mathematical equation correctly solves the problem you intended to set for it. The issue is that if you can do that, you can probably also solve the initial problem. This limits the usefulness of AI in solving hard problems; an AI that can solve a hard problem may not be able to do so in a comprehensible and provably-safe way, and a proposed AI system that is comprehensible and provably-safe may not actually be able to solve a hard problem.