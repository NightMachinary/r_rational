:PROPERTIES:
:Author: lumenwrites
:Score: 4
:DateUnix: 1562295760.0
:DateShort: 2019-Jul-05
:END:

I think I do understand your argument, it does make a lot of sense. Thanks for thoughtful replies by the way, I'll need to think about this more.

But one counterargument does come to mind immediately. What if creating any kind of AGI is difficult and will remain difficult? What if it'll take google-sized or government-sized amount of computing resources and competent people to make an AGI that works, instead of it being available to any average joe.

In that case, wouldn't it be safer to rely on google engineers getting narrow-goal subservient AGI right, rather than a god-like AGI that takes over and does the right thing?

Right now we can control nuclear weapons and dangerous viruses because they're very difficult to create. If anyone could make a nuke in their basement, the world would be destroyed already.

*If* AI could only be created with a lot of effort from a lot of smart and competent people, and *if* it is much more difficult to create value-AGI than goal-AGI, wouldn't it be safer for people to stay in charge and try to do the right thing, and manage the risk the way we manage risks of all potentially world-destroying technologies, rather than trying to create a god that is aligned and controls all these things for us?

I guess it comes down to this - what is more difficult, creating and controlling non-evil AGI, or controlling people and making sure they don't do stupid and crazy things. If there's only 1-3 corporations in the world that are capable of creating AGI, I'd probably bet on google engineers, if anyone can make AGI in their basement, we would need a value-AGI to prevent that.