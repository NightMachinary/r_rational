:PROPERTIES:
:Author: Detsuahxe
:Score: 21
:DateUnix: 1562290819.0
:DateShort: 2019-Jul-05
:END:

If you want a thing that is legitimately smarter than you, you may as well call that thing an intelligence. That's what "smart" means. Doesn't matter if it's running on silicon or flesh.

All AI safety issues are technology safety issues. The only difference is that with AI safety, if one mistake is made, it has a very real chance of ballooning to the point where it destroys the world. Similar to how people were once worried that a nuclear bomb being detonated would ignite the atmosphere and destroy all life on the planet.

The limits you describe, even assuming they work when used, only need to not be used once. One mistake, and the GAI eats the planet. And once self-improving AIs can be made, it's only a matter of time until that one mistake happens. This is why the race to make a "good" GAI is portrayed as a race. The only genuine, permanent solution to the risk of GAI is to make the first one right, in such a way that it has an insurmountable advantage over all subsequent GAIs, and can therefore prevent them from eating us.