:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 10
:DateUnix: 1463425083.0
:DateShort: 2016-May-16
:END:

Came up with an interesting variation on the Prisoner's Dilemma which I think has interesting implications. I call it the Dilemma Of The Magi for reasons I think should be discernible.

Two people are presented with two buttons, and each must choose to press one of them. The Rescue Button always kills whoever presses it. The Rest Button also kills whoever presses it - unless the other person presses the Rescue Button, in which case the Rest Button does nothing at all.

I am not sure what the dominant strategy is here if there is no communication between the people involved. Societies composed entirely of cooperate-bots and societies composed entirely of defect-bots will both go extinct, while random button choosers will survive among their own kind a quarter of the time. A defect-bot introduced into a society of random button choosers will survive half the time, twice as often as the random button choosers, but the advantage disintegrates once the society is taken over by defect-bots.

On the other hand, it seems that if there is communication between the people, altruistic behavior would be forced, as clearly one person dying (which might be you) is preferable to two people dying (one of which is definitely you), at least in the generic case. So the two people would have to argue until they could unanimously decide who should sacrifice themselves - with no decision being made until then. This challenges our intuition that altruistic behavior is extra-rational.