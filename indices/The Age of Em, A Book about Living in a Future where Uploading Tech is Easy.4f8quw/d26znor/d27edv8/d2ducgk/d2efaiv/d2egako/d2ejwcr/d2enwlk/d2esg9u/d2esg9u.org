:PROPERTIES:
:Author: whywhisperwhy
:Score: 1
:DateUnix: 1461454327.0
:DateShort: 2016-Apr-24
:END:

First, if your utility function guarantees that your happiness is strongly tied to working, do you really have a choice whether to work or not?

Also, my argument is not "it just is," it's based on the principle that intelligent beings should have self-determination. As I said, in general I would not want to be created with a utility function locked such that my purpose is to work and serve people, even if it made me happy. I do see what you're saying that if some people naturally developed into that attitude, I wouldn't see anything wrong with that, but the difference is that they had some choice in the matter. That having been said, the more I think about it the more I think it's not "wrong" per se, just not what I would prefer (if this were CMV you'd be receiving a delta right now).

I think the optimal situation would be that an artificial intelligence be treated like any human and at least be given some measure of choice in what it decides to pursue (for example, if it decides to pursue physics research then great, recruit it for that, if it decided to become interest in social welfare, also great) and also some ability to change its utility function. Obviously that has to be tempered with practicality (we don't want it to become genocidal and we also want it to be productive), though.

Edit: Out of curiosity, do you think these AGI or modified uploaded humans should be property?