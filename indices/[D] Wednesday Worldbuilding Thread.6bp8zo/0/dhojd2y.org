:PROPERTIES:
:Author: callmebrotherg
:Score: 7
:DateUnix: 1495039160.0
:DateShort: 2017-May-17
:END:

I've started working on a list of possible justifications for super intelligent AI being absent from a setting whose scientific understanding should make one possible. Feel free to add to it:

- It just isn't possible to make an AI like that, for some weird reason nobody understands (ala Three Worlds Collide).
- It was deemed too high a risk to create an AI capable of recursive improvement, so even if the political state has otherwise atrophied by this point, there remains one last function to perform: working on the wrong kind of AI is a death sentence, and there is an otherwise-invisible group that that concerns itself solely with existential threats, which is more than happy to carry out your execution.
- Somebody thought that it would be too hard to make an AI that wouldn't go wrong if you were trying to get it to do a huge number of complex things. Far better was to program it to do exactly one thing, and if that was the case then you would want to program your recursively-improving intelligence to hunt down and destroy others of the same sort before they got out of hand. At some point during development, or maybe right after the thing was switched on, it was destroyed: it turns out that some other civilization had the same idea, thousands or millions of years ago, and every star system is patrolled by stealthy bots whose only goal is to destroy intelligences with too much potential for runaway self-enhancement. Even biological lifeforms can be hunted down if they play too much with cognitive enhancement.
- Similar to the above, another civilization already created an AI. Its values are mostly unknown, but it really doesn't like competition and is willing to leave us alone only if we don't try to build that competition.