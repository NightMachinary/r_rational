:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1495096508.0
:DateShort: 2017-May-18
:END:

/The Genocide Man/ had an interesting take on it. Recursive AI was possible, even easy - but the more intelligent a given AI was, the faster it went both homocidal and (to some degree) suicidal. Anything /super/human quickly (and very obviously) started killing everyone it could reach, usually while leaving itself deliberately vulnerable in some way. It was possible to find a mathematical correlation between the intelligence of the AI and the amount of time before it went crazy, so limited-intelligence AIs could still be short-term useful...