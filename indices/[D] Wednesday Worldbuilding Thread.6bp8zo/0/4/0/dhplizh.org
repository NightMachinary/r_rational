:PROPERTIES:
:Author: MagicWeasel
:Score: 4
:DateUnix: 1495086524.0
:DateShort: 2017-May-18
:END:

The issue with that is that if the AI has magic powers, it's really not threatened by humans so has no reason to leave; if we accept that it IS threatened by humans, then either its utility function is pro-human or human-neutral.

If pro-human, it is duty bound to become a friendly(ish) AI - either doing ACTUAL friendly AI things and giving us a beautiful perfect life, or doing friendly(ish) AI things (AKA unfriendly AI things) and putting us all into camps and feeding us gruel.

If human-neutral, then it's got no reason to let us live, so it can use its magic powers to kill everyone /now/ rather than let humanity remain a danger to it.

The idea of a dormant superintelligent AI is intriguing but I think that could be part of "laying low until it has the resources to deliver a decisive blow" type of strategy rather than what you suggest.