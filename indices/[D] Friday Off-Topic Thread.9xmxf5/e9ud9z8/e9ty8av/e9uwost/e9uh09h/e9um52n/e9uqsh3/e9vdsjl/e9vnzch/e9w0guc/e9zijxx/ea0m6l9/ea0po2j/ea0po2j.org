:PROPERTIES:
:Author: hh26
:Score: 2
:DateUnix: 1542606710.0
:DateShort: 2018-Nov-19
:END:

It depends on the scenario and your probabilities over possible outcomes.

If you have a 50-50 prior on the AI being friendly or not, then absent any disaster it's not worth opening the box and having a 50% chance to doom humanity. But if some sort of disaster is going to occur, like a meteor or a plague or something that has a >50% chance of destroying humanity, and a friendly AI would save everyone, then it would be worth it to open to box despite the uncertainty, because it lowers your risk. Maybe there's some sort of Newcomb's box thing going on where being willing to open the box in the case of a disaster incentivizes an unfriendly AI to cause a disaster, but even then your analysis would depend on the odds of the AI even being capable of doing such a thing as compared to the odds of a disaster happening naturally.

So I guess what I'm saying is that there probably isn't a good argument an AI could make all by itself that would be a good reason to let it out, because any unfriendly AI could make an identical argument. But under circumstances outside of its control that made not letting it out more dangerous than letting it out, then it would be good for someone to be convinceable.