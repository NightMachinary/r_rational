:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1542776446.0
:DateShort: 2018-Nov-21
:END:

Higher population of humans also means more chance that at least /someone/ will survive any given catastrophe.

#+begin_quote
  I think this is the multiplier that could potentially have a huuuuuge variance, I don't think you can just say that it's 1, when my mental model was assuming it would be closer to 0.01. But it's really hard to say, it depends on how much influence the AI's decisions carry in the real world and the nature of our interactions with the box.
#+end_quote

If we're using any information from the box, we're giving the AI in the box some degree of influence. (And if we're not using any of the information, then why have the AI in there in the first place?)

We can ask it for a cure for cancer - but if it's Unfriendly, we risk getting a 'cure' that interacts with something reasonably common (like a flu virus) to create a humanity-ending superbug. We can ask it to position our satellites in fuel-saving orbits and direct our probes to study other planets - but we risk it using cumulative gravitational effects over a period of decades to line up an Earth-striking asteroid. It might not be able to hijack our nukes - but if it's designing the security systems for them, it can probably arrange for them to be easily hijackable by someone who will use them. With an Unfriendly AI, every output that it gives could be the first domino in a chain that ends in disaster - or it could simply be the setup for a domino chain with a very common trigger.