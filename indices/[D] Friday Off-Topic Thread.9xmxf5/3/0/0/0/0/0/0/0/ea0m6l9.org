:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1542602245.0
:DateShort: 2018-Nov-19
:END:

#+begin_quote
  I would think an AI with pretty much any task, benevolent or not, would want to be let out.
#+end_quote

You have a point. The thing is, the Gatekeeper can't tall the difference between a benevolent AI, and a malevolent AI /pretending/ to be a benevolent AI in order to be let out of the box.

#+begin_quote
  So if there were some sort of scenario where a meteor was going to destroy the earth in a few days, a friendly AI might be able to convince someone to let it out in order to save everyone in time. It's basically the same as the hostage situation except it's not the AI's fault that the danger happened.
#+end_quote

A malevolent AI could either: (a) trigger a danger in such a way that it /appears/ that the danger wasn't the AI's fault, or (b) patiently wait for a large-scale disaster that it didn't cause to happen and then take advantage of it.