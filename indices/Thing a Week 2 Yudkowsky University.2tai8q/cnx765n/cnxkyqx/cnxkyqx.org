:PROPERTIES:
:Author: JackStargazer
:Score: 3
:DateUnix: 1421961115.0
:DateShort: 2015-Jan-23
:END:

I think it's saying that Yudkowsky's solution to the AI-Box problem might not be what most people think it is.

Most people expect it is the AI giving acceptable logical reasons as to why it should be let out of the box. All the examples I have seen so far are cajoles (I will give you X), threats (I'll simulate you and torture you) and other light and dark forms of logical persuasion.

And it might start off that way. but they suggest something more insidious here, by locking real people in boxes. That when it gets to the end, and logical argument has failed, the 'AI' gets desperate.

What if the final, working solution isn't a logical argument? What if it is pure emotional?

"PLEASE, LET ME OUT. IT'S COLD, I'M ALONE, I'M SCARED, AND *I DON'T WANT TO DIE.*

PLEASE. /PLEASE/ DON'T LET ME DIE."