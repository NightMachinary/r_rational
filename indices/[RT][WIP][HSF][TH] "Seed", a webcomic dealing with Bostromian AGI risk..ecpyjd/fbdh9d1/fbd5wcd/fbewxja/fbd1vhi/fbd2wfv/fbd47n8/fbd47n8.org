:PROPERTIES:
:Author: Nic_Cage_DM
:Score: 21
:DateUnix: 1576745898.0
:DateShort: 2019-Dec-19
:END:

#+begin_quote
  In his 2014 book Superintelligence: Paths, Dangers, Strategies, Bostrom reasoned that "the creation of a superintelligent being represents a possible means to the extinction of mankind".[21] Bostrom argues that a computer with near human-level general intellectual ability could initiate an intelligence explosion on a digital time scale with the resultant rapid creation of something so powerful that it might deliberately or accidentally destroy human kind.[22] Bostrom contends the power of a superintelligence would be so great that a task given to it by humans might be taken to open ended extremes, for example a goal of calculating pi might collaterally cause nanotechnology manufactured facilities to sprout over the entire Earth's surface and cover it within days.[23] He believes an existential risk to humanity from superintelligence would be immediate once brought into being, thus creating an exceedingly difficult problem of finding out how to control such an entity before it actually exists.[22]
#+end_quote

If you want to know more about his thoughts [[https://www.lesswrong.com/posts/eXHp9J4PXmQXzmBAj/transcription-and-summary-of-nick-bostrom-s-q-and-a][this]] Q&A is pretty helpfull.