:PROPERTIES:
:Author: AugSphere
:Score: 2
:DateUnix: 1415106560.0
:DateShort: 2014-Nov-04
:END:

#+begin_quote
  What value would a Culture citizen derive from massively upgrading his or her mind?
#+end_quote

That is the pertinent question, is it not? I think it should be answered by the most capable mind that has an optimal representation of the subject's values as opposed to the default intelligence the subject has at the moment. My key point here is that, if it's possible to tune yourself down in intelligence in case you decide it's the best choice, then you ought to consider this choice while employing the most powerful feasible cognitive system available. The choice of what to do with oneself is of immense importance in post-scarcity society after all. If it's not possible to fluidly tune your own cognitive power then I would consider the matter more carefully, but as it is, with the magical level of tech in The Culture, I just don't see any reasons not to do this.

#+begin_quote
  Although... if you're going to force him to decide according to an algorithm, why don't you just run the algorithm for him and tell him the answer?
#+end_quote

For what I envision here the difference would be minimal in the end. In one case you uplift the individual and he decides what to do, in the other, the very same optimal decision, that he himself would make, is calculated for him. I think the first one is a bit more polite, but it's not really a deal-breaker in my mind. The question of guaranteeing the trustworthiness of all the systems involved is a separate matter here, but it case of The Culture it's not terribly pertinent from what I can see.

#+begin_quote
  Regarding the earthworm thing: it's not a technical challenge, it's the fact that in order to make a human brain you need to fill it up with something. There just isn't enough stuff in an earthworm's brain to fill a human brain, so the majority of this human's personality will have to be created on the spot or derived from some other source.
#+end_quote

An earthworm makes for a bit of a tricky analogy here. If we are uplifting an earthworm (and we might as well go for the truly representative example of this kind and uplift a simple replicator), then, sure, we'll have a difficult time choosing a set of values for it, since it does not really have any in it's base form. I've implicitly used the assumption that there is no such paradigm shift when uplifting to superintelligence level from human one. There are no values 2.0, for which humans don't have analogues. I think it's human-understandable values all the way up.

#+begin_quote
  And would you really recognize it when you see it?
#+end_quote

You're not implying the authors have been secretly writing superintelligent agents for years and nobody recognised them, are you? That would be pretty hilarious. On a more serious note, yeah it would take one hell of an author. Maybe Eliezer will try his hand at it, when he's finished with HPMOR. He, apparently, likes a challenge.