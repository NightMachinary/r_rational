:PROPERTIES:
:Author: AugSphere
:Score: 5
:DateUnix: 1473490775.0
:DateShort: 2016-Sep-10
:END:

I'm going to give my own perspective on it, which is symbolical, rather than visual. What we call matrix multiplication is a special case of operating on multidimensional containers.

You matrix is a container of numbers indexed along two dimensions: =A_{i,j}= is the number inside your container, positioned at coordinates =i= and =j=. The numbers for all values of =i= and =j= taken together are called a 'matrix'.

When you do matrix multiplication you're basically mixing the containers along the shared dimension: =P_{k,l} = âˆ‘_i A_{k,i}*B_{i,l}=, the summation is along the shared index =i= and the non-shared indexes are preserved. The order on the right side doesn't matter, since the multiplication of numbers is commutative (it's better to write them in the same order as the matrices though, this way the repeated index is on the inside and the outside ones are identical on the left and right), but the shared index =i= obviously must have the same range of values in both matrices for it to make sense, which you can figure out from the formula itself.

If you're familiar with usual imperative programming languages (=for= loops in particular), then [[https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/][this]] might shed some light on how various inner and outer products in linear algebra are all basically the same thing under the hood.