:PROPERTIES:
:Author: AbysmalLion
:Score: 2
:DateUnix: 1577071332.0
:DateShort: 2019-Dec-23
:END:

It does seem plausible. Some thoughts though:

The FTL only with biological sentient works, but the question then comes, what happens when an AI starts puppeting people. What stops a machine from sticking some probes in a brain to control a biological for matter?

Also, FTL isn't really that big of a hard limit for a "singularity"-risk AI. It can just expand the old fashion way, what prevents that?

What happens when AIs go rouge? Can they go rogue?

For my science-fantasy universe I simply put a limit on intelligence vs. size of brain. Which is to say one can have a sentient building, but it can't be much smarter than a human, it will have a central brain, and most of it's ability to sort through it's sensorium is "plain old software" (no sentient pattern matching all the camera feeds at once, only on ones it is "paying attention to"). Anything bigger shards into multiple personalities and sentience can't think faster than a smart human.

My point is I think you can solve your problems:

- Magic is special, only the actual will of the sentient will activate it.
- Divergence of purpose when the AI separates thought, splitting up computation causes drift and internal conflicts. This is unlikely in reality, but as a universal axiom it's fine.
- Again as a universal axiom "AIs arrive at a common morality" is fine, but I think it might need to be fleshed out a little more.