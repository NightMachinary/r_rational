:PROPERTIES:
:Author: ZeroNihilist
:Score: 4
:DateUnix: 1469646342.0
:DateShort: 2016-Jul-27
:END:

Virtually every task that is currently computationally expensive but solved would become trivial.

Humans lose to robots at every single sport or game ever invented, possibly even to the same fundamental system (i.e. a general-purpose game-playing algorithm that requires only training data).

Humans are relegated to aesthetic roles in engineering or architecture except on the bleeding edge where people haven't yet made algorithms for it. Basically, you mould the output of the designing algorithm to get the desired appearance and user experience while it ensures that the technical requirements are met.

Algorithms predict the expected performance of new products based on various combinations of prior art, marketing, and target demographic. This results in some people pushing back against the dominant culture, favouring "unlikely to succeed" products, until the algorithms adapt to the new data and try to pin them down again. This is already happening, albeit with people instead of explicit algorithms.

Likewise, there's a resurgence in artisanal products allegedly designed without the aid of any algorithms. People associate these with homeliness and personality, whereas algorithmic products are considered impersonal and sterile. Naturally, there are algorithmic products that seek to recreate this feel, including introducing artificial imperfections to give it the craftsman touch. Even self-proclaimed experts can't easily tell them apart (though actual experts can).

Humans are almost completely removed from roles where failure could cause death where those roles are amenable to algorithms. For example, the entire aviation industry (maintenance, piloting, air traffic control, even scheduling) is monitored by humans but actually executed by algorithms.

The above algorithms themselves are all checked by simulation and static analysis and any failure modes are corrected if possible. Most of the remaining cases are to do with humans in some way, such as when they fail to follow procedure or override the system.

All the "@home" projects (e.g. SETI@home, Folding@home) are quickly completed and any potential benefits follow on from that. People keep finding replacement problems; in the long term this process solves, well, everything solvable. Have a look [[https://en.wikipedia.org/wiki/List_of_distributed_computing_projects][at this Wikipedia page]] for inspiration.

Larger organisms can be simulated at the neuron level. I believe we've mapped the connectome of a flatworm (or something like that) and verified its correctness, but that's only ~14 neurons. We're working on a mouse connectome, but I think that's going to be limited by our scan rate even if we had the CPU for it. That's a whole other can of worms, especially if simulating a human mind is reasonable.

There's probably way more. Really, the limiting factors are what we can actually program (for explicit algorithms) and collecting data (for machine learning).

Oh and google or its in-universe equivalent would be an unstoppable juggernaut floating ever higher on a tide of data.