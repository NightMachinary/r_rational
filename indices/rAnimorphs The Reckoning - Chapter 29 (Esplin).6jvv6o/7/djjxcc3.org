:PROPERTIES:
:Author: ZeroNihilist
:Score: 6
:DateUnix: 1498737763.0
:DateShort: 2017-Jun-29
:END:

I think the really interesting thing here is that it implies that it's Esplin, not the Yeerk council, that is the chosen champion of one of the gods. Things have been engineered to give him the perfect opportunity to achieve his propagation of self. It's not guaranteed, because the other god meddles too, but it may be the best possible situation given the existence of a hostile omnipotent entity opposing his benefactor.

Going off the god-demarcation I presented [[https://www.reddit.com/r/rational/comments/4hn9hr/ranimorphs_chapter_18_cassie/d2spj0u/?context=10000][way back in the chapter 18 comments]] (TL;DR: Ellimist and Crayak are Order+Unity+Silence and Chaos+Harmony+Noise maximisers, though not sure which is which), InfinitEsplin is a key goal of Order+Unity+Silence.

After all, an infinite entity linked by thought-speech and enforced value compatibility maximises all three literally and figuratively. Order, because InfinitEsplin will enforce it. Unity both as "unity of purpose" and the literal meaning "one". Silence, because there is minimal need to communicate with yourself (and, literally, because they could use thought-speech).

If InfinitEsplin is a goal of Order+Unity+Silence (for compatibility with canon, Crayak), what is the goal of Chaos+Harmony+Noise (Ellimist)?

Well, harmony makes me think that Yeerks are going to be involved here too. Instead of a massive hive-mind, however, we'll have a universal sharing; all sapients united by understanding despite their differences.

Chaos would imply that the situation would be highly unpredictable. No one entity nor group of entities would have full and indefinite control of the universe.

As for noise, I'm interpreting it as a stand-in for dialogue/communication. The universe would be full of people talking and arguing and being persuaded, converging on optimality by the glacial, faltering process of democracy.

Initially I thought that Chaos+Harmony+Noise (Ellimist) would be bad for humanity, but I'm coming round to the idea that their goals might be compatible with our aggregate utility function (even if it would take radical changes of society for us to accept it). Bring on the dissolution of arbitrary barriers through implanted empathy.