:PROPERTIES:
:Author: artifex0
:Score: 2
:DateUnix: 1443468973.0
:DateShort: 2015-Sep-28
:END:

Frankly, I'm not convinced that the paperclip maximizer issue is nearly as difficult a problem as people make it out to be.

To begin with, unless you used evolutionary algorithms (which would be insane), designing an AI will require a much better understanding of utility functions than we currently have. There'll need to be a science of utility functions to get the thing working at all, and that should turn the question of which motivations are pro-social from our vague speculation into something more like an engineering problem. Of all of the problems they'll need to account for, my suspicion is that anti-social behavior resulting from an overly specific motivation will be, at the same time, one of the most obvious and one of the most dramatic in it's consequences.

Secondly, the first true AI isn't going to have super-human intelligence. Even if some implausibly sudden breakthrough let researchers to build an AI that could be scaled up to super-human levels with more processing power, the obvious thing to do would be to begin by testing it at sub-human levels. I don't think an AI with a sub-human or even a human level of intelligence would immediately understand the need or have the ability to hide unexpected emergent motivations- and this would let researchers refine both their theories and their programming before ever starting to experiment with super-human intelligence.

I honestly think that to realistic AI researchers, the paperclip maximizer problem would be as obvious and testable as a sealed hull to a shipbuilder. I think it's something they'd likely have a good idea about how to solve very early in the development process, and I think they'd develop a theoretical understanding of it that could be generalized to more intelligent AIs.