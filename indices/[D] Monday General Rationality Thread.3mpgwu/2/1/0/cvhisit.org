:PROPERTIES:
:Score: 5
:DateUnix: 1443485870.0
:DateShort: 2015-Sep-29
:END:

Personally, having done a fair amount of reading on issues like, "How to design a mind", there are ways you could write the code to avoid the AI valuing something /completely random/, but even though that gets you past the "Paperclips barrier", it doesn't get you past the "Happy sponge barrier" where you programmed the AI for something that /honestly sounded like a good idea at the time/ but turned out to be Very Bad when taken to programmed extremes.

The simplest solution to this that /I/ can think of is, "Give the AI enough social and natural-language reasoning to understand that what I'm saying to it is an imperfect signal, and it needs to do all kinds of inference to determine what I Really Mean rather than just taking my initial program literally". And that's actually a rather difficult research problem. That, or "Program enough knowledge of human minds into the AI at the preverbal level that it uses a human-values evaluator for its utility assignments in the first fucking place, before even turning it on to give it training data", which is the traditional proposal.