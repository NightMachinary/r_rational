:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1476149387.0
:DateShort: 2016-Oct-11
:END:

It seems like followed to its logical conclusion this would be the exact sort of thing that would quickly wipe out humanity if put as a GAI's utility function.

This ought to incentivize the creation of a singleton wiping out humanity, /after all people are made of resources that could be used to simulate lots of perfectly happy ems/, simulated people can live their lives /absurdly fast/, so these kinds of problems aren't just going to come up after massive amounts of time either.

Of course since your model places some small penalty on death, by far the better solution would be to just create a singleton that can just simulate all that pleasure for itself and doesn't have any diminishing returns; if you still penalize memory erasable enough for that to come up then the AI will just replace itself with a predecessor every so often. So obviously the much simpler solution is just effectively a paperclipper. Of course if you place utility on distinctively human forms of pleasure the AI might make itself a bit more human, so maybe it'll even feel bad, which i'm /sure/ will console you.

Of course I think the big reason to reject you premises as *AugSphere* pointed out, is that this type of valuing /potential/ people is extremely fishy. It also requires you /place value on people based on their happiness/, that's kind of unavoidable.