:PROPERTIES:
:Author: LieGroupE8
:Score: 1
:DateUnix: 1476140571.0
:DateShort: 2016-Oct-11
:END:

"I prefer to think of creating new agents only in terms of the impact on already existing ones"

The point is that existing agents /do/ in fact assign value to creating new agents - thus they are morally incentivized to die for someone else. It is not much different from jumping in front of a trolley to save someone else, and possibly much less bad, if the agent has lived a long and fulfilling life to the brink of memory capacity.