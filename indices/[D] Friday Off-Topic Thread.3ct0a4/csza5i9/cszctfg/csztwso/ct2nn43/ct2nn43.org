:PROPERTIES:
:Author: itaibn0
:Score: 1
:DateUnix: 1436854494.0
:DateShort: 2015-Jul-14
:END:

I skimmed the paper on Calude's paper, and I'm suspicious. On page 9, if I understand correctly, they show that under certain conditions the halting time for a computation U (x) is in some set of density less than 1/s, and then conclude from this that the probability ("according to density", whatever that means) that U (x) halts is less than 1/s. Can you explain how this inference is made?