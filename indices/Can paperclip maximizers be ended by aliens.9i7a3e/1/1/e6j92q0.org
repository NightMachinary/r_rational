:PROPERTIES:
:Author: TeMPOraL_PL
:Score: 2
:DateUnix: 1537766572.0
:DateShort: 2018-Sep-24
:END:

#+begin_quote
  The best case scenario I can think of is that some alien civilization has developed a universe-destroying weapon.
#+end_quote

Whether or not that will work depends on how close that alien civilization is to some sort of AI/unified mind/unified society. As with all game-theoretic gambles, you should not hesitate to deliver on your threat, or you'll loose. If the aliens get second thoughts about destroying the universe, they'll lose the weapon to AI, and then the universe.

A similar situation is actually covered in Cixin Liu's The Dark Forest, where (light spoiler) humanity is keeping alien invasion at bay with a MAD threat, but the person responsible for triggering it hesitated just a little too long, long enough for the aliens to destroy the threat delivery mechanism .