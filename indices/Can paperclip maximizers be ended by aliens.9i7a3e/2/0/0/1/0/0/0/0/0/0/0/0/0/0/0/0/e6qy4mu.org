:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1538072920.0
:DateShort: 2018-Sep-27
:END:

NO

/rant

The reason I say superinteligent AI wont go war for dumb reasons is that presumably its reasonably good at achieving its .

Humans want different things , and have a lot of limitations that the ai doesn't (not that wanting different stuff means that yu cant cooperate, or even do a values handshake) Not everything fails the same ways humans do.

There is math and easy experiments that show ways that humans are being irrational ,and how optimal agents behave, and consistently do better and get whatever they want. Competent paperclip maximizers that conquer a solar system without destroying themselves presumably are closer to optimally rational agent ,even if not necessarily perfect.

Humans are a badly implemented mess that just happens to work well enough to survive , humans doing stupid stuff is not correlated to better designed agents doing the same.

There is 0 reason to go to war , any amount of resources wasted in it as higher than the negligible cost just convincing the other , there is nothing to overestimate ,(and overestimating is something the ai should do less than humans ,expecting it to fail in a way that its obvious to you its like expecting a master chess player failing in the kind of thing you expect yourself to fail f you were playing a chess game.)

The other either can be convinced or has gone crazy.

You are saying that its likely that the ai are going to be dumb . Humans not cooperating when it would benefit them is humans taking bad decisions , something better at taking decisions will do it less often ,and not in ways so obvious even I can see it.

Do the paperclips suspect the other has a bug and its not being a rational(in the decision theory sense, before you start saying something weird about irrationality being good) agent? Because then they should just follow Augman's agreement theorem and converge on the same beliefs.

If war happens it will be because one of them is doing something really wrong.

if the paperclipers suddenly care more for their paperclip design than the original metric you just sneaked in a value change , which might happen but The Ai will try to prevent , and its a case of one ai malfunctionng and having to be put down instead of paperclipers starting wars by themselves.

we do x because of emotions> emotions are part of our code > the ai has code > the ai does x Is not a valid inference chain /rant

Also as a aside , even if there is severe drift things a value handshake is generally better than killing each other .