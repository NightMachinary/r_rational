:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1538158440.0
:DateShort: 2018-Sep-28
:END:

I originally understood you as saying that you had two AIs that still maximized the same definition of paperclip but had different designs and somehow decided that their design was better and started a war because of that . And I put an example whith AI whith different kinds of paperclips that cost diferently ,but still operating whith the same definition. And you responded whith something weird about paperclips caring about their design.

The case of the definition of paperclip changing is a risk,and a bug might cause it at some point. But its not going to happen on its own, the AI wants to remain making paperclips* , and not something else the humans would also call paperclips(witch is the only relation in both concepts). So it should already be working on avoiding value drift and had a lot of contingencies for it. It might happen , but its not the most likely thing to happen if you send a prove to the nearest star and have taken the apropiate measures. Or at least I hope , that would imply really bad things about the future of the universe even if we develop FAI . And that would make expansion slower , if you can never trust any copy of you that you aren't actively monitoring . But then you cant trust your future self either. An this seems to be a computer science question we only have vague speculation about so dunno.

How flexible it is depends on what the definition is , the category of paperclips maximizer its indeed wide and contains lots of agents that are maximizing really different things , and the difference between them is not smaller than between paperclips and thumbtacks. I'm imagining something slightly flexible, but my point its that flexibility doen't matter for this kind of thing. If cooperating was better than war , then both paperclipers will cooperate, if it isn't they arent taking a secure action.

But value differences don't mean the only alternative is war , a lot of times cooperation is better. The AI might have cooperated whith humans before killing them all once it got more powerful and it no longer was in its interests .

War betweenAIs or civilizations that own entire solar systems can get nasty. If you want to precomit to do as much damage as possible there are really damaging thing you can do,and you can start taking more stars before the other can react.

Value handshakes ,and cooperation don't need flexibility. The paperclip maximize doesn't have as an objetive turn the universe into paperclips, its objetive its making as much paperclips as possible. It can share the universe if the alternative is less paperclips. It could work perfectly in a economy of similarly powerful AIs, do things for other AIs in exchange of money to pay people to make paperclips, form alliances etc . If it has to modify itself to care less for paperclips , it wont matter.

A paperclip maximizer doesn't have this property you seem to be adding to it of never compromising , it can compromise on paperclip number and paperclip probability(which means that yes, agents that have binary goals can also compromise) as long as compromising means getting more paperclips than war.

Not sure how war between solar system looks like , and if MAl is a thing there , but at least I think There would be significative damage,and if you want to precommit you can waste material , destroy ,do damage.

Humans that want different things compromise all the time without deciding to want the thing the other wants, even selfish bastards cooperate if it benefits them .There are some things humans feel they can't compromise on , but that's part of some social strategies evolution "hard coded"(yes I know you can choose to act differently) on us and not something all agents have.