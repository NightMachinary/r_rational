:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1537978374.0
:DateShort: 2018-Sep-26
:END:

Barring severe drift, yes, they should be able to talk it out. The question is if they don't. We tend to think that sort of conflict or disagreement is a human exclusive because in us we see it as driven by emotions, but "emotions" is basically the name we give to our basic programming directives, the ones that don't need any rational basis but just are there. So basically two humans fighting instead of settling on a mutually beneficial solution are often just two humans whose core programming is different enough that they can't fully appreciate the benefits of cooperating with the other (and, for example, overestimate the cost of giving up their current position). If the two AIs have refined their designs separately through slowly drifting criteria and now are each convinced that their own design is by all means the best possible (or, at least, that the other design isn't better than their own), the next obvious step is cannibalising the other's paperclips and infrastructure to make it into their own paperclips and infrastructure. And that's war.