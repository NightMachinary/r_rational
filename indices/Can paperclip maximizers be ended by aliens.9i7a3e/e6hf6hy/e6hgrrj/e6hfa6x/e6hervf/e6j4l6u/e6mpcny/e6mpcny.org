:PROPERTIES:
:Author: Chosen_Pun
:Score: 2
:DateUnix: 1537905918.0
:DateShort: 2018-Sep-25
:END:

There's an argument in there that humans tend to care about the things they care about /because/ they have utility re: reproducing. (Sometimes humans care about things that don't have that utility at all, but they don't tend to reproduce as successfully, almost by definition. This should all be review.) The point being that /apparently/ complex goals /can/ arise from, and be traced back to, a single imperative, and deviations from that are, from a certain perspective, just that. Errors to be corrected. (Not /my/ perspective, mind you. Just setting up the analogy)

The rhetoric goes that sufficiently intelligent paperclip maximizer would at times appear to value things very unrelated to maximizing paperclips, for example, improving the human condition, by, for example, /curing cancer/; the logic being that in its early stages, the best way for it to maximize paperclips in the long term is to maximize its resources in the short term /by convincing humans that it is friendly and can be trusted/ and does not need its functions limited please and thank you, would you like world peace with that?

I'm given to understand that the game at [[http://www.decisionproblem.com/paperclips/]] has helped a lot of us grok these and other concepts related to the problem, if you've got a couple cumulative days to get through it.