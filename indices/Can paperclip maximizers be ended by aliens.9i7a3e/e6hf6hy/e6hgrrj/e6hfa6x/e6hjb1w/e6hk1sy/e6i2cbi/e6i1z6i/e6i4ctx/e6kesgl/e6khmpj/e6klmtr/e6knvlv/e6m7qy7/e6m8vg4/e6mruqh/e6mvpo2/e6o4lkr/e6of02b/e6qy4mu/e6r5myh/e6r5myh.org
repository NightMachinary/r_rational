:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1538078571.0
:DateShort: 2018-Sep-27
:END:

I'm sorry, but we ARE talking about Paperclip Maximizers. The entire point being that these are AI simultaneously so smart that they can annihilate humanity and so dumb that they don't see how doing so /defeats the purpose of making paperclips entirely/. We have some key directives, like surviving and reproducing, but we can override them, by committing suicide or choosing chastity voluntarily. If the Maximizer was able to be so flexible, it'd have acted more sensibly long time ago. The premise of a Maximizer is that there are certain things it just. Won't. Compromise on. No matter what. So if there are directives concerning designs in their core programming, the one with which they were created, it's likely that they can't just be flexible about them.