:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1538151263.0
:DateShort: 2018-Sep-28
:END:

The problem is that your own idea of a "value handshake" implies that sort of flexibility: for example, flexibility on what design of paperclips is acceptable. The handshake makes sense if an AI thinks "meh, even if it's not MY paperclips, it still IS paperclips, and that's better than nothing; so I better not waste resources fighting this guy". That's (partial) flexibility. I realise absolute flexibility is an unrealistic extreme. But an absolutely inflexible AI instead will think that flawed, 'imperfect' paperclips are just as bad as thumbtacks or smiley faces or humans. They're just /not HIS idea of paperclips/. And must subsequently be eradicated. War is not then a costly possibility to be dodged, but a painful inevitability. Yes, it is expensive, but to not go to war would mean to back up on the road to maximizing paperclips. And that's just not done. At best, if the enemy was more powerful, the AI could back up, feign submission, but still scheme to eventually defeat its opponent. And turn them into paperclips.