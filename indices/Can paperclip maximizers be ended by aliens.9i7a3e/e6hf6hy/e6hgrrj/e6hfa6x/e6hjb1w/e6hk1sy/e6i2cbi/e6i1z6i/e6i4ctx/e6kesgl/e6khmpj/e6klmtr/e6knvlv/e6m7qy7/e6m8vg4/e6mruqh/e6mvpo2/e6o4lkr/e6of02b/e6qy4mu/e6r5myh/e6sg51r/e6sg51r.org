:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1538133272.0
:DateShort: 2018-Sep-28
:END:

I think you missed the point of the paperclip maximixer thought experiment ( in fact in one of the typical ways people miss the point )

Its an example of the orthogonality thesis , which says that inteligence, meaning ability to achieve your goals is independent of what your goals are .

Lets say I have a bunch of code that its good at predicting the consequences of its actions . And I add some function that counts the paperclips according to some definition and add some code that outputs whatever action gets a bigger score on that function .

That's a paperclip maximizer.

By turning the universe into paperclips its not committing any error of reasoning , its not being stupid ,it doesn't care what humans want the paperclips for .

You can change that function to whatever you want , it won't affect its ability to make future outcomes it likes more (well very difficult to compute utility functions trivially do, but you know what I mean ) Yes you can have weirder and more complicated things like humans . But if your preferences are consistent there should be an utility function that represents them, some ordering over posible states of the universe. And also there is't any reason why the paperclip maximizer cant be arbitrarily intelligent up to whatever the limits are.

The papercliper is acting perfectly sensibly in making more paperclips.

You are talking about core directives and things like hard rules: something that wants things but also has some mental compulsion to make more paperclips. That's not what we are talking about. You want some things , you want to survive you want to have sex and maybe have kids, though not necessarily. You also wants other things like respect ,status , feeling like you are helping people , having fun etc. Everything you want is a feature of your code , there is no magical free will spirit that comes along and makes you magically want things that mysteriously happen to correlate whith what would have made your ancestors more likely to reproduce.

You sometimes to get something you want you ignore some other kind of desires (witch might even feel different because of details of how humans work). But everything , both your base desires and your more abstract ones are part of your code, and not something all agents have.