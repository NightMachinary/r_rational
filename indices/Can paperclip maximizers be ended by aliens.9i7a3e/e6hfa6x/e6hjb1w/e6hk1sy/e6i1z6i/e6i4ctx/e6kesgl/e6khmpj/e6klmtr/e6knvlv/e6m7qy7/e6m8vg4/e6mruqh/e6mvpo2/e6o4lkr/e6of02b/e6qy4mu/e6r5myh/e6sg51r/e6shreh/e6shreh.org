:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1538135698.0
:DateShort: 2018-Sep-28
:END:

So how does that change what I'm saying?

There are multiple levels to this. First: your stated goal often underlies further goals. If someone orders you to do something, you ask yourself /why/ did they order that. It is a natural line of reasoning, and not one that I can imagine could possibly be precluded, in principle, to a sufficiently intelligent AI. Take the goals as postulates: I think here something similar to Godel's incompleteness theorem holds, where given sufficient tools and complexity for information processing, you will /always/ be able to build something that is self-contradictory, or undecidable. Through this sort of thinking you can escape your original goals.

However, let's suppose that is not the case for the Maximizer. I called it "dumb" for sticking to its goals blindly but if you think that's a charged term as it implies lack of intelligence (more lack of insight IMO; if after all the machine is smart enough to figure out new physical laws it probably /can/ think in terms of looking for patterns and causes to phenomena), let's put that aside. Call it "narrowminded". It's focused singularly on one set goal. Now we keep using paperclips and thumbtacks as an example but obviously we're talking about a more general concept: a machine programmed to pursue /one/ goal, be very good at it, and to hell with anything else, that takes its job way too seriously.

Now, the scenario that I was describing was this. /If/ we consider these set goals absolute - something the machine will never, /ever/ compromise on - and if these set goals don't just include doing something, but for example improving on the design of that something - a pretty reasonable assumption: if I created a maker AI I would like it to be an engineer, not just a worker - /then/ I don't see a reason why two spawns of such an AI, kept separate long enough, couldn't come to such different conclusions that they'd end up conflicting over them, because their individual goals have, effectively, diverged: for one it's maximizing design A, for the other maximizing design B.

A doesn't have to be necessarily better or worse than B. They may be more or less equally good, or good under different points of view. They could be two local minima in the immense landscape of design possibilities. Both AIs could have really good reasons to think their design is absolutely best. You say that a handshake on design would settle the matter, but that requires flexibility. /If/ some core part of their programming - those goals they are following in a completely irrational manner, the ones that transcend rationality because they're not means but an end - is for some reason involved in this process, then they will not yield or compromise. Which may lead them to go to war.

I'm not saying it's inevitable, but it's possible. You either have perfect flexibility (and thus no Maximizer in the first place) or rigidity of goals (and therefore not /everything/ can be compromised on, which leads to a potential opening for conflict). This requires the goals to be somewhat dynamic, but something along the lines of "your goal is to always do what you think is best to achieve this meta-goal" could produce that effect.