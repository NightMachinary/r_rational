:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1538161666.0
:DateShort: 2018-Sep-28
:END:

I guess the real difference here is whether you assume that the AI by virtue of being superintelligent also has perfect information and perfect insight. I don't think it would, because it'd still be a material being, not a God of some sort. As such, it will have probabilities and error margins to deal with, even though margins much tighter than ours. Within those margins, conflict may be a possible avenue. Not the only one, but I don't see why it would be so necessarily off the table. Same goes about this notion that as long as things count as "paperclips", that's fine. "Paperclips" are not a fundamental particle. The AI must have circuits dedicated to recognise paperclips as such, and a working definition that probably goes beyond its core directive. That definition won't be too hard, and it will probably be mutable.

For the AI to accept a value handshake it needs to compromise on that definition, and yes, it requires flexibility especially /if the AI knows it's strong enough to win a war/ and it must weight its options. So basically the only case where your idea is right IMO is if war has a high likelihood of resulting in Mutual Assured Destruction. Though I think we agree on that much at least.