:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1537891774.0
:DateShort: 2018-Sep-25
:END:

"Is it okay with maximizing the number of paperclips, or does it want them to be its paperclips? Like, for example, it improves on the design, and consider other paperclips dissatisfactory? In which case a long enough separation would produce two different competing paperclips designs and it'd be war."

Not really . If it had more complex goals maybe. But if its a paperclip maxmimizer , it just maximizes paperclips(Its on the name) A magical humanlike spirit doesn't come into the code and becomes attached to its paperclips designs. IF it maximizes the number of paperclips it maximizes the number of paperclips , and takes whatever action it thinks will result into more paperclips.

Paperclips can't be worse or better, what matters is the number. You could have something that maximized something else but that would be other kind of hypothetical ai.

If the new AGI disagrees about which design allows them to make more paperclips , they compare data and try figure out were the disagreement comes from , war would be stupid (this also happens it they did care about some quality of paperclips and disagree on which design rated higher on their utility function)

Self modifying to favor some paperclip design would mean less paperclips in the long run(and war means less paperclips)

And yes maybe radiation flips some bits , or there is a bug, or it makes a mistake when self modifying , changing its definition of paperclips .\\
But it won't happen if everything is working correctly.