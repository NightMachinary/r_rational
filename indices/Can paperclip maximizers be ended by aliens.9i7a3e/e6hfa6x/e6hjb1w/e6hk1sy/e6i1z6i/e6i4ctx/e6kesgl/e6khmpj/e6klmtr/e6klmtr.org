:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1537823116.0
:DateShort: 2018-Sep-25
:END:

Not necessarily, you can make nano-machines that don't work if there are changes on duplication . Something that self replicates doesn't necessarily evolve unless versions whith small changes are viable. Life's replication fails really often , and lots of variations in adn are viable , this doen't have to be the case whith nanotech. For example you can encrypt some vital part of the code whith a hash of the rest of it.

Yes whith enough optimization pressure eventually some of them will work anyway, , but eventually can be a long time. (I remember reading something about that , maybe form the future of humanity institute but I cant find it right now ) You can do stuff like having some of them check the others for damage ,and that sort of thing .

Value drift is a bigger danger for copies of the AI but Its not very clear how inevitable it is , It will depend on details of the AI , and what safeguards can be made. So maybe .

It would have to be a bug , a paperclip maximizer doesn't become a competitor by becoming independent , it has to become a thumbtack maximizer or something. That will also be a problem for expanding in general. And for the ai itself since it could also mutate and become a thumbtack maximizer without traveling, so it already needs to have safeguards against that.

The problem whith waiting is that you loose material , since it exits your Hubble volume which means less paperclips in total .

And all stars are wasting resources. So you can be patient but not too patient.