:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1538163041.0
:DateShort: 2018-Sep-28
:END:

he AI must have circuits dedicated to recognize paperclips as such, and a working definition that probably goes beyond its core directive.

Its core directive is maximize x. If you change the definition of x you are telling it to maximize something else entirely. So it wants to keep its definition. Since changing its definition means less x , since the new x doent count as the same thing for its current self. Make paperclips its not a goal you can code. You code a specific criterion of what its a paperclip , and it maximizes that thing.

Also if there are more AI its more likely that they will cooperate, since the benefits of wining the war become less .

Again turning the universe into half one kind of paperclips and half the other kind doesnt require flexibility .

There are almost always ways to add preference orderings to get something both agents prefer to whatever probability of annihilation they calculate.

Imagine if the AIs were still on earth before taking over and there were thousands of them . value handshashakes would be even more comon there.

Or if two ais have most of the galaxy ,they certainly wont want to start that war , its just too costly , would take absurd amounts of time and migh not ever have a clear winner ever . Since you can always send probes everywere once you have a solar system it will become difficult to completely end an opponent .

So cooperation kind of becomes the best option unless you are smarter or have some kind of advantage over the other .

There is no magical perfect information involved, but you can be very accurate and extract much more info from data than humans , and you don't need perfect information to do things ,just slightly higher probability of paperclips than the alternative.

Humans cooperate without perfect information all the time ,and I'm not convinced "flexibility" has to be involved always.

In fact thats actually easy to show whith simple decision theory thought experiments.

Having one goal doesnt mean your payoffs are always the same. You can choose a 90%chance of 1/2 universe of paperclips .

Being unsure about the power of the other or whatever just changes your probability estimates.