:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1538149958.0
:DateShort: 2018-Sep-28
:END:

Ok I think I found my main disagreement.

If you have an agent that wants paperclips and an agent that wants thumbtacks or paperclips the second agent trivially has more options and can be satisfied in more situations.

The agent that literally wants anything is always satisfied , and its perfectly flexible.

And I don't feel like that's any advantage, its just optimizing for an easier goal.

The best agent at maximizing paperclips is a paperclip maximizer .An AGI could maximize all the things humans care about .That the paperclip maximizer is maximizing paperclips instead of maximizing what humans care about is not a mistake on its part.The same way its not a mistake that you aren't sorting pebbles.[[https://www.lesswrong.com/posts/mMBTPTjRbsrqbSkZE/sorting-pebbles-into-correct-heaps]]

Would you defect or cooperate on the true prisioner's dilema?.

[[https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma]]

If you cooperate ,do you think its because of having flexible goals.

Humans aren't perfectly empty flexible agents that decide to interpret things without any criterion.

A truly "flexible" agent could compromise always.This doesn't feel like any advantage of that agent.I don't feel especially compelled to become that kind of agent to be more flexible.

You said that if the agent was more flexible it would't be a paperclip maximizer.But then what would it be?If it doesn't have any more goals there is no criterion that would lead it to choose anything else that isn't maximizing paperclips.The way you are saying things it sounds like agent that has any objetive except liking whatever happens is being irrational, since they aren't getting what they want in some situations.

You keep saying things like :

"programmed to pursue /one/ goal, be very good at it, and to hell with anything else, that takes its job way too seriously "

But way too seriously depends on your goals.There is no universal scale were you can measure how good a set of terminal goals(you do realize the ai has making paperclips as a terminal goal right?, and it can have instrumental goals like learning about the universe , building dyson spheres etc, as a means to make more paperclips).

You don't care that much about paperclips , so paperclips sound inherently worthless to you , but that's a fact about human psychology , not about agents in general.

Talking about the benefits of having certain terminal goals , aka utility functions seems like a category error .Better means"rates higher in whatever function I'm evaluating".

Phrases like that make you sound to me like you were talking about a human so obsessed whith paperclips that doesn't notices that he forgets about everything else , and that doesn't realize that killing humans is bad and he should't do it because paperclips only matter if there are humans to use them.

But that's like the peblesorters imagining you as a someone so obsessed whith things like surviving or altruism that you forget that living only makes sense to sort heaps of pebbles.