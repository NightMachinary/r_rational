:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1537907926.0
:DateShort: 2018-Sep-26
:END:

I think it has a higher likehood of maximizing wireheading by tiling the universe whith computronium adding numbers to its reward score or whateverer it has. Or a weird combination of goals , if foom doesn't happen and multiple ai take over instead of one and end up compromising on a ai that maximizes a combination of their utilities.

But anyway that doesn't affect my point , and we were talking about paperclip maximizers. Two copies of the same AI should have the same definition ,and want to keep it. So unless it has some goals attached to some definition of personal identity , it shouldn't care . Something could go wrong being uncomunicated , but the AI is going to try as hard as possible to maintain its definition of paperclip-ness intact, and it wont change by default . Humans would change if we sent them to a travel like that , but its not a fact inherent to all agents . In fact most agents have a common instrumental goal of preventing changes on their utility function. And a value handshake is going to be a better alternative to war anyway.

Also btw not really relevant , but you can view it as paperclipness or likehood of being a paperclip ,the math is equivalent.