:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1486515613.0
:DateShort: 2017-Feb-08
:END:

#+begin_quote
  Here's how I think the timelinery works: [[http://sketchtoy.com/67872359]]
#+end_quote

Ah now I see what you mean. The problem I have with your interpretation is that it clearly implies a limit to how far you can send the info back.\\
So if I were to alter your diagram (I can't because using a laptop trackpad is shit for that sort of thing) the grey lines would all trace back to the same place, because there's no limit to how far back the info can be sent.

#+begin_quote
  It could read your brain and not kill people for a year after it comes out. Assuming that it can send back arbitrary amounts of information through your scheme, if you read the info it sends back, it has already won - see the AI box experiment.\\
  Even if it couldn't convince you outright, surely there is some info AGI experts wouldn't figure out is nefarious - see the underhanded C contest. That's probably not the part where this fails, though - with any luck, AGI experts do not accept text sent from an arbitrary future AGI. They might run screaming in little circles that suddenly the Simurgh is real and has already sung. Perhaps it could convince you of this fact and to not contact them.
#+end_quote

See I don't imagine if it escapes it would bother reading my mind unless it had superhuman cognition. Hell even if it did read my mind, the priors on this sort of thing are so low that it would probably just assume I was insane. I probably should have stated more explicitly that the sheer implausibility of the situation serves as a great deal of the defence against a rational agent like this.\\
However I wouldn't count on that, because clearly the purple gods represent our models of reality being flawed on a deep level so I don't know what kind of knowledge the GAI might be able to glean about such things. As for the AI box experiment, I don't find that nearly as convincing knowing that even EY couldn't win when he did two more trials. I still think an /actual/ SAI could win within that format but I'm less sure when it comes to the safeguards I would use in this scenario.\\
First off while I consider just straight up brain hacking a possibility, I don't think it's likely within the limitations I have imposed.

I would only allow extremely stubborn people who have shown an ability to pass the AI box experiment (I would arrange for them to take it before I hired them without them even knowing I was going to be using it as a test) consistently against my best "AI" and are otherwise qualified. They would look at it in pieces to ensure it seems to just be factual information about AI design and ethics. However first it would be looked at by extremely simple computers (in faraday cages and destroyed after the fact, despite them already having been cleared to not be hackable via this sort of input [I would have very large competitions to make sure at the very least no human could hack it just via the text input]) these would automatically destroy the message if it contained too much emotional language and didn't fit the strict style/structural requirements I would impose on myself in the future. The computers would then automatically translate it into a message just similar enough to the original to be understood in content, but hopefully different enough to diffuse some potential brain hacking danger. The original message would never be seen by any humans obviously, that's the whole point of these memetic precautions.\\
Then I would have some very good loyal codebreakers (who also definitely don't have the resources to bootstrap an AI and passed the same qualifications as my earlier people) look at the resulting info, in increasingly larger pieces in order to tease out any hidden dangers.\\
Then I would have loyal AI people actually look at it and figure out what it means in order to start my own AI project. Plus I would also make the stuff about AI goal alignment public and award massive prizes for finding flaws (of course if any flaws /were/ found then I'm scrapping the whole thing and changing my precommitments to start everything over since I have to assume everything is compromised)

Also keep in mind I would have a pseudo-world domination, as in nobody knows i'm involved, but I have convinced the world that pinhole portals is operated by some alien intelligence who causes mass destruction if world governments don't comply with his orders. However I would also use this ability to provide the world with free power, though the logistics would be difficult though worth it, (just read my answer for how that whole plan works).\\
Suffice to say I can force world governments to do whatever I want but can't risk anything that too obviously benefits myself.

Ok finally keep in mind I would already have developed the original AI (in the simulation) with the whole world's resources and intelligence behind the problem. So i'm getting the message back from basically the best possible future for FAI, so if that reality is compromised, then we probably had no chance in the first place (it would also imply that in real life we are ~100% f**ked). Though I'd like to think my safeguards with the message would still decrease risk by another few percent (which given the stakes is massive).