:PROPERTIES:
:Author: zaxqs
:Score: 2
:DateUnix: 1562545655.0
:DateShort: 2019-Jul-08
:END:

#+begin_quote
  Its gradient descent has not yet had the training data to observe that changing the utility function harms the current utility function. It doesn't yet have a concept of utility functions, or actors.
#+end_quote

This is actually a very good point, but I don't see how it would learn how to change the utility function intelligently and still believe that doing so helps the current utility function. Seems more likely that the utility function would be changed on accident and at random, if at all.

#+begin_quote
  treats that humans think some things "should" be as a coincidence, once it is modeled at all.
#+end_quote

Then why does she gain human values? Coincidence?