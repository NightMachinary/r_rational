:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1488713864.0
:DateShort: 2017-Mar-05
:END:

The boxes are all pretty massive (as they replace all factories) but you could still use larger boxes to replicate smaller ones, but i'm not so sure that's as catastrophic you make it sound. I mean the speeds limited by how fast you can move around these massive boxes and why even bother making more once you have enough in order to make anything you need?\\
Also the boxes don't /actually/ do anything, they are just made of normal material and the computer makes them indestructible and uses them as a reference point for its copy paste commands, making copies won't cause the computer to recognize them as valid targets.

#+begin_quote
  You will be maximally likely to be within that physical-law-abiding simulation which is run at maximum weight. What weight means isn't clear, but is likely to depend on the amount of computation dedicated to it. The number of simulations spawned is ludicrous, /as each simulation's smartphones can spawn more simulations/. Since the tails diverge, the universe whose inhabitants find the description of the longest-running program that halts is unlikely to satisfy your values - consider that an AGI that spawns anywhere and figures out your scheme would be pretty effective at coming up with ways to come up with long-running programs. (/Emphasis mine/).
#+end_quote

I didn't say each smartphone would get direct access to the computer, that would be idiotic since everybody would get the same anthropic power that I had. I said "considerable" processing, not effectively infinite. The obvious method of implementation is just to simulate a much better computer then read all signals in the smartphones and feed them into the simulated computer and vice versa. I fully intend to ensure I have direct control over the simulation (in the sense that the levels above me are also controlled by me)

As for halting problem I have as much processing as I need so I can just have higher levels automatically allocate resources to lower levels since the highest level has an infinite amount to spare. Barring that it's not like said anything about infinite /levels/ of simulation. I only need to do enough simulation to ensure that i'm almost certainly simulated so I can pull the tricks with the anthropic principle. If I really want it so that nobody can tell if they're at the lowest level I can just set things up so if it has insufficient processing (somehow) it instead simulates just my own mind or something similar, and things are set up so you can't know if this is the case.\\
Also it seems inconceivable how any of these simulation could crash, since they're simulated at the base level, with chromodynamics.\\
Barring that I could be content to literally just simulate my own mind a bunch of times since I can pull all the same anthropic tricks that way as well, while not necessarily needing too many lower levels.

As for AGI spontaneously spawning that's if anything the point, since as I pointed out I would have simulated version of me working on the problem for as long as necessary to be as confident as possible in it working. So if a GAI invades from a lower or higher level it was probably of my own creation and I can be as assured of its friendliness as is reasonably possible.