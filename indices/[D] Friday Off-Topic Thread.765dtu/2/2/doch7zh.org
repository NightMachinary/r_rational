:PROPERTIES:
:Author: scruiser
:Score: 10
:DateUnix: 1507957627.0
:DateShort: 2017-Oct-14
:END:

As another commenters said, these are three separate issues with some common points.

Without HPMOR it may have taken me significantly longer to break out of my fundamentalist Christian mindset, so I guess I owe EY one for that (I can elaborate more on this if you are interested). In general... I think EY has done a good job shifting the conversation so that some people are actually taking super intelligent AI seriously. I think EY has over-hyped himself somewhat... for instance, his response to Roko's basilisk and the internet flamewars he has gotten into over his response to it (for instance after XKCD made a joke about it), it is kind of counterproductive, I have a hard time understanding how he can make "learning to lose" a key morale of HPMOR and then waste the effort/reputation on continuing to fight a battle that isn't worth his time.

In general, I don't think the hard-takeoff scenario (recursive self-improvement in an exponential fashion) particularly likely... but it is catastrophic enough to be worth being aware of. However, I also recognize that a strongly super intelligent AI could still be an existential threat even without a hard-takeoff in self-improvement, and even non-super intelligent AI could still be a problem if it had sufficient resources and it wasn't aligned with human values. So I think in general "friendliness"/human-value alignment is a worthwhile problem, however the number of unknown unknowns related to it makes it difficult to properly address right now.

As for MIRI's work... well actually I haven't read any of their papers in the last few years. From the last time I did read through their work... it seemed they were focusing on mathematical formalisms that the think will be relevant to friendly AI. My problem with this was that it is kind of assuming that the first AI capable of self-improvement would fit into the constraints and assumptions of their mathematical formalisms. I wasn't really sure how to evaluate their claims at the time, and their publication rate looked kind of low. Looking at their website now, it seems like they've picked four categories to focus on and explained why the think those categories are meaningful to friendly AI. Their rate of publication also seems better, and they've actually gotten a few things published (besides internal publication and conference papers). So at worse they are at least as productive as academics working on abstract mathematics and/or philosophy. At best, some of their ideas will actually prove relevant to an actual AI.