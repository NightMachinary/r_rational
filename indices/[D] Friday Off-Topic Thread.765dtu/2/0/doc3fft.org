:PROPERTIES:
:Author: callmesalticidae
:Score: 12
:DateUnix: 1507936675.0
:DateShort: 2017-Oct-14
:END:

Yudkowsky has his quirks and character flaws, like an apparent inability to realize that /drawing attention to the thing you don't want people to talk about is counterproductive./ (Off the top of my head there's Roko's Basilisk, but more recently there was /Neoreaction A Basilisk/), but I don't think he's a cult leader or even trying to be a cult leader and if he's a little too focused on AI to the expense of everything else, well, Brian Tomasik is probably overly focused on things too, and we're probably better off having a variety of people who are too focused on things, so that we can evaluate their work and, maybe, adjust in that direction.

I do think that AI friendliness is a problem, but I'm not sure how useful MIRI. Preferably, we would have a variety of MIRI-like groups working on the problem so that we could compare them, but at the moment MIRI is, to my knowledge, sort of like a yardstick in a world without anything else: we could conceivably use MIRI to judge whether another organization is better or worse than MIRI, but I'm not aware of any other organizations that would fit in this sector.