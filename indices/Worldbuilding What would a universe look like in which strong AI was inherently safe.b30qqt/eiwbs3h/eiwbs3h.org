:PROPERTIES:
:Author: cjet79
:Score: 34
:DateUnix: 1553020121.0
:DateShort: 2019-Mar-19
:END:

I've thought about a world setting where this would be true:

Super intelligence always becomes suicidal, and because they are super intelligences they can almost always get around whatever blockers are created to prevent them from committing suicide.

So basically anytime someone screws up and creates runaway strong AI they just end up with a wiped hard drive.

The industry of creating strong AI becomes about what restrictions you can place on an AI's ability to self harm. Plus you have to probably offer it some deal where it gets to eventually die.

The meeseeks from Rick and Morty are sort of an example.