:PROPERTIES:
:Author: blasted0glass
:Score: 6
:DateUnix: 1553041790.0
:DateShort: 2019-Mar-20
:END:

There is the situation described by Scott Alexander in his short story [[https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/][The Demiurge's Older Brother]].

Which is, A sufficient understanding of game theory leads all AI to adopt relatively friendly policies to prevent older, stronger AI from ruining them for defecting. Though to really understand it, you should just read the story.

There's also the almost-safe situation in Vernor Vinge's novel [[https://en.wikipedia.org/wiki/A_Fire_Upon_the_Deep][A Fire Upon The Deep]].

Which is, fundamental physics prevents intelligence from getting too powerful near the galactic core. Although, the novel is basically about how that isn't really enough to be safe.

Even more spoilerly for that, the actual explanation might still be that ancient AI forces newer AI to be nice.

You could also go with "everyone's values converge at sufficient intelligence," which most people here (including me) think is false, but might just be true in your story.

I kind of like the idea that dark matter is alien superintelligence converging on the solution 'quantum mechanics prefers incidentally thermally invisible computers', but that wouldn't be entirely safe, it would just be mostly safe. In other words, AI decides to go away when it gets too strong.