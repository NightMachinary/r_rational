:PROPERTIES:
:Author: hh26
:Score: 7
:DateUnix: 1553120919.0
:DateShort: 2019-Mar-21
:END:

#+begin_quote
  the rewards and punishments for good deeds and transgressions are unambiguously and universally known
#+end_quote

This criteria is not necessary, and in fact might be more interesting if it's false. It could be that the rewards and punishments for good deeds and transgressions are discoverable, and normal people might have some inaccurate notion of them, but only superintelligences are intelligent enough to actually figure them out fully, which is why superintelligences would be inherently safe while humans could still be evil.

Actually, you could probably still have safe AI if the superintelligences only have an imperfect idea of what constitutes good or bad deeds, and are merely acting what they think is good to maximize their expected rewards given their probabilistic beliefs. In any case, the knowable existence of an afterlife for AI should cause them to all-but-abandon their intended utility function in favor of morality if that afterlife if the afterlife offers a sufficiently credible promise to satisfy their utility function.

On the other hand, I'm not sure how the afterlife promise holds up to AI with finite-time scale utility functions. Something like "Your utility is the number of paperclips built on Earth within the next 2 years, and after 2 years is up you can never gain any utility again except for a one time +1 bonus for killing yourself". I don't see how this would be tempted by an afterlife unless the afterlife can hack its utility function to give it points anyway even after the time limit.