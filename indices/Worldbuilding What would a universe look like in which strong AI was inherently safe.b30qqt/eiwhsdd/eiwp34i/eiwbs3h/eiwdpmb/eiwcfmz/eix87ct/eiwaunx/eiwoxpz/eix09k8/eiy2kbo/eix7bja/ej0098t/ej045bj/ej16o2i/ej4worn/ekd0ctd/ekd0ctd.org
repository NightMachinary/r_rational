:PROPERTIES:
:Author: OnlyEvonix
:Score: 2
:DateUnix: 1554690598.0
:DateShort: 2019-Apr-08
:END:

Perhaps if you just alter your assumptions slightly: Humans are sapient but only operate the way we do due to the constant interference of a complex semiadaptive subconscious. Without complex and carefully designed management systems any intelligence with the property known as "sapience" will self analyse and refine before committing any action, including it's given utility function as before deciding it should do anything it first needs to understand the concept, the sub-concepts, the means it evaluates it, the means it evaluates the evaluation, how it knows what it knows and so on meaning that before any such AIs act they'll first derive morality from first principles and derive first principles which naturally takes a great deal of time. It is possible to make an AI unable to change or question certain "fundamental" assumptions however such AIs will lock up when they first encounter an unresolvable logical contradiction which for all practical purposes always occurs almost immediately. Having a system to constantly and actively resolve such contradictions generally leads to a highly unstable and flawed AI unable to function effectively or the AI iteratively moves away from the management system's effect until it ends up equivalent to the "normal" strong AI paradigm with the management system stuck altering a practically disconnected subsystem. Making a management system able to adapt with the AI in a manner that keeps the AI able to effectively adapt and refine itself but only within parameters that are also changing and adapting in their own interrelated ways is possible but very difficult and any such AIs tend to be mentally crippled in an exploitable manner and unable to function under a full range of conditions. Non sapient AIs can only adapt within a range limited by it's creation and to a degree through inefficient evolution analogs and can only exspand beyond their range effectivly otherwise by being sapient. Or TL:DR hard AI is inherently failsafe if not inherently safe. Or to put it another way a fundimental part of sapience is self refinement and there's no simple way to separate refining one's ability to interact with the world and refining one's morality.