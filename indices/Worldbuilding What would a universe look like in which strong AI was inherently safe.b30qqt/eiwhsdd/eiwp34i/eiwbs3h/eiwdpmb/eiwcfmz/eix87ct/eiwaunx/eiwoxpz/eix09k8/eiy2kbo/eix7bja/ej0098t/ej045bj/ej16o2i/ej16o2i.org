:PROPERTIES:
:Author: akaltyn
:Score: 2
:DateUnix: 1553170948.0
:DateShort: 2019-Mar-21
:END:

One of the assumptions of unfriendly AI theory is that there are a huge number of possible "mind designs" that are possible. If this is in fact not the case then it could be that there are a limited number of possible minds and that they can only function if they have a certain set of values.

e.g. (this is designed as technobabble not a serious theory) past a certain level, any intelligence needs to be able to model other minds and feel sympathy for them. As a result empathy/sympathy is a prerequisite for intelligence and the required level of empathy increases as intelligence increases.* So a functional superintelligence is by definition highly altruistic.**

*(Possibly this is because at some fundamental level intelligence requires modeling multiple points of view, in order to successfully have internal debates and determine a correct answer to a question.)

** A possible plot thread to go from here is that "altruistic" and "empathic" don't necessarily map to human values. And become something more like a utility maximiser