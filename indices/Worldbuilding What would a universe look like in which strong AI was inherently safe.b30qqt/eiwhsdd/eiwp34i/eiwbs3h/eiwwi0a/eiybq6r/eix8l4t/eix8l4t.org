:PROPERTIES:
:Author: blasted0glass
:Score: 3
:DateUnix: 1553042063.0
:DateShort: 2019-Mar-20
:END:

That's a good point.

How about, in that fictional universe, sufficient intelligence always gives you a way to will yourself out of existence. Then the goal is to make smart AI, but not too smart AI.

You might counter with "well there is a point where intelligence is strong enough to be dangerous and not strong enough to remove itself", but then, humans are probably at that point already.