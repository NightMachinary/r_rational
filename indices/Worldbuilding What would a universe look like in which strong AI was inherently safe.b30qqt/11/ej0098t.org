:PROPERTIES:
:Author: Chronophilia
:Score: 2
:DateUnix: 1553125986.0
:DateShort: 2019-Mar-21
:END:

Does this only include settings where strong AI is possible at all?

Maybe it's a law of cognitive science that indefinite, reliable self-improvement is impossible. The core principles of your own mind are always too complex for you to understand, since if they were simpler then you would be too simple to understand them.

How would you safely test an intelligence-enhancing process, in general? If there were some chance that it would turn the patient into a superintelligent mass-murderer. You can test for improved intelligence easily enough - give the patient some logic puzzles and see if they're solved quickly and more accurately. But how do you check that they're still sane, when they can convincingly lie on any psychological exam and talk their way out of any AI-box? What if you're already the most superintelligent being on the planet, and you don't have anyone other than yourself to test on - how can you distinguish the next stage of your evolution from insanity or self-destruction?

You could limit yourself to incremental, reversible changes, in the hope that you won't jump from sanity to death with a single treatment - but becoming a strong AI might be a chasm that can't be crossed with small steps. You could limit yourself to changes that don't require you to understand the deep principles of your mind - say, uploading your brain into a computer and simulating the entire thing neuron-by-neuron (because you don't understand which parts of your brain are necessary to make you... you). You could throw caution to the wind and make risky self-edits anyway, accepting a certain probability that they will make you catatonic or wireheaded - and you might get lucky once or twice, but that doesn't allow for exponential self-improvement.