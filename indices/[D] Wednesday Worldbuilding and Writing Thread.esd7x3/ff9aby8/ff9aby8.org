:PROPERTIES:
:Author: AbysmalLion
:Score: 2
:DateUnix: 1579708447.0
:DateShort: 2020-Jan-22
:END:

So I am not sure if this really counts as world building or munchkinry, but I am posting it here because it's more world building for where it is in my process.

Let's say that you - a rationalist - find yourself in 1990 with the following advantages:

- You have worked out the rough trajectory of the future through intelligence (and a bit of luck).

  - What the internet will become.
  - Cell phone computers and 3d printing.
  - A guess about things like CRISPR and Deep Neural Networks.
  - Whatever else you can reasonably expect someone to have predicted about things that exist today (perhaps in prototype form) if they were very lucky.

- In 1995 you will have approximately 5 billion dollars in assets. And a liquid income from your primary venture (some sort of technology company) in at-least the billion dollar range every year following.
- Given your ability to project out what the future will be like, you have the ability to reach it sooner for cheaper.
- No moral or ethical boundaries.
- A small group of deeply loyal people who will do anything you ask.

Now here is the problem:

- You believe super-scaling AI will destroy humanity. You believe that this must be stopped.
- You believe super-scaling AI will first be achievable by 2020, this is your hard cut off for any plans, you have 30 years, 25 years with money.

  - You believe publicly releasing any projected technological advances you have made early would be detrimental to this number, so you will not do it unless necessary and only in a strictly controlled fashion.

- You believe that any attempt at social, political, cultural, or other soft power solution will not be enough to prevent super scaling AI from being created.

  - They may be useful tools, but in general any solution relying on them you believe will eventually fail.

- The solution must survive your lifetime. Any self sustaining human structure would be inadequate (re: the previous), it must either prevent super-scaling AI or push the problem back by a significant amount.

  - Some sort of semi-permanent progress toward preventing super scaling AI must have been made.

- Humanity must exist in some self-sustaining form.

  - Preferably space faring.
  - Your own, or anyone's, survival is not particularly important.

What do you do?

(I have my own thoughts and solutions, I am curious what others would do/think).