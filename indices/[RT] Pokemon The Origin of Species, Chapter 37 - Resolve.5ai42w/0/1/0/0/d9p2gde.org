:PROPERTIES:
:Author: Galap
:Score: 3
:DateUnix: 1478488045.0
:DateShort: 2016-Nov-07
:END:

Probably the best way to explain my viewpoint is to explain what it would take to convince me that the whole intelligence explosion/AI risk is valid.

Humans have been at about the same level of intelligence for approximately 50000 years. That intelligence itself was not sufficient to directly lead to our increased capabilities. It indirectly led to it through development of knowledge and understanding, and it took humanity 50000 years to get to that point. But while it took milennia the first time, it only takes someone a few years of learning to get up to speed. That means the gap in humans' ability to understand things and their ability to /figure them out the first time/ is very large. And I think that you can't get away from this problem, that this fact is based on deep aspects of the nature of reality and information theory, rather than being a shortcoming of just humans.

In addition it's pretty obvious to me that humans' capability of understanding things is far above the extent to which we actually understand anything, i.e. anatomically modern humans can attain a vastly greater technological and scientific level than we currently are at. So I think the window of ideas that a superintelligence could come up with that humans would not, but that humans could understand once presented to them, is actually quite large as well, contrary to what seems to be the main view.

So that means that humans 'could' have theoretically performed actions that took them from the stone age to our current level in a single lifetime, right? Well yes and no, like so many things. You can't build the LHC in a day with 21st century technology, which is all it would have to work with to start out. We tend to make basic discoveries out-of-sequence, we uncover something that seems to be useless because we don't yet have the other discoveries or the advances in the tools we need to put it to use. But if we hadn't discovered it earlier, when we did get to the point we could make use of it we wouldn't know to go looking for it because it doesn't itself follow from that point. And those advances in tools take a lot of long iterative refinements to come into being.

The second part is somehwat related. People seem to think that a superintelligence would immediately be able to make things that work without having to go through the long process of testing them, or that it will kind of inherently have ideas that are /correct/, by virture of its intelligence. I don't think that this is how it works. It won't start out knowing any more than we do, and the only way it can learn more is to perform actual experiments, and try to build things, fail for various reasons, and revise its designs based on those results. There's no free way to expand your models to include the cases where they break down: you have to do it the hard way, which is to actually perform those actions and see what happens. Sure it would be /better/ at doing all these things than we are, but I don't think it could get to be so earth-shatteringly better so quickly, and here's why:

As someone who works in experimental science, I can say that the rate limiting factor of progress is not in understanding the results of our experiments, but rather in imagining the correct experiments to perform which will reveal phenomena that we did not anticipate (which I think for the reasons above is a HARD problem) and in the time it takes to actually perform the experiments, and perform them with the rigor required to be confident of the resutlts.

In other words, the way to convince me that AI risk is real is to demonstrate to me that the following two things are not inherent to the information-theoretic nature of reality but rather human-specific failings:

1: the fact that it is MUCH easier to understand something after the fact or with an already existing example or proof than it is to devise it from nothing. (examples: it's much easier to learn the concepts of relativity than it was for Einstein to devise them. We can understand the process of technological process and see how the changes in our society came from the technological changes we've experienced, but no one could predict this in advance.)

2: theoretical models can only work well when combined with actual experimentation. You can't design something and have it work the first time without testing it and modifying it based on what happens in the tests.