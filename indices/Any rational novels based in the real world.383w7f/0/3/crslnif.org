:PROPERTIES:
:Author: IWantUsToMerge
:Score: 2
:DateUnix: 1433214344.0
:DateShort: 2015-Jun-02
:END:

Ack, yeah. Real life seems to be primarily about navigating webs of acausal cooperation protocols and trying to figure out what one's goal actually is, which makes actually applying instrumental rationality to everyday problems the last thing on one's list of shit to do.

"Build an AI that can figure out what one's goal actually is" is a nice subgoal, it's a fairly concrete thing one can work towards right now. But you still have to wrestle with the facts that doing any AI research imminentizes UFAI, and deep seated doubt as to the inaccessibility of the human's end-goals to the human itself, I mean, how the fuck could the human's end-goals be less accessible to the human than to the machine the human builds? How could that be right?! Surely there's a more direct route to self-actualization? What if that route can be traversed quicker than UFAI but FAI without self-actualization can't, and focussing on AI research instead of walking that path would damn us all?