:PROPERTIES:
:Author: Lord_Drol
:Score: 6
:DateUnix: 1497462114.0
:DateShort: 2017-Jun-14
:END:

That's not remotely true. First of all: even superintelligences are not omnipotent, and there is nothing stopping events outside ones own control from changing your utility function. Whether that's a bit-flip from a stray cosmic ray, or a more powerful superintelligence twisting you to suit its own goals, or some other reason, it's entirely imaginable that an entity's utility function changes over time. (Against that entity's wishes, of course. Well, against that entity's original wishes.)

But even aside from that: it's flat out wrong that "having a utility function" is in any way an unusual case. The VNM theorem says that any agent who satisfies four axioms has a utility function. Not that they necessarily have an explicit function that they consciously try to maximize, but rather that it is possible to construct a function that their behavior always seeks to maximize.

Now, are you VNM-rational? For instance, are your preferences transitive? If they're not, I can money-pump you for everything you've got. So if you're not money-pumpable, you must necessarily have transitive preferences.

Likewise, all the rest of the VNM axioms are ones that any non-trivially exploitable agent follows. This means you, /yes you/, have a utility function. Not that you're consistently trying to optimize it, but there is a function out there that you could be accurately modeled as trying your best to maximize.

So don't treat having a utility function as something special that only AIs do. Perhaps only an AI might be aware of the explicit formula for its own utility function, but you, me, and most other humans are utility-maximizers all the same.