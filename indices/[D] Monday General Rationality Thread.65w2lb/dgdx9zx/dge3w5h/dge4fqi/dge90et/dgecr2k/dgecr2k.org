:PROPERTIES:
:Author: eniteris
:Score: 3
:DateUnix: 1492473716.0
:DateShort: 2017-Apr-18
:END:

The main question is "why can't we make an AI in the human mindspace"

What is the difference between a human and a paperclipper? Why is it that humans don't seek to maximise (what seems to be) their utility (whether it be wealth, reproduction or status). Why does akrasia exist, and why do humans behave counter to their own goals?

And are there ways to implement these into AIs?

Although that is a good question. Why don't humans end up as paperclippers? Why do we have maximal limits on our goals, and why don't we fall prey to the fallacies that AIs do? (ie: spending the rest of the universe's mass-energy double-checking that the right number of paperclips are made)