:PROPERTIES:
:Author: eniteris
:Score: 4
:DateUnix: 1492455092.0
:DateShort: 2017-Apr-17
:END:

I've been thinking about irrational artificial intelligences.

If humans had well-defined utility functions, would they become paperclippers? I'm thinking not, given that humans have a number of utility functions that often conflict, and that no human has consolidated and ranked their utility functions in order of utility. Is it because humans are irrational that they don't end up becoming paperclippers? Or is it because they can't integrate their utility functions?

Following from that thought: where do human utility functions come from? At the most basic level of evolution, humans are merely a collection of selfish genes, each "aiming" to self-replicate (because really it's more of an anthropic principle: we only see the genes that are able to self-replicate). All behaviours derive from the function/interaction of the genes, and thus our drives, simple (reproduction, survival) and complex (beauty, justice, social status) all derive from the functions of the genes. How do these goals arise from the self-replication of genes? And can we create a "safe" AI with emergent utility functions from these principles?

(Would it have to be irrational by definition? After all, a fully rational AI should be able integrate all utility functions and still become a paperclipper.)