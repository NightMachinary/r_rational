:PROPERTIES:
:Author: Nimelennar
:Score: 2
:DateUnix: 1523378138.0
:DateShort: 2018-Apr-10
:END:

I think that utilitarianism is a fine moral system, given the ability to predict, with reasonable accuracy, the consequences of your actions. Individual humans, in my opinion, generally don't have enough good data to make those decisions, nor the detached, unbiased perspective necessary to determine all of the probable effects of their actions, even with sufficient data.

I want my /government/ to be utilitarian. It (ideally) has the data and processing power to effectively make that work. I want the people /around/ me to have a moral code created /through/ utilitarianism, which probably wouldn't be utilitarian itself (short of massively expanding the human brain's storage and processing power, and correcting its natural logic). It'd probably be a series of simple rules that our pathetic brains can understand and adhere to, starting with "Question everything, including the rules of this moral code." I can't say for certain what the other rules would be (as I don't consider my predictive power anywhere near sufficient to approach functional utilitarianism), but I imagine "Thou shalt not torture" would make the list.

On an /individual/ basis, which is what I was referring to in the text you quoted, the thought patterns for "Do this bad thing" and "Let this bad thing happen" are much different, which is what I mean by saying that they're morally quite different. And since thought patterns reinforce themselves the more that they are used, if I were writing this story and could therefore predict all of the consequences of the characters' actions, I would consider the /utilitarian/ thing to do would be /not/ to have the characters reinforce the pathways which allow them to ignore pain that they are deliberately causing (or to only do so to the minimum extent necessary to escape the loop and save the world).