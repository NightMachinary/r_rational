:PROPERTIES:
:Author: Artaxerxes3rd
:Score: 7
:DateUnix: 1419277925.0
:DateShort: 2014-Dec-22
:END:

It all sounds perfectly reasonable to me, but considering this is [[/r/rational][r/rational]], where the particular brand of 'rational' in use is the LessWrong variant (due to Yudkowsky's HPMoR being the codifier), I feel like someone should really mention that while all the stuff about labour replacement and increased automation is relevant and a more or less probably highly accurate description of the future, the video doesn't really begin to take the next important step.

I mean, if you're in this subreddit you've probably heard it a million times already, but first AI gets better than humans at chess, then Jeopardy, then driving cars, then a bunch of other things (and lots of people lose their jobs), and eventually, the day will come when AI is better than humans at building AI. And then FOOM, right? Recursive self-improvement occurs and the result is possibly a 'superintelligence', an AI drastically better than humans at pretty much everything. All of a sudden the labour concerns aren't as big of a deal, and the big question is whether or not we've built the AI correctly, right?

That's not to say that it wouldn't be a bad idea to make sure the labour concerns are dealt with effectively - I'm sure there are plenty of utilons up for grabs if we transition through increasing AI capabilities effectively. Personally I think UBI is very promising as a solution for the most part, but it's politically seemingly very far away from getting anywhere anywhere and ofc it might not actually be a very good policy for some reason I'm yet to know.

But the big one to get right, at least if we're talking about AI, is the very possible intelligence explosion.

Or, you know, any of the other potential scenarios people talk about, like for example the multi-polar malthusian race to the bottom by brain emulations or whatever. The future is kinda tricky to predict.