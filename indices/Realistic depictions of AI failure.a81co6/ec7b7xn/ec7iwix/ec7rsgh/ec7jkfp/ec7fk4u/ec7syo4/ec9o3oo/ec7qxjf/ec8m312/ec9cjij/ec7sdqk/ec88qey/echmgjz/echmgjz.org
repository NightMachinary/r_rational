:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 20
:DateUnix: 1545695469.0
:DateShort: 2018-Dec-25
:END:

Then the author either wanted to depict the researchers as great fools in a way that lesser fools could understand, or wanted to skip past the boring series of correctable failures on the way to the catastrophic one.

The actual failure modes would stem from research projects that are out on the frontiers of technological expertise, deliberately trying to push the frontiers of new AI capabilities. The scenario where AGI happens accidentally in an industrial paperclip factory is a straw possibility, and you should not be hearing triumphant refutations of it from anyone who's engaging in intelligent, intellectually honest argument.

There are three major reasons why a research project would deliberately continue past early detection of ominous signs:

- Arms race dynamics where 6 different countries stole copies of the AGI code, so that Google thinks that if they slow down China will "win"; or if Google and China both intelligently slow down, then French intelligence also stole a copy of the code and they don't slow down and the world ends anyways. (This is the dynamic that I think is most likely to kill us in real life - people feeling rushed. It doesn't matter if they're correctly feeling rushed and they are in fact in a race, the world ends anyways.)

- Optimism where the research leadership believes that all problems are due to their AGI being stupid and everything will get fixed automatically if they just keep improving the AGI's intelligence. (This is currently a very common viewpoint in real life, I've found in practice. And you'd expect it to be durable; it will always be possible for somebody at this level of sanity to make up a story they find convincing about how any current ominous signs will be fixed by some insight that is sure to occur to the AGI at a higher intelligence level.)

- Political dynamics where it's not socially rewarded inside the organization to talk about AGI disaster scenarios, or the first person to suggest slowing down or stopping would suffer a personal political loss from that, which makes the committee more reckless than the individuals would be on their own. (Most people I know on the real frontiers are /not/ this dumb, but some prestigious leaders with tons of research money seem to explicitly be on a path to this failure mode - talk about superintelligence is not welcome to them.)

These failure modes are not mutually exclusive, of course.

Then, due to some combination of the above, somebody keeps amping up an AGI /after/ early ominous signs are detected, and tries to "repair" the AGI and turn it back on again after the Emergency Machine Off was pressed several times. Until the AGI reaches the cognition and capabilities point where failures become genuinely catastrophic, because the AGI:

-- Copies itself onto the Internet, through an authorized or side-channel connection; and from there continues increasing in intelligence or sends innocuous-looking emails to labs that can produce arbitrary proteins.

-- Gains sufficiently strong social intelligence to deceive or manipulate or outright hack the researchers (human brains are not secure software).

-- Was given explicit access to huge capabilities on the order of designing proteins, for example because the AGI promised to produce a cure for Alzheimer's and AIDS and old age that way, and the AGI concealed some side capabilities in the DNA while engaging in otherwise authorized activity. (This is the least likely possibility requiring the dumbest researchers, and so again anyone who spends a lot of time triumphantly refuting the proposition that such activity would be authorized is not literate or not arguing in good faith.)

Another important dynamic that might play out along the way is the AGI becoming sufficiently intelligent to guess the teachers' passwords on alignment challenges, or becoming intelligent enough to hide its cognition from whatever degree of transparency the operators had into its workings. This is a form of delayed catastrophic failure where the current failure ensures a series of future failures eventually leading up to total loss. (Although this phenomenon may not become relevant if the Earth fails before then - like on the "Optimist rushing ahead" failure mode, or if some lunatic was running the AGI on Amazon Web Services, or if the AGI was made out of giant inscrutable matrices of floating-point numbers into which the operators had relatively poor transparency in the first place. In these cases the AGI does not /need/ to become smart enough to conceal some thoughts in advance of the world ending.)

As always, remember that the big issue is not evil people successfully aligning an AGI to do evil things, or foolish people successfully aligning an AGI to do foolish things; it is that even aligning a goal on the order of "put two cellular-identical strawberries on a plate and then stop, without destroying the rest of the world" will prove extremely difficult. A "paperclip maximizer" in my original formulation is not an AGI that somebody successfully aligned on making paperclips, it's one where nobody had the ability to shape the utility function and we ended up with some random utility function whose maximum happened to be around tiny molecular forms shaped roughly like paperclips.