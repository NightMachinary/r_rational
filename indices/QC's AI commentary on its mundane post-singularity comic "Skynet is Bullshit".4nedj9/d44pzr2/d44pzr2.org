:PROPERTIES:
:Author: Galap
:Score: 1
:DateUnix: 1465636732.0
:DateShort: 2016-Jun-11
:END:

Yeah, this is partly why I don't agree with the 'AI intelligence expolsion' idea. The concept that making a self-improving system will allow it to extremely rapidly progress to omnipotence seems really simplistic. Ideas that sound like they're going to work are much easier to come by than ideas that actually do, and I think that applies to this situation in a couple of ways.

This would apply to the AIs as well. It seems to me that there's a very important property of the universe whose consequence is that many things are easy to undestand or verify after the fact but extremely difficult to arrive at from nothing. As one particular example of this, I would be EXTREMELY surprised if P=NP.

I suppose it's conceivable that the limitation is only a part of us, and not inherent to the universe, but that seems very unlikely. However, if for example, it was proved that P=NP, I would believe it more likely that intelligence explosion is possible.