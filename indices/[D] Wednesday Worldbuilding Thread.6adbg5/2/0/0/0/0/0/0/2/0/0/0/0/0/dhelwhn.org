:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 1
:DateUnix: 1494471582.0
:DateShort: 2017-May-11
:END:

#+begin_quote
  What I said is hardly vague, since all you would need to do is run simulations of a vast number of minds and compare them to simulations of my own mind in order to determine which conditions will lead to minds within a range that produce informed moral judgements the simulations of me deem acceptable. That's why I mentioned the bit about using conditionals based on simulations of yourself.
#+end_quote

Okay, let me try a different tack. This part of what you said, right here?

#+begin_quote
  run simulations of a vast number of minds and compare them to simulations of my own mind
#+end_quote

/I can't do that./ You haven't given me a /mind/; you've given me a /process for getting a mind/, and it's not even a process I'm capable of carrying out. To put it in programming terms: my original query asked for an object of type Mind; instead, however, you provided me a call to a function with /return type/ Mind. The problem is that this function is nothing more than a prototype, and so when I try to call it, I get an error. It's in this sense that I say your suggestion doesn't answer my question.

#+begin_quote
  From the perspective of a truly alien amoral entity it would likely appear that most humans are already hard coded with a relatively small range of moral systems. I think it's underappreciated just how similar most people's moral beliefs already are once you strip away differing models of reality and just how complex people's moral instincts are, and i'm sure you're aware that some of these things like a desire for fairness are present in other animals.
#+end_quote

The thing you're missing here is that human behavior, like that of most animals, is largely driven by /instinct/, not moral systems. Now, we happen to have a high-enough level of abstract reasoning skill that we're able to come up with and /describe/ a moral system that our actions are roughly consistent with, but from a purely biological perspective, it's our subconscious tendencies and desires that drive us (what Freud would call the id).

In other words: if you're trying to describe a (biological) mind in terms of moral imperatives, you're working on a higher level of abstraction that, from a reductionist point of view, simply does not exist. It's fine to talk about morality, but when your reference class is the /space of biologically plausible minds/, you're much better off talking about psychological tendencies (such as, again, the empathy-driven hivemind example). Which is to say:

#+begin_quote
  It's only necessary for my purposes that people be much more skeptical (to avoid bizarre models of reality confounding things) and have gut feelings about ethics very similar to my own.
#+end_quote

/This/ is a much better way of putting things than "everyone has the same morals I do". But even so, we run into the same problem as before: by describing these hypothetical people in terms of your own mind, you're offloading the vast majority of the complexity into a single word, "I". You're not giving any detail here--a black box labeled "I" would be about as informative. Here, try this question:

Would a society full of [[/u/vakusdrake]]'s mind-clones with insta-kill powers /really/ be stable? How sure are you that, given Death Note powers, you /wouldn't/ give into the temptation after a while? Maybe you're quite sure, I don't know--but that's the point: /I don't know./ I don't know because I don't have a good description of what your mind is like because /you didn't give me one/. Sure, you gave me a hypothetical process for finding out, but all that does is make a call to a function that doesn't exist. As far as worldbuilding goes, it's a non-answer, a dodge.

Hope that makes my viewpoint a bit more clear.