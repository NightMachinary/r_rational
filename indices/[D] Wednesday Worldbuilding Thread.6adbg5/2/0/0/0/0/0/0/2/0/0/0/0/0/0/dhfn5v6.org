:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1494530859.0
:DateShort: 2017-May-11
:END:

#+begin_quote
  The thing you're missing here is that human behavior, like that of most animals, is largely driven by instinct, not moral systems. Now, we happen to have a high-enough level of abstract reasoning skill that we're able to come up with and describe a moral system that our actions are roughly consistent with, but from a purely biological perspective, it's our subconscious tendencies and desires that drive us (what Freud would call the id).
#+end_quote

As I said before the point here is not to imbue some complex arbitrary moral system, but merely to insure they all instinctively have the same moral intuitions as myself. After all as I said before my ethical systems are generally the most self consistent interpretation of my moral intuitions. So the idea here is that when it comes to authoritarianism, murder or anything else they will have gut feelings which align with my own. Like I mentioned previously a massive amount of the core of people's ethics seems to be greatly determined by genetics, so I don't think the idea that you could control the gut feelings that one's ethical systems are built on is implausible.

However when it comes to specificity I think I was looking at this rather differently than you since my strategy was essentially to come up with the best strategy in universe as it were, based on what I know about the specifications of the setting (and thus I would assume that if I give clear instructions to the genie for how to get the information it needs to fulfill my instructions things ought to work out).

But if we're not allowing those sorts of tricks then I would just describe a laundry list of moral intuitions that I would be instilling, as a sort of poor approximation for just directly instilling my ethical system which while a terrible idea for inhuman or superintelligent entities should work quite well for humans. Ultimately I find that solution in many ways less satisfying since it forces a clear divergence in my behavior in and out of setting, since in setting I would have massive incentive to spend as much time and effort as possible coming up with an approximation for my moral system.

Whereas out of setting (so what I'm giving you as an example of something that may sort of resemble what I real version ought to resemble) I'm not going to do that (not that coming up with an explicit list of moral intuitions to stick in ideal humans isn't my idea of fun, just that I don't currently have too much time on my hands and i'm probably already spending more time writing this than I can justify) and would instead try something simpler like Rawlsian veil-of ignorance style contractualism as a base (so sort of like some of the slatestarcodex articles on the topic), then tack on much more intense versions of some of my moral intuitions so as to try to compensate for the lack of certain important intuitions I would likely not think to stick in. So yeah stick in a /much/ stronger aversion to violating people's preferences and a far great desire for altruism (with primary focus on making people able to satisfy their preferences, to guard against paternalism).\\
Then as I mentioned before just go through say the sequences and eliminate every flaw in human thinking you can, make people care far more about the truth and less personally attached to their beliefs as well as generally more curious. Plus since you said no superhuman intelligence I'll increase mental abilities as much as I can within those bounds, so everybody's a genius with an amazing ability to model others thinking.