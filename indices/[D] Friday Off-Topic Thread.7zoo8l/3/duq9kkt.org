:PROPERTIES:
:Author: phylogenik
:Score: 4
:DateUnix: 1519423094.0
:DateShort: 2018-Feb-24
:END:

I have a basic question about an inference problem I'm working on -- if I have a prior of 0 and a likelihood of +âˆž am I ok? To simplify, let's say the model I'm fitting has 3 continuous parameters with two normals and one exponential for priors. When the first two parameters have identical values and the third has value zero (so positive prior densities all around) I get singularities in my likelihood surface. But the set of these combinations has measure zero, so despite infinite posterior density (?) at infinitely many points I think I'm still good? Obviously in ML-inference you'd be in big trouble but I think Bayes is ok (i.e. I'm semi-confident-ish that region of the posterior integrates to some small finite value but sadly my math background is v. lacking so idk quite how to formally demonstrate that -- edit, to clarify, through calc 3, diff eq, linear algebra, real analysis, though quite rusty. Understanding of stats/prob theory is very cobbled together lol from different papers, seminars, non-rigorous machine learning/stats application-focused books, etc. Really need to sit down some month with a proper textbook and have at it)? I'm also approximating the joint posterior numerically via mcmc (there's no analytic solution) and the chain never even wanders into that region of parameter space, but even if it's not a practical concern I'm worried it might be a theoretical one, despite brief assurances from a few math/stats PhD friends that it's not. I'll ask them for references when I next see them but figure I could ask here first. Does anyone know of any good papers or book chapters I could read (or cite)?

edit: actually, come to think, this would be an issue in any regression problem with a normal likelihood where the variance is a free parameter, right? Even ones that don't allow measurement error, since you only need one infinite log-likelihood for their sum to be infinite. Although hmmm I guess then all the others would be -inf, so accommodating measurement uncertainty /would/ be necessary? So now I think there has to be a name or paper for this... although come to think would that mean maximum likelihood can't accommodate measurement error for those models? (I've only ever worked on those problems in a Bayesian framework)