:PROPERTIES:
:Author: Nimelennar
:Score: 4
:DateUnix: 1572220675.0
:DateShort: 2019-Oct-28
:END:

#+begin_quote
  I was positive that was what you were referring to.
#+end_quote

I can't see why; I never made any reference to what values are, well, valued, and, while the story hints at a lack of empathy on Luna's part after ascension, all that's made clear is that her values have suddenly become incomprehensible.

#+begin_quote
  I'd at least look into it, especially since there's only one data point and the outcome wasn't a paperclipper.
#+end_quote

Look into it how? The only person Twilight can experiment upon is herself, which risks corrupting her value system. Cadence's mind is functionally gone, and Celestia doesn't seem to be volunteering for experimentation, and /no one else exists/.

It should also be noted that she may consider her value system as /already/ having been corrupted - she has already found, from the last incarnation of Equestria, that she can no longer value the company of new ponies.

#+begin_quote
  I have to say this is a false analogy. Again, one data point. You can't really say that most ascensions result in brutal suppression, or even that it's likely. All we know is that is happened.
#+end_quote

Yes, we have one data point, which means it seems to have happened /one hundred percent of the times it's been tried./ And they don't seem to have any understanding of /why/ it happened, either. That, if anything, says the Goa'uld metaphor is /underselling/ the risk (you've /heard tales/ of these supposed Tok'ra, but neither you nor anyone you've met has actually encountered one; the one Goa'uld any of you have met has been of the "brutally suppress the original personality" variety).

Imagine a rocket that can only launch with human guidance. The first time it launches, it explodes catastrophically, killing its pilot, and you have no idea why that happened, because you can't even simulate it properly without a human consciousness attached and at risk.

How can you ethically test that rocket a second time, knowing that the most likely outcome is that it will explode again and kill the pilot again (and again, and again, until you have done enough simulations to track down the factor which is causing the rocket to explode)?

And that analogy doesn't even do the situation justice, because what we're talking about is a radical shift in core values. The /first/ time, the shift was towards something seemingly harmless, but completely alien, something that looks upon normal people like bacteria, but doesn't care enough to harm them. Yes, the first attempt didn't become a paperclipper, but if you admit the second attempt might turn out /better/ than the first, you should also admit that the second attempt might turn out /worse/.

#+begin_quote
  What is stupid is never even trying to investigate a way to perform an uplift while still holding your previous values.
#+end_quote

By definition, you're creating a new person who thinks differently than you do; if not, what is the point? Since they think differently than you do, you cannot predict how they'd think; if you could predict how a person thinks, you can become that person /without/ an uplift (or, at least, with just a boost in processing power and memory retention, which probably wouldn't do much to fix ennui).

Despite all of that, I'll grant that it might be /possible/ to come up with a way to do a safe upload, where values are retained. But it's made clear that Twilight and Celestia are the last two intelligent life forms on the planet. They'd have to seek out, or create, a whole other civilization in order to start those tests, which will take who-knows-how-long, and Twilight (who already seems to be experiencing value decay) doesn't want to go through that again. And, for a prize which is far out of reach, and which the only data point she has suggests /may not even exist/, why should she?