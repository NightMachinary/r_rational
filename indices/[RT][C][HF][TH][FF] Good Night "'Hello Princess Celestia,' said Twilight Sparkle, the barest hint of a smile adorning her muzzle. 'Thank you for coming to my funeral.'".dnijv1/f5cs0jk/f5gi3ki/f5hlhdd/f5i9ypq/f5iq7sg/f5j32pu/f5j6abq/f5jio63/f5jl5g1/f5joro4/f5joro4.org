:PROPERTIES:
:Author: Nimelennar
:Score: 3
:DateUnix: 1572234860.0
:DateShort: 2019-Oct-28
:END:

#+begin_quote
  base rate fallacy
#+end_quote

The base rate fallacy is only a fallacy *if* the base rate is different than the specific information. /We don't know/ what the base rate is. Sure, it's /probably/ not 100%, but if Luna is the only subject who has been upgraded, it's probably not 0.0001% either (or, there'd only have been a 1:1,000,000 chance that she'd be corrupted if it were).

If you have some in-universe information to suggest that Twilight should know that the base rate of value drift when ascending is low enough to be worth the risk, I'm happy to hear it.

#+begin_quote
  Excellent. Wipe it clean and start over.
#+end_quote

...Okay, you've taken a decided turn towards the evil here. Creating new minds to be subjected to experimentation is one thing, but going against the express wishes of a friend as to the disposal of her body/consciousness?

I'll skip the assumptions you're making why Luna became what she became, and state that it doesn't really matter /why/ she did; all that matters is /Twilight's perception/ of why she did. Because that's what she's making her decision based upon (and she can't really obtain more data on this, because Luna has already left). And, in her perception, it was due to the ascension.

And yes, there's only one data point, but one data point is /still a data point/. All you have to weigh /against/ that data point is supposition.

#+begin_quote
  You can certainly imagine ways to use processing power to emulate this, yes, but you're not engaging with the core point I was making. There are ways we can imagine that modify how we think and that are beneficial.
#+end_quote

Yes, but you're missing /my/ point. My point is that any mind that you can sufficiently emulate with your own mind is, pretty much by definition, already present within your own mind. Any mind that you /can't/ emulate, you /can't/ predict. So, anything /safe/ (like processor speed) won't relieve your ennui, because you can pretty much become that person by choice, just slower. Anything sufficiently different from you as to relieve your ennui, if /everything/ bores you, isn't someone you can safely assume will retain your values, because you can't sufficiently emulate them (and, if you could, you wouldn't be stuck in a state of ennui).

#+begin_quote
  Perhaps I failed to convey the point. Copy consciousness. Place it at root, with root access. Set emulation speed at many times higher than the secondary consciousness.
#+end_quote

So, you have a slow-thinking subprocessor. .../How/ exactly is this supposed to relieve ennui?

#+begin_quote
  Wrong angle. These are two questions. Why not die, and why not live. She has answered why she doesn't want to continue /as she is/
#+end_quote

Yes, and, by your own admission, she'd /have to continue as she is/ in order to do the research necessary to safely continue as something else. Which, as you also admit, she /doesn't want to do./

#+begin_quote
  has failed to adequately consider alternatives because she is tired
#+end_quote

Even if I concede this (which I don't; we haven't seen how long she's spent considering alternatives to declare whether it's adequate or not; we certainly can't assume that based on the conclusion she reached), "tired" is not "stupid."

#+begin_quote
  She has then defaulted to why not die. She has defaulted to the position of evil.
#+end_quote

And now we're back to values. You consider her death evil. Which, okay, that's your value judgement. But you're imposing /your/ values on /her./ *Values are not universal constants.* If her values are such that, after many, many lifetimes of rational consideration, she has concluded that it is time for her life to end, I think that is her choice to make. /Her/ values should decide what becomes of /her/ body and /her/ consciousness (just as Cadence's values, a preference that her happiness should be maximized, determined what happened to her).

If you think death is evil, you are well within your rights to never die, if you can manage to pull it off. But, as far as /my/ moral values state, you have no right to make that determination for others.