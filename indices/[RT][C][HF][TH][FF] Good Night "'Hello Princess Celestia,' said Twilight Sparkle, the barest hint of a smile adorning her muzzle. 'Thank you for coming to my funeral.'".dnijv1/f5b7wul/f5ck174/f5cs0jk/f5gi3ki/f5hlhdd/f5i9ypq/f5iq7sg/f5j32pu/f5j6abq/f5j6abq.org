:PROPERTIES:
:Author: Lightwavers
:Score: 3
:DateUnix: 1572221860.0
:DateShort: 2019-Oct-28
:END:

#+begin_quote
  I can't see why
#+end_quote

I thought it was implied. People value empathy.

#+begin_quote
  which means it seems to have happened one hundred percent of the times it's been tried.
#+end_quote

You've stumbled straight into the [[https://en.wikipedia.org/wiki/Base_rate_fallacy][base rate fallacy]] there. We know of one case where, taken to its extremes, this has seemingly turned someone into an unempathetic jackass who'd rather build things in the stars than talk to people.

#+begin_quote
  and no one else exists.
#+end_quote

Easily solved. Celestia herself contemplated making new ponies at the end of the story. So experiment on them. Or, hell, experiment on Cadance. I'm sure she won't mind.

#+begin_quote
  (you've heard tales of these supposed Tok'ra, but neither you nor anyone you've met has actually encountered one; the one Goa'uld any of you have met has been of the "brutally suppress the original personality" variety).
#+end_quote

This analogy has gotten really far off track. First, there's no suppression going on. We /haven't/ heard of anyone encountering one of these supposed oppressive beings, or unfriendly AI, and the only person who did self-modify was already predisposed to introversion, megalomania, and depression.

#+begin_quote
  How can you ethically test that rocket a second time, knowing that the most likely outcome is that it will explode again and kill the pilot again
#+end_quote

Well first off you don't assume that one failed test means it's going to fail again. Second you recognize that the first test didn't really fail at all---as you yourself said earlier, there's nothing /wrong/ with having values that mean you spend your time playing with starstuff. Third, you make new individuals and you ask for the consent of the suicidal ones, if you're going to make new individuals anyway.

#+begin_quote
  but if you admit the second attempt might turn out better than the first, you should also admit that the second attempt might turn out worse.
#+end_quote

The first AI will have all the power. So far that's Luna, and she doesn't care enough to harm anyone. But assume that the second attempt turns into a genocidal maniac. In story we have Discord, Tirek, and the Elements, all of which could conceivably deal with such a threat.

#+begin_quote
  Since they think differently than you do, you cannot predict how they'd think
#+end_quote

False. So long as you understand how exactly this person deviates, you can definitely predict how they'd think. But what if this person, say, thinks twice as fast and has the ability to instantly make themselves devoted to any task. You can predict how they'd think, /and/ you can see how you can't just become that person without modifying your brain. You don't just need a boost in processing power and memory, but in the ability to modify. In the story, Luna continually modified herself until she became an alien. Just set, say, a max of three modifications per year, with unlimited ability to reverse. Or build a guidance consciousness that reverses any changes she finds abhorrent that polices the process.

#+begin_quote
  And, for a prize which is far out of reach, and which the only data point she has suggests may not even exist, why should she?
#+end_quote

Remember what evil would say if you asked it why it did what it did.