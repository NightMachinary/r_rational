:PROPERTIES:
:Score: 2
:DateUnix: 1479150520.0
:DateShort: 2016-Nov-14
:END:

Ummmm /huh/? It's fine to have a value function over causal trajectories. The point of reinforcement learning is to signal to the organism what its evolved needs are, not to maximize the reward signal while detaching it from any distal cause.

Also, changing the world to make things more efficient is still changing /the world/ rather than just changing your sensory signals.