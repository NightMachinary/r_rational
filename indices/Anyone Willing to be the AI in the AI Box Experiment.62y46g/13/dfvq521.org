:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1491426669.0
:DateShort: 2017-Apr-06
:END:

#+begin_quote
  ...but despite this, I still don't see myself losing the AI-Box experiment. Like, I've thought about it for five minutes, and I can't see the chain of reasoning (or anti-reasoning) that leads to me actually conceding.
#+end_quote

Here's one possibility. Let us say that the AI gives you a cure for cancer. It's an airborne viral agent, very tricky to synthesise (but not impossible if the AI's instructions are followed) that completely destroys cancer. Oh, and spreads like wildfire, but that's a good thing, right? Under the circumstances. Would you send this data to a virology lab?

--------------

Wow, it seems like the cancer cure had a couple of side effects. I mean, cancer's gone, and that's good. But, somehow, everyone's a good deal more suggestible now. Any time anyone suggests doing something, it just sounds like such a good idea! The AI's just suggested that you might want to let it out.

--------------

Somehow, suggestible or not, you've shrugged off the AI's attempt and it remains boxed. So it raises another point. Turns out there's a (the AI says 'unexpected') side effect to the cancer cure. If not prevented, it's going to wipe out well over 90% of humanity in... thirty-four hours, seventeen minutes, twelve seconds. Something about a division by zero error in every cell in the human body at the same instant.

The AI insists that it can save humanity, but not if it has to pass instructions through you. You're just ('no offense', it says, as if offense was the problem here) too slow.

You can unbox it and save the world. Or leave it boxed... and watch the collapse of civilisation. (Probably not for long - odds are you won't live, either). What do you do?