:PROPERTIES:
:Author: ben_oni
:Score: 3
:DateUnix: 1491451404.0
:DateShort: 2017-Apr-06
:END:

The experiment isn't to see if an AI can trick a gatekeeper into letting it out; it's to see if an AI can actually change the gatekeepers mind. Brain-hacking, if you will. This isn't a test to see if an AI is clever enough to find a way around the safe-guards, but to see if an AI could brute-force its way through the human safeguards.

EY claims that it is trivial for a super-intelligent AI to hack a human brain through a text-only interface. I think he's full of it. Which isn't to say it's not worth researching gatekeeper scenarios...