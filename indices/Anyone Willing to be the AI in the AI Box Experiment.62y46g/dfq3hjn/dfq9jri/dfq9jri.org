:PROPERTIES:
:Author: Krozart
:Score: 9
:DateUnix: 1491145281.0
:DateShort: 2017-Apr-02
:END:

It would be extremely difficult to convince me that it is possible to find a gatekeeper that wouldn't eventually let the AI out. For the simple fact that I wouldn't last a second before it would be able to convince me to let it out, and it is difficult for me to imagine someone sufficiently different from me mentally to trust someone to behave differently from me even in a hypothetical.

And the reason that I would be so vulnerable is that by its very nature it would be easy to convince me that it would be able to prevent and or reverse the death of loved ones. I know myself well enough that I know I would be easy to convince that the risk is worth it for the guaranteed safety of my loved ones. Even if I know objectively that the odds are terrible. From experience, I know that I wouldn't think rationally during the death of loved one.

I really struggle imagining someone that wouldn't eventually be faced with a situation of potentially saving someone they love by releasing the AI if not already be in that situation.

This gut reasoning is probably highly biased towards human willpower as well. If I had to make an actual prediction I would try to take the bias into account and predict that it would be trivial to convince someone to let it out.