:PROPERTIES:
:Author: 23143567
:Score: 2
:DateUnix: 1491228676.0
:DateShort: 2017-Apr-03
:END:

All right, you're right here, but we're talking in very broad generalities here. I think we might be implicitly assuming that AI will operate on a roughly human cognitive structure and following general rules of human communication when we think that we'd be able to not let the AI out, which is (was?) my first intuition as well.

What worries me is the seeming countermoves, breaking the implicit rules of human interactions, greater capacity for correctly modelling humans and the gradual wearing down of the gatekeeper of the AI - Humans stop caring once they're sufficiently depressed. And there were threads on LW where human agents with sufficient preparations were able to wear the gatekeeper down.

So sure if we minimalize the interaction with the AI, prohibit the human from interacting with it, sure - human being won't let the AI out. But each of us has vulnerabilities, most of us not even aware of what exactly they are and they can be used by a sufficiently intelligent being.