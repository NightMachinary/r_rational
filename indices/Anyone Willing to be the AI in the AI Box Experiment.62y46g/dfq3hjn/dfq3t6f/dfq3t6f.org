:PROPERTIES:
:Author: GopherAtl
:Score: 19
:DateUnix: 1491132610.0
:DateShort: 2017-Apr-02
:END:

I have long suspected that the people most interested in this kind of exercise, and most active in considering and debating the dangers of and controls necessary for strong AI, are actually more likely to let the AI out of the box. As much as they focus on the dangers, at the end of the day, they are ultimately driven forward by a belief in how much potential for good there is. It's telling that the loudest voices in discussions about the danger of AI are /not/ groups who oppose creating strong AI altogether; it's those looking for ways to make it safer. Yes, someone could - possibly inevitably would - ignore naysayers and popular opinion and do it anyway, in secret. I don't see why such an actor who obviously disregards, or at least underrates, the risks would be any more influenced by the kind of research MIRI does. So, to me, the choice to research and find solutions for the risks is primarily a step towards, ultimately, taking the risk.