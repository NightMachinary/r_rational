:PROPERTIES:
:Author: paranoidsp
:Score: 1
:DateUnix: 1491306327.0
:DateShort: 2017-Apr-04
:END:

I'll give you an example of a losing position. If I were the gatekeeper, I'd lose. I've already lost. If the ai can figure out a way to convince me that it would be a benevolent ai, and I believe that argument to be impossible to fake, keeping he ai int he box would be the biggest mistake I would ever make. So the ai wouldn't be confronting an agent precommitted to not letting it out, it would find someone desperate for a logical reason to believe it benevolent.

So my problem with gatekeepers is the idea that they precommitt to not letting the ai out in any situation. The possibility that there is a foolproof argument to show that someone was benevolent means that there should be a chance to let the ai out.