:PROPERTIES:
:Author: ben_oni
:Score: 3
:DateUnix: 1491325738.0
:DateShort: 2017-Apr-04
:END:

I've been thinking about this the last couple days. After reading up about this exercise, I'm convinced that it can only be done in specific narrow circumstances.

The basic game is for the "AI" player to convince the "Gatekeeper" player to forfeit and pay out. The context of that is the Boxed AI problem. A certain sort of actor is much more likely to respond to AI-related arguments, particularly those sorts of people who are interested in developing transcendent AI.

That isn't to say most people won't eventually be susceptible to some sort of argument, line of reasoning, or emotional manipulation. Allow me to rephrase he game rules: One person says, "We are going to talk for two hours, and then you'll give me ten dollars," and the other person says, "Oh really?" This is just a kind of con, and as everyone knows, an important part of any con is picking your mark. For every potential gatekeeper who won't be tricked into letting out the AI, they'll almost always be caught in other tricks and pay out again and again and again, often without even knowing they've been had. While one might be able to pre-commit to keeping the AI in the box, no one can pre-commit to never being conned again.