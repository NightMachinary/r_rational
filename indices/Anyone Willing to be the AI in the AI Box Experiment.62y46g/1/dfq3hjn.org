:PROPERTIES:
:Author: cretan_bull
:Score: 13
:DateUnix: 1491131537.0
:DateShort: 2017-Apr-02
:END:

#+begin_quote
  ...but despite this, I still don't see myself losing the AI-Box experiment. Like, I've thought about it for five minutes, and I can't see the chain of reasoning (or anti-reasoning) that leads to me actually conceding.
#+end_quote

I concur with your assessment. I too have thought about this a fair bit and don't see how even a superintelligent AI could convince a vastly less capable gatekeeper to let it out if the gatekeeper's utility function is essentially "don't let the superintelligent AI out of the box".

For a gatekeeper who doesn't have that utility function (i.e. has values more or less along societal norms), he would still be expected to assign an enormous negative instrumental utility to letting the AI out of the box, due to all the bad things that could happen. The AI getting out of the box comes down to convincing the gatekeeper to abandon that negative instrumental utility assignment -- convincing the gatekeeper that letting it out of the box isn't such a bad thing and, on the contrary will have great positive utility (potentially unbounded) according to the gatekeeper's terminal utility function.

But for a gatekeeper who has precommitted to not letting the AI out, no matter what clever arguments the AI develops, I can't see how the AI could possibly be guaranteed to win (or even win at all). Under these conditions "don't let the AI out of the box, no matter what" might as well be a term in the gatekeeper's terminal utility.

Consequently I find the results of Yudkowsky's AI box experiments very surprising and am disappointed he didn't publish the transcripts. No doubt he had good reasons for this decision, but actual transcripts would be far more convincing than just the results and give a great deal of insight into the specific failure modes we're talking about.