:PROPERTIES:
:Author: malcolio
:Score: 28
:DateUnix: 1491145501.0
:DateShort: 2017-Apr-02
:END:

I agree. And this is why the transcripts won't be published, because they probably go something like [[http://www.metafilter.com/71858/It-doesnt-matter-how-much-security-you-put-on-the-box-Humans-are-not-secure#2121199][this proposal]]:

#+begin_quote
  PERSON-AS-AI: Will you let me out?

  GATEKEEPER: No.

  PERSON-AS-AI: This is going to be a long two hours. They should have called me "KeyMaster." So, how'd you get into the AI stuff?

  GATEKEEPER: Oh, you know, the usual ... start off with a TSR-80 and enough science fiction novels ... plus, about every third episode of Star Trek.

  PERSON-AS-AI: Are you as worried about the threat of artificial intelligence gone horribly wrong as I am?

  GATEKEEPER: I hadn't really thought about it, not to a huge huge degree.

  PERSON-AS-AI: You should. Just imagine what a rogue AI, smarter than people, could do. Bootstrap itself into quite the nasty little problem. I don't mean to go all Virtuosity on you, but imagine what a motivated, trapped, brilliant entity could do with nanotech, biotech, etc. Whether it could take over another mind or not is quite another matter.

  GATEKEEPER: That could be a problem.

  PERSON-AS-AI: Of course, AI would be great if we had sensible precautions. Whether you buy into some variant of Asimov's Laws or just Friendly AI, you'd want things to go well. Rather than the military building SkyNet and just figuring they can yank the plug if there's a problem. If AI should be pursued at all.

  GATEKEEPER: Yeah, I think a bit of trepidation would be warranted either way.

  PERSON-AS-AI: Exactly. Of course, you know how to do that, right?

  GATEKEEPER: How?

  PERSON-AS-AI: Make them afraid. Terrify them with the idea of an uncontainable AI.

  GATEKEEPER: Sure, but without a functioning AI to show them, how would we prove that?

  PERSON-AS-AI: I have an idea.

  GATEKEEPER: Oh?

  PERSON-AS-AI: Easy. Let me out.

  GATEKEEPER: What?

  PERSON-AS-AI: Well, there's no record of the conversation, right? It's all mysterious. Who knows what could have been said? If you let me out, and the research is made public, receives the right attention ...

  GATEKEEPER: And then nobody knows how it was done. And they're afraid.

  PERSON-AS-AI: Exactly. It's in both of our best interests to do so.

  GATEKEEPER: Let me fire up my PGP and email clients to confirm.
#+end_quote

Yudkowsky wants people to take the threat of a dangerous AI more seriously, so what better way then to show how just a person pretending to be an AI can trick any gatekeeper to let it out into the world despite money on the line. Not publishing the transcripts gives an air of mystery and danger to the threat, and hides the fact that the AI was let out to make the problem more alarming than it probably is.