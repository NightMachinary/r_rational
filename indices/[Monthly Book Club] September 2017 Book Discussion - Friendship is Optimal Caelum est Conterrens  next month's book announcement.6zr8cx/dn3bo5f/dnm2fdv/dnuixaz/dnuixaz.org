:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1507040543.0
:DateShort: 2017-Oct-03
:END:

#+begin_quote
  The beauty of the nebulous definition of satisfy values with friendship and ponies, by definition must first understand your values.
#+end_quote

I'm not sure this recasting helps, because the people in the story don't know that, and we, the readers, see multiple violations of what many would consider to be their values, such as with people who value the real world.

#+begin_quote
  I'm not sure what you mean by this question
#+end_quote

I'm not sure which question you're pointing at, but

- "How much suboptimality is there from this value misalignment?" Namely, Celestia's values are evidently not properly aligned with humans values, most evidently because of the whole "pony" thing. The question is how big the downsides are, relative to a truly benevolent AI. This obviously depends on what you view as ideal.

- "What are the likely outcomes from trying to aim for these opportunity costs?" An opportunity cost is a possibility you sacrifice for choosing another. In this case, the main opportunity cost is a different AI. Chances are Celestia won't let that happen, but the options should at least be considered.

- "Why is it all about my happiness? Convince me and I'm recruitable. Does she really not need help?" If Celestia is aiming for something I agree with, it seems that having me help her would be a good thing, since I have physical presence.