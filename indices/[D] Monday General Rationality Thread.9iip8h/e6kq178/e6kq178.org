:PROPERTIES:
:Author: xamueljones
:Score: 6
:DateUnix: 1537826964.0
:DateShort: 2018-Sep-25
:END:

Last night, I had a strange dream where I became a [[https://en.wikipedia.org/wiki/Posthuman][posthuman]] in a variety of ways. One resulted in me becoming an unfeeling machine of logic, another I turned into a hive-mind of millions, an individual accelerated beyond all reason with the ability to think for thousands of years on a problem within the span of one second, a godlike being with the power to shape everything within my view, an omnisavant capable of learning every field of knowledge, and other superhuman feats of imagination.

I blame these dreams on reading [[https://gamesoftranscendi.wordpress.com/][Simulacrum: A Post-Singularity Story]] before bed the night before. I wouldn't consider it a rational story, but it's a very intruging take on the Singularity.

However with the passing of my dreams as I woke up, I find myself pondering questions about what would happen if a human, /not an AI/, were to become a superintelligence.

- Would they have emotions like we do? Many people seem to think being smart means being cold and unfeeling. I call bullshit on this. Emotions are not a force that opposes logic. Our emotions dictate goals and desires that we fulfill. Logic is simply how we determine the path to best fulfill out goals. This [[http://strongfemaleprotagonist.com/issue-7/page-83-3/][comic page]] is the best statement of this idea I have ever encountered. Therefore I can't help but think a posthuman would actually feel more strongly and diversely than we do as humans.
- Will communication be possible among all posthumans? Because humans are a tiny dot on the [[https://raw.githubusercontent.com/tricycle/lesswrong/master/r2/r2/public/static/imported/2008/06/24/mindspace_2.png][space of all possible minds]], and it is virtually guaranteed that there will be more ways to be a posthuman than there are to be a human. With such radically divergent minds, would communication be possible between any two existing minds despite having incredible intelligence at their command?
- Would they compete over resources? In many stories about an AI becoming superintelligent, there is a common fear and worry that they would eliminate humanity not out of any fear or hatred, but because we use/are resources that can be better used for their goals. I wonder if posthumans will complete over resources like we do today or if they would be capable of making a utopian society where everyone cooperates instead?
- Will there be more than a few posthumans? It's an extension of the previous question. It's understood that cognition requires energy and while humans as designed by evolution are pretty inefficient, it can be understood that a posthuman will likely require enormous levels of energy beyond what is easily available to a human today. Much like how the AI to first appear might act to prevent the development of any future AIs to monopolize any life-sustaining resources, I wonder if the first posthuman will act to stop any future posthumans to ensure the monopolization of resources?

I would love to discuss any of the questions above or anything else about the idea of posthumans.