:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 6
:DateUnix: 1516422807.0
:DateShort: 2018-Jan-20
:END:

I have been thinking about utilitarianism and villainy, and am starting to think we need to pre-commit to a very irrational course of action even if we choose to be utilitarians.

Let me explain the thought process: imagine a villain constructs a doomsday device, and threatens to activate it unless his/her selfish demands are met, which may include all kinds of things like money and slavery and rape and murder, but only affect a tiny fraction of the population.

In the current world, this course of action is stupid. There's too many irrational people that will rebel even with the threat of doomsday. Even those that don't take up arms will still treat this as a moral dilemma and be unsure about whether to obey or rebel. So the villain will most likely just get him/herself killed.

But what if utilitarians became the majority of the population? In this situation, the utilitarian thing to do seems to be obey. And not just obey, but help put down any rebels, deliver the slaves, carry out the murders, etc. etc. After all, the more rebels, the more likely it is that the villain will simply activate the device and kill everyone, which results in an absolute minimal utility that is irrecoverable, since everyone is dead. The relatively small number of sacrifices needed to appease the villain is insignificant in comparison. And whatever other actions and outcomes are possible, they aren't worth the risk of human extinction in pretty much every utilitarian system of utility calculation.

Therefore, if utilitarianism ever becomes the dominant ethical system, every villain gains a perverse incentive to construct doomsday devices. After all, most of the population will jump to serve them, and even put down the crazies that try to rebel. This is terrible, because the more doomsday devices are built, the more likely one of them is to be activated (possibly by malfunction). Then we all die.

So, as strange as it sounds, it seems that in order to avoid human extinction, we should pre-commit to the irrational act of rebelling against anyone who makes a doomsday device even if it risks killing us all.

More generally, it seems that by the same logic, we should pre-commit to essentially defying any kind of utilitarianism-exploiting villainous threat. For example, if some villain creates a bomb that will kill X people and demands we kill or enslave some targets to prevent the bomb exploding, we should pre-commit to rebelling and attacking the villain anyway even if it kills the X people. Otherwise every villain gains perverse incentives to create all kinds of bombs and we end up with a lot more dead people.

Does this thought process make sense? I have a number of bias concerning ethical systems, so I need a second opinion.