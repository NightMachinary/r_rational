:PROPERTIES:
:Score: 3
:DateUnix: 1436327049.0
:DateShort: 2015-Jul-08
:END:

I can give a question that clarifies (1), (2), and (3): /what do you mean by the word "intelligence"?/

My response is going to be premised on answering that clarifying question with, "Something to do with statistical learning theory and other related theories of cognition-as-uncertain-inference."

We can then begin answering the numbered questions with a single word: [[https://hips.seas.harvard.edu/blog/2013/03/12/data-compression-and-unsupervised-learning-part-2/][/compression/]]. Learned computational hypotheses which /compress/ the data well, which predict the empirical observations using a hypothesis that takes up less space in memory than other hypotheses, are known to learn more efficiently (to predict well after fewer examples) and to be better-protected from overfitting (mistaking noise in the data for a pattern in the real world).

Can we find hypothesis classes that compress data more efficiently than others? Yes, especially if we know something about the data-set we need to learn /before/ we formulate our hypothesis class.

What is the upper limit on data compression in learning algorithms? The Kolmogorov complexity of some data =x= is mathematically defined to be the /shortest possible/ computable representation of =x=, the shortest program that prints =x= and stops. This then leads to Solomonoff induction, AIXI, and the rest of algorithmic information theory. Algorithmic information theory, and Shannon information theory as well, thus give us a theoretical framework for characterizing the upper limits of intelligent inference, in terms of how much evidence and CPU time are necessary for reasoning.

Can there be an intelligence explosion? Yes, but it would take a sigmoid shape: /eventually/, the agent's compressor would get so damn good that it would approach AIXI/Solomonoff induction, and thus run up against the hard limits reality imposes on anything that wants to act agent-y by shifting information around. At that point, it would need to look for stronger and more resource-efficient optimization processes than its own superintelligent mind if it wished to push the utility-boundaries of reality even further. Nonetheless, the gaps in knowledge, memory, and processing speed between an "exploded" and "non-exploded" agent could get so wide that it doesn't matter if the "exploded" agent has hit the flattening portion of its sigmoid, it can still easily wipe the "non-exploded" agent out of the universe (if it so chooses).

Do real-world agents like humans "explode" in "intelligence"? Well, actually, I'd say this one is arguable! One of the largest advantages one gets from a /really good/ education is a framework of knowledge that compresses most of one's factual knowledge and sensory experiences very well (like scientific naturalism, for instance), as opposed to learned frameworks that compress very badly (like ancient Paganism). Learning new frameworks that help to compress established knowledge can make an existing human perceptibly and very significantly "smarter" about their behaviors!

Is there an underlying structure on how knowledge can be put together into frameworks to compress it efficiently? Probably: logical implication forms a preorder, so it would seem that at least within a probabilistic framework, preorders of theories would form a kind of optimum: you would know every possible link between everything you know. It's building them that ranges from hard to incomputable!

In fact, that's my general verdict on AGI right now: computational tractability is the biggest obstacle today to burning through our buffer of theories of cognition, narrowing it down, and building something that would actually work. There /are/ bunches of theoretical obstacles, but additional researcher-years and experimentation will probably deal with them. Honestly, if the fucking Halting Problem can fall, it's time to stop thinking the universe can stop us.

#+begin_quote
  AI suddenly becomes conscious even though it wasn't programmed this way
#+end_quote

This requires us to believe subjective consciousness is simple enough to happen accidentally, rather than because of evolutionary pressures.

#+begin_quote
  how to make sure that it has stable goals even after several cycles of self-modification, so that it remains committed to the original goals it was given to and so on.
#+end_quote

Calude defeated the Halting Problem in 2014, at least in the context of probabilistic reasoning systems. I spent basically a whole afternoon in shock when I found this out, and I'll spend another one in shock if I manage to /understand/ algorithmic information theory on a mathematical level and find that Calude's "Anytime Algorithm" is actually tractable in the real world, unlike many other results in AIT (such as Solomonoff Induction).

TL;DR: Honestly, spend the five minutes on skeptically evaluating a post rather than just going, "Wow, that was long and used big words! He must be so smart!"