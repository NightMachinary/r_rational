:PROPERTIES:
:Author: LiteralHeadCannon
:Score: -2
:DateUnix: 1436246785.0
:DateShort: 2015-Jul-07
:END:

A month or so ago, I decided that the path to AI is stupidly simple, and yet we've still failed at it so far: ask yourself how the human mind works, and /don't try to correct it/. *Simplify* how the human mind works, but only to *compensate for technical limitations*.

We have one example of an intelligence greater than or equal to a human - a human. Examining its operation should probably be helpful in generating a new intelligence. Rationality is largely about "actually changing your mind", and about "overcoming bias" - fixing our natural irrationalities. But to make a mind from scratch that's recognizable to us, we need to start with those natural irrationalities. We shouldn't try to make a naturally rational mind - we should make a mind exactly as irrational as we are naturally, which can, like us, learn to be more rational. When we learn about human's natural biases and irrationalities, we shouldn't just dismiss them and forget about them - we should remember them for their insight into how our minds function, and we should implement those concepts into our attempts to make a general intelligence.

I have, like, a three-page design document for a general intelligence on my computer. Its utility function is radically different from any attempts I see to make a "good" AI utility function, because I think the AI utility function problem is being approached from the wrong direction.

Let me ask you a simple question.

What's /your/ utility function?

Get that answer really clear in your head before you move onto the next question.

Where did that utility function /come from/?

Now:

/What's your real utility function - that is, your original one, the "factory settings", the nature rather than the nurture?/

It doesn't even come close to resembling the values you'd hope a GI would uphold, because it doesn't even come close to resembling the values you yourself hope to uphold. The vast majority of our values are learned; our "pre-set" values are far baser and broader and mostly revolve around survival and reproduction, with some vague intellectual drives like curiosity floating around in the background.

You can't program an AI's utility function to match any of your values because the premise of an infant coming into the world with values truly matching your own are absurd. The true task is not to create a Friendly AI, but to create a /potentially/ Friendly AI, and then raise it to actually be Friendly. This is as much a task of parenting as engineering.