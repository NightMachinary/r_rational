:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1436240174.0
:DateShort: 2015-Jul-07
:END:

#+begin_quote
  Do you think intelligence can be increased tenfold, thousandfold and so on? What does it mean, exactly, to increase intelligence? Is there a fundamental difference in the brain architecture between high IQ individuals and low IQ individuals?
#+end_quote

Increasing general intelligence means decreasing the complexity costs of the algorithms used in a mind, including architectural changes in the mind itself (on the level of hardware as well as software). No one can currently speak to how efficient the process of GI can become, as we only have a lower bound of humanity's most intelligent individuals, and, as you said, we don't have a generalized theory of GI to estimate a higher bound from.

Differences in brain architecture between high-IQ and low-IQ individuals don't really matter to determining an upper bound of the efficiency of GI, although you will definitely find them. I believe Einstein had an increased number of connections from the temporal lobe? Anyway, it's not really representative of the plausibility of intelligence explosion, since intelligence differences in humans are the results of fairly tiny differences in brain dynamics, and the brain is the stupidest device necessary to build civilization.

Intelligence can also be made to mean rationality, as in freedom from biases and using the best predictors and evaluators possible to achieve your goals. In this sense, science itself can be said to contribute to intelligence via the efficiency and accuracy of its resultant tools, and an increase in intelligence would be gained by building a device that relies on those tools instead of the hard-to-reverse-engineer and massively suboptimal tools in the brain. Now it just so happens that some of the tools the brain uses are magnitudes better than any tool we have, but these are mostly in the field of perception and the process of GI itself, although things like self-modification and learning are not the best in either the brain or our current knowledge. If you measure intelligence as the effectiveness with which one gets shit done, then yes, I can imagine a thousandfold improvement in intelligence easily.

#+begin_quote
  Currently AI research seems to be a big number of subfields, do you think this will remain so, or will there ever be an all-encompassing theory of intelligence?
#+end_quote

General intelligence itself, which would be formalized in an "all-encompassing" theory of intelligence, is not the entirety of the field of AI. Rather, AI is the study of how to get computers to better do the things that humans can do better than computers. General intelligence as in learning, self-direction, and acting well in a variety of domains, particularly the domain of learning to act well in other domains, is one limited, somewhat stymied field of AI. This is due to the fact that learning depends on perception, which isn't too hot right now.

Your all-encompassing theory of intelligence will not get rid of AI's subfields, because there are subfields of AI that don't care about GI. However, once a sufficiently advanced GI is made, we will eventually no longer be able to do science faster than it can, so in some essence the field of AI will be unified into the AI itself.

#+begin_quote
  To what extent can you do useful theoretical research separately from actually building an AI?
#+end_quote

I'm not sure. However, since safety is starting to come into its own as a cross-cutting concern in AI, the theory will probably direct practical research, and vice versa.