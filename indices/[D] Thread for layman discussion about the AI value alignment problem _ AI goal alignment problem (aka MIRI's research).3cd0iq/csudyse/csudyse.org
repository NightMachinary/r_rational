:PROPERTIES:
:Author: avret
:Score: 3
:DateUnix: 1436225975.0
:DateShort: 2015-Jul-07
:END:

It would seem that a reasonably simple theoretical solution would be to have the ai predict its future actions given some change to its utility function and then evaluate those actions in light of its current utility function, with that resultant utility as the expected utility of any given modification. Would this not work, or is it too difficult to implement, or...?