:PROPERTIES:
:Author: Gurkenglas
:Score: 3
:DateUnix: 1436240785.0
:DateShort: 2015-Jul-07
:END:

You just took the method of [[https://en.wikipedia.org/wiki/Gradient_descent][gradient descent]] on its source code, then restricted its modifications to the one part of itself (its utility function) where the only way you're going to get any changes is when either your predictor of future actions or your evaluator of actions is still flawed enough that /modifying your utility function/ makes you /better at implementing your unmodified utility function/.

The way this would go wrong is that your code would keep looking for slightly modified utility functions until it finds one, U, where all the neighboring utility functions are worse at achieving U than U is.

Why did you think U would have anything to do with human values?