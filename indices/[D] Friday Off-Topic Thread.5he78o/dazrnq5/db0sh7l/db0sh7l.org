:PROPERTIES:
:Author: sir_pirriplin
:Score: 3
:DateUnix: 1481379797.0
:DateShort: 2016-Dec-10
:END:

The /Moral Event Horizon/ is not really about morality. It's about the sympathy the audience can feel for a fictional character. When a character crosses that line, there is a chance they may still be "good" but no chance that they will be sympathetic.

If a good character crosses it, they do it expecting to be hated by the other characters.

In real life the issue is more complex. If you are crossing a /Moral Event Horizon/ for the greater good it is more likely that you are wrong about what good is. That would be narratively unsatisfying in fiction (/Unexpected Consequences/ is a total cliche and /Well-Intentioned Extremist/ has been done to death) but Real Life doesn't care.

An example of Moral Event Horizon people worry about in real is an AI lying to its programmers for the Greater Good. Eliezer says AIs should be programmed in a way that "If [I decide that] fooling my programmers is the right thing to do, execute a controlled shutdown [instead of doing the right thing to do]."

[[http://lesswrong.com/lw/v1/ethical_injunctions/]]