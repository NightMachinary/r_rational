:PROPERTIES:
:Author: sprague-grundy
:Score: 3
:DateUnix: 1621044508.0
:DateShort: 2021-May-15
:END:

I've heard arguments like that before, but to be honest I've never managed to actually follow the math, and in any case they seemed to be arguments about the theoretical physical limits of computation rather than the performance of actual or hypothetical technology. (There's an argument that you should expect us to get to the theoretical physical limits of computation, but I'm not sure I actually buy it.)

I'm doing more of a handwavy extrapolation of current technology into the future. At least with current tech, power draw is usually superlinear in clock speed, and I'd expect that to continue to be the case. There's an old anantech thread that goes into some details on where the extra power goes as you overclock an i7 that's pretty interesting: [[https://web.archive.org/web/20160627101731/https://forums.anandtech.com/showthread.php?t=2195927]]

I think the biggest handwave is the assertion that you'll keep getting superlinear gains down to very very very low clock speeds. That might not actually be true.