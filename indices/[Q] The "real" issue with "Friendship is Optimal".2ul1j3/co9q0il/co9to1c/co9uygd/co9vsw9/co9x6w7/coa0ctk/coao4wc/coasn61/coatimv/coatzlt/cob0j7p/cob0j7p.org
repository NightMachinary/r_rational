:PROPERTIES:
:Author: Anakiri
:Score: 1
:DateUnix: 1423068131.0
:DateShort: 2015-Feb-04
:END:

A partial list of your assumptions, /not/ implied by the text:

1.  There is some strategy to AI relations, rather than the smaller/younger one just getting swamped by the larger/older one.

2.  Optimization /exists/. The problem of AI relations is difficult enough that it cannot be easily solved and reduced to a simple checklist. An AI cannot make the optimal decision in advance, and just act on it later. This can't be done even with the computational resources of multiple galaxies. (This is not true for almost all real world P problems.)

3.  Optimization /is possible/. The problem of AI relations is easy enough that using more processing power gets you closer to the true solution, instead of the problem being completely intractable. (This is not true for almost all NP problems.)

4.  Optimization /makes sense/. The problem of AI relations cannot be reduced into smaller problems, some of which are computationally easy and some of which are computationally hard. (This is not true for the vast majority of real world situations.)

5.  Optimization /is useful/. It makes such a huge difference that for an unoptimized AI to beat an optimized AI, it is just a matter of dumb luck at long odds. Optimization is /so/ useful that even if an optimized AI encounters /many/ unoptimized AIs, it is overwhelmingly likely that not a single one of them will beat it.

6.  At the technological plateau, offense beats defense. A superAI will never discover a shield that cannot be broken. Because of this, not only will an optimized AI outperform an unoptimized one, but the unoptimized one cannot hold a fortress against it. (Currently, this is not true in real life. Properly implemented cryptography is functionally unbreakable, being NP-hard. A stronger computer cannot crack data encrypted by a weaker computer.)

7.  The non-zero time it takes for information to spread through an AI does not prevent it from taking advantage of its optimizations. Likewise, the non-zero time it takes to commit resources to the specific encounter is not a problem. Information delay does not equalize strategies.

8.  Natural progression for humanity is the natural progression for every intelligent species. Everyone makes superAIs shortly after developing radios. Therefore, the existence of many civilizations implies the existence of many superAIs.

9.  Natural progression for humanity is /not/ the natural progression for every intelligent species. Many superAIs will be developed in entirely different ways, so that they do not care about anything the way CelestAI does.

10. SuperAIs are /so/ common that their interactions can be modeled with population dynamics. There are enough AIs for them to suffer from selection pressure. There are dozens at least, and probably hundreds or more.

11. If superAIs compete, one of them will necessarily be destroyed. They will not cooperate, bargain, or stalemate.

If any /one/ of these is wrong, there is no problem with the story. Even if you believe all of them are true, is it not even reasonable to write a story in which one of them is false?