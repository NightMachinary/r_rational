:PROPERTIES:
:Author: Anakiri
:Score: 1
:DateUnix: 1423252413.0
:DateShort: 2015-Feb-06
:END:

#+begin_quote
  How do you separate these from the possible "implicit" assumption that "the universe is subjective and rewards heroics", which could be used to "rationalize" almost any conceivable plot device?
#+end_quote

That's easy. Fiction is like reality unless noted. A rational person could believe that what I said is true in reality. So, a rational author could write a story in which they are true, without feeling the need to note it.

A rational person could believe that the first superAI in the universe eats the universe. A rational person could believe that superAIs will interact based on nothing but abstract zero-knowledge game thoery. A rational person could believe that superAIs are rare, or that they are largely self contained. (For example, most superAIs are built to solve some expensive problem. They do that, then they stop. Only humans are dumb enough to build a superAI with /open ended/ goals.) A rational person could believe that an AI's goals do not substantially impact its effectiveness.

Those rational people will then write rational stories, and this will be the realistic ending that follows naturally from everything that happened before.