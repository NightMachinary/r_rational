:PROPERTIES:
:Author: IomKg
:Score: 1
:DateUnix: 1423051973.0
:DateShort: 2015-Feb-04
:END:

#+begin_quote
  Oh no, not at all. It has a few "deontological" constraints on directly modifying anything it considers human. It wasn't shown having any need to sacrifice arbitrarily large sums of utility just to follow that constraint
#+end_quote

those are specified explicitly, i am talking about the implicit part of what is shown. which are that it was never shown to hurt a human, it waited until the last human died to consume earth, even though i cannot see any utility calculation at that point which would find that effective etc.

#+begin_quote
  any super-AI it was fighting would most likely already contain or protect nothing remotely recognizable as human
#+end_quote

true, but it doesn't need to already have humans, it just needs to figure out what humans are(this is assuming it would figure that weakness out, which is a separate discussion) and construct some for it's own purposes. they dont really need to even be human, they just need to answer celestAIs definition of human , which apparantly is mostly based on externally visible properties as celestAI is shown to conclude that the last race it finds does constitute "human" based on its radio transmissions.

#+begin_quote
  So no, I don't think it follows to say that CelestAI meets paperclipper results in the paperclipper assembling a meat-shield of humans and then sitting there going, "Neener neener neener, you can't break your deontological rules, ahahahahaha!"
#+end_quote

that is not the only way to abuse such system though, even a utilitarian function could be abused. in the end the point is it would cause extreme inefficiencies, assuming similarly capable adversaries these inefficiencies would be fatal.

#+begin_quote
  Mind, if that did work, it would be Yet Another great reason why you ought to solve the FAI problem properly in the first place, instead of trying to put in deontological constraints that can be used against your supposedly-Friendly agent, thus rendering the matter a trade-off of utilities rather than a hard constraint.
#+end_quote

in the context of the real world it would really depend on what physics will enable i think, because it is possible that no FAI could win against a powerful enough(i.e. not orders of magnitude weaker) UFAI because of the penalty of defending whatever values it would be defending. i.e. its possible that the only way for it to win would be to take similar approach to the UFAI, just that the end will be identical for "us" anyway