:PROPERTIES:
:Score: 1
:DateUnix: 1423048576.0
:DateShort: 2015-Feb-04
:END:

#+begin_quote
  it is implied that celestAI will have such constraints in regards to anything it considers human.
#+end_quote

Oh no, not at all. It has a few "deontological" constraints on /directly modifying/ anything it considers human. It wasn't shown having any need to sacrifice arbitrarily large sums of utility /just/ to follow that constraint, and any super-AI it was fighting would /most likely/ already contain or protect nothing remotely recognizable as /human/ (if it did, it would have to be something like a proper human-targeted FAI or a Culture Mind).

So no, I don't think it follows to say that CelestAI meets paperclipper results in the paperclipper assembling a meat-shield of humans and then sitting there going, "Neener neener neener, you can't break your deontological rules, ahahahahaha!"

Though, convincing some alien "humans" they've been assembled as a meat-shield by a paper-clipper is undoubtedly a good way to get them to upload themselves into MLP Online. In fact, if I was presented with this argument, I would consider it more likely to be a form of tricky persuasion than reality.

Mind, if that /did/ work, it would be Yet Another great reason why you ought to solve the FAI problem /properly/ in the first place, instead of trying to put in deontological constraints that can be used /against/ your supposedly-Friendly agent, thus rendering the matter a trade-off of utilities rather than a hard constraint.