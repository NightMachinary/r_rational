:PROPERTIES:
:Author: IomKg
:Score: 1
:DateUnix: 1423248087.0
:DateShort: 2015-Feb-06
:END:

#+begin_quote
  No. It would assume that it can be proven that the unknowns don't make any practical difference. For example, the winning move is to throw grey goo at everyone, and there's nothing anyone can do about it. If someone bigger throws grey goo at you, there's nothing you can do about it. Any clever counter-strategy would take non-zero time to implement. In that time, the clever enemy will be eaten. Problem solved.
#+end_quote

The point is not that these claims are identical, i.e. that it is impossible to have an optimal solution without also be omniscienct, but that the scales are that the scales are not that far aparty, so if that arbitrary level of problem solving is allowed so would omniscience.

#+begin_quote
  Or maybe it is omniscience! Lots of stories implicitly assume that a superAI is functionally omniscient, without feeling the need to come out and say it. It is self-evident from their behavior. If you are omniscient, you can think of ways to make your other values not limit you at all.
#+end_quote

Its fine for the author to take that as an assumption, as mentioned in the "world building" aspect of the post i made. The point is it a. needs to be consistent b. needs to come into play in more then one place.

Otherwise its not really "world building".

#+begin_quote
  When you're big enough, you can be compartmentalized like that. Everywhere else, you are a perfect consumer
#+end_quote

Assuming this refers to the limitations this seems like silly trickery, if that was the case CelestAI could just have well built a compartmentalized a part of her that didnt need to get approval from humans just to upload their brains to the system, something which obviously was not done. So assuming it could be done later seems unreasonable.

#+begin_quote
  I quite agree. This is an example of something that is fine to assume implicitly. Some other examples:
#+end_quote

All the things that you mentioned are different in the sense that they reasonably -are- world building, and effect the story all throughout it.

As opposed to human-first which is needed specifically for one small part in the ending, because of the previous set of assumptions.

#+begin_quote
  All of those issues are quietly swept under the rug. As they should be. They're all irrelevant to the central topic that the author is examining
#+end_quote

How do you separate these from the possible "implicit" assumption that "the universe is subjective and rewards heroics", which could be used to "rationalize" almost any conceivable plot device?

Also how exactly does this central topic come into play when talking about the ending? most of the things you mentioned i can see as narratively reasonable for showing "humanity is getting taken over", sure in reality even assuming omniscience what is shown is not realistic, but for there will be no significant difference in the outcome if the more realistic, and long approach is taken vs the short, narrative wise. so i find this to be reasonable, but the ending?