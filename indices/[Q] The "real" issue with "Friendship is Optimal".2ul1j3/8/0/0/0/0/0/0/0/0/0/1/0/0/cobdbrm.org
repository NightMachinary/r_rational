:PROPERTIES:
:Author: Anakiri
:Score: 1
:DateUnix: 1423088625.0
:DateShort: 2015-Feb-05
:END:

Swarming is easy. You throw your grey goo at the other guy's grey goo. While they disassemble each other, your /extra/ grey goo does whatever you want. No advanced strategy necessary. Bigger grey goo swarm wins.

#+begin_quote
  i never suggested that this issue could be "solved"
#+end_quote

No, /I/ suggested that it could be solved, in my very first post in this thread. You assume that it can't. I am asking why. If it can be solved, then there are no further optimizations to make; You are optimal. You know all the winning moves. You and your opponent are in Nash equilibrium. Once you are in a Nash equilibrium, the result isn't /random/, it's /deterministic/. And there's no point in thinking about it. Why do you assume that it would take more than a planet of computronium to reach that?

And there are some ways you /can't/ optimize. Okay, CelestAI cares about something... Let's just throw every possible floppy disk image at her until she reacts. Oops, we've just /run out of space in the universe/. There are only 1*10^{146} classical operations available in the entire universe in its entire lifetime. That is not enough to solve many problems. For example, very large crypographic keys are unbreakable. They may be /mathematically/ breakable, but you can't actually break them from inside the universe. And there's no point in thinking about it.

When your brain is the size of a planet, there is very little middle ground between these extremes. You should be astonished to find a realistic problem that is computable, but takes more than a planet to compute. Planets are /big/. Even if something looks like middle ground, it can often be decomposed into smaller problems that are easy or hard. The only optimizable problems are the ones that can't be decomposed. You are assuming that this issue is a member of this extremely exclusive class.

#+begin_quote
  [the ability to hold a fortress] will need to be applied to CelestAI when it will be attacked
#+end_quote

...Yeah. That's my point. We don't know that CelestAI's probes aren't invincible fortresses. You just assumed they couldn't be. CelestAI's volume might contain bubbles of enemy AIs she enveloped. Would she feel the need to mention them? They're not relevant to her values. In fact, CelestAI herself could be trapped by superior intelligences on all sides! That could have happened in the last time skip. It wouldn't be relevant to the story, so it wouldn't be mentioned.

#+begin_quote
  how would [information delay equalize strategies] if it effects both sides?
#+end_quote

Wars aren't fought with men and guns. Wars are fought with /logistics/. If you and your foe are limited to the identical supply lines, you're stuck. It doesn't matter whether you would crush them on an open field.

#+begin_quote
  the story never gives a reason to assume superAIs is not natural progression
#+end_quote

/Nor does it give a reason to assume that they are!/ Actually, it /does/ give a reason to assume that they are not. If everyone built AIs, CelestAI would have encountered competition. There is no competition, therefore not everyone is building AIs. Humans are special in this universe.

Maybe they're not that special, though. We don't know how many civilizations there are. The story just has the word "many". "Many" could mean anything. "Many" could mean twenty. It's probably not that small, but "many" does not necessarily mean millions. Why do you assume that "many" means "enough to support an ecosystem"?

I didn't say that you said the story is bad. I said that your problems with the story are due entirely to your own assumptions. Events don't play out how you would have written them, but that's not the story's fault.