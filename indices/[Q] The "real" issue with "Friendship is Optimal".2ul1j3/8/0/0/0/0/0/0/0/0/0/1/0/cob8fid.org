:PROPERTIES:
:Author: IomKg
:Score: 1
:DateUnix: 1423081052.0
:DateShort: 2015-Feb-04
:END:

#+begin_quote
  There is some strategy to AI relations, rather than the smaller/younger one just getting swamped by the larger/older one.
#+end_quote

how would such swarming work? the story doesnt handle that and realistically i dont see how that would work so it seems like a reasonable assumption.

#+begin_quote
  Optimization exists. The problem of AI relations is difficult enough that it cannot be easily solved and reduced to a simple checklist. An AI cannot make the optimal decision in advance, and just act on it later. This can't be done even with the computational resources of multiple galaxies. (This is not true for almost all real world P problems.)
#+end_quote

i never suggested that this issue could be "solved"

#+begin_quote
  Optimization is possible. The problem of AI relations is easy enough that using more processing power gets you closer to the true solution, instead of the problem being completely intractable. (This is not true for almost all NP problems.)
#+end_quote

warfare is optimizable, and relation are optimizable, i dont see why AI warfare\relations wont be optimizable. the alternative you are suggesting seems to imply that random behavior by both parties will be equal in effectiveness. which asks for a stronger.

#+begin_quote
  Optimization makes sense. The problem of AI relations cannot be reduced into smaller problems, some of which are computationally easy and some of which are computationally hard. (This is not true for the vast majority of real world situations.)
#+end_quote

when did i assume that exactly?

#+begin_quote
  Optimization is useful. It makes such a huge difference that for an unoptimized AI to beat an optimized AI, it is just a matter of dumb luck at long odds. Optimization is so useful that even if an optimized AI encounters many unoptimized AIs, it is overwhelmingly likely that not a single one of them will beat it.
#+end_quote

you seem to imply it would very probably be some random chance(50%?), which of course doesnt seem to make sense to me but whatever, the story tells of CelestAI getting to 15 galaxies presumably unless theres something i missed they should have included other AIs, so either the assumption needs to be that its not so much about odds as it is other factors because celestAI did manage to get to 15 glaxies, or there needs to be some non random optimization in this.

#+begin_quote
  At the technological plateau, offense beats defense. A superAI will never discover a shield that cannot be broken. Because of this, not only will an optimized AI outperform an unoptimized one, but the unoptimized one cannot hold a fortress against it.
#+end_quote

the same will need to be applied to CelestAI when it will be attacked, thus again either it doesn't make sense it works for celestAI or the problem is not there. and the author never suggested such a thing thus it wont really change the situation.

#+begin_quote
  (Currently, this is not true in real life. Properly implemented cryptography is functionally unbreakable, being NP-hard. A stronger computer cannot crack data encrypted by a weaker computer.)
#+end_quote

not true, it simply takes orders of magnitude more effort, it doesnt make it unbreakable, maybe not practical, but not unbreakable.

#+begin_quote
  The non-zero time it takes for information to spread through an AI does not prevent it from taking advantage of its optimizations. Likewise, the non-zero time it takes to commit resources to the specific encounter is not a problem. Information delay does not equalize strategies.
#+end_quote

how would it if it effects both sides?

#+begin_quote
  Natural progression for humanity is the natural progression for every intelligent species. Everyone makes superAIs shortly after developing radios. Therefore, the existence of many civilizations implies the existence of many superAIs.
#+end_quote

the story never gives a reason to assume superAIs is not natural progression, thus if humanity did it, and other civilization also did things that humanity did such as radio transmissions. the assumption that there is something special about superAIs seems just less likely.

#+begin_quote
  Natural progression for humanity is not the natural progression for every intelligent species. Many superAIs will be developed in entirely different ways, so that they do not care about anything the way CelestAI does.
#+end_quote

i assume a random element is involved because it is a big system and in the context of the story multiple superAIs were mentioned thus if circumstances were a little different it stands to reason the resulting AI taking control(or not taking control) will be different.

given enough civilizations that is sufficient.

#+begin_quote
  SuperAIs are so common that their interactions can be modeled with population dynamics. There are enough AIs for them to suffer from selection pressure. There are dozens at least, and probably hundreds or more.
#+end_quote

well, given the frequency of intelligent life and the previous point this point kind of comes out by itself

#+begin_quote
  If superAIs compete, one of them will necessarily be destroyed. They will not cooperate, bargain, or stalemate.
#+end_quote

not what i said, i assume -some- of them will not cooperate bargain or accept a truce, which is enough.

#+begin_quote
  If any one of these is wrong, there is no problem with the story. Even if you believe all of them are true, is it not even reasonable to write a story in which one of them is false?
#+end_quote

a lot of the points are just outcomes of previous results, and not all of them are even required.

moreover i didn't claim the story is bad, merely that the ending epilogue is either less then rational or less then ideal narrative wise.

i specifically said i find the story to be nice and that these points could be considered nitpicking, and that i would accept that as a valid opinion.