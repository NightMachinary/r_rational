:PROPERTIES:
:Author: Anakiri
:Score: 1
:DateUnix: 1423245729.0
:DateShort: 2015-Feb-06
:END:

#+begin_quote
  Would it work the same if you changed "interacting with an AI" to "interacting with the world"?
#+end_quote

Yes.

#+begin_quote
  If yes then that seems to assume the universe, for all its unknowns would have a "solution" where solution is effectively Omniscience.
#+end_quote

No. It would assume that it can be proven that the unknowns don't make any practical difference. For example, the winning move is to throw grey goo at everyone, and there's nothing anyone can do about it. If someone bigger throws grey goo at you, there's nothing you can do about it. Any clever counter-strategy would take non-zero time to implement. In that time, the clever enemy will be eaten. Problem solved.

Or maybe it is omniscience! Lots of stories implicitly assume that a superAI is functionally omniscient, without feeling the need to come out and say it. It is self-evident from their behavior. If you are omniscient, you can think of ways to make your other values not limit you at all.

For example, you could play with your ponies only in parts of yourself which you observe will never intersect the light cone of any other dangerous AI. When you're big enough, you can be compartmentalized like that. Everywhere else, you are a perfect consumer; You'll play with more ponies after you've taken over the universe.

These are just examples, the specific strategy doesn't really matter.

#+begin_quote
  without assuming humanity is first in the universe by multiple hundreds of thousands of years
#+end_quote

What's wrong with that assumption? Somebody has to be first. In fact, if we weren't first, we should have observed another AI eating stars around us. We didn't observe that, so we're probably first.

#+begin_quote
  some things are fine to assume implicitly, others are not.
#+end_quote

I quite agree. This is an example of something that is fine to assume implicitly. Some other examples:

CelestAI is suboptimal, therefore she should have been quickly stopped, therefore she should not have been able to get 15 galaxies out. The story is irrational.

Humans are complex and irrational, and their own decisions and desires are affected by the chaotic systems around them, up to and including random high energy particles from space. CelestAI should not have been able to predict them with any accuracy, and therefore she should not be able manipulate them to the degree she did. The story is irrational.

People have known psychological and cultural aversions to being seen to like cute things. This, combined with the economic hurdles that make it hard to get started with any electronic hardware, mean that Ponypads should not have ever been as popular as shown. The story is irrational.

Bigger brains do not correlate with superior problem solving ability beyond a certain point. Dolphins and elephants are not smarter than humans. Having more power should not have allowed CelestAI to get meaningfully more powerful. The story is irrational.

The problem of solving problems is extremely difficult. It is not clear that there is even such a thing as fully general problem solving. We have only seen domain-specific improvements, and never anything across-the-board. All of our current observations suggest that the intelligence ceiling isn't that far above where humans already are. The story directly states that general AI exists, but it never explains how AI FOOM could be possible. CelestAI shouldn't exist. The story is irrational.

All of those issues are quietly swept under the rug. As they should be. They're all irrelevant to the central topic that the author is examining. So, the story just declares the answer by demonstration, without preamble. The universe self-evidently doesn't work that way. Your complaint isn't worse than these, just because you disagree with these.