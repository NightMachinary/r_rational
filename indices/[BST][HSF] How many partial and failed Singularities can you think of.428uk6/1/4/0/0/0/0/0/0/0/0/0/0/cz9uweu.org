:PROPERTIES:
:Author: ArgentStonecutter
:Score: 1
:DateUnix: 1453625266.0
:DateShort: 2016-Jan-24
:END:

#+begin_quote
  Not sure what you're arguing with, I already covered that possibility.
#+end_quote

Your statement was that "deals with AIs can be enforced through credible threats" without qualification.

I challenged that, you responded "Humans often can't credibly precommit to non-subgame-perfect strategies. AIs can." as if that was a given for any AI. Then qualified it with "given they have access to each other's source codes, and the ability to change their own".

That is a very strong assumption. It needs more support.

#+begin_quote
  I think you're more sure of that than the evidence suggests that you should be. So I ask, how do you think that you know this?
#+end_quote

Probabilities. AIs would be developed, initially, by humans. Out of all the possible paths to the development of an AI, how many would involve a process that is completely unlike the development of any other software system of comparable complexity to date? Even systems of far less complexity have unexpected behavior, that is, it's not possible to predict or even describe describe their "utility function", if they have one. Having been developed, how many would impose an artificial constraint on their actions?