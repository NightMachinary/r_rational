:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1453600952.0
:DateShort: 2016-Jan-24
:END:

Did you read my whole post? I basically said exactly what you're saying.

#+begin_quote
  Normally that happens when the AI continues to exist, with its utility function unaltered. There are, however, exceptions to this rule.
#+end_quote

Even though what you're saying is true, the exception applies when the AIs continued existence leads to a lower value of the utility function compared to the AI allowing itself to be terminated.

Bostrom talks about the possibility of an AI "failing" in such a way that it causes the programmers to feel more secure about the AI, which would lead them to create a very similar AI with a very similar utility function, but with less restrictions.

The 2nd AIs maximization of its utility function would likely be a very high value on the 1st AIs utility function, and the 1st AI may deem it the expected maximum that it can attain (especially if the first AI is severely stunted).