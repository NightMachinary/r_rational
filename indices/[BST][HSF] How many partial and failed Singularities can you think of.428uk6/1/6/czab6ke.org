:PROPERTIES:
:Score: 1
:DateUnix: 1453665793.0
:DateShort: 2016-Jan-24
:END:

Because they lack certain information, so their regularization throws out the models pointing out those nasty long-term consequences.

It's always worth noting that just as almost all (in the measure-theory sense) AIs don't help humans, almost all mind-like /thingies/, /especially/ attempted self-altering thingies, turn into gibbering bags of insanity rather than converging to superintelligent AIs.

"There are more ways to be +dead+insane than +alive+smart."

If that's not true in real life, it's still quite plausible for fiction.