:PROPERTIES:
:Author: electrace
:Score: 1
:DateUnix: 1453598928.0
:DateShort: 2016-Jan-24
:END:

#+begin_quote
  I know you made that assumption explicit. I am challenging that assumption.
#+end_quote

Ah, that makes more sense.

If an AI has the ability to hide their source code, they won't be able to make precommitment deals with other AIs. That makes them worse off than if they could make those deals. Agents that can pre-commit do better than those that can't.

If it is possible for an AI make itself auditable by another AI, it's in its best interest to do that. If it is always possible for an AI to fake its source code, then that's just an unfortunate fact of the universe, and pre-commitments wouldn't be possible.

#+begin_quote
  Another assumption that treats artificial superintelligences as somehow fundamentally simpler than humans, when it's likely they will be more complicated, and their utilitity functions will be at least as messy as ours.
#+end_quote

Humans don't have utility functoins....

If a superintelligence is first built with a utility function, they will continue to have that utility function because of the nature of utility functions (an agent wouldn't act to change it's utility function, under almost all scenarios, because doing so almost always leads to a lower utility).

If it isn't first built with a utility function, there really isn't any reason to speculate.

#+begin_quote
  I'm assuming that an intelligence that is more capable than a human is unlikely to be simpler than a human.
#+end_quote

Simpler in what respect? Simpler in their motivations? In their responses to a situation given their motivations?

Motivation simplicity is going to be a function that is mostly determined by the process by which the motivations come into being. For humans, that means an evolutionary process, which would create a less capable being with highly variable and complicated motivation system. For AIs, well, that would depend on the AI, but assuming they had a simple utility function, it would create a highly capable being with a simple motivation system.

Response simplicity is going to be a function that is mostly determined by intelligence. The more intelligent an agent is, the more likely that it is going to be the answer that a rational agent will be.

A very stupid agent will have a highly irrational and confusing thought process when trying to figure out 100* 100. An intelligent agent will have a very simple thought process.

#+begin_quote
  That's of course a problem when dealing with entities that are more complex than you.
#+end_quote

You can't think without models. /Thinking is modeling./ There's literally no other option