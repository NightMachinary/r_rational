:PROPERTIES:
:Author: OffColorCommentary
:Score: 2
:DateUnix: 1425365227.0
:DateShort: 2015-Mar-03
:END:

Now, let's change this scenario up. Your accomplice is your clone. Not a genetic clone like Dolly, but rather a sci-fi clone created shortly before the heist, who thinks just like you do.

Since your clone thinks just like you do, you don't have to guess what their choice will be. Whatever logic you use, you can assume your clone will use the same logic, and come up with the same decision as you. If you choose to rat, so will they, and you'll both get 10 years in prison. If you choose to stay silent, so will they, and you'll both get 1 year in prison. So you choose the good one, obviously. This resolves the entire problem nicely.

However, we just drew a very strange causal arrow - "I choose this so someone else I have no communication with will too" - which means there are people who object to it, of course.

Being limited to using this if you have a clone isn't very good. Eliezer is mostly interested in the case where you have two AI agents that have access to each other's source code. If they both understand TDT, they can just examine the other's code to find the most mutually beneficial trade, trusting that the other will uphold their end of it (and it's not like they can defect, you'd read their "and now I defect code" in the process).

You can also sort of extend it, loosely, to anyone who you know knows TDT and who thinks a lot like you. Humans are all pretty similar for a lot of purposes, so that might come in handy.