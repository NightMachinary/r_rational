:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1479713004.0
:DateShort: 2016-Nov-21
:END:

#+begin_quote
  It's not that they only count as one, but Gladys' utility function should have been designed to discount the value of extra identical simulations so as to preserve diversity.
#+end_quote

See that's always going to be really tricky, because there's no way to avoid that making people's lives worth less, if they are not the only existing iteration of themselves. Given a infinite or staggeringly huge number of iterations of every person will exist elsewhere in most cosmological models this is a uncomfortable idea because it means the lives of /currently eexisting/ humans are somehow worth less than you would initially expect.

Ok as for Blake not being Todd in various senses; as I said before you really can't argue that from a subjective experience Blake expects oblivion when you is forcibly turned into Todd. After all in real life you would have no reason to expect sudden oblivion if you were given a temporary drug that made you unable to retrieve your memories.\\
There's just not really good reason to think that maintaining continuous memories, will have a massive effect on subjective experience.

#+begin_quote
  Or are you saying that A!Todd = A!Blake because of the continuity and A!Blake = B!Blake because of the continuity there, meaning that if you shifted every human's simulation through every computer then everybody would be everybody else at the same time?
#+end_quote

I think the fact that you shifted it through every computer wouldn't actually make any difference. What matters I think is the continuity of people's mental processes, the medium isn't really super important here. But sure you could change everyone's personality so they would at some point be copies of everyone else, however you could do the same thing gradually in your view.\\
As for being everyone else /at the same time/, i'm really not sure what to subjectively expect from having one's mind merged with others or split into duplicate, I just don't think there's any way to make confident predictions. So I would likely want to err on caution and code GAI to see that as death; so it doesn't get any funny ideas of just merging all humans into its mind so it can stop worrying about their desires.

Also yes you're right that once somebody has been forcibly Todd-ified then turning them back to normal would be wrong and against their wishes. However just because you consider Todd and Blake the same person /doesn't mean massive mental changes against their will are any more ok/.

Even in your view where massive personality changes have to be gradual for you to count as the same person (is that about right). There's still plenty of ways Gladys could alter your mind against your will that would make you more "happy" /that you sure as fuck wouldn't be on board with/.\\
For instance I think the justification galdys gives for not granting constant ecstasy is kind of weak, she could just give you false memories of some really shittty nonexxistent prior life for you to compare it to. Even without to much drastic changes, she could keep you the same, but just give you a shit tons of drugs for the first time so that they would be extremely novel and enjoyable. Then wipe your memories after a day and start over so you couldn't tire of it. Hell if you're maximizing joy it makes way more sense to go the false memories route and stick you in a repeating loop of some really awesome few seconds of experience.\\
Of course if I'm being serious a gradual lobotomy to some minimal level of awareness and wireheading is probably always going to be optimal for most happiness focused GAI.

Ultimately the problem here is that a GAI that values your happiness /over/ your autonomy, is pretty much /always/ going to lead to horrifying situations.