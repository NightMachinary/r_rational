:PROPERTIES:
:Author: ZeroNihilist
:Score: 1
:DateUnix: 1479716760.0
:DateShort: 2016-Nov-21
:END:

#+begin_quote
  Given a infinite or staggeringly huge number of iterations of every person will exist elsewhere in most cosmological models this is a uncomfortable idea because it means the lives of currently eexisting humans are somehow worth less than you would initially expect.
#+end_quote

Not necessarily. Unretrievable copies are extremely similar to non-existent copies. It doesn't really matter if there's a trillion trillion other versions of me, because I could never be recreated from them.

#+begin_quote
  Ok as for Blake not being Todd in various senses; as I said before you really can't argue that from a subjective experience Blake expects oblivion when you is forcibly turned into Todd. After all in real life you would have no reason to expect sudden oblivion if you were given a temporary drug that made you unable to retrieve your memories.
#+end_quote

Actually, I think that being unable to retrieve memories would be a temporary oblivion. This is a difference of models that probably can't be reconciled.

#+begin_quote
  There's just not really good reason to think that maintaining continuous memories, will have a massive effect on subjective experience.
#+end_quote

This may be the sticking point: I think that subjective experience is an illusion of memory, not a real phenomenon.

We think that time is continuous because we literally cannot experience it any other way, but it's entirely possible that time is running in reverse, or that it skips from point to point, or that it only began a microsecond ago, or even that it isn't moving at all. We cannot know we have experienced anything unless we assume that our memories are authoritative.

#+begin_quote
  I think the fact that you shifted it through every computer wouldn't actually make any difference. What matters I think is the continuity of people's mental processes, the medium isn't really super important here. But sure you could change everyone's personality so they would at some point be copies of everyone else, however you could do the same thing gradually in your view.
#+end_quote

I'm saying that if overwriting Blake with Todd doesn't change Blake's identity, then overwriting Todd with Sam wouldn't either, nor would Sam with Jane, and Jane with Horatio, etc.

And if you did that with everybody in sequence, then Xanthe would be Blake and Todd and Sam and Jane and Horatio, etc.

Further, if you believe that clones of Blake are still Blake, then by keeping copies of the people at every stage of the process you would have to conclude that everybody was everybody else.

This isn't possible in my model, because the copies would be sufficiently different from the end result to be different people.

#+begin_quote
  Even in your view where massive personality changes have to be gradual for you to count as the same person (is that about right).
#+end_quote

Pretty much. Mine is sort of an information-based model of identity.

Your current state is a function of your past state and your environment. As your state mutates normally (both in response to environmental changes and normal decay/shift of memory), the divergence between states increases as a function of time.

Normally, you're pretty much the same. There are obviously parts of you which are in significant flux (like thoughts, working memory, emotions, sensory data), but there are other parts which change much slower (habits, major memories, personality, behavioural tics). So even over long periods of time you're going to be broadly similar to your past self, and over short periods of time you'll be almost identical.

A radical change in memory or personality represents a significant discontinuity in this trend. Essentially, you've lost a portion of your statistical link to your past selves. If the change is to your "core" identity (defined as the parts of you most resistant to change over time), that's akin to a death of some of the information that comprises you.

Clones are an interesting edge case, because technically you're not losing much information. However, reducing the number of copies of that information is still comparable to murder.

#+begin_quote
  There's still plenty of ways Gladys could alter your mind against your will that would make you more "happy" that you sure as fuck wouldn't be on board with.
#+end_quote

That's a good point. If the changes were gradual and each of them made Blake happier, would it be immoral? My gut reaction is to say yes, but I'm not sure I could justify it.

#+begin_quote
  Of course if I'm being serious a gradual lobotomy to some minimal level of awareness and wireheading is probably always going to be optimal for most happiness focused GAI.
#+end_quote

True, and theoretically some safeguards put in to prevent this scenario would lead to the other one you mentioned.

Anyway, thank you for the stimulating conversation, but I think this is getting a little unproductive for both of us. I'm happy to end with the understanding that our models are incompatible and prone to different weaknesses.