:PROPERTIES:
:Author: ConscientiousPath
:Score: 1
:DateUnix: 1599186838.0
:DateShort: 2020-Sep-04
:END:

#+begin_quote
  shouldn't the focus be somewhat broader in the endings? Looking at the world reacting to the events rather than a culmination of the character/s efforts from a first person view?
#+end_quote

I don't think so, no. The ideal of rationality is to achieve your goals through perfection of awareness, improved speed and accuracy in predictions, improved quality of the choices you think to make, and elite control of self to consistently take advantage of all those things. It doesn't dictate what your goals /are/, and doesn't promise more change to the world than is possible for you to effect. Therefore it doesn't promise that the broader world will change or react significantly.

The ideal of rationality is something that's very hard to make progress towards in real life because there's a large gap between knowing scientific findings and statistics, and using them to /correctly/ implement significant behavioral changes that don't backfire, burn you out, over-correct, make you miserable, just straight up prove impossible to do. If it weren't extremely hard to do, especially on your own, CFAR wouldn't have a business model.

Rationalist non-fiction often tries to extrapolate what a world where rational people lived might look like because people often see politics as a place that especially needs more rationality, and imagining futures is part of coming up with policy. I think that for most people this is jumping the gun because you're unlikely to have a positive effect when attempting enormous tasks like improving an institution if you can't manage simpler things in your personal life. People with messy rooms shouldn't be trying to clean up government. But I'm getting off topic...

Rationalist fiction, as opposed to rationalist non-fiction like the Sequences, is in many ways the art of writing enjoyable Mary Sue characters. The stories are often about characters who are far more industrious than anyone willing to stop working long enough to find and read these books. MC self control is often far above average, their attempts at deduction turn out to be correct, and when they're unable to control their emotions the problem is limited in duration and/or scale enough that they aren't prevented from reaching their goals. The whole genre is basically "stories by people who are smart enough to write characters with a high wisdom stat, for people who are smart enough to care about plot holes."

#+begin_quote
  Why is humanity so front and stage in thoughts of the future? I don't mean in regards to talking about possible alien life, I mean why is humanity being in charge of everything something people see as set in stone? "Friendly AI" is something that is discussed (that I really need to read more discussions of), AI that helps humanity, but with the idea of an AI singularity being a thing that is being actively researched, AI that is (several times) smarter than humans seems a distinct possibility (even if AI growth is restricted to the exponential growth rate of Moore's Law or something similar).
#+end_quote

The whole concept of the Singularity is partly that we can't predict what would happen afterwards. Many authors aren't smart enough to write smart characters well because brains aren't capable of simulating anyone smarter than themselves. Similarly if/when GAI exists and has greater than human level intelligence, no current human can effectively write about what that will be like because we aren't smart enough to know how a thing greatly smarter than us will act outside of generalities like "it will be very good at learning to achieve its goals".

Part of the reason too is that not all rationalist stories are told within a world that matches our current state of technology. Given the difficulty of writing AI in well, and the singularity's tendency to take over an entire setting, it's easier to just leave it out.

#+begin_quote
  Anyway, to relate back to the actual reason for posting in this subreddit - is the idea of (rather than directly making the world better) making something that itself makes the world better something that meshes with rationalism, either as it is defined here, as you relate to it yourself, or as it is reflected in some works that I am just not aware of?
#+end_quote

Making the world better on a large scale (directly, indirectly, or with positive feedback loops) isn't precluded in the genres in this sub, and some authors want to include that plot point in their narrative, but IMO it isn't a required component of any of them. "Rational" is mostly just "the plot doesn't have huge obvious holes or characters making decisions that they wouldn't if they were actual humans." "Rationalist" is mostly just "explicitly uses some cogsci, in a story about how a smart person with good self control wins by being smart".