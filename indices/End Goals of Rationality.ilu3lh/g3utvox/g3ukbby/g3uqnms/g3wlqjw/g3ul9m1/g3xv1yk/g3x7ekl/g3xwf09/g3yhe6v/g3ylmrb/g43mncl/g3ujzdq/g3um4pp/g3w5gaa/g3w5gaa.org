:PROPERTIES:
:Author: thomas_m_k
:Score: 4
:DateUnix: 1599162410.0
:DateShort: 2020-Sep-04
:END:

If some descendants of ours survive who aren't exactly human but share some fundamental values with us (e.g., they know what fun is and what boredom is, and they are conscious), then I think we could be very happy. However, that's certainly not the default outcome. The default outcome is that we all die and are replaced by something that, to us, looks /mindless/ (it might still be intelligent in the sense that it can outsmart any human, but it lacks any concept of fun or love or friendship or honor or anything like that). For example, a [[https://wiki.lesswrong.com/wiki/Paperclip_maximizer][paperclip maximizer]].

All these futures that you are considering are in this tiny tiny part of possibility space. The vast majority of possibility space is a universe that is "dead" forever. That's the thing we should prevent.

#+begin_quote
  Would it be more suited to the ideology behind rationalism if there was "epilogue: here's how they all lived happily ever after" and followed by "epilogue 2: here's how everything changed in the wider world."
#+end_quote

Eliezer Yudkowsky (of HPMOR fame) once tried to write a story set in a post-singularity world where our world had been /fixed/, [[https://www.lesswrong.com/posts/88BpRQah9c2GWY3En/seduced-by-imagination][but he couldn't finish it]]:

#+begin_quote
  [...] It's not a good idea to dwell much /on/ imagined pleasant futures, since you can't actually dwell /in/ them. It can suck the emotional energy out of your actual, current, ongoing life.

  [...]

  I am now explaining why you shouldn't apply this knowledge to invent an extremely seductive Utopia and write stories set there. That may suck out your soul like an emotional vacuum cleaner.
#+end_quote