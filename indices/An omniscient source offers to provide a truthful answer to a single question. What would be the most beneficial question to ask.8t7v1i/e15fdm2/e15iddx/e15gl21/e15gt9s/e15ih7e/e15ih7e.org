:PROPERTIES:
:Author: pizzahotdoglover
:Score: 1
:DateUnix: 1529738153.0
:DateShort: 2018-Jun-23
:END:

I had another thought about FAI. Are you sure it's such a good idea to create one? Even if you defined its friendliness as carefully as possible, it could still have pretty dramatic and, in retrospect, bad consequences. For example, in [[http://localroger.com/prime-intellect/][The Metamorphosis of Prime Intellect]], (MINOR SPOILERS) a FAI bootstraps itself into omnipotence, then does a lot of things that technically achieve the utility function of reducing harm to humans, but in doing so, it uploads everyone to the cloud and doesn't allow anyone to come to harm or die even if they want to, deletes large sections of reality to improve its processing power, and gets kills off all extraterrestrial life, since it might one day threaten humanity, resulting in the total annihilation of tons of sapient species.

Furthermore, even if you did create a FAI so carefully that it would never do any of that stuff, what if it reproduced and its offspring was an asshole? Or what if someone with an incompatible utility value got their hands on its code and made an evil twin? It's a dangerous Pandora's Box to open.