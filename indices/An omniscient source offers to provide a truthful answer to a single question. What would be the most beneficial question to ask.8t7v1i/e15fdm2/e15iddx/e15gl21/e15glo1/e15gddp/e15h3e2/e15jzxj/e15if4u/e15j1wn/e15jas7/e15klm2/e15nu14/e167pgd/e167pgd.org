:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1529776669.0
:DateShort: 2018-Jun-23
:END:

#+begin_quote
  That's what "non-counterproductive" means.
#+end_quote

What exactly does that mean though, quantitatively? Is any policy that increases the chance of a UFAI considered counterproductive? Is there some ratio threshold of FAI to UFAI probability that a policy must have to be non-counterproductive? If the restrictions are too tight, you might end up with policies that have very weak effects that barely change any of the probabilities.

#+begin_quote
  Also, we seem to vastly disagree what the probabilities before the message is;
#+end_quote

Eh, they were just numbers I chose to illustrate the problem. My real opinion is 0% FAI 99% UFAI 1% some catastrophic event(s) wipes out all/most of humanity before they build a UFAI, simply because I don't believe FAIs are possible. I can't use this for the example since every policy would have no effect on the probability of an FAI.