:PROPERTIES:
:Author: scruiser
:Score: 8
:DateUnix: 1417390557.0
:DateShort: 2014-Dec-01
:END:

#+begin_quote
  suggesting we more-or-less deliberately get the FAI problem wrong, or at least, less right than we can possibly get it. You have my tentative apologies, on the condition that you never try to actually construct an FAI.
#+end_quote

Let me give you a scenario. Imagine 30 years from now, the first high-resolution (high enough to design a neural net off of) neural scans of humans have been used to create barely nonsapient programs which multinational corporations are just beginning to exploit. You are on a committee deciding whether or not to build a general AI. MIRI has a bunch of interesting math, but nothing that you can actually implement as an AI yet. What you are able to do is take your neural scans and design a sapient AI with high levels of empathy for humans and that vaguely meets some of MIRI's criteria and theories. Do you choose to put off building the general AI until some indeterminate point where it is provably friendly (which may not be even entirely possible)? Meanwhile you have multinational corporations building stronger and stronger AIs with no concern for existential risk at all. Or do you go with the best you can do right then?

In the optimalverse, Hanna had already released her theories so it was really a matter of time till strong AI came about. Some guy was already working on a smiley face paperclipper.