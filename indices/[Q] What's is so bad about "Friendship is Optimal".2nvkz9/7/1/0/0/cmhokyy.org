:PROPERTIES:
:Author: scruiser
:Score: 3
:DateUnix: 1417412912.0
:DateShort: 2014-Dec-01
:END:

Yeah, the question is a worst case scenario... I can see MIRI making progress in solving alot of the math for ideal cases like AIXI, but I would expect them to have issues with actually implementing all their math and theories into a working AI.

So a slightly modified version: MIRI has all the math for FAI(to the extent that it is possible), but they don't actually know how to implement all of their steps in real hardware (some of the math assumes infinite computational power, some of algorithms execution time explodes combinatorial with problem complexity making them unusable in the real world, etc.) You at least have to admit this is plausible given the direction of all their current research. It is also plausible that /de novo/ AI research will be outpaced by biologically inspired AI. Thus you could end up in a scenario where you can only loosely apply MIRI's theories, because there wouldn't be a way to have something that is both intelligent and designed from the ground up as opposed to trained/taught.

It is also possible that MIRI could discover there simply isn't a way for an intelligent agent to guarantee that its goals will stay stable throughout recursive self-improvement. Then they will have to decide if they are willing to risk an AI with possibly changing goals (because if they don't make it someone else will first).

But anyway, the whole point of a hypothetical situation like that is to test where your values lie.