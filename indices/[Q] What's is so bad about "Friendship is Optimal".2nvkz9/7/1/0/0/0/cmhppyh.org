:PROPERTIES:
:Score: 1
:DateUnix: 1417416307.0
:DateShort: 2014-Dec-01
:END:

#+begin_quote
  But anyway, the whole point of a hypothetical situation like that is to test where your values lie.
#+end_quote

No, the point is to /get the right answer/. There's no points for being counterfactually virtuous.

#+begin_quote
  So a slightly modified version: MIRI has all the math for FAI(to the extent that it is possible), but they don't actually know how to implement all of their steps in real hardware (some of the math assumes infinite computational power, some of algorithms execution time explodes combinatorial with problem complexity making them unusable in the real world, etc.) You at least have to admit this is plausible given the direction of all their current research.
#+end_quote

Oh, it's plausible given the material they've already published. That's why we ought to fix it. The concept that 30 years go by and nobody has an idea for a direction so bloody-obvious I already noticed it and a MIRI paper already mentioned it a few weeks ago is... well, completely implausible.

#+begin_quote
  It is also possible that MIRI could discover there simply isn't a way for an intelligent agent to guarantee that its goals will stay stable throughout recursive self-improvement.
#+end_quote

They've already made a significant attack on the Loebian Obstacle.