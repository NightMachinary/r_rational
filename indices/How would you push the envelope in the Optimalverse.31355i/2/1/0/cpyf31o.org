:PROPERTIES:
:Author: E-o_o-3
:Score: 3
:DateUnix: 1427936331.0
:DateShort: 2015-Apr-02
:END:

Idk, depends on her decision theory.

For example, I could precommit to suicide if she doesn't agree (my death is against her values) and thereby trade utility functions, or set things up that none shall know of whether she accepts the deal or not such that her acceptance doesn't incentivize others to try blackmailing her.

If CelestAI runs on timeless decision theory this probably won't work. I haven't really settled philosophical debates about what's rational and where "trade" ends and "blackmail" begins so it's hard to predict CelestAI.