:PROPERTIES:
:Author: HeckDang
:Score: 13
:DateUnix: 1492598226.0
:DateShort: 2017-Apr-19
:END:

#+begin_quote
  I like many things about this story, but I don't like the "True Ending." I genuinely don't understand why we are supposed to consider the murder of fifteen billion people an acceptable sacrifice in order to...retain the ability to be unhappy? Preserve our right to allow our children to suffer? Avoid desiring to kill and eat things that look like babies but aren't even sentient?

  What rational reason is there to refuse the Superhappies' compromise? It seems like the entire line of thinking is "there's something intrinsically valuable about the way humans are--and particularly, our capacity to suffer is intrinsically valuable for some goddamn reason--so we can't allow ourselves to be changed, even for the better." Which seems irrational as all hell to me, and inconsistent with the rest of EY's work. Since when do rationalists value suffering for its own sake?
#+end_quote

The Superhappies' compromise reminds me of [[http://lesswrong.com/lw/xu/failed_utopia_42/][Failed Utopia #4-2]], in that in each there has been an attempt to satisfy human values, but in each there have been other factors at play. I think on the sliding scale of utopias, while the Superhappies' offer is much better than #4-2, in each there have been controlling influences very clearly orthogonal to human values affecting how people live their lives and how, which of, and to what extent their values are satisfied.

Like the Gandhi that doesn't want to eat the pill that will turn him into an unstoppable murderer, I think it is not unnatural to front resistance to changing your core values. Is it not possible that humanity, if unaided by Superhappies, may bring to fruition a future more closely aligned with human values than the literally compromised utopia offered unto them? If so I think it's possible to make an argument that the 12 billion people is an insignificant number compared to the unimaginably significantly larger number to come in the far future.

The question I think is how well have the superhappies captured human values. If you think they've basically done enough, and the value in both fast-tracking and securing their version of utopia is worth the compromise necessary, then I think that's fair. But I think it's also fairly reasonable to suggest that humanity could eventually produce a future for themselves that better and more purely satisfies human values without capitulating to alien influence.