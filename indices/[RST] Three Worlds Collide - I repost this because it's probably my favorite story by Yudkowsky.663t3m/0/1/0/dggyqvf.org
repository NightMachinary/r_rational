:PROPERTIES:
:Author: CeruleanTresses
:Score: 3
:DateUnix: 1492615232.0
:DateShort: 2017-Apr-19
:END:

I'm certainly not arguing that it's unnatural to resist changes to one's core values; my objection is that the story frames that resistance as rational.

An argument could certainly be made that murdering the 15 billion is the only path to the "golden ending" where humanity optimally satisfies its existing values, and therefore the actions of the characters were rational. However, I have a couple of objections to that interpretation.

First, this is a lot more of a gray area than the characters imply. If it were a straight trade-off of 15 billion lives for an optimized human utopia, their absolute certainty that it was the right choice would make sense. But in reality the cost is (15 billion humans die + our opportunity for alliance and cultural/technology exchange with two advanced alien species is lost + we continue to satisfy our core values less optimally than the Superhappy compromise would have for god knows how long), and the reward is (perfect human utopia for some unknown length of time x the probability that we ever achieve such a utopia). The story treats this dilemma as a hell of a lot more black-and-white than it is.

Second, the particular human value that the characters most emphasize is our capacity for suffering. In the "bad ending," the last thing Akon does before being re-engineered is to pinch himself and regret that that's the last time he will feel pain. In the true ending, the Pilot's "triumphant" final words are "To live, and occasionally be unhappy!"

I argue that the story portrays it as rational to hold suffering as a core value, and I also argue that it's /not/ rational to do so. I've never seen any other rationalist work endorse the unfortunately common human habit of fetishizing unhappiness.