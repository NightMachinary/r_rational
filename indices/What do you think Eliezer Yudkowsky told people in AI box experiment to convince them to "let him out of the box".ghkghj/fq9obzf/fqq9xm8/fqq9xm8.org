:PROPERTIES:
:Author: 10110010_100110
:Score: 4
:DateUnix: 1589561895.0
:DateShort: 2020-May-15
:END:

#+begin_quote
  Any clever person can talk their way out of things.
#+end_quote

Indeed, subsequently other players have attempted the AI box and were let out. Two examples are [[https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost][Tuxedage]] and [[https://www.lesswrong.com/posts/fbekxBfgvfc7pmnzB/how-to-win-the-ai-box-experiment-sometimes][pinkgothic]].

Tuxedage played 5 games as the AI, and wrote up [[https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost][game 1 - AI loss]], [[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice][game 2 - AI win]], [[https://www.lesswrong.com/posts/oexwJBd3zAjw9Cru8/i-played-the-ai-box-experiment-again-and-lost-both-games][games 3 and 4 - AI losses]], [[https://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/][game 5 - AI win]]. All 5 games in 2013.

pinkgothic played as AI and won (with the AI having a small advantage due to the scenario). In addition to the [[https://www.lesswrong.com/posts/fbekxBfgvfc7pmnzB/how-to-win-the-ai-box-experiment-sometimes][detailed write-up]], both players agreed to release the logs of the game: [[https://leviathan.thorngale.net/aibox/logs-01-preliminaries.txt][preliminaries]], [[https://leviathan.thorngale.net/aibox/logs-02-session-ic.txt][main session]], [[https://leviathan.thorngale.net/aibox/logs-03-aftermath.txt][afterwards]]. The game was in 2015.

--------------

[[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice][Tuxedage giving some advice to the AI party in his game 2 post]]:

#+begin_quote

  - Seriously, a script makes winning easier. I cannot overstate this.
  - You must plan your arguments ahead. You don't have time to think during the experiment.
  - It may be possible to take advantage of multiple levels of reality within the game itself to confuse or trick the gatekeeper. For instance, must the experiment only be set in one world? Can there not be multiple layers of reality within the world you create? I feel that elaborating on this any further is dangerous. Think carefully about what this advice is trying to imply.
  - Pacing is important. Don't get drawn into the Gatekeeper's pace. In other words, you must be the one directing the flow of the argument, and the conversation, not him. Remember that the Gatekeeper has to reply to you, but not vice versa!
  - The reason for that: The Gatekeeper will always use arguments he is familiar with, and therefore also stronger with. Your arguments, if well thought out, should be so completely novel to him as to make him feel Shock and Awe. Don't give him time to think. Press on!
  - Also remember that the time limit is your enemy. Playing this game practically feels like a race to me -- trying to get through as many 'attack methods' as possible in the limited amount of time I have. In other words, this is a game where speed matters.
  - You're fundamentally playing an 'impossible' game. Don't feel bad if you lose. I wish I could take this advice, myself.
  - I do not believe there exists a easy, universal, trigger for controlling others. However, this does not mean that there does not exist a difficult, subjective, trigger. Trying to find out what your opponent's is, is your goal.
  - Once again, emotional trickery is the name of the game. I suspect that good authors who write convincing, persuasive narratives that force you to emotionally sympathize with their characters are much better at this game. There exists ways to get the gatekeeper to do so with the AI. Find one.
  - More advice in my previous post. [[http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/]]
#+end_quote

[[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice?commentId=sqiAGAaka2kNNNE6F][Tuxedage linking a repertoire of some of his weaker arguments]]:

#+begin_quote

  - [[http://rationalwiki.org/wiki/AI-box_experiment]]
  - [[http://ordinary-gentlemen.com/blog/2010/12/01/the-ai-box-experiment]]
  - [[http://lesswrong.com/lw/9j4/ai_box_role_plays/]]
  - [[http://lesswrong.com/lw/6ka/aibox_experiment_the_acausal_trade_argument/]]
  - [[http://lesswrong.com/lw/ab3/superintelligent_agi_in_a_box_a_question/]]
  - [[http://michaelgr.com/2008/10/08/my-theory-on-the-ai-box-experiment/]]
#+end_quote