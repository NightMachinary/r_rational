:PROPERTIES:
:Score: 5
:DateUnix: 1479059921.0
:DateShort: 2016-Nov-13
:END:

I mean, on the one hand, fair enough. On the other hand, an embodied Bayesian reinforcement learner with multiple reinforcement modalities isn't really gonna have "goals". "Goal-directedness" is going to be the mode of behavior such an agent engages when it optimizes expected reward with no reward prediction error. Saying that it /has/ a utility function is incorrect, even though given sufficient knowledge of its cognitive structure we should be able to construct or induce one for it.