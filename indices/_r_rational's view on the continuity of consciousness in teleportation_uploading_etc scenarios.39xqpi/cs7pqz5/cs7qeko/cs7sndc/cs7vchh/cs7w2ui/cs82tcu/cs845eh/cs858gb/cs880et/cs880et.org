:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434450935.0
:DateShort: 2015-Jun-16
:END:

#+begin_quote
  I think that the biggest issue is that we have differing value systems.
#+end_quote

I value individual life and freedom. I abhor the crypto-collectivist neo-altruism I see as so common in certain circles. It is very hip in some places for modern internet futurists to compete on who can be most self-sacrificing for other people and the future. Knowing a little something about anthropology, world political history, and psychology, that sort of thing leaves a sick taste in my mouth.

If that wasn't happening intentionally or otherwise, I apologize. I thought I was seeing the presentation of regurgitated material, not original thought. As you say, perhaps weakly stated and disorganized thoughts were the only thing there. In any case, please carefully consider why you believe what you think you believe.

--------------

I'm not some Luddite, however this sounds. Personally, I want an expanding cloud of /me/ exiting the Earth's atmosphere at high speed as much as the next powergaming internet egomaniac; but not if it kills me. And I see no reason to discard the scientifically supported and generally accepted theory that the human brain is everything that creates thought, however you define consciousness or self or any other hard to nail down terms.

If I am my brain, it has to have some closely interconnected, ongoing, additive processing relationship with any new processing substrate if the original is to be destroyed as a result. Otherwise my brain, which is all that I am, will die in functional isolation from any other cognitive processes, and leave only a copy of its state behind to spawn off new (if identical) cognitive processes. Fine for everyone else in the universe, if they don't think like I do, but not good for /me/.

The idea that someone is just the pattern of their brainstate, or a functional description of their goals or actions, divorced from the physical matter running it, is not even wrong. It ignores so much it doesn't even vaguely match reality. It'll also get me killed when the tech comes around if people continue to hold these unsupported personal delusions.

That destructive uploading is the way some very promising and popular existing tech seems to be heading, things like ways of recovering cryonic suspensions, should be a warning sign that people might be seeing and believing what they want, not what is. Being biased very strongly by a desire for this technology to work and give a way out of oblivion seems a real danger to any knowledgeable modern futurist debating this topic.

--------------

#+begin_quote
  People in general, including myself, only have instrumental value in my system.
#+end_quote

I think that's sort of sad. It isn't supported in any way by reality, is pointlessly reductionist in a very flawed way, and isn't required as a theoretical framework for software intelligence to also work, or even human augmentation paths to uploaded human minds. Rather, it seems like an intellectual hairshirt, something to justify sacrificing the sense of self that admittedly helps so little in rational thought.

If this becomes a popular opinion, and tech goes on the path it seems to be, eventually a lot of people are going to die and be replaced by software murder-clones. If the artificial substrate ends up working better and faster (likely), and the copy process is perfect (or people think it is), that's a very dark potential future. One you inexplicably seem perfectly fine with. This doesn't engender much sympathy, or make for a good impression of your level of rationality.

How can you fulfill not wanting to die tomorrow rationally if you suddenly have "die for awesome murder-clone" added into the mix? That seems like an incorrectly designed goal with some really off-the-wall assumptions built in. How can you not want to die tomorrow, and have that as a repeated and continuing goal, if you don't even know /what you are/?

It seems ITT people's answer is largely to pull an Asimov and redefine themselves as merely functional states, not as the generally scientifically accepted definition of the collective and continuing process of matter contemplating itself.

--------------

#+begin_quote
  Isn't rationality dependent on what your goals are to begin with?
#+end_quote

In the weakest sense, of course. But having goals that functionally conflict with reality because of poor assumptions shows a lack of rationality.

If your goal is in support of optimal murder-uploading strategies, I think this conversation might be over.

#+begin_quote
  But if, for some reason, teleportation requires that there be only one of me at the end, then I don't really care that much about the continuity.
#+end_quote

This and other comments suggests a state of self-worth that I feel is dangerous. The talk of suicide added in is making me very uncomfortable. I'm not shutting this conversation down, but I do want to make sure I'm not doing some sort of damage here. Perhaps this is a good place to wrap the topic up. But feel free, if you wish. I'll at least read a reply, though don't be offended if I don't comment again myself.