:PROPERTIES:
:Author: TimeLoopedPowerGamer
:Score: 1
:DateUnix: 1434614211.0
:DateShort: 2015-Jun-18
:END:

Glad you asked. People are wasting time arguing for software minds being the same operationally as the meaty originals instead of the issues actually I have with OP's suicide-clone teleporters. This confuses the issue, when it is really straight forward.

In fact, I'm the *strongest* strong AI supporter you're ever likely to meet. I think software brain simulation, uploading of human minds, human-level AI, IA-to-uploading, non-human superintelligent "Virtual" Intelligences (a dumb term), and all the ways in which self-aware and self-reflective thought can occur and be represented in software are not just scientifically possible, but likely in the relatively near-term.

There is nothing /unique/ about the human brain that makes it non-simulatable or unapproachable by science. Worst case, use chopped up brain matter to make a wetware computer add-on to run your software. But I don't think even that is necessary. The process of human cognition is likely much, much more redundant and parallel than a computer program would need to be.

There is, however, something unique about the continuing process of a specific human brain in operation. It is self-aware and has, like other physical objects, unique identity. If you simply outright destroy it, it doesn't matter if you have a copy for other people (who aren't now dead) to enjoy. You've killed someone.

I don't want to die simply to get a copy of what I /was/ uploaded or cloned, and lots of my peers are seeing that and then thinking with their unsupported and illogical ideologies, not their rational skills. It's frustrating.