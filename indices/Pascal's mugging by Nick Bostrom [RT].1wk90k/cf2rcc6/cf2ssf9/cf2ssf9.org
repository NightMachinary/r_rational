:PROPERTIES:
:Score: 4
:DateUnix: 1391101351.0
:DateShort: 2014-Jan-30
:END:

[[http://kruel.co/2013/01/13/the-singularity-institute-how-their-arguments-are-broken/][It's actually been argued]] that the whole argument for Friendly AI research is just a sugar coated version of Pascal's mugging. [[http://johncarlosbaez.wordpress.com/2011/04/24/what-to-do/#comment-5515][Like Greg Egan said]], "All of Yudkowsky's arguments about the dangers and benefits of AI are just appeals to intuition of various kinds, as indeed are the counter-arguments." There is no real empirical evidence about the dangers and benefits of AI. It might even be impossible to get evidence about it, at least before it's too late.

#+begin_quote
  [[http://kruel.co/2012/11/03/what-i-would-like-the-singularity-institute-to-publish/][The gist of the matter is that a coherent and consistent framework of sound argumentation based on unsupported inference is nothing more than its description implies. It is fiction.]]
#+end_quote

** 
   :PROPERTIES:
   :CUSTOM_ID: section
   :END:

#+begin_quote
  If you argue that it is more reasonable to contribute to the mitigation of risks associated with artificial general intelligence than to contribute to more or less probable risks then, in case you are not just appealing to intuition, there must be some formalized argument that favors AI risk mitigation over all other possible actions. In other words, you need to formally define “reasonable”.

  Note that the difference between AI risks and other possible risks can't be its expected utility, because that results in Pascal's mugging. The difference can neither be that it is more probable. Because that argument also works against AI risks by choosing risks that are even more probable than AI risks.
#+end_quote

[[http://kruel.co/2013/01/13/the-singularity-institute-addendum-to-whats-wrong-with-their-arguments/]]

Well, we don't have direct evidence of the dark lords of Matrix or the powers Seventh Dimension and we *have* evidence of certain kinds of more rudimental AIs so there's that. But in the end choosing AI risks instead of more or less probable existential risks comes down to feelings and there is not yet a proper formal analysis for it.