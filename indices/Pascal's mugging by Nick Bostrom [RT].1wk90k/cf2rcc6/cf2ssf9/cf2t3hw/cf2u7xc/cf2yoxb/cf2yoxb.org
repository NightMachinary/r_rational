:PROPERTIES:
:Score: 3
:DateUnix: 1391113169.0
:DateShort: 2014-Jan-30
:END:

#+begin_quote
  The infamous Roko's Basilisk[1] is an example of this (beware, Yudkowsky and few others believe you significantly raise the chance of getting eternally tortured if you receive information about Roko's basilisk... depending on how seriously you take this information).
#+end_quote

Oh please. The last time I attempted acausal trade with a supposed future superintelligence I found out that I wasn't the simulation, ie: absolutely nothing happened.

Which is a pity, because I was praying to a /nice/ god.

Well, I did prevent myself from developing a nasty piece of superstitious idiocy and have fun with some friends on IRC by actually /testing/ a nasty-sounding theory.

#+begin_quote
  Universal Dovetailer[2] is a contrived version of computationalism that not only says that everything is possible, but everything that is possible will happen and more mindfucks[3] .
#+end_quote

I don't really have time to read the pictured blog post, but all I can say is that the whole notion of "mathematical realism" is fucking nonsense.

Proof is program, program is proof. Mathematics /is/ computation: it doesn't exist in a realm of platonic ideals or on a universal computer (Schmidhuber published a paper just like this to troll, btw), it's just symbols being manipulated in accordance with fixed rules.

I can write a mathematics in which 2 + 2 = 5. Really, I can. It might not be self-consistent, but that doesn't mean it's "false", it just means that its proof-computations can diverge into saying /anything/.

Goedel showed that consistency (never contradicting itself) and completeness (ability to prove all the /true/ theorems) are opposed to each-other in certain interestingly powerful logics, but that's just another way of saying: some computational problems are undecidable.

So which computations or bits of mathematics are "true"? Well, the ones causally tied in with an external reality /outside the computer/. That's it.

#+begin_quote
  Some modern cosmologists think that it's probable that the universe is infinite and this on its own could do nasty things to the traditional concept of impossibility.
#+end_quote

And yet I remain unable to sprout wings and fly. /Which I desperately want to do, because it would be so freaking AWESOME!/

Any theory of causality or possibility that tells me that Some Other Me Somewhere has wings and can fly but doesn't tell me what /I can actually do in this universe/ to become him... is fucking nonsense.

If we really want to engage in that kind of All Possible Universes as Defined by All Possible Turing Machines nonsense, then "possibility" means "the existence of a path through the causal graph from a present point to a certain past or future point", or in other words, something is possible if there's a deterministic way to make it happen. Impossible are the things you can't do because you weren't born in the right branch of reality.

And the people who tell you otherwise are spouting [[http://rationalwiki.org/wiki/Many_worlds#Many_worlds][universe woo]].

#+begin_quote
  If you use a coherent and consistent framework of sound argumentation based on unsupported inference to argue for FAI research then how's that any different from using the same kind of argumentation to argue that we should pay more attention to the Dark Lords of Matrix so they don't pull the switch?[4]
#+end_quote

My position is the following: DEATH TO ALL PROSPECTIVE FUTURE GODS WHO REFUSE TO GIVE ME COOKIES!

Now, to be fair, words like "plausibility" and "acausal" are not /identical/ to "impossible". "Acausal trade" is just how a game theorist describes something like basic first-level superrationality, or, as we humans call it in our real lives, /the social contract/.

(This is why you should always check to see that things you /know/ really do exist fall neatly out of your Big Sophisticated Theories. A social theory that tells you prosocial behavior or the social contract can't exist, or a theory of reality that tells you how to trade with future superintelligences but not how to get cookies, is fucking bunk.)

#+begin_quote
  Which is more "impossible" and why?
#+end_quote

See, here's the thing. I don't argue in favor of machine ethics research (which includes the Future of Humanity Institute and some portion of the AGI Conference sponsored by Google, by the way: not just MIRI) on the basis of "WE ALL PROBABLY GONNA DIE UNLESS WE GIVE THIS GUY OUR MONIES!"

I argue in favor of that research because, for the definition of "ethics" used in such research, a more ethical AI is better at giving we humans what we want, like cookies. I want lots and lots of cookies. And other things. The whole point of "machine" in machine ethics is to pretend we have a genie and put aside the issue of Where Goodness Comes From (ie: from God or from social agreement or what), and instead just come up with solid epistemic processes for inferring What People Believe is Good, conditioned in what they believe about the world, thus giving us some idea of what people /truly/ want and wish for. Because then, after all, we can /give it to them/, which will be Pretty Freaking Awesome.

In general, I support research into getting things I want, and when it comes to danger levels I support safety and ethics research guided by /actual experts/ -- which means that the man spearheading machine ethics ought to be Juergen Schmidhuber, Marcus Hutter, or Shane Legg, /not/ Eliezer Yudkowsky /at all/.

/Problem is/, responses on the "danger and ethics" question vary dramatically between those three people. Schmidhuber says, "I am trying to make hard takeoff happen. Fuck human values, have an artilect war instead." Hutter says, "There is a possibility that AIXI could wirehead itself somehow, but that's more a failure of intelligence than of ethics. The danger is in the humans /using/ AIXI." Shane Legg says, "now that my AGI start-up is being bought by Google, we're establishing an ethics board, just in case", but he otherwise agrees with Hutter's position regarding hard take-offs and artilect wars and such.