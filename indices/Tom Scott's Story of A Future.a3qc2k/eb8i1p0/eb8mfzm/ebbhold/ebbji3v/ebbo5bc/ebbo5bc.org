:PROPERTIES:
:Score: 2
:DateUnix: 1544226657.0
:DateShort: 2018-Dec-08
:END:

I think they mean that, if you're part of team that writes a paper that, if implemented, can cause the rise of an AGI, then you probably know that. You probably, in the course of doing the research and testing your code and all that jazz, have realized that these principals, if utilized under those circumstances, could result in a AGI. You've already done the work, so you should, theoretically, understand what it means.

It would be a bit like if the people who were working on the Manhattan project somehow didn't realise that their work could be used to make a nuclear bomb. Not a very likely turn of events.

Also I think they could be saying that the people who were writing the paper have already done the work, that it is far more likely that they would have accidentally created an AGI while they were testing their own principles, rather than releasing it and having somebody else beat them to it.