:PROPERTIES:
:Author: akaltyn
:Score: 2
:DateUnix: 1519403578.0
:DateShort: 2018-Feb-23
:END:

Assuming that its new or novel technologies that are the risk I would tr and get funding channeled into better implementation of current technologies. Try and build up a belief in academia and the general public that we've focused on abstraction too much (How many minds were wasted for decades on string theory who could have been amazing engineers and brought cheap electricity and clean water to the world?). This allows me to maximise general human happiness while avoiding existential risks.

If there are specific technologies I'm worried about I'd lobby behind the scenes to have them treated like chemical, nuclear and biological weapons. Putting in place something like the oversight organisations at the UN that exist for those, and have them limit it. The advantage of something like that is its self sustaining, once the major powers buy into it they have an incentive to keep it going to keep eachother from doing it