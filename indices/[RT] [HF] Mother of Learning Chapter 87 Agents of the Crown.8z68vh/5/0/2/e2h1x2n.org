:PROPERTIES:
:Author: SoylentRox
:Score: 3
:DateUnix: 1531734621.0
:DateShort: 2018-Jul-16
:END:

Maybe. Remember we rationalists are essentially hoping, even expecting based on the evidence, this very thing to happen in the real world.

We're expecting that once advanced AI begins to earnestly be developed, for the tools to self improve to the point that you have a machine agent that will be able to make as much usable progress in key areas of science and technology as all of the progress we have made since the Enlightenment.

Well, if we don't screw up and it blows up in our faces.

But the goal we need as primates made with deeply flawed bodies and programming code is we need to develop medical science as far as we'd get in about 10,000 years at the present rate of progress. Or all 7 billion people alive today are going to die within their 'natural' lifetimes as well as some unknown number of billions of people who haven't been born yet.

(the number isn't known but I would expect even if this AI singularity never happens for some future generation to arrive at a solution so the number isn't unbounded. A mere 20 billion dead or something, you know, more than who ever died in warfare in probably all of human history)