:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1392048332.0
:DateShort: 2014-Feb-10
:END:

I'd read maybe a third of those links - thanks, reading more should be helpful. I would definitely do the experiment, but I'm in the wrong category of participant - I think that a transhuman superintelligence could argue itself out of most varieties of box, and I think that a person could argue another person out of a box in the experiment as stated, but I don't think that another person could argue /me/ into letting them out of the box.

The big problem is finding someone who actually wants to run the experiment with me. I'd definitely be down for it, but as Tuxeage clearly shows, the demand to be the human is much higher than the supply. And then add to that the fact that if I did the experiment it's probable that I would just win, which wouldn't teach me all that much since I already know pretty much all the arguments used. (Probable not because I'm super awesome, but because from what I've been able to find the AI usually loses.) What I really want is that single, powerful moment where the human has been broken down and finally decides to let the AI out - and there's no guarantee that I would get that from playing a game.

Edit: Okay, I'm dumb, the very first link you gave was a collection of logs. I've read them all now, including the [[http://tuxedage.wordpress.com/drafts-and-notes/leotal-gk-vs-n9-2600-ai-gatekeeper-victory/soundlogic-gk-vs-smoothporcupine-ai-gatekeeper-victory/][Soundlogic vs Smoothporcupine]] one that was a broken link, and I'm even more convinced that there's not really a reason for me to actually run the experiment myself. What I'd really like to see is an experiment where a gatekeeper loses, and I don't think that I would lose as gatekeeper or win as AI.