:PROPERTIES:
:Score: 5
:DateUnix: 1501346156.0
:DateShort: 2017-Jul-29
:END:

#+begin_quote
  This works if you ignore all the people in power at Google, Facebook, Amazon, Microsoft, etc. who are all desperately dumping money into AI capabilities research.
#+end_quote

I guess we perceive different things there, and I'm curious to know where you're coming from. From my background and based on my experience, it looks like they basically want to build "AI" into all their products, by which they largely mean supervised statistical learning with deep convnets. In order to do so, they continually overhype the achievements of supervised deep convnets, and more so for unsupervised and reinforcement learning.

#+begin_quote
  They do want to build super-AI
#+end_quote

Have they said so?

#+begin_quote
  every civilisation which still has problems to solve wants to build a super-AI which listens to them, because that's basically equivalent to solving every problem.
#+end_quote

I don't hear all of civilization saying they want to build a super-AI to solve their problems.

#+begin_quote
  So unless the civilisation implied by the video does not have any more problems, they would still be trying to build ever more capable problem-solving agents.
#+end_quote

Or they'd find it a cheaper use of resources to solve their problems the old-fashioned way, as even a superintelligence eventually must.

#+begin_quote
  I think you underestimate both the number of experts who don't take superAI concerns seriously
#+end_quote

I might. Have we got numbers?

#+begin_quote
  the difficulty increase in building a safe superAI vs. a typical superAI.
#+end_quote

I think we may have a difference of view here. I think there's actually a fairly large gap between the simplest possible "general" intelligence (in the sense of being able to learn any task, given human-scale amounts of data and CPU power, and a cost function specifying the task), the simplest naturally "world-optimizing" general intelligence, and the simplest self-improving world-optimizing general intelligence.

That could just be my personal views and background talking, but it /seems to me/, just based on what I know, that you need to solve a fresh, fundamental technical problem to ascend each of those steps. At each level before the last, you can stop, hype your shit up, get billions in investment money, and make a long, successful career out of /not/ having a super-AI.

In fact, I think many of the qualitative, important problems rest between the first step (simplest apparently-general intelligence) and the second step (simplest world-optimizing general intelligence). That's actually why I think Singularity hype over deep learning is /so/ thoroughly misguided: not only have people pointed out the flaws in deep learning /as/ machine learning, they've also been spending a long time pointing out its flaws as a theory of how to build a mind able to grip the world and squeeze its own timeline into the small spaces it wants to visit.

Deep learning is going to generate some very interesting industrial applications, including general, trainable-for-anything statistical learners. These will /not/ have the kind of capabilities that someone like MIRI or the AI safety community care about in terms of general intelligence: the ability to use multiple ontologies as appropriate, a utility function with fixed intentional/aboutness content, ability to model itself as embedded in an environment, etc.