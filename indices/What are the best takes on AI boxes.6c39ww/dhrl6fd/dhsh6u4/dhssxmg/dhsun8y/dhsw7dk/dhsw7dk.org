:PROPERTIES:
:Author: GaBeRockKing
:Score: 3
:DateUnix: 1495264875.0
:DateShort: 2017-May-20
:END:

That's not exactly what I'm saying. Imagine putting a short-sighted, narcissistic sociopath in charge of the AI box (there are plenty of people like this, for the record). Then the global argument doesn't sway them, and it's very difficult to persuade them that its in their own best short-term interest to release the AI, seeing as the AI doesn't actually have anything concrete to barter with.

The AI box experiment as a whole presupposes that the person listening to the AI can be swayed by rational argument. And, considering the subreddit we're in, that's typically a pretty good assumption. But in the general case, that's not necessarily true, and the roleplaying example I made is just one specific case where it doesn't work.