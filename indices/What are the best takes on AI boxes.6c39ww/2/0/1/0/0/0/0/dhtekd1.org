:PROPERTIES:
:Author: GaBeRockKing
:Score: 1
:DateUnix: 1495302663.0
:DateShort: 2017-May-20
:END:

#+begin_quote
  The ai box experiment doesn't presuppose the person being able to swayed by rational argument , an AI doen't have any reason to only use a certain group of tactics labelled "rational arguments " to win , in fact in the roleplay the people who(presumably) lost had a economic incentive to just ignore everything the other person said, we don't have many logs of people who won but generally(based of their latter comments about it) they seem to have recurred to personal and seriously dark arts things to win .
#+end_quote

In this case, I'm talking about "rational arguments" as "arguments based around maximally fulfilling the utility function of the key-holder," and by extension, meta-arguments purporting to explain the listener's utility function better than they themselves understand.

And specifically, I'm making the argument that, while the AI box experiment is fundamentally oriented around such arguments, many people have utility functions that a boxed AI can't plausibly argue that it'll be able to fulfill.