:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 5
:DateUnix: 1485578158.0
:DateShort: 2017-Jan-28
:END:

#+begin_quote
  Thanks for this conversation, it has given me some good ideas for a CYOA adventure series I am working on.
#+end_quote

No problem! I find this sort of stuff to be a lot of fun to discuss anyway--and if helped you come up with some ideas, all the better!

#+begin_quote
  It is true that every phenomena observed by science has so far been, in principle reducible to smaller, non-conscious components. That said, is there really any fundamental rule of logic that says this has to be true?
#+end_quote

Well, I don't think there's a handy name for the concept, but the short answer is yes. Here's a (relatively) brief explanation as to why:

Consider a computer program. At the most basic level, every computer program is merely a collection of bits--little 0's and 1's that act as instructions for the processor. Now, these bits are an example of something /ontologically fundamental/. What this means is simply that /bits are not made of anything else/--they just /are/. (Well, at least from a computational perspective. Physically, of course, most computer chips use currents and circuits to represent the 0's and 1's--but that's a matter of representation. When we consider the computer program /as a mathematical object/, the bits are fundamental.) This is all well and good--it's perfectly fine for bits to be fundamental. This is because a bit is a very simple mathematical object--in fact, a bit is the /simplest possible mathematical object/.

However, suppose you're a (rather dull) programmer who's been programming in binary all your life. One day, while entering strings of 0's and 1's into your terminal, a jolt of realization strikes you: /there appear to be some sequences of bits that occur repeatedly across all of your programs/. Moreover, you realize, the reason these sequences appear so often is because they accomplish some task that needs to be performed a /lot/ by your programs, in all sorts of different contexts. So, you reason, wouldn't it be nice if, instead of typing in the same sequence for, say, integer addition every single time, you programmed the computer to recognize a certain string of characters--say, "ADD"--and automatically run the corresponding bit sequence? You do this, and it turns out that you were right: it /is/ a lot easier. Eventually, this innovation catches on amongst your colleagues, and soon everyone is programming using these shorthands. (Note: This is not a historically accurate parable.)

The important thing to note here, however, is that despite making programming much easier, those shorthands are still exactly that: shorthands. The underlying processing still occurs using bits; at no point does the character sequence "ADD" ever appear in the bitstream. Because of this, we say that the ADD command is /not/ ontologically fundamental (although it's still simple enough that people feel mostly comfortable treating it as though it were); it can be decomposed into simpler parts.

Now, try to imagine a command that both (a) gets the computer to recognize it and do something, and (b) does not correspond to any bit sequence. If this seems impossible... well, that's because it is: no such command exists and no such command ever could. (More formally: for any action the computer can execute, there exists a bit sequence that, if entered, would cause it to perform that action. From this it trivially follows that there is no action the computer can execute for which there is not a corresponding sequence of bits.) And this situation is /exactly analogous/ to the claim that I originally made: that there exists no object that both (a) exhibits highly complex behavior and (b) does not consist of simpler components! Complicated objects /cannot/ be fundamental, because it is always possible to represent the behavior of such an object using a simpler description. (Corollary: sensation cannot be ontologically fundamental, because sensation is a very complicated thing indeed.)

Hope that helps!