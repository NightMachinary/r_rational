:PROPERTIES:
:Score: 9
:DateUnix: 1502808164.0
:DateShort: 2017-Aug-15
:END:

Oh, that's even one object-level down from where I was. I just get exasperated that lots of them /acknowledge/ the technological capacity we're going to have incoming this century, but seem to have decided that it's appalling and there's no good outcome. They say, "well, if you can automate away all the labor, at least the AIs will kill us all quickly, so we don't have to live through the dreadful hell of emptiness and meaninglessness left in the wake of automating the labor." Then they insist that we need to focus on breeding higher-IQ people... because apparently society has gone stagnant for lack of scientific super-geniuses at the same time as we're barreling towards death-by-AI.

I'd call it [[https://www.theatlantic.com/magazine/archive/2017/06/his-kampf/524505/][the Richard Spencer Principle: "I'm adopting this large ideology because if I don't, I might as well sit home and masturbate in my own filth."]] Their ideas don't seem to be a causal structure giving rise to distinct confirmed predictions, but instead [[https://www.reddit.com/r/slatestarcodex/comments/6q7hqj/why_do_rationalist_writingscommunities_seem_to/][a series of propositions constructed to just make everything seem weighty, terrible, and apocalyptic.]]

To which my only reply is: "Wuh? But there's lots of things to do besides upholding some awful ideology or basement-dwelling. There are more things in Heaven and Earth, internet Horatio, than are dream't-of in your frankly really obscure, weird philosophy."

Like, if you talk to me about Bayes and AI Safety, I'm gonna nod my head along, but if literally everything you try to talk about ends with, "And that's why X is the downfall of Western civilization and/or the human race", it's just kinda... a massive WTF.