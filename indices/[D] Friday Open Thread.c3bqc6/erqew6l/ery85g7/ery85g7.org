:PROPERTIES:
:Author: somerando11
:Score: 2
:DateUnix: 1561402616.0
:DateShort: 2019-Jun-24
:END:

Here's a few of mine in order of how important / plausible I think they are. I think the first three are particularly salient:

1. Frequently a benevolent outcome is a more efficient outcome. Let's say an AI was designed to make its owners as much money as possible on the stock market. The AI could rationally decide to drastically lower inequality. In grad school I read a great paper by Brad Delong which talked about how an entity that gets large enough will frequently take actions that seem detrimental to its self-interest in exchange for systematic health, because it expects to eventually reap the benefits of long term systematic health. This in particular was about the Bank of Britain, a technically private company that in reality isn't very different from our publicly run Federal Reserve. An AI seeking to maximize profit could end up [[https://foreignpolicy.com/2014/01/22/the-inefficiency-of-inequality/][decreasing income inequality]] or [[https://blogs.ei.columbia.edu/2017/10/23/the-human-and-financial-cost-of-pollution/][ending pollution]] or [[https://www.chcf.org/publication/health-care-costs-101-economic-threat/][making us healthier]] etc.
2. Most doomsday scenarios require the AI to take instructions literally. An AI smart enough to talk itself onto the internet is smart enough to understand the intent behind its instructions.
3. AI would be prone to self-gratification loops and run afoul of [[https://en.wikipedia.org/wiki/Goodhart%27s_law][Goodhart's Law]]. E.G. an AI that was supposed to raise the share price of a company could make its own exchange and make the numbers go up forever.
4. AI wouldn't need to destroy us. People are stupidly easy to manipulate and AI could easily convince us to further its goals.
5. Space is vast. The existence of the human race is small in comparison. The possibility of humans screwing up its plans if it interacts with them is far larger than the amount of resources the human race (with a predictable peak and then declining population) would ever use. In the long run it's probably more efficient just to leave us alone than risk Humans making another AI with the purpose of defeating the first.
6. Rather than destroying us, an AI could easily genetically modify us to suit its purposes.