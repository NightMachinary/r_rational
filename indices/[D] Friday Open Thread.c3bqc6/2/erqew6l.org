:PROPERTIES:
:Author: Veedrac
:Score: 6
:DateUnix: 1561141747.0
:DateShort: 2019-Jun-21
:END:

I need a list of every reason people have heard used to argue that we shouldn't be worried about AI from an existential risk perspective. Even if the arguments are bad, please give them, I just want an exhaustive list.

Here are some I know:

- We don't know what intelligence means.
- Intelligence largely arises from the social environment; a human in chimp society is much less productive than one in human society.
- We don't know that intelligence is substrate independent. We don't know what qualia is.
- Fast takeoff scenarios assume they will happen in world that look like today, rather than one with a lot of slightly-weaker AIs.
- AIs smart enough to kill us are smart enough to know not to do it, or smart enough to have better moral judgement than us.
- [[https://www.ted.com/talks/grady_booch_don_t_fear_superintelligence/transcript?language=en][You can just train AI on data to align them.]]
- If we're smart enough to build AGI, we're smart enough to make them do what we want.
- Just shoot it with a gun, it's not like it has arms.
- If AGI is so smart, why does it matter if it replaces us?
- I've seen AI overhyping, this is an extension of that.
- It's just sci-fi.
- “Much, if not all of the argument for existential risks from superintelligence seems to rest on mere logical possibility.”
- It's male chauvinist storytelling.
- Brains are fundamentally different from silicon computers. Typically the argument is referring to a lack of an explicit data store, and the brain being particularly asynchronous and parallel.
- Current AIs are incredibly narrow, AGI is stretching beyond current science.
- “ML in general is just applied statistics. That's not going to get you to AGI.”
- Current hardware is vastly smaller and less capable than the brain; Moore's law won't last to close the gap.
- We don't know how brains work.
- Brains are really complicated.
- Individual neurons are so complicated we can't accurately simulate them.
- We can't even accurately simulate a worm brain, or we can't reproduce behaviours from doing so.
- Even if you could make a computer much smarter than a human, it wouldn't make it all that dangerous.
- Not all AIs are agentful, just build ones that work.
- People building AIs won't want to destroy the world, there's no point panicking about them being evil like that.
- You're assuming you can be much smarter than a human.
- This is a modelling error; intelligence is highly multidimensional, you won't have a machine that's universally smarter, just machines that are smarter in some axes and dumber in others, like a chess engine.
- Superintelligence is so far out ([[https://isps.yale.edu/news/blog/2016/11/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence][>25 years]]) that it's premature to worry about it.
- It distracts from ‘real' risks, like racial bias in current AI.
- I do AI work today and have no idea how to build AGI.
- People are terrible at programming. [[https://www.reddit.com/r/bestof/comments/axmtnj/uptitz_learns_some_hard_truths_about_machine/ehv6ki9/][“Anyone who's afraid of the AI apocalypse has never seen just how fragile and 'good enough' a lot of these systems really are.”]]
- AGI will take incredible amounts of data.
- “I'm fairly sure there isn't really such a thing as disembodied cognition. You have to build the fancy sciency stuff on top of the sensorimotor prediction-and-control stuff.” (I'm not sure this is actually anti-AGI, but it could be interpreted that way.)
- We already have AGI in the form of corporations, and (they haven't been disastrous) or (we should worry about that instead).
- Experts don't take the idea seriously.
- [[https://news.ycombinator.com/item?id=19363444][The brain isn't the only biological organ needed for thought.]]
- Robin Hanson's arguments. I'm not going to summarise them accurately here, but IIUC they are roughly:

  - We should model things using historically relevant models, which say AI will result in faster exponential growth, not FOOM.
  - AI well be decentralized and will be traded in parts, modularly.
  - Whole brain emulation will come first. Further, whole brain emulation may stay competitive with raw AI systems.
  - Most scenarios posited are market failures, which have standard solutions.
  - Research is generally distributed in many small, sparse innovations. Hence we should expect no single overwhelmingly powerful AI system. This also holds for AI currently.
  - AI has diseconomies of scale, since complex systems are less reliable and harder to change.

- [[https://news.ycombinator.com/item?id=17215455][We should ignore AI risk advocates because they're weird.]]
- [[https://www.reddit.com/r/ControlProblem/comments/8ohodx/superintelligence_is_a_myth/][This set of arguments,]]

  - Humans might be closer to upper bounds on intelligence.
  - Biology is already incredibly optimized for harnessing resources and turning them into work; this upper-bounds what intelligence can do.
  - Society is more robust than we are modelling.
  - AI risk advocates are using oversimplified models of intelligence.

- We made this same mistake before (see: AI winter).

Please add anything you've heard or believe.