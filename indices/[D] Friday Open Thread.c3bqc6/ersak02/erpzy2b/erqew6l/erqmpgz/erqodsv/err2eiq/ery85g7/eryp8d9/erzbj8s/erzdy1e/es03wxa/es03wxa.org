:PROPERTIES:
:Author: somerando11
:Score: 1
:DateUnix: 1561464817.0
:DateShort: 2019-Jun-25
:END:

My (uninformed) opinion isn't that AI isn't an existential risk. To be perfectly pessimistic I think AI is inevitable.

There are too many commercial applications, leading to too much money. Even if there weren't, it's applications for "security" makes it too tempting for government. I think some systems create such strong short-term incentives that people behave in ways that is destructive over the long term even when they know this is happening. (Slavery, the arms race, environmental degradation like depletion of fisheries). The only thing that can break those systems are effective governmental control and technological paradigm shift. To successfully govern AI we would need a world government, which controlled research over AI in the long term. Otherwise there's too much incentive for independent actors to defect. I would give the chance of an effective world government happening in the next 50 years a 2% chance.

I would give a 20% chance of human-controlled AI enabling perfect totalitarianism; 50% chance of it causing dystopian levels of inequality; 20% chance of things staying mostly the same; 9% chance of massively improving everyone's life; and 1% chance of causing a human singularity.

So I'm relatively sanguine about the possibility of AI taking over, because I think there's a higher chance of AI-controlled AI massively improving everyone's life than human controlled AI. (10% chance everything stays the same, 20% chance it leads to some form of utopia (which could itself be a benign existential threat like Solaria in the foundation series).

Either way, humanity is about to try to thread the needle through some unpleasant times.