:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1561432882.0
:DateShort: 2019-Jun-25
:END:

Thanks again, this is useful.

Hypothetically, if you were convinced that far-term superintelligence---as in post-singularity---was a probable existential risk (say, 70% confidence), how much would your opinion change?