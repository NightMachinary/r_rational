:PROPERTIES:
:Score: 2
:DateUnix: 1436926803.0
:DateShort: 2015-Jul-15
:END:

It would be competing with humans for resources. It doesn't have to destroy humans outright; it can just outcompete us. In order for that not to be in its interests, it would have to specifically value humans, or it would have to be uninterested in acquiring additional resources. The former requires carefully designing its utility function or relying on sheer dumb luck, and with that sort of luck I could win every lottery in the world at once. And an AI not interested in acquiring more resources has a pretty limited goal.