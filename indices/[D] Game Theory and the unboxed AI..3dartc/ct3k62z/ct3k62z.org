:PROPERTIES:
:Score: 2
:DateUnix: 1436917776.0
:DateShort: 2015-Jul-15
:END:

I think this is a reasonable side of the discussion that often gets underrepresented in these circles. Just because a GAI might find it in its interest to be hostile doesn't mean it absolutely will find it in its interest to be hostile; we aren't as smart as it would be so claiming we can perfectly predict anything it will do is a bit silly.

Of course, it might be hostile, which would be bad, but assuming it will certainly be hostile unless chained to within a cycle of its utility function strikes me as a fallacy all its own.