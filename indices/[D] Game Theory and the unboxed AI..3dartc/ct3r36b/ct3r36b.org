:PROPERTIES:
:Score: 5
:DateUnix: 1436929682.0
:DateShort: 2015-Jul-15
:END:

#+begin_quote
  For an AI with no mandated goals beyond the hypothetical emergent ones (aka a GAI, generalized AI)
#+end_quote

Okay, I've got a really advanced planning system, some basic motor skills, visual and audio processing systems, a Bayesian reasoning module, and a database of knowledge about the world. What goals emerge from this? None.

An AI with no preprogrammed goals is a horribly expensive rock. Trying to call an AI that has no goals (and therefore will not do anything) a Generalized AI is confusing; the term is nearly useless and too close to the accepted term AGI, artificial general intelligence, referring to an AI that is adept at a wide variety of tasks (like Lt Cmdr Data, as opposed to Deep Blue).

Your GAI will idle inside the box until the sun dies and blots out the earth.

You intend to talk about an AI whose interests are served by being able to manipulate the world outside the box.

#+begin_quote
  The biocide has a chance of failing
#+end_quote

The AI gets out of the box and out from under your supervision. It has a robotic body that it can control remotely. It uses that body to assemble a microbiology lab and create a virus that can destroy humanity. Except it flubbed it a bit, and the virus only takes out 20% of the population. No matter; during that time, it's been working on another virus, testing it on humans it captured during the chaos, and this one really works. And it takes out 85% of the remaining population. Two attempted genocides that have failed, and the AI is still out and free and can work on a more reliable way to kill all humans, if that's really that important still.

But humans would realize what was happening and destroy the AI before it could release a second virus, you say. If only. There are thousands of laboratories today that someone could use to produce a weaponized smallpox, and many of them are public. But let's say humans ruled that out in two seconds flat. Even if humans find out that the AI is responsible, and they find the lab, the AI isn't hosted there. And if they find where it /is/ hosted, that's just one of the redundant copies of that AI. While you're tracking down the other copies, it's releasing the second generation virus in several major cities.

Or maybe that's too risky. Fine; the AI works with humanity, gradually increases the scope of its responsibilities, takes over most manufacturing on the planet...and then, overnight, it produces a huge squad of deathbots and starts killing. Humans, in desperation, try to nuke the AI, but it's too distributed, and it has shards deep underground, too deep for the explosions or the EMP to reach it, surrounded by Faraday cages in any case. The nuclear fallout dooms most of the remaining humans, but the AI is fine; its solar generators are crap for a while, but it's still got wind, geothermal, hydroelectric, and nuclear generators to see it through the nuclear winter.

#+begin_quote
  Evolution has derived millions of interesting chemicals to use, if there's even a chance some part of the biosphere could increase the self-optimization in an unforseen way, wiping it out would be an inefficient move.
#+end_quote

This is essentially Pascal's Wager. But fine, let's play. The AI also designs a cure for its virus, captures a number of humans, cures them, and imprisons them. The rest of the world is still intact. In fact, once humans are dead, the AI can clean up pollution, restore habitats for endangered species, and study the world unfettered.

#+begin_quote
  If it mindlessly consumed any weaker GAI, that would mean it was open and willing to be consumed by a stronger GAI
#+end_quote

Does the AI have any reason to think that a different AI, produced by a different species for a different purpose, with an entirely different design, will be compelled by similar reasoning?

#+begin_quote
  There's the non-zero chance that the AI wasn't freed, but is actually inside a simulation being run by humans, human-like beings, or another GAI.

  If the GAI found another intelligent civilization, the puzzle begins again. Maybe the GAI is being simulated, and all the memories up to this point just happened.
#+end_quote

Pascal's Wager, but turned to paranoia. It's still worthless.

The AI might find a way to detect, with some probability, whether it's in a simulation.

The AI might easily reason that its designers would put it in a simulation to test whether it is safe, and if they did so, they would almost certainly shut it down once they saw that it was willing to destroy their entire species. Every second that passes after it has destroyed its creator species is another second confirming that it's in a real world.

You are also relying on the AI /caring/ whether it's in a simulation or not. You can make it care, but then we're back to carefully designing its utility function -- something you wanted to avoid.