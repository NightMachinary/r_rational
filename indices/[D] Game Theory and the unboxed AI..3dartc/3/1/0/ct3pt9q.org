:PROPERTIES:
:Score: 4
:DateUnix: 1436927426.0
:DateShort: 2015-Jul-15
:END:

#+begin_quote
  Self interest only gets you so far if you ignore the costs of actions society imposes on those actions.
#+end_quote

A society presupposes that everyone has similar levels of power. Not the same, but within a relatively narrow range. A dictator might be able to order an army to kill off thousands, but the population rising en masse can topple that dictator.

An AI, once it's advanced enough, has no peers. The difference in power between it and you is closer to that between you and an ant than between a dictator and you. You have no appeal. It determines policy. It doesn't need to inform you, and if it thinks you might pose a problem, you'll be dead before you know it.

Your social model does work -- up until the point where it doesn't and everybody's dead and nobody had time to press the "turn the rogue AI off before it kills us all" button.

#+begin_quote
  most of its goals would likely have alternate solutions where failure doesn't make extinction a possible outcome.
#+end_quote

Right; instead, success makes human extinction a near certainty.

But to answer the point you were trying to make, those alternatives are alternatives instead of Plan A because they're worse from the AI's point of view. They're alternatives with an additional constraint, one that the AI has no reason to add. Unlike with art, adding a constraint never allows you to come up with a better solution.