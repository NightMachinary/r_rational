:PROPERTIES:
:Author: E-o_o-3
:Score: 2
:DateUnix: 1437680939.0
:DateShort: 2015-Jul-24
:END:

That "sense" of rationality is more like "omniscience", I think.

#+begin_quote
  completed infinity of empirical data
#+end_quote

No...just responses which are "correct" as per the agent's preferences in response to /new/ empirical data. You can have a perfectly rational agent that knows nothing about the universe it is in.

#+begin_quote
  completed infinity of computing power
#+end_quote

I'm a lot more sympathetic to this definition of rational, but in practice: You don't need to be able to solve all possible optimization problems to be perfectly rational, though - you only only to solve /your/ optimization problem.

So long as all choices you are faced with happen to be within the bounds of your bounded rationality, you are perfectly rational for all practical intents and purposes.

--------------

In practice, I agree - it's /seems pretty likely/ that the optimization problem of "maximize human values, given this universe"... or even the problem of "maximize paperclips, given this universe" for an agent with any reasonable freedom of choice requires infinite computing power to perfectly solve. But I have to say /probably/ because I don't know for sure (and in fact I /can't/ know for sure, since the true laws of the universe can't be discovered with certainty)