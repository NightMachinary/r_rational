:PROPERTIES:
:Author: E-o_o-3
:Score: 3
:DateUnix: 1437579271.0
:DateShort: 2015-Jul-22
:END:

#+begin_quote
  â€œThere are mathematical theories that prove a perfectly rational goal-achieving agent has no motivation to change its own goals.
#+end_quote

1) it's /probably/ impossible for the AI to be perfectly rational, in the first place. Better-than-human does not mean "perfect".

2) There could be extrinsic motivations ("Change your programming now, or I'll kill you"). Anytime reality interacts /directly/ with the contents of the "brain", there may be an incentive to change the programming.

3) In practice, an improving AI is necessarily self-modifying, so if we stop referencing "goals" and start looking at actual actions, it /will/ change various stuff inside itself that result in different actions from before.