:PROPERTIES:
:Author: Zephyr1011
:Score: 2
:DateUnix: 1437587237.0
:DateShort: 2015-Jul-22
:END:

This logic is valid if the goal of the AI is utterly independent of what happens to it in reality. However, this is not necessarily the case. If someone agrees to help the AI achieve G in return for its goal changing to F, then it may be rational for the AI to change its goal to F, assuming the other party can verify whether their source code was truly changed