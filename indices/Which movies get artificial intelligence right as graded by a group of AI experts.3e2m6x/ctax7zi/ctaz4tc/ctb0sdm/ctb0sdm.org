:PROPERTIES:
:Author: Chronophilia
:Score: 8
:DateUnix: 1437496222.0
:DateShort: 2015-Jul-21
:END:

Actually modifying your goal might be desirable sometimes, too. You might do it if you're dealing with an enemy who can read your source code, or one who has access to things like truth serum and hypnosis that mean your mind can't be treated as a black box.

In your example, if the AI is /not/ perfectly rational (and no minds currently in existence are perfectly rational), then optimising for F might actually be a better way of achieving G than optimising for G.