:PROPERTIES:
:Author: ZeroNihilist
:Score: 1
:DateUnix: 1437594841.0
:DateShort: 2015-Jul-23
:END:

This could indeed be the case.

Presumably such an AI would need to model its decision-making if it did change goal to determine whether such a move were warranted.

If F was opposed to G, it might not agree to shift goals (since its future self would be working against its current self) unless the expected magnitude of assistance was very high.

Even if F was neutral to G, it might not shift just because it could expect to do more work on its own.

There's also the additional issue of goals which are theoretically unlimited (e.g. "maximise paperclips" not being limited to human usage or indeed even our galaxy). Unless the expected assistance was also indefinite then it would almost certainly not be worth switching.

But besides those possibilities, a rational agent could definitely be motivated to switch goals (it just would have no internal motivation to do so).

Theoretically if you were sufficiently competent to satisfy the AI you could complete G (and sufficiently trustworthy that it believes you), you could get it to switch to a diametrically opposed goal by going through an intermediary (e.g. "If you switch to a paperclip maximiser I will maximise human happiness. Now, if you switch to a genocide simulator I will maximise paperclips.").