:PROPERTIES:
:Author: LunarTulip
:Score: 3
:DateUnix: 1433606488.0
:DateShort: 2015-Jun-06
:END:

The way I tend to interpret it is that the alignments have molded people's values, rather than the other way around. When there's an objectively-measurable thing which mostly lines up with what they already think of as good, they're likely to adopt it as their standard measurement for goodness, because people tend to find the idea of objective morality to be comforting.

Once it becomes the standard, the points of inconsistency between their original values and the “good” that they can objectively measure start to melt away; people reason that anything which registers as “good” must be morally good, even if it wasn't previously labeled as such.

Also, of course, I'd expect different cultures to latch on to different alignments. For instance, I'd expect drow culture to see the alignment which the rulebooks refer to as "Evil" as the /good/ one, since it's the one most in line with their values.

A side effect of this interpretation is that the round-peg-square-hole problem stops being a problem. Someone with complex values will /keep having/ those same complex values, regardless of how the universe's alignment-measuring algorithms decide to label them.