:PROPERTIES:
:Author: xartab
:Score: 1
:DateUnix: 1534606050.0
:DateShort: 2018-Aug-18
:END:

#+begin_quote
  Okay, but you're not actually demonstrating any actual harm being caused at all. You're presuming that value-changing is harmful. I'm saying "show me how."
#+end_quote

How do you assess harm? Not by physical pain alone, as sometime we suffer pain in order to gain something we value more. Not by psychological pain alone, as sometime we accept experiences that will cause us anguish in order to gain something we value more. We can propose that harm is equivalent to how much the world moves away from how we would want it to be, not by superficial desires but by deep wants.

Also, we don't value the perception of satisfaction in itself, we value how reality is, despite our perception (mostly).

For example, most people would prefer to suffer by discovering that their partner has cheated on them, rather than live happily all their life without being aware of the betrayal. Another example, we value things that will happen to out bodies after we are dead, despite the fact that we won't be around to perceive them.

This means that if you dissatisfy a value, the individual being aware of it doesn't come into play.

Now, I admit that there are people who don't agree with this, they don't care if their values are infringed when they're not aware of it, or after they become unable to keep caring. Maybe you belong to this category.

It doesn't matter, because harm is not decided by how those people feel, it's decided by how /everyone to which a decision applies/ feel. It's decided by the satisfaction or dissatisfaction of the value function of everyone. And I will point out that physical facts have no influence on what one /should/ (terminally) value, because of Hume's guillotine. Provided, that is, that specific value satisfactions aren't contingent on physical reality.

Oh, by the way, I don't know if you already watch Robert Miles' YT channel, but it's very interesting. In [[https://youtu.be/ZeecOKBus3Q?t=262][this video]], he goes into convergent instrumental values (he calls them goals... which is kinda better than values, I should do it too) and later arrives at /goal preservation/ (6:28). I don't think it will convince you of anything, but you can never know. Maybe his eloquence, which is a world apart from mine, will give you some sort of epiphany. Or maybe not, but it's neat anyway.

#+begin_quote
  Right, "no brainwashing" is deontological, not consequentialist.
#+end_quote

I don't agree. I think it's consequentialist in a way that takes into account previous states as relevant states. Like, if tomorrow a Superintelligent AI had the power to change all the values of humanity, to the very last one, into a value of not-existing, and then destroyed humanity to fulfil that value, by your definition it would have done nothing wrong.

The reason why I bring up brainwashing is that I think it's difficult to visualise the condition of having your values changed, as it happens so rarely in reality, but it's a common trope in fiction. When we see it in fiction, it's usually presented as an evil, meaning that authors of fiction, at the very least, think value-changing is evil... evil means bad, bad means that it decreases value satisfaction, and that means that there must be a value against it.

#+begin_quote
  No, because now you're changing more than just people's values, you're actually messing with millions of happy homosexual relationships, which is clearly harmful.
#+end_quote

You could probably have deduced from context that my question was meant to ask what your opinion would be if the only appreciable change in value satisfaction was changing sexual orientation, while preserving other variables, as the total quantity and happiness of relationships. I'm going to extrapolate that if you'd answered the latter, you would have said that yes, you do find it equally favourable. Please correct me if I'm wrong.

If that's the case, I'll point out again that your system of values is not shared equally by everyone.

(You can then argue that we would take into account the amount of value each to-be-snapped person would put on not having their values changed, and I would agree. I think for large populations a representative sample would do, so we could gather a lot of people chosen at random, interview them, and then determine their average amount of aversion to the thought of having their value function changed)

#+begin_quote
  They care about keeping caring because their caring /is itself valuable./
#+end_quote

This is precisely my point.

#+begin_quote
  But I don't care about the former. I don't value their value of their mindless, pointless hate.
#+end_quote

[[https://lh5.googleusercontent.com/tPvbV4OfLbgcERXPv722tqo9FVQYUwAewJeWW2EoGDMpvg-8TcnGvvHJGWLrrByAa0jxB6753tZHwBDVTpFo7oTyM_6Zcbf9Lnf3RTQeDmdWcltNlf3s1DvGSg6kPSUD3kQFdA89][Relevant.]] By which I don't mean that you're wrong in not valuing it, but that hurt isn't calculated on the basis of what /you/ value.

Nice discussion by the way, I love this subreddit.