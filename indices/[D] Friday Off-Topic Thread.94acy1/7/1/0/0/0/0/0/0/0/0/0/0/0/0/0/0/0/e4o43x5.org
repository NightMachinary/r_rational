:PROPERTIES:
:Author: crivtox
:Score: 2
:DateUnix: 1534988951.0
:DateShort: 2018-Aug-23
:END:

Values changing is clearly harmful in a preference utilitarianism way.like if you had a papercliper and modified it to not want to tile the universe whith paperclips y the papercliper would not want you to do that .And if someone actually wants to be homophobic then changing them to not be homophobic will rate negatively on their utility functions .And people generally consider that doing things to someone that they wouldn't want you to do to is bad . It just happens that it balances whith the good generated by happy homosexual relationships in your preferences.

I think people's cev probably doesn't include homophobia and if they knew enough they would want to want to be homofobic .But this is not trivially correct and there is room for someone to disagree there .

A papercliper would want to make everyone want to make more paperclips , and for the perspective of the papercliper thats only positive .But you could also have an agent that minimizes the changes the utility functions of humans , and that also seems perfectly consecuentialist so. (though now that I think about it It I'm confused about if there is any kind of deontology that you can't see as some kind of consequentialism if you go meta enough.huh).There is nothing inherently silly about caring about changes in the preferences of other people.

And In any case there are good reasons to have rules against changing people's values like that.Its better if everyone agrees to that norm so our enemies don't brainwash everyone into something we dislike .Even if it would actually be good if you actually did it.