:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1534833267.0
:DateShort: 2018-Aug-21
:END:

Hmm. I feel like we're missing each other's cruxes. Particularly because of things like this:

#+begin_quote
  I don't agree. I think it's consequentialist in a way that takes into account previous states as relevant states. Like, if tomorrow a Superintelligent AI had the power to change all the values of humanity, to the very last one, into a value of not-existing, and then destroyed humanity to fulfil that value, by your definition it would have done nothing wrong.
#+end_quote

It seems like you keep bringing up examples of changing people's values that lead to them then objectively losing something in some way that we can from our vantage point obviously determine is negative. If you can't posit a situation in which people's values are changed /without/ it actually being a bad thing, then I think you may, in fact, truly, despite your repeated insistence otherwise, deep-down consider value-changing to be bad deontologically, and not consequentially, especially when you bring up how it's so often bad = evil = harmful in fiction :P

The homo-to-hetero snap that also takes into account all the different changes in life circumstances to equalize happiness seems like it's stretching things beyond the scope of the question in order to come up with a scenario that proves your point, but if it helps, I /would/ say that snapping to make everyone bisexual is another thing I would do and consider an obvious net positive.