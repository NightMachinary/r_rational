:PROPERTIES:
:Author: NotUnusualYet
:Score: 1
:DateUnix: 1442892930.0
:DateShort: 2015-Sep-22
:END:

Didn't see this response until just now, sorry for the wait.

Anyway, the problem is that you simply can't afford to take the risk of building a powerful AI that doesn't care about human values, especially an AI that's going to improve itself. Even if the entirety of humanity spent 100 years thinking through safeguards it wouldn't be enough, because by definition humans cannot accurately predict how a superintelligence will act.