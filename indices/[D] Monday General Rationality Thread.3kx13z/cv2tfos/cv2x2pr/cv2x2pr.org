:PROPERTIES:
:Author: NotUnusualYet
:Score: 1
:DateUnix: 1442359346.0
:DateShort: 2015-Sep-16
:END:

Are we assuming that the AIs can't increase their own intelligence in any way? Otherwise if there's a fast takeoff in intelligence, some AIs will end up by chance much more intelligent and can leverage that into permanent domination for their user's utility. The result would be equivalent to randomly elevating a human to godhood, which isn't the worst outcome but certainly not ideal.

More importantly, I feel like this would lead to an incredibly aggressive society in which everyone (or at least, their AI) is trying very hard to increase their own power so their utility function can dominate. I don't particularly want a humanity where everyone is a supergenius trying to take over the world, even if it's done without violence or manipulation.