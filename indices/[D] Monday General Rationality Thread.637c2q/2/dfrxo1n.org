:PROPERTIES:
:Author: liveoi
:Score: 9
:DateUnix: 1491236642.0
:DateShort: 2017-Apr-03
:END:

Re: AI in a box experiment. (I thought to comment in the original thread, but I'm a little late to the party)

I always thought that the source of the problem is that you actually want something from the AI (for example, a cure for cancer). Else, why build a gate at all? (or the AI itself for that matter)

The gate keeper's goal is to allow some information flow (that could be helpful and beneficial) without risking freeing the AI (and world destruction).

The point is, when you're dealing with an entity that is vastly more intelligent than you, you can never be sure of the full consequences of your actions (the cure for cancer could somehow lead to freedom for the AI).

On a more general note, I'm not entirely sure that the required level of intelligence for that kind of trick is even possible. A lot of people fear an AI because it might be able to improve itself, but I'm not sure that it is possible to self improve in a consistent way. Moreover, intelligence itself is not a linear property, i.e. , in order to be twice as intelligent, you would have to invest a lot more than twice the effort. And that means that even if some entity could self improve, this exponential process does not lead to an intelligence explosion.

Edit: Formatting