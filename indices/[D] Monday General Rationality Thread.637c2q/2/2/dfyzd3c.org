:PROPERTIES:
:Author: Brightlinger
:Score: 1
:DateUnix: 1491597039.0
:DateShort: 2017-Apr-08
:END:

#+begin_quote
  And that means that even if some entity could self improve, this exponential process does not lead to an intelligence explosion.
#+end_quote

If true, this implies that recursive self-improvement should level off somewhere. It doesn't imply that it has to level off near any particular threshold: if the process "only" becomes as smart as a network of ten thousand geniuses, or even only as smart as /one/ human genius, that's still a pretty big deal.