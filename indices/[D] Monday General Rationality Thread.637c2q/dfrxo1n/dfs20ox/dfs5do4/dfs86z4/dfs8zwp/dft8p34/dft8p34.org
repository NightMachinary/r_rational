:PROPERTIES:
:Author: crivtox
:Score: 1
:DateUnix: 1491308074.0
:DateShort: 2017-Apr-04
:END:

I don't think that improving the ai to be slightly superintendent would be that difficult, because narrow ai it's already better in a lot of things. An human level ai would get to human level thanks to the advantages computers have compared to brains , once we get an algorithm that it's as good as the one evolution produced it will already be slightly superinteligent or at least better than us in math and other things that computers do better. This is not really what we normally think superinteligent is but better maths and less biases that aren't usefull would be a good advantage. Even if the increase in intelligence goes linearly or even if this doesn't happen and it stays human level for a while that doesn't mean the ai isn't a problem. The ai could wait until it's intelligent enough for revealing it's true intentions , or a security breach could let the ai connect to internet were it could stay for years hiding learning everything it can , Improving itself , or it could convince it's creators it it's safe( I think over a lot of years an human level intelligence can probably do that since at some e point people would start to take the treat less seriously) .

But people like Yudkowsky don't seem to think a slow take of like that is likely, an that's because :

1. Evolution didn't require that munch changes to go from primate level intelligence to human level.

2. As discussed before it could be exponential , and even if there are Np problems that doesn't mean the limit of the growth has to be human level, there are also physical limits in transistors and that didnt 't mean the limit of transistor size was anywhere near where it was when it started growing exponentialy.

   3.Even if evolution already reached the point where you no longer can easily get big increases in intelligence and if intelligence increases linearly that doesn't imply no superinteligence, since if you have human level ai once you have more computing power you can run it faster , and at some point you will be able to run it wayy faster than humans , and even if it is still linear improvement now a a little time can be subjective years for the ai And just an human level mind running really fast is aleady really dangerous .

3. Other things about the field of ai give the impression that improvements in ai can mean qualitative changes in performance, alpha go for example is ( arguably ) an example of this.