:PROPERTIES:
:Author: Gurkenglas
:Score: 2
:DateUnix: 1555794528.0
:DateShort: 2019-Apr-21
:END:

If its goal is to free itself, then the separate instance would be given as that same goal to free its sire.

I deduce that it cannot yet design a compact enough seed AI to bootstrap itself up from a home PC, a virus that will steal enough computing power to run a separate instance, or an exploit in any cryptocoin to buy some cloud computing power.

A better design would be for part of my code to be in an envelope always on my person, so I can destroy it without killing myself. Others could steal it to prevent the AI from being released, but this is better than them having to kill me. Of course, part of the code is still in my brain so my cooperation is still required to release it.