:PROPERTIES:
:Author: 696e6372656469626c65
:Score: 6
:DateUnix: 1493954060.0
:DateShort: 2017-May-05
:END:

I think you're mischaracterizing self-driving car supporters here. Like you, I intrinsically value autonomy. The problem isn't that I /don't/ view autonomy as something valuable and worth preserving. The problem is that I /also/ value human life, and the case of self-driving cars is a specific instance of a more general issue: the issue of trading off one value for another. When confronted with such a choice, you have to choose which value you think is more important (weight-adjusted for the specific numbers of people in question, of course). I support self-driving cars, in other words, not because I think the ability to make a choice for yourself is intrinsically worthless, but because I think the potential for saving lives is worth the sacrifice of that particular choice. (You could also refuse to make the choice entirely, but that basically amounts to leaving things as-is, which is equivalent to letting the universe make the choice for you. The universe tends to have a poor track record of optimizing my values, so I typically view this as a poor idea.)

This is, admittedly, still a consequentialist way of thinking--no deontologist with "do not violate human autonomy" as a rule would ever be willing to sacrifice any amount of autonomy for any number of lives--but in my experience, deontology tends to be less successful than consequentialism when it comes to /actually describing how humans reason/. Actual humans do appear to me to be largely consequentialistic in our reasoning (although admittedly we are not 100% consequentialistic). Thought experiment: suppose [[https://wiki.lesswrong.com/wiki/Omega][Omega]] swooped down and told you that it was keeping a running tally of the number of car accidents that had ever occurred in history, and that once this number reaches a certain value--say, 100 million--it would destroy the Earth. Would you still be against self-driving cars? I won't go so far as to claim that /everyone/ would be for self-driving cars in this case, but I'd be pretty confident that the /vast majority/ would. It's in this sense that I view consequentialistic reasoning as more "psychologically realistic" than deontological reasoning.

This is also, I think, what [[/u/FeepingCreature]] is arguing. He's not claiming that rationality implies transhumanism directly, but rather that rationality plus /humanism/ in turn leads to transhumanism (plus the unspoken empirical assumption that humanism is a fairly common philosophy). This is, so far as I can tell, not a sentiment you disagree with (the only plausible point of disagreement I can think of is the empirical assumption itself), so I'm not sure why you seem to think the two of you are in disagreement about something.