:PROPERTIES:
:Author: Zephyr101198
:Score: 1
:DateUnix: 1622233272.0
:DateShort: 2021-May-29
:END:

I'm not aware of much work that tries to map out concrete bad scenarios from AI, that is also actually trying to be a good story. Though I'd definitely love to see some!

The classic MIRI conception of AI going bad involves an agent that gets incredibly powerful, incredibly fast and takes over basically instantly, which doesn't make for a great story. But there's also a bunch of other perspectives, especially focusing on a slower world, with many agents. In particular, some bits of work you might find interesting that try to somewhat flesh out these scenarios:

[[https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like][What Failure Looks Like]] by Paul Christiano

[[https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story][Another (Outer) Alignment Failure Story]] by Paul Christiano

[[https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic][What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes]] by Andrew Critch