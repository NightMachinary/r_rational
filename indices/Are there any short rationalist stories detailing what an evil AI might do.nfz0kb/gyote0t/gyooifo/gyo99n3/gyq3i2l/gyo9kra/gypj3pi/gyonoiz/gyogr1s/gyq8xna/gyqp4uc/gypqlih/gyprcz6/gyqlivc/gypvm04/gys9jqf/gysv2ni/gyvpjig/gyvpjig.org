:PROPERTIES:
:Author: Veedrac
:Score: 3
:DateUnix: 1621551308.0
:DateShort: 2021-May-21
:END:

AI risk only becomes AI risk once the AI is either generally more intelligent than a human, or at least sufficiently generally intelligent, and also sufficiently more capable than humans in at least some axes. You should start out by assuming that the AI /already/ has a significant cognitive advantage over humans in at least a significant number of respects.

On that basis it might be worth brainstorming a few ways different sorts of AIs that meet the above criteria might achieve greater levels of power, or improve their own cognitive abilities. Say, if they had a year to do it. There are a lot of answers to that question. (If this sounds evasive, it's actually mostly just laziness, but I still recommend the attempt.) Then you know that a dangerous AGI would do something at least as smart.