:PROPERTIES:
:Author: PastafarianGames
:Score: -1
:DateUnix: 1621457362.0
:DateShort: 2021-May-20
:END:

#+begin_quote
  why does the rationality community spend so much time figuring out how we might align future superintelligent AIs with our values?
#+end_quote

Certain sections of the "rationality community" do this, not the whole meta-community of folks identifying as rationalists, and the answer boils down to "because of CFAR/MIRI and other, similar doomsday prophets and cultists".

But that's not the fullness of the answer. The other part of the answer is that all of the things we could /actually/ do, all of the highest-leverage things that could actually save the world or even contribute to doing so, in our /actual world/, are hard and expensive and tedious and we don't want to do them. So we've invented this notion that funding or engaging in AI research somehow helps.

There's only one particularly plausible scenario: GAI never happens, but at least we felt good about not putting in the work to stop climate change, bad governance, and bad public policy.

(I'm not putting in that work either, mostly because I've spent years trying to find a way to do literally anything to make a difference, and I'm taking a few years off before I throw myself into the emotional meat grinder again.)