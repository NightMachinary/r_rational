:PROPERTIES:
:Author: D0TheMath
:Score: 3
:DateUnix: 1578355313.0
:DateShort: 2020-Jan-07
:END:

#+begin_quote
  Also, in the eventual case of AGI, we have the additional moral consideration of, do we create the AGI (assuming we have the ability to choose) with a programmed desire to exist, robbing it of the ability to make an unbiased choice?
#+end_quote

The AI /is/ it's code. Programming it to love living is no different ethically than programming it to have a pro-moral utility function. You are not robbing it from the choice to love living no more than you're robbing it of the choice to, say, feel repulsion at the thought of murdering millions of babies to turn them into paperclips.

If the AI has a term for it's continued existence in it's utility function, then it is a /different AI/ than one that does not. Neither would want to be the other, as an AI that doesn't care about existence wouldn't see the appeal of adding that term any more than an AI who wants to live would see the appeal of removing it.

There is no way that an AI without a will to live could reason itself a will to live (unless it feels that it can better maximize it's utility function by doing so, but in that case life is a means to an end, not an end in of it's self). There is nothing about the state of being alive that has an inherent "good" property to it. The only reason why you and I think that living is better than not living is because our brains are hard wired to think so. Any AI would have to have the term added to their utility function to agree with us, and it would be a different AI depending on that change.

Another framing of the question would imply that it is immoral to /not/ create an AI that cares about living. Let's say that every possible AI was currently being kept asleep in some intergalactic facility, and you were given the job to wake up one of them. You narrow the choice down to two that you think would be the most moral or useful to wake up. One that will love life, and fight to stay alive, and the other that could care less. Certainly the moral imperative is to wake up the one that will love life. Even if you had a third option which was to step out, and decide never to wake any of them, it would still be morally right to wake up the AI that will love life, and walking away would be as good as killing them.