:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 2
:DateUnix: 1578243941.0
:DateShort: 2020-Jan-05
:END:

So another way of posing the core of what I'm wondering is maybe easier to express in the context of an AI or other fully created intelligence:

When you are designing such an intelligence, is it moral to create it such that it finds purpose/joy/whatever you want to call it in serving you/some other group or entity. In other words, is it immoral to make something in such a way that it fundamentally won't choose an existence other than the one you designed it for?

If that kind of thing /is/ immoral, then is it similarly immoral to create a being that can't help but prefer existence? That's sort of what we are (accidentally) doing when we have children. We are creating a new being that is designed in such a way that, post facto, is incapable (in the majority of cases) of making a truly free choice about existing or not existing since it is designed to prefer one over the other. Does the fact hat this designing was done by evolution rather than the parent matter?

Again, sorry that this isn't very organized. I don't think I've fully nailed down exactly what it is I'm thinking about so I'm kind of jumping from point to point.

Maybe this all boils down to a question about "free choice"? The whole reason that I'm shying away from using my own empathy is that I feel like humans are fundamentally incapable of making a free choice about their preference of existence over non-existence since we have been programmed to prefer one over the other, and so I'm trying to figure out whether purposefully creating another being that is similarly limited (even if it's in a different way such as might be the case with AGI) has any negative moral connotations.