:PROPERTIES:
:Author: Fresh_C
:Score: 2
:DateUnix: 1454724188.0
:DateShort: 2016-Feb-06
:END:

Okay, that makes a lot more sense. I suppose you could include a value system for those safety features, but most of them would be hard to quantify. And as you said it would try everything it possibly could to obey the letter of the safety feature even if it violated the spirit of it.

It seems the real problem is getting the AI to understand the underlying purpose of the security itself. Only once it can set a utility value based on our expectations that it acts ethically will it stop trying to do something that benefits it, but would be considered detrimental to us.