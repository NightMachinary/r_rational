:PROPERTIES:
:Author: Fresh_C
:Score: 1
:DateUnix: 1454721167.0
:DateShort: 2016-Feb-06
:END:

I understand the idea laid out by the article, but I don't see why starting with a basic goal of "Play chess with the limitation that you are willing to be turned off at anytime" would violate that principle.

The AI may realize that it would be better at playing chess if it wasn't turned off, but part of its stated goals are to be turned off at the whims of humanity. Thus in order to maximize its goals it cannot impede the process of allowing itself to be turned off.

Basically I'm saying that the safety features are included in the goals. So the AI will never want to achieve its goals at the cost of violating the safety features. Because the safety features ARE its goals.