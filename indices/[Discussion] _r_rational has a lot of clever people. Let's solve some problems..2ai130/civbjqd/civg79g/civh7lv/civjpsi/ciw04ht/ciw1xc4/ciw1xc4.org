:PROPERTIES:
:Score: 3
:DateUnix: 1405233757.0
:DateShort: 2014-Jul-13
:END:

#+begin_quote
  Friendly is a integer, not a boolean.
#+end_quote

Not if it's done right, it's not. I think Eliezer's "meta-ethics sequence" was dangerously close to being /philosophy/, and I personally would hold that you /do not ever/ build superintelligent AIs based on /philosophy/ (attempting to do so is likely to get you killed, wireheaded, or turned into a pony). I think Eliezer would probably point out that /he invented this view/, and that he didn't actually intend the meta-ethics "sequence" to convey that you should use philosophical methods but rather than this is a hard problem on which we need a lot more /scientific/ evidence to successfully dissolve the problem into a matter of algorithms.

/Given/ a dissolution of AI Friendliness into algorithms, it becomes a matter of formal proof from axioms and the probabilities we assign to the axioms.