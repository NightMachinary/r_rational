:PROPERTIES:
:Score: 4
:DateUnix: 1435796903.0
:DateShort: 2015-Jul-02
:END:

#+begin_quote
  The canonical example being "train my machine-learner against images of smiling people" (leading to a universe tiled with the minimal amount of a face required to register a 'hit' from the classifier)
#+end_quote

I always wonder how someone was so fucking stupid that they managed to build a causal inference engine (aka: the AI itself), but managed to define its utility function in terms of purely feature-governed concepts rather than causal-role concepts.

Actually, no, I very recently started wondering that.