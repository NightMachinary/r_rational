:PROPERTIES:
:Author: DataPacRat
:Score: 3
:DateUnix: 1435766374.0
:DateShort: 2015-Jul-01
:END:

"Minimize the odds of the permanent extinction of sapience" sounds like one of the more ideal utility functions - after all, sapience is required in order for any minds to exist to /have/ any other goals. But a superintelligent AI with access to nanotech, space travel, and all that other good stuff is reasonably likely to take that 'minimize' in strange directions, few of which are likely to be all that beneficial to biological humanity.

CelestAI minus the urge to convert regular old humans into shining examples of sapient ponydom is just one Really Horrible outcome. After all, as the saying goes, "The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else."

(Which is why I modify /my/ 'avoid sapience extinction' utility function with the parallel utility function, 'avoid my own personal extinction'...)