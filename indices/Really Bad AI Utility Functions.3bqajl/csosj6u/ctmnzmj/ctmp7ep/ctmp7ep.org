:PROPERTIES:
:Author: DataPacRat
:Score: 1
:DateUnix: 1438345882.0
:DateShort: 2015-Jul-31
:END:

"Tiling" (depending on how you define it) may not be the result, if the utility function is "minimize the odds of permanent extinction" as opposed to "maximize the number of". The obvious extreme version of a different strategy would be to have just one sapient entity in the universe - and to convert all mass and energy in the universe into machinery to keep that one sapient entity alive (and resurrect it as necessary).

I suspect that the probabilities involved would lead such an AI to adopt a hybrid approach; create enough sapient organisms to minimize the odds that a single death would lead to extinction, and then pull a CelestAI to convert the universe into various mechanisms to protect against as many large-scale extinction events as possible.

(Then again, I could be wrong... :) )