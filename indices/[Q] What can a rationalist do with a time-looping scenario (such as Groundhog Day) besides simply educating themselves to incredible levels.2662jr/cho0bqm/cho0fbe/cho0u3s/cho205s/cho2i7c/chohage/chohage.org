:PROPERTIES:
:Author: Pluvialis
:Score: 4
:DateUnix: 1400775371.0
:DateShort: 2014-May-22
:END:

This is a moral question that I've been pondering over for a while now, and I haven't reached a conclusion yet.

My intuition is definitely very strongly that in a Groundhog Day scenario everyone else has no appreciable utility value whatsoever.

My intuition regards them as mindless automatons. And even though I know they're not mindless, there's something significant about the fact that on any given day you could commit whatever acts of atrocity you could think of upon them and that the next morning and every morning after they'd continue to get up as usual, no trace of your previous acts remaining.

But when I compare this to my thoughts about temporary intelligences simulated on a computer, I come up with a different response. If you could spawn a new intelligence in a computer simulation, I would be quite upset with you if you didn't look after it, even if you were planning to delete it after the next day.

I think it's the inevitability of the Groundhog Day scenario, the fact that it's not your fault that they're being respawned every day for 24 hours. If a computer were simulating somebody afresh every day and there was nothing you could do to prevent it, I think the same ennui about their utilitarian value would result.

EDIT: This starts to get even weirder for me, and completely beyond my ability to come up with an intuitive response, if you consider the following question.

What is the utility value of a series of saved snapshots of a simulated intelligence?

Assuming a simulated intelligence has utility, at what point does the utility arise? As it's being calculated? As it's being read from memory? What if you read the snapshots non-chronologically? How would I know if I were a simulation being run non-chronologically? Could I just be a series of snapshots stored in memory? Could I be just a single snapshot? Could I even just be a theoretically possible arrangement of molecules?

Makes my brain hurt.