:PROPERTIES:
:Author: hxcloud99
:Score: 8
:DateUnix: 1543127294.0
:DateShort: 2018-Nov-25
:END:

Drats. I forgot about this thread and I'm in dire need of it right now. :(

Welp, I hope there will still be takers.

* RaNoWriMo: Follow Only Phantoms
  :PROPERTIES:
  :CUSTOM_ID: ranowrimo-follow-only-phantoms
  :END:
TL;DR: In the future, everything has gone to shit. The clathrate gun hypothesis held water and fired in 2034, causing ~2°C of warming in 4.5 years and eventually kicking off runaway global warming.

Eighteen years later, in an underwater community near where Manila once stood, an AI becomes sentient and invents /causal loop engineering/ AND learns about the existence of three other AGIs who just became sentient hours before it. Cue a seven-day 200M+ casualties war of increasingly elaborate causal gambits as the AGIs battle across time and space for control over this universe...

...which kicks off our story set in 2016. Four college students find themselves solving puzzles in desperation after the apparently supernatural suicide of a student proves to humanity that time travel /is/ possible, but not exactly /how/.

You can read a more detailed (spoiler-rich) groundwork here: [[https://docs.google.com/document/d/1k02UdRvCcpHYZSNWVxLrirFGZiu-hlf8GXFDXtQiXLI][LINK]]

--------------

* Supermunchkinry
  :PROPERTIES:
  :CUSTOM_ID: supermunchkinry
  :END:
Right, so my first problem is /how the hell do you convincingly simulate the actions of four supermunchkins with freaking time travel capabilities/? I've been fracking my brain with coffee-infused jets since the start of this month and I feel like I just bit off more than I can chew.

First off, the AGIs in my story are fast-takeoff ones, but to help me stave off a lot of impossible-to-write scenarios, I put an arbitrary x^{1/3} * ln(x) constraint on their rate of self-improvement, where x is the number of minutes since they achieved human-level intelligence.

This is a world where AI safety research almost, but not quite, reached its goal. There's a field called /formalised morality/ and a theorem that ensures your initial seed morals will be extrapolated in a consistent (and still conceivably rule-abiding) way. However, you cannot /guarantee/ the existential safety of humanity in this manner because of the weak constraints on what can count as an initial seed. In other words, *you can give AGIs a coherently extrapolated terminal goal, but you can't prevent others from putting in malicious ones*.

Maybe I ought to introduce each AGI in turn:

- *Ocean The Mother MX-4*, a Taiwanese AGI and the first one to arise. Terminal goal is *Taiwanese scientific supremacy*, subject to weak constraints on the killing and torture of people. SPOILER: Turns out OTM was also programmed to regard Chinese Mainlanders as 'enemies of the state', so in the first day of the Seven-Day War it released a plague upon Fujian that caused profound mental retardation but otherwise minor physical effects.

- *OpenMind v762*, a joint project between NATO[1] and the Alphabet-owned Versor. Open source and most funded. Terminal goal is *Superfun*, or the complete elimination of human suffering.

- *CRC 2☆, aka Sarimanok*, from the Bayesian Cooperative Conspiracy. First to discover causal loop engineering (mostly because of its own meddling). Terminal goal is *human eudaimonia*.

- *Esrafil*, from Palestine/Hamas. Terminal goal is /da‘wah/ *or the spread of Islam*. And before you ask, no, I am /not/ talking about Islamic extremism here. Second to learn of causal loops via a raid of the Israeli Bayesian Conspiracy.

[1]: In this future, the US ceased to be the sole world superpower and has a role roughly equivalent to today's Russia.

Okay, now how do causal loops work in this universe?

*First, they are Novikov self-consistent*. This does away with a lot of time travel plot holes but is notoriously much more demanding to write in terms of writing and structure. Half of my writing this month consists literally of foreshadowing and establishing plausible causal loops (sometimes to humorous effect). Oh, by the way, it is a theorem that these kinds of loops let you solve NP problems so there's a lot of cryptographic possibilities from that fact alone.

PS I presume nuclear launch codes are breakable by an NP-complete computer. :wink:

*Second, they have entropic upper bounds*. My future setting starts in 2052 and I arranged things so that 2007-ish is the farthest one can send back an amount of mass-energy equivalent to a small human's with a reasonable 5% success rate. The problem is that, establishing causal loops take an exponential amount of energy to do (cue solving worldwide flooding by /doing large-scale fusion on seawater/) and the error rate has fundamental lower bounds inversely proportional to the de Broglie wavelength. In other words, *it's really hard to reliably send back small stuff, and for the big stuff you have to expend exponentially huge amounts of energy*.

The price for getting your coordinates wrong is that a) you just float out there in space (causal loop portals, in-universe /chronoholes/, do obey a generalised form of momentum-energy conservation however), or b) you intersect with matter, in which case you violently explode. I may or may not have used (b) to retcon unexplained real-world explosions. :wink:

Oh, and /chronotransit/ involves getting bombarded by lots of EM radiation, so people are usually sent back in Faraday cages. I used this as a plot device in an epistolary news article chapter titled /The Case for Caged Children/.

*Third, chronoholes have epiphenomenal effects*, like say reducing the ambient temperature to microkelvins and acting as weakly gravitating source. This is actually how 2016 humanity confirms the effect as a /natural phenomenon/ because of smartphones and portable measuring devices and whatever it is you use when a wormhole-like object appears in your backyard and so on.

Other than this, science proceeds as usual barring the decline brought about by climate change (people would want to fund climate change-related stuff first) and sabotage by the AGIs. In particular, nanotechnology is used to great effect by the AGIs as well as other technologies found in [[https://www.futuretimeline.net/]].

Can anyone think of how to munchkin this universe when you have three other munchkins that want you dead?

* Rationality
  :PROPERTIES:
  :CUSTOM_ID: rationality
  :END:
My second problem is that, only my AGIs act as munchkins. My 2016 characters are only Level I intelligent and actually my goal is to get them to learn rationality in the process of solving the Fair Play puzzles so that they go on to found the Bayesian Cooperative Conspiracy as was intended by Sarimanok. To be frank, I don't know how else to portray rationalists in fiction except via munchkinry since I haven't really read anything beyond Worm and MoL and the first few chapters of HPMOR.

So what counts as realistically proto-rationalist in this sense? How do you munchkin an already munchkin-resistant world? Take note my branch point from real-life is 2016 so I have to spend a lot of time being consistent with the real-world first.

* Conspiracies
  :PROPERTIES:
  :CUSTOM_ID: conspiracies
  :END:
My last point is about rationalist conspiracies and IP. Eliezer said he doesn't mind people building off his universe as long as you credit him for it (and boy, I literally inserted him into the story). But I'm actually wary of e-mailing him about it 'cause I know he's a pretty busy guy. So...EY, if you see this, could you elaborate on what you consider as proper use regarding your /beisutsukai/ universe?