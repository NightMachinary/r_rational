:PROPERTIES:
:Author: hxcloud99
:Score: 2
:DateUnix: 1543205214.0
:DateShort: 2018-Nov-26
:END:

#+begin_quote
  So, for example, an AI might maintain an ace in the hole by studiously ignoring a particular region so that it can send something back to that area to prepare from the start of the sensor blackout
#+end_quote

Oh cool, this implication from Novikov is something I'd missed. I was thinking something like this should happen for information loops: AGI has a question -> AGI sets up loop to send answer back at an earlier date -> AGI immediately remembers answer upon pressing the button. Your alternative sounds like it would make more sense given the rules, though I'm thinking how a superbeisutsukai can actually avoid knowing what's inside a box since it must have a causal effect on its environment by virtue of existing.

#+begin_quote
  majority of warfare between the AIs would be information warfare: minimising the information available to opponents and maximising the information known to oneself.
#+end_quote

Yes! The brunt of the conflict so far consists of sabotaging key technologies (esp. causal loop engineering) in the human-only past, plus or minus manipulating initial conditions to delay each other's intelligence take-off. I'm still not sure how big of a lie you can pass off when there's three other super-Bayesians watching though.

#+begin_quote
  If you first send an agent with a collection of zero-days back
#+end_quote

Oh cool I have that as well. That means I'm doing it right lol.

#+begin_quote
  having successfully reduced your state of knowledge about the warehouse.
#+end_quote

Okay I'm still trying to wrap my head around this idea of deliberately reducing one's state of knowledge for future us. But thanks, this seems an important direction I'll have to tackle eventually.

#+begin_quote
  you can send stuff back to arbitrary locations, why not colonise other planets?
#+end_quote

:)

#+begin_quote
  I'd assume the AIs would try to go back further by setting up time machines at earlier points in the past if they could
#+end_quote

Yeaah, as the timeline got closer and closer to 2052 the effects of causal loops got really out of hand. I'm thinking past-gambit, countergambit, counter-countergambit and so on, like four Simurghs battling each other simultaneously.

That's a useful rule of thumb, though I'd wager a more equivalent comparison would be 2007->1907 : 2052->2007 due to exponential technological growth and accounting for technological sabotage. Also, the AGIs don't start off really smart at the onset (they reach 200 x human-level only after an entire week's worth of self-modification, and the conflict only lasts that long anyway) so I think it's also plausible to deny the other AGIs of key tech just by breaking human stuff in the past.

Anyway, I really appreciate your input. This is my first story (let alone ratfic) and you've given me confidence that I'm on the right track. :)