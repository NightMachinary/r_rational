:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 1
:DateUnix: 1505514004.0
:DateShort: 2017-Sep-16
:END:

#+begin_quote
  why would you think an AGI would be able to do any better than humans?
#+end_quote

I guess I should clarify something here. When we speak of AGI, are we not referring to *superintelligent* artificial *general* intelligence? As in, something that is much, much smarter than humans in general? As in, a [[https://en.wikipedia.org/wiki/Technological_singularity][technological singularity]] or something close to one?

I mean, isn't the whole point of making an AGI to have something much much smarter than humanity, so it can solve the problems that humanity has not? Like immortality or space travel or reversing entropy or future prediction? If it wouldn't do any better than humans, then it's just as useless as another human.

Now you can argue that creating a technological singularity or any kind of superintelligence is impossible, that AGIs can't actually be smarter than humans because we are already maximally smart or something. But in that case, the entire discussion about keeping AGIs in boxes becomes pointless, since they wouldn't be any more of a threat than regular human bad guys and wouldn't be any better at escaping mundane prisons.