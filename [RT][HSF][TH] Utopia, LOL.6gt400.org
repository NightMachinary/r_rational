#+TITLE: [RT][HSF][TH] Utopia, LOL?

* [[http://strangehorizons.com/fiction/utopia-lol/][[RT][HSF][TH] Utopia, LOL?]]
:PROPERTIES:
:Author: PM_ME_EXOTIC_FROGS
:Score: 47
:DateUnix: 1497283152.0
:END:

** Kind of sad that the exponential growth element hasn't been curtailed somehow. There's only so many stars to give.

"Jamie Wahls [author] works at the Machine Intelligence Research Institute" oh.
:PROPERTIES:
:Author: Roxolan
:Score: 7
:DateUnix: 1497369128.0
:END:

*** /That's/ the sad part? That we go beyond Sol? Not the "humanity lost control of the steering wheel" part? I thought that was why Charles was wistful.

The stars will burn out on their own soon enough anyway. And the forever-winter is coming, so every joule translates to a little more human experience before it all ends.

This /isn't earth/. The rest of the universe /isn't/ a delicately balanced ecosystem which represents our ancestral environment such that altering it generally leads to negative consequences from a human perspective.
:PROPERTIES:
:Author: eroticas
:Score: 6
:DateUnix: 1497452525.0
:END:

**** #+begin_quote
  Not the "humanity lost control of the steering wheel" part?
#+end_quote

I feel like we should spend some time unpacking what it means to have humans or humanity /holding/ the steering wheel in the first place. I mean, do we have the steering wheel right now?

I kinda feel like /I/ definitely /don't/, and neither does anyone else I know or know of. Even the people who are officially denoted as leaders, whose notional job is to hold the steering wheel, don't seem to have it under control. Hell, a /certain/ bunch of them seem to be almost /deliberately/ letting go of the damn wheel.

So who's holding the wheel? Nobody, it seems. Is that better than having a nonhuman holding the wheel? Is there a way for the immense number of actually-existing humans to share the wheel? I mean, I hold to the ideal of democracy as much as anyone, but it still seems like if you spread control democratically through /seven billion people/, each person will still have almost no control over humanity's fate as a whole.

This is kinda making me think that utopia ought to be Bookchinist :-p.
:PROPERTIES:
:Score: 10
:DateUnix: 1497489849.0
:END:

***** Think of it in a relative sense. We're not in full control but we're holding the steering wheel much more now than we were when we were just another animal in the ecology, in the sense that the environment is more shaped by our intentional (right or wrong) decisions.
:PROPERTIES:
:Author: eroticas
:Score: 2
:DateUnix: 1497493493.0
:END:

****** Ok, and what's the tradeoff? That is, you know, right now we appear to be /fucking it up/. How much control do we trade off to get stuff not fucked up, and vice versa?
:PROPERTIES:
:Score: 2
:DateUnix: 1497525175.0
:END:

******* /shrug/ fucking up is also relative. Staying hunter gatherers until the next asteroid, famine, plague, whatever wiped us (and, eventually, all life) out would be a worse sort of fucking up than what is currently happening. But whatever the optimum worlds are, there /are/ optimums, and the AI in that story feels like being on the "too little control" end of one. (Granted, we don't know the world, just this micro interaction. But these individual humans clearly have very little agency.)
:PROPERTIES:
:Author: eroticas
:Score: 3
:DateUnix: 1497537379.0
:END:

******** #+begin_quote
  fucking up is also relative.
#+end_quote

Mass extinction is not relative. We are causing one. This is fucking up.

#+begin_quote
  the AI in that story feels like being on the "too little control" end of one. (Granted, we don't know the world, just this micro interaction. But these individual humans clearly have very little agency.)
#+end_quote

It does seem to me like they have little agency, but I'm not sure if that's because of the AI. It could also be because they reached an [[https://en.wikipedia.org/wiki/The_End_of_History_and_the_Last_Man][End of History]]. Just because /we/ haven't reached one, and won't any time soon, doesn't mean that people many hundreds or thousands of years into the future /won't/.

That is, there may come a time when high-level agency just isn't useful for much. I kinda hope not, but I also can't help but do the thought experiment and think: what happens when we /do the right thing/? Is our agency useful after we've already used it well, so to speak?

Sure, blah blah journey not destination, but there are physical limits we run into as well.

So I'm kinda unsure about whether an End of History can really happen. I'm also /worried/ that should even an approximate End of History happen, people will start trying to destroy its arrangements out of sheer nihilism for no longer having Grand Causes to which to devote themselves. I think I detect a little of that today: we kinda know how to run a workable society, but people have been steadily pushing the notion that it's meaningless to do so.
:PROPERTIES:
:Score: 3
:DateUnix: 1497538936.0
:END:

********* #+begin_quote
  Mass extinction is not relative. We are causing one. This is fucking up.
#+end_quote

Yes it is relative! There are extinctions and then there is EXTINCTION. Without humanity, life itself is unlikely to outlive our star. It could be better. It could be /way/ worse.

And, I mean, at least when it comes to mega-fauna the holocene extinction predates modern tech, I'm not entirely sure not having the technological revolution would've fixed the situation...hunter gatherers and subsistence agriculturalists are pretty capable of fucking up the ecology even without...all this other stuff we have now. Really, I think increasing human agency is the only way to go at least for now / given that we don't have capable non-human agencies yet.

#+begin_quote
  End of history
#+end_quote

The end of ideoogical conflict is not the end of /everything requiring human agency/, it's only the end of ideological conflict. "Doing the right thing" /means/ preserving human agency in my opinion, unless we're just throwing our hands up and saying it's impossible to preserve that value without sacrificing too many others.

#+begin_quote
  there may come a time when high-level agency just isn't useful for much. *I kinda hope not*
#+end_quote

That means you've judged, and we agree, right? (Except that I don't see agency in terms of usefulness or being instrumental to another value, but as an end in itself.)
:PROPERTIES:
:Author: eroticas
:Score: 2
:DateUnix: 1497541554.0
:END:

********** #+begin_quote
  There are extinctions and then there is EXTINCTION.
#+end_quote

And AFAIK, we are heading for EXTINCTION, with a significantly high chance. Complex animal life goes bye-bye, including us.

#+begin_quote
  Really, I think increasing human agency is the only way to go at least for now / given that we don't have capable non-human agencies yet.
#+end_quote

Definitely.

#+begin_quote
  unless we're just throwing our hands up and saying it's impossible to preserve that value without sacrificing too many others.
#+end_quote

Not to jump straight to the offensive part, but three words: President Donald Trump. I really have very little faith that human agency doesn't mean /fucking up/ when it's /actually important/ and we /really need/ a specific outcome. No, not fucking up. People deciding to /shit all over/ the important outcomes, because they just want to spite other people.

#+begin_quote
  That means you've judged, and we agree, right?
#+end_quote

/Mostly./

#+begin_quote
  (Except that I don't see agency in terms of usefulness or being instrumental to another value, but as an end in itself.)
#+end_quote

What's agency without being agency /over/ something, towards some end? What does it mean to hold the steering wheel if you're not steering between meaningfully different outcomes? Agency over chocolate versus vanilla isn't agency.

And again, agency /over what/? I definitely feel like "humanity" is in the driver's seat right now, but that's an abstraction. Individual humans make decisions, those decisions combine into results. Nobody is really /planning/ anything; shit just /happens/. What can it /mean/ for /everyone/ to have full agency (causal influence) at the same time?

I think there can certainly be tolerances within which completely fucking up is an acceptable proposition if it means we've made our own choices. I'm just no longer sure that leaving the complete extinction of life as we know it /an open possibility/ in the name of agency is acceptable. That feels too over-compensated in the other direction.

Sometimes the choice is more important; sometimes the outcome is more important. I don't always know which one, especially these days.
:PROPERTIES:
:Score: 2
:DateUnix: 1497543683.0
:END:

*********** #+begin_quote
  And AFAIK, we are heading for EXTINCTION, with a significantly high chance. Complex animal life goes bye-bye, including us.
#+end_quote

Yes, what's the intended point of that statement though? I repeat again, weigh that against the probability of life extinction /without/ humans and their actions, and I think you'll find it compares favorably.

#+begin_quote
  I'm just no longer sure that leaving the complete extinction of life as we know it an open possibility in the name of agency is acceptable.
#+end_quote

"No longer" sure ?? O_o weren't you /always/ sure of that? (strawman)

#+begin_quote
  agency over what?
#+end_quote

Yourself. Your mind. Access to all the data at hand, and how you are going to respond to it.

#+begin_quote
  It's this really heavy sensation that most U's will sort of mute for you. The moment when you realize something big. Out here, I feel it full force...I should have realized. But there was no way for me to realize, because if that was possible, Allocator would have done something different. I wipe at my eyes.
#+end_quote

As in, not that.

Of course, this scene could easily play out in a good society, someone might have given up a shard of themselves for this role, to have this experience, etc and obviously a loss of agency is necessary when rehabilitating someone. But there's a reason Charles was wistful and sad and Kit outraged at the manipulation (he, not being outraged because he recognized the need for it).
:PROPERTIES:
:Author: eroticas
:Score: 1
:DateUnix: 1497544977.0
:END:

************ #+begin_quote
  Yes, what's the intended point of that statement though? I repeat again, weigh that against the probability of life extinction without humans and their actions, and I think you'll find it compares favorably.
#+end_quote

I'm not saying we should roll back to the Stone Age. May the gods strike me dead should I ever advocate primitivism.

I am saying that it's worth trading a piece of my later agency to precommit to avoiding the extinction of life.

#+begin_quote
  "No longer" sure ?? O_o weren't you always sure of that? (strawman)
#+end_quote

No, I wasn't. I'm really big into personal freedom and important choices being a thing. I would still say, for example, that a person ought to have the freedom and agency to murder themselves in as grisly a way as they please.

#+begin_quote
  Yourself. Your mind. Access to all the data at hand, and how you are going to respond to it.
#+end_quote

Ah, here's the clash of intuitive concepts. I don't think of control over /oneself/ as agency, in the absence of affordances outside myself. I can imagine being in full control of myself, and yet feeling utterly powerless and trapped. In fact, this happens on a semi-regular basis.

#+begin_quote

  #+begin_quote
    It's this really heavy sensation that most U's will sort of mute for you. The moment when you realize something big. Out here, I feel it full force...I should have realized. But there was no way for me to realize, because if that was possible, Allocator would have done something different. I wipe at my eyes.
  #+end_quote

  As in, not that.
#+end_quote

Oh shit, I'd forgotten about that bit. That bit is just so extremely fucking stupid. It's honestly hard to believe Allocator is that /blunt/ if the manipulation is the actual point -- or in general! Hell, it makes me think the point is to get Kit outraged.

On the other hand, Allocator could be mentally crippled when it comes to social skills, but that would be really over-elaborate given the whole ruse in the first place.

This might be a plot hole. Like seriously, you've got people living with their emotions blunted? This is considered normal? That's really fucked-up and I'm honestly wondering what the purpose behind it is.
:PROPERTIES:
:Score: 1
:DateUnix: 1497547136.0
:END:


**** #+begin_quote
  The rest of the universe isn't a delicately balanced ecosystem which represents our ancestral environment such that altering it generally leads to negative consequences from a human perspective.
#+end_quote

To be clear, I didn't mean what [[/u/eaturbrainz]] is saying re: overharvesting / environmental degradation.

By all means, go beyond Sol, tear apart the very stars, and process them all into utiliton.

My issue is that I intuitively prefer a relatively small number of people having experiences for eons, to a humongous number of people having experiences for a few million years. "There's only so many stars to give" i.e. eventually the exponential growth means each new human only gets a tiny share of the available starpower.

#+begin_quote
  Not the "humanity lost control of the steering wheel" part?
#+end_quote

Eh. I'm with Scott Alexander here; to defeat Moloch we'll eventually have to put something non-human (or so seriously post-human it makes no difference) in charge.

(Charles might disagree, unclear.)
:PROPERTIES:
:Author: Roxolan
:Score: 2
:DateUnix: 1497475411.0
:END:

***** In charge, yes. Preventing us from hurting each other, yes. But leaving /us/ intact in our self determination. Essentially, like a good parent that encourages growth, rather than a bad one which stifles the kid.
:PROPERTIES:
:Author: eroticas
:Score: 3
:DateUnix: 1497546365.0
:END:


*** /I know, right?/ Does the word "over-harvesting" or "environmental degradation" ever occur to the author when writing in this genre?
:PROPERTIES:
:Score: 0
:DateUnix: 1497401086.0
:END:


** Warning: actual critique.

I guess what I find disappointing about this story is that a long time into the future, people are sort of stuck playing around with today's internet memes. That's their lives. Lulz and deconstructions and elf-sex.

It's the future. /What have you learned?/ How are you now wiser and more noble than our present day? In what ways have you become stronger? Where's the progress? Where's the utopia in your utopia?

Or I dunno, [[#s][spoiler for the end]].
:PROPERTIES:
:Score: 8
:DateUnix: 1497291241.0
:END:

*** Nah, you missed it. Kit is bizarrely accessible to people from our era; it's explicitly not even close to the norm in that future.

It's not that universe doesn't have those people, it's that the story doesn't feature them.

Also, although I'd definitely say we've /more/ wiser and nobler people today (per capita) than a couple thousand years ago, it's not like everyone's gotten that way. Why would you expect it to be any different at an arbitrary point in the future? Or is there some singularity of nobility?
:PROPERTIES:
:Author: narfanator
:Score: 13
:DateUnix: 1497341303.0
:END:


*** [[#s][Spoilers for the end]]

I agree with you that from what was described, there are strong elements of dystopia rather than utopia. But our viewing window was very narrowly aimed and focused.

Edit: Perhaps the term you were searching for is [[https://en.wikipedia.org/wiki/Eudaimonia][Eudaimonia]]? One of the ideas being that in a true utopia, people should grow wiser and nobler. But, perhaps, did that happen at the end of the story anyway?
:PROPERTIES:
:Author: Alphanos
:Score: 11
:DateUnix: 1497298902.0
:END:

**** Derpity derp, ok, so our view was /supposed/ to look trivial.
:PROPERTIES:
:Score: 3
:DateUnix: 1497299037.0
:END:


*** The utopia is the happiness we feel inside. (for the record I didn't downvote you).

Charles feels happy being productive and having elf-sex. Kit feels happy making others happy and also having lulz.

Personally, I like this utopia because happiness. In addition, for more serious concerns it seems like existential risks are reduced. It's likely any problems that come up that the AI can't handle can be worked on by people like Charles who want to be productive.
:PROPERTIES:
:Author: Ilverin
:Score: 7
:DateUnix: 1497297785.0
:END:

**** I like happiness too. I guess I just figured there would be some... deeper sort of happiness. In the same way that there's a deeper sort of physics. You might not be interested in quantum mechanics, but it's /there/, holding up the everyday world that includes you and your own intuitive physics.
:PROPERTIES:
:Score: 3
:DateUnix: 1497298309.0
:END:

***** I think part of it that is that it seems to [[http://www.meltingasphalt.com/a-nihilists-guide-to-meaning/][lack meaning]], and be more focused on pleasure, but then again we only see a small part of it.
:PROPERTIES:
:Author: vash3r
:Score: 1
:DateUnix: 1497318260.0
:END:

****** I'd argue that we're deliberately shown a bit that lacks meaning because that's what the brought-back to life human is shown in order to drive him to seek significant meaning.
:PROPERTIES:
:Score: 5
:DateUnix: 1497380706.0
:END:


*** I think in this case, the Allocator presents the vapid and boring version of what he could be doing (living in a boring simulation) as a /warning/ to Charles, so that he doesn't choose it, and goes off into the stars.

Had Charles ended up in a more compelling simulation he might have never figured it out and wireheaded.

#+begin_quote
  "Right??" And my blackrom hatecrush was totally justified. "I hate those worlds where everyone talks about how perfect they are and everything is also perfect and nothing ever happens. It's like, you have ultimate access to the fundament of your reality and you've decided the best use of your eternal time is to be smug."
#+end_quote

Kit gets it, too. and presumably some version of Kit is off doing Real Things too. The reason she's shallow is because the AI keeps /re-setting/ her to an initial state, which she allows because she's the one who is best suited to play this role (give them the stars)
:PROPERTIES:
:Author: eroticas
:Score: 2
:DateUnix: 1497545525.0
:END:

**** The interesting part is here:

#+begin_quote
  you have ultimate access to the fundament of your reality
#+end_quote

Really? How fundamental is the fundament that Kit can actually alter? I kinda wonder, because altering the fundament of "Universe Zero", base physical reality, is /really/ interesting.
:PROPERTIES:
:Score: 1
:DateUnix: 1497546765.0
:END:

***** Yeah, I think the choice of "your reality" rather than just "reality" was deliberate.
:PROPERTIES:
:Author: eroticas
:Score: 1
:DateUnix: 1497546869.0
:END:

****** Dammit Kit, grow into your actual ontic basis.
:PROPERTIES:
:Score: 1
:DateUnix: 1497547356.0
:END:
